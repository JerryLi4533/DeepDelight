{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "ff_dropout = 0.2\n",
    "attn_dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('./input/tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(ff_dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, res1_dropout):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.dropout = nn.Dropout(res1_dropout)\n",
    "        self.res1_dropout = res1_dropout\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     # apply self attention and dropout\n",
    "    #     x = self.ln1(x)\n",
    "    #     y = x + self.dropout(self.sa(x))\n",
    "    #     # apply feedforward and dropout\n",
    "    #     y = self.ln2(x)\n",
    "    #     x = y + self.dropout(self.ffwd(y))\n",
    "    #     return x\n",
    "    \n",
    "    # def forward(self, x):\n",
    "    #     # apply self attention and dropout\n",
    "    #     y = self.dropout(x) + self.sa(self.ln1(x))\n",
    "    #     # apply feedforward and dropout\n",
    "    #     z = self.dropout(y) + self.ffwd(self.ln2(y))\n",
    "    #     return z\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply self attention and dropout\n",
    "        x = self.ln1(x)\n",
    "        y = self.dropout(x) * (1 - self.res1_dropout) + self.sa(x)\n",
    "        # apply feedforward and dropout\n",
    "        y = self.ln2(y)\n",
    "        z = self.dropout(y) * (1 - self.res1_dropout) + self.ffwd(y)\n",
    "        return z\n",
    "\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     # apply self attention and dropout\n",
    "    #     y = x + self.dropout(self.sa(self.ln1(x)))\n",
    "    #     # apply feedforward and dropout\n",
    "    #     z = y + self.dropout(self.ffwd(self.ln2(y)))\n",
    "    #     return z\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, res1_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, res1_dropout=res1_dropout) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with dropout=0.0\n",
      "step 0: train loss 4.4283, val loss 4.4185\n",
      "step 100: train loss 2.6480, val loss 2.6604\n",
      "step 200: train loss 2.5184, val loss 2.5105\n",
      "step 300: train loss 2.4420, val loss 2.4551\n",
      "step 400: train loss 2.3771, val loss 2.3806\n",
      "step 500: train loss 2.3267, val loss 2.3447\n",
      "step 600: train loss 2.2805, val loss 2.2843\n",
      "step 700: train loss 2.2431, val loss 2.2532\n",
      "step 800: train loss 2.2048, val loss 2.2230\n",
      "step 900: train loss 2.1710, val loss 2.1879\n",
      "step 1000: train loss 2.1487, val loss 2.1655\n",
      "step 1100: train loss 2.1182, val loss 2.1531\n",
      "step 1200: train loss 2.0903, val loss 2.1183\n",
      "step 1300: train loss 2.0816, val loss 2.1041\n",
      "step 1400: train loss 2.0496, val loss 2.0769\n",
      "step 1500: train loss 2.0305, val loss 2.0733\n",
      "step 1600: train loss 2.0116, val loss 2.0747\n",
      "step 1700: train loss 2.0039, val loss 2.0547\n",
      "step 1800: train loss 1.9751, val loss 2.0423\n",
      "step 1900: train loss 1.9638, val loss 2.0170\n",
      "step 2000: train loss 1.9465, val loss 2.0311\n",
      "step 2100: train loss 1.9387, val loss 2.0187\n",
      "step 2200: train loss 1.9223, val loss 1.9994\n",
      "step 2300: train loss 1.9175, val loss 1.9979\n",
      "step 2400: train loss 1.9113, val loss 1.9853\n",
      "step 2500: train loss 1.8854, val loss 1.9813\n",
      "step 2600: train loss 1.8960, val loss 1.9889\n",
      "step 2700: train loss 1.8895, val loss 1.9849\n",
      "step 2800: train loss 1.8808, val loss 1.9722\n",
      "step 2900: train loss 1.8672, val loss 1.9640\n",
      "step 3000: train loss 1.8604, val loss 1.9553\n",
      "step 3100: train loss 1.8457, val loss 1.9615\n",
      "step 3200: train loss 1.8262, val loss 1.9452\n",
      "step 3300: train loss 1.8310, val loss 1.9503\n",
      "step 3400: train loss 1.8214, val loss 1.9365\n",
      "step 3500: train loss 1.8124, val loss 1.9294\n",
      "step 3600: train loss 1.8078, val loss 1.9391\n",
      "step 3700: train loss 1.8050, val loss 1.9287\n",
      "step 3800: train loss 1.7971, val loss 1.9452\n",
      "step 3900: train loss 1.7941, val loss 1.9241\n",
      "step 4000: train loss 1.7875, val loss 1.9158\n",
      "step 4100: train loss 1.7879, val loss 1.9294\n",
      "step 4200: train loss 1.7755, val loss 1.9056\n",
      "step 4300: train loss 1.7771, val loss 1.8955\n",
      "step 4400: train loss 1.7781, val loss 1.9182\n",
      "step 4500: train loss 1.7622, val loss 1.9003\n",
      "step 4600: train loss 1.7578, val loss 1.8841\n",
      "step 4700: train loss 1.7526, val loss 1.8907\n",
      "step 4800: train loss 1.7393, val loss 1.8915\n",
      "step 4900: train loss 1.7454, val loss 1.8898\n",
      "step 5000: train loss 1.7357, val loss 1.8807\n",
      "step 5100: train loss 1.7387, val loss 1.8793\n",
      "step 5200: train loss 1.7310, val loss 1.8762\n",
      "step 5300: train loss 1.7397, val loss 1.8721\n",
      "step 5400: train loss 1.7253, val loss 1.8678\n",
      "step 5500: train loss 1.7224, val loss 1.8616\n",
      "step 5600: train loss 1.7312, val loss 1.8665\n",
      "step 5700: train loss 1.7212, val loss 1.8734\n",
      "step 5800: train loss 1.7127, val loss 1.8558\n",
      "step 5900: train loss 1.7186, val loss 1.8636\n",
      "step 6000: train loss 1.7128, val loss 1.8584\n",
      "step 6100: train loss 1.7128, val loss 1.8499\n",
      "step 6200: train loss 1.7135, val loss 1.8437\n",
      "step 6300: train loss 1.7064, val loss 1.8685\n",
      "step 6400: train loss 1.6983, val loss 1.8570\n",
      "step 6500: train loss 1.6956, val loss 1.8390\n",
      "step 6600: train loss 1.6969, val loss 1.8403\n",
      "step 6700: train loss 1.6954, val loss 1.8479\n",
      "step 6800: train loss 1.6945, val loss 1.8562\n",
      "step 6900: train loss 1.6919, val loss 1.8434\n",
      "step 7000: train loss 1.6887, val loss 1.8447\n",
      "step 7100: train loss 1.6937, val loss 1.8530\n",
      "step 7200: train loss 1.6811, val loss 1.8516\n",
      "step 7300: train loss 1.6832, val loss 1.8374\n",
      "step 7400: train loss 1.6855, val loss 1.8476\n",
      "step 7500: train loss 1.6628, val loss 1.8312\n",
      "step 7600: train loss 1.6706, val loss 1.8269\n",
      "step 7700: train loss 1.6630, val loss 1.8254\n",
      "step 7800: train loss 1.6734, val loss 1.8214\n",
      "step 7900: train loss 1.6731, val loss 1.8308\n",
      "step 8000: train loss 1.6609, val loss 1.8271\n",
      "step 8100: train loss 1.6666, val loss 1.8317\n",
      "step 8200: train loss 1.6729, val loss 1.8284\n",
      "step 8300: train loss 1.6663, val loss 1.8293\n",
      "step 8400: train loss 1.6612, val loss 1.8307\n",
      "step 8500: train loss 1.6704, val loss 1.8305\n",
      "step 8600: train loss 1.6620, val loss 1.8339\n",
      "step 8700: train loss 1.6577, val loss 1.8332\n",
      "step 8800: train loss 1.6446, val loss 1.8289\n",
      "step 8900: train loss 1.6591, val loss 1.8107\n",
      "step 9000: train loss 1.6540, val loss 1.8218\n",
      "step 9100: train loss 1.6426, val loss 1.8081\n",
      "step 9200: train loss 1.6600, val loss 1.8199\n",
      "step 9300: train loss 1.6413, val loss 1.7944\n",
      "step 9400: train loss 1.6419, val loss 1.7943\n",
      "step 9500: train loss 1.6476, val loss 1.8114\n",
      "step 9600: train loss 1.6450, val loss 1.8063\n",
      "step 9700: train loss 1.6425, val loss 1.7959\n",
      "step 9800: train loss 1.6361, val loss 1.8144\n",
      "step 9900: train loss 1.6372, val loss 1.8030\n",
      "step 10000: train loss 1.6405, val loss 1.8068\n",
      "step 10100: train loss 1.6345, val loss 1.8155\n",
      "step 10200: train loss 1.6246, val loss 1.7994\n",
      "step 10300: train loss 1.6222, val loss 1.7928\n",
      "step 10400: train loss 1.6300, val loss 1.7997\n",
      "step 10500: train loss 1.6348, val loss 1.7919\n",
      "step 10600: train loss 1.6352, val loss 1.7948\n",
      "step 10700: train loss 1.6312, val loss 1.8010\n",
      "step 10800: train loss 1.6309, val loss 1.8023\n",
      "step 10900: train loss 1.6220, val loss 1.8028\n",
      "step 11000: train loss 1.6195, val loss 1.8044\n",
      "step 11100: train loss 1.6158, val loss 1.7915\n",
      "step 11200: train loss 1.6243, val loss 1.7786\n",
      "step 11300: train loss 1.6219, val loss 1.8025\n",
      "step 11400: train loss 1.6267, val loss 1.7915\n",
      "step 11500: train loss 1.6211, val loss 1.7818\n",
      "step 11600: train loss 1.6250, val loss 1.7881\n",
      "step 11700: train loss 1.6150, val loss 1.7908\n",
      "step 11800: train loss 1.6095, val loss 1.7824\n",
      "step 11900: train loss 1.6203, val loss 1.7805\n",
      "step 12000: train loss 1.6009, val loss 1.7732\n",
      "step 12100: train loss 1.6107, val loss 1.7867\n",
      "step 12200: train loss 1.6111, val loss 1.7923\n",
      "step 12300: train loss 1.5969, val loss 1.7828\n",
      "step 12400: train loss 1.6152, val loss 1.7801\n",
      "step 12500: train loss 1.6161, val loss 1.7798\n",
      "step 12600: train loss 1.6191, val loss 1.7688\n",
      "step 12700: train loss 1.6181, val loss 1.7708\n",
      "step 12800: train loss 1.6111, val loss 1.7990\n",
      "step 12900: train loss 1.6089, val loss 1.7879\n",
      "step 13000: train loss 1.6031, val loss 1.7873\n",
      "step 13100: train loss 1.6062, val loss 1.7737\n",
      "step 13200: train loss 1.6060, val loss 1.7703\n",
      "step 13300: train loss 1.5995, val loss 1.7699\n",
      "step 13400: train loss 1.6031, val loss 1.7636\n",
      "step 13500: train loss 1.6030, val loss 1.7838\n",
      "step 13600: train loss 1.6032, val loss 1.7748\n",
      "step 13700: train loss 1.5968, val loss 1.7822\n",
      "step 13800: train loss 1.6079, val loss 1.7872\n",
      "step 13900: train loss 1.6021, val loss 1.7742\n",
      "step 14000: train loss 1.5958, val loss 1.7698\n",
      "step 14100: train loss 1.6000, val loss 1.7779\n",
      "step 14200: train loss 1.6057, val loss 1.7767\n",
      "step 14300: train loss 1.5970, val loss 1.7752\n",
      "step 14400: train loss 1.5872, val loss 1.7645\n",
      "step 14500: train loss 1.5968, val loss 1.7561\n",
      "step 14600: train loss 1.5959, val loss 1.7640\n",
      "step 14700: train loss 1.5994, val loss 1.7570\n",
      "step 14800: train loss 1.6050, val loss 1.7573\n",
      "step 14900: train loss 1.5939, val loss 1.7467\n",
      "step 15000: train loss 1.5912, val loss 1.7583\n",
      "step 15100: train loss 1.5905, val loss 1.7672\n",
      "step 15200: train loss 1.5914, val loss 1.7517\n",
      "step 15300: train loss 1.5878, val loss 1.7478\n",
      "step 15400: train loss 1.5971, val loss 1.7483\n",
      "step 15500: train loss 1.5847, val loss 1.7513\n",
      "step 15600: train loss 1.5938, val loss 1.7466\n",
      "step 15700: train loss 1.5927, val loss 1.7593\n",
      "step 15800: train loss 1.5903, val loss 1.7601\n",
      "step 15900: train loss 1.5830, val loss 1.7470\n",
      "step 16000: train loss 1.5853, val loss 1.7522\n",
      "step 16100: train loss 1.5805, val loss 1.7474\n",
      "step 16200: train loss 1.5773, val loss 1.7606\n",
      "step 16300: train loss 1.5746, val loss 1.7572\n",
      "step 16400: train loss 1.5897, val loss 1.7476\n",
      "step 16500: train loss 1.5765, val loss 1.7545\n",
      "step 16600: train loss 1.5792, val loss 1.7535\n",
      "step 16700: train loss 1.5768, val loss 1.7483\n",
      "step 16800: train loss 1.5781, val loss 1.7564\n",
      "step 16900: train loss 1.5878, val loss 1.7548\n",
      "step 17000: train loss 1.5878, val loss 1.7603\n",
      "step 17100: train loss 1.5843, val loss 1.7723\n",
      "step 17200: train loss 1.5835, val loss 1.7605\n",
      "step 17300: train loss 1.5762, val loss 1.7401\n",
      "step 17400: train loss 1.5812, val loss 1.7568\n",
      "step 17500: train loss 1.5744, val loss 1.7382\n",
      "step 17600: train loss 1.5732, val loss 1.7419\n",
      "step 17700: train loss 1.5710, val loss 1.7547\n",
      "step 17800: train loss 1.5694, val loss 1.7286\n",
      "step 17900: train loss 1.5794, val loss 1.7682\n",
      "step 18000: train loss 1.5670, val loss 1.7385\n",
      "step 18100: train loss 1.5644, val loss 1.7483\n",
      "step 18200: train loss 1.5673, val loss 1.7511\n",
      "step 18300: train loss 1.5709, val loss 1.7231\n",
      "step 18400: train loss 1.5777, val loss 1.7523\n",
      "step 18500: train loss 1.5674, val loss 1.7384\n",
      "step 18600: train loss 1.5676, val loss 1.7441\n",
      "step 18700: train loss 1.5736, val loss 1.7566\n",
      "step 18800: train loss 1.5735, val loss 1.7586\n",
      "step 18900: train loss 1.5808, val loss 1.7576\n",
      "step 19000: train loss 1.5698, val loss 1.7378\n",
      "step 19100: train loss 1.5607, val loss 1.7396\n",
      "step 19200: train loss 1.5755, val loss 1.7313\n",
      "step 19300: train loss 1.5748, val loss 1.7254\n",
      "step 19400: train loss 1.5612, val loss 1.7445\n",
      "step 19500: train loss 1.5694, val loss 1.7465\n",
      "step 19600: train loss 1.5674, val loss 1.7387\n",
      "step 19700: train loss 1.5654, val loss 1.7409\n",
      "step 19800: train loss 1.5647, val loss 1.7433\n",
      "step 19900: train loss 1.5592, val loss 1.7393\n",
      "step 20000: train loss 1.5521, val loss 1.7298\n",
      "step 20100: train loss 1.5507, val loss 1.7291\n",
      "step 20200: train loss 1.5630, val loss 1.7267\n",
      "step 20300: train loss 1.5594, val loss 1.7289\n",
      "step 20400: train loss 1.5592, val loss 1.7335\n",
      "step 20500: train loss 1.5627, val loss 1.7339\n",
      "step 20600: train loss 1.5677, val loss 1.7372\n",
      "step 20700: train loss 1.5511, val loss 1.7265\n",
      "step 20800: train loss 1.5593, val loss 1.7381\n",
      "step 20900: train loss 1.5573, val loss 1.7231\n",
      "step 21000: train loss 1.5575, val loss 1.7391\n",
      "step 21100: train loss 1.5578, val loss 1.7209\n",
      "step 21200: train loss 1.5545, val loss 1.7367\n",
      "step 21300: train loss 1.5613, val loss 1.7505\n",
      "step 21400: train loss 1.5473, val loss 1.7299\n",
      "step 21500: train loss 1.5562, val loss 1.7418\n",
      "step 21600: train loss 1.5555, val loss 1.7287\n",
      "step 21700: train loss 1.5476, val loss 1.7452\n",
      "step 21800: train loss 1.5490, val loss 1.7349\n",
      "step 21900: train loss 1.5583, val loss 1.7432\n",
      "step 22000: train loss 1.5527, val loss 1.7445\n",
      "step 22100: train loss 1.5541, val loss 1.7571\n",
      "step 22200: train loss 1.5500, val loss 1.7362\n",
      "step 22300: train loss 1.5518, val loss 1.7282\n",
      "step 22400: train loss 1.5572, val loss 1.7219\n",
      "step 22500: train loss 1.5596, val loss 1.7413\n",
      "step 22600: train loss 1.5557, val loss 1.7429\n",
      "step 22700: train loss 1.5557, val loss 1.7360\n",
      "step 22800: train loss 1.5534, val loss 1.7227\n",
      "step 22900: train loss 1.5423, val loss 1.7269\n",
      "step 23000: train loss 1.5517, val loss 1.7344\n",
      "step 23100: train loss 1.5464, val loss 1.7454\n",
      "step 23200: train loss 1.5375, val loss 1.7179\n",
      "step 23300: train loss 1.5548, val loss 1.7275\n",
      "step 23400: train loss 1.5486, val loss 1.7208\n",
      "step 23500: train loss 1.5452, val loss 1.7414\n",
      "step 23600: train loss 1.5492, val loss 1.7218\n",
      "step 23700: train loss 1.5429, val loss 1.7377\n",
      "step 23800: train loss 1.5357, val loss 1.7311\n",
      "step 23900: train loss 1.5463, val loss 1.7342\n",
      "step 24000: train loss 1.5424, val loss 1.7252\n",
      "step 24100: train loss 1.5440, val loss 1.7261\n",
      "step 24200: train loss 1.5380, val loss 1.7228\n",
      "step 24300: train loss 1.5377, val loss 1.7240\n",
      "step 24400: train loss 1.5394, val loss 1.7243\n",
      "step 24500: train loss 1.5421, val loss 1.7303\n",
      "step 24600: train loss 1.5401, val loss 1.7406\n",
      "step 24700: train loss 1.5349, val loss 1.7391\n",
      "step 24800: train loss 1.5391, val loss 1.7134\n",
      "step 24900: train loss 1.5414, val loss 1.7361\n",
      "step 25000: train loss 1.5386, val loss 1.7307\n",
      "step 25100: train loss 1.5445, val loss 1.7211\n",
      "step 25200: train loss 1.5370, val loss 1.7333\n",
      "step 25300: train loss 1.5261, val loss 1.7343\n",
      "step 25400: train loss 1.5416, val loss 1.7304\n",
      "step 25500: train loss 1.5403, val loss 1.7355\n",
      "step 25600: train loss 1.5385, val loss 1.7274\n",
      "step 25700: train loss 1.5318, val loss 1.7280\n",
      "step 25800: train loss 1.5502, val loss 1.7300\n",
      "step 25900: train loss 1.5202, val loss 1.7193\n",
      "step 26000: train loss 1.5324, val loss 1.7295\n",
      "step 26100: train loss 1.5434, val loss 1.7136\n",
      "step 26200: train loss 1.5305, val loss 1.7281\n",
      "step 26300: train loss 1.5354, val loss 1.7224\n",
      "step 26400: train loss 1.5431, val loss 1.7345\n",
      "step 26500: train loss 1.5402, val loss 1.7128\n",
      "step 26600: train loss 1.5336, val loss 1.7168\n",
      "step 26700: train loss 1.5335, val loss 1.7258\n",
      "step 26800: train loss 1.5285, val loss 1.7011\n",
      "step 26900: train loss 1.5298, val loss 1.7222\n",
      "step 27000: train loss 1.5303, val loss 1.7156\n",
      "step 27100: train loss 1.5280, val loss 1.7085\n",
      "step 27200: train loss 1.5373, val loss 1.7104\n",
      "step 27300: train loss 1.5300, val loss 1.7158\n",
      "step 27400: train loss 1.5390, val loss 1.7205\n",
      "step 27500: train loss 1.5443, val loss 1.7157\n",
      "step 27600: train loss 1.5315, val loss 1.7234\n",
      "step 27700: train loss 1.5293, val loss 1.7091\n",
      "step 27800: train loss 1.5274, val loss 1.7037\n",
      "step 27900: train loss 1.5267, val loss 1.7241\n",
      "step 28000: train loss 1.5214, val loss 1.7069\n",
      "step 28100: train loss 1.5471, val loss 1.7037\n",
      "step 28200: train loss 1.5290, val loss 1.7098\n",
      "step 28300: train loss 1.5286, val loss 1.7066\n",
      "step 28400: train loss 1.5281, val loss 1.7060\n",
      "step 28500: train loss 1.5239, val loss 1.7209\n",
      "step 28600: train loss 1.5327, val loss 1.7127\n",
      "step 28700: train loss 1.5175, val loss 1.7070\n",
      "step 28800: train loss 1.5261, val loss 1.7078\n",
      "step 28900: train loss 1.5286, val loss 1.7083\n",
      "step 29000: train loss 1.5394, val loss 1.7150\n",
      "step 29100: train loss 1.5247, val loss 1.7196\n",
      "step 29200: train loss 1.5305, val loss 1.7084\n",
      "step 29300: train loss 1.5236, val loss 1.6943\n",
      "step 29400: train loss 1.5270, val loss 1.7032\n",
      "step 29500: train loss 1.5257, val loss 1.6966\n",
      "step 29600: train loss 1.5226, val loss 1.7068\n",
      "step 29700: train loss 1.5302, val loss 1.7077\n",
      "step 29800: train loss 1.5376, val loss 1.7130\n",
      "step 29900: train loss 1.5309, val loss 1.6992\n",
      "step 30000: train loss 1.5208, val loss 1.7104\n",
      "step 30100: train loss 1.5190, val loss 1.7033\n",
      "step 30200: train loss 1.5170, val loss 1.7074\n",
      "step 30300: train loss 1.5275, val loss 1.7030\n",
      "step 30400: train loss 1.5346, val loss 1.7235\n",
      "step 30500: train loss 1.5339, val loss 1.7113\n",
      "step 30600: train loss 1.5313, val loss 1.7047\n",
      "step 30700: train loss 1.5201, val loss 1.6911\n",
      "step 30800: train loss 1.5174, val loss 1.7002\n",
      "step 30900: train loss 1.5261, val loss 1.7035\n",
      "step 31000: train loss 1.5189, val loss 1.7104\n",
      "step 31100: train loss 1.5142, val loss 1.7124\n",
      "step 31200: train loss 1.5260, val loss 1.7104\n",
      "step 31300: train loss 1.5247, val loss 1.7027\n",
      "step 31400: train loss 1.5292, val loss 1.7008\n",
      "step 31500: train loss 1.5272, val loss 1.7175\n",
      "step 31600: train loss 1.5194, val loss 1.7252\n",
      "step 31700: train loss 1.5208, val loss 1.7175\n",
      "step 31800: train loss 1.5178, val loss 1.7190\n",
      "step 31900: train loss 1.5140, val loss 1.7147\n",
      "step 32000: train loss 1.5043, val loss 1.7119\n",
      "step 32100: train loss 1.5231, val loss 1.7141\n",
      "step 32200: train loss 1.5212, val loss 1.7124\n",
      "step 32300: train loss 1.5218, val loss 1.7140\n",
      "step 32400: train loss 1.5166, val loss 1.7057\n",
      "step 32500: train loss 1.5189, val loss 1.7106\n",
      "step 32600: train loss 1.5185, val loss 1.7002\n",
      "step 32700: train loss 1.5298, val loss 1.7130\n",
      "step 32800: train loss 1.5177, val loss 1.7071\n",
      "step 32900: train loss 1.5165, val loss 1.7102\n",
      "step 33000: train loss 1.5150, val loss 1.6949\n",
      "step 33100: train loss 1.5250, val loss 1.6977\n",
      "step 33200: train loss 1.5277, val loss 1.7144\n",
      "step 33300: train loss 1.5361, val loss 1.7012\n",
      "step 33400: train loss 1.5207, val loss 1.7133\n",
      "step 33500: train loss 1.5221, val loss 1.7063\n",
      "step 33600: train loss 1.5119, val loss 1.7069\n",
      "step 33700: train loss 1.5160, val loss 1.6998\n",
      "step 33800: train loss 1.5178, val loss 1.7042\n",
      "step 33900: train loss 1.5129, val loss 1.7050\n",
      "step 34000: train loss 1.5176, val loss 1.7030\n",
      "step 34100: train loss 1.5209, val loss 1.7116\n",
      "step 34200: train loss 1.5064, val loss 1.6996\n",
      "step 34300: train loss 1.5177, val loss 1.7040\n",
      "step 34400: train loss 1.5150, val loss 1.7075\n",
      "step 34500: train loss 1.5119, val loss 1.6933\n",
      "step 34600: train loss 1.5125, val loss 1.7093\n",
      "step 34700: train loss 1.5191, val loss 1.7088\n",
      "step 34800: train loss 1.5190, val loss 1.7140\n",
      "step 34900: train loss 1.5241, val loss 1.7089\n",
      "step 35000: train loss 1.5123, val loss 1.6973\n",
      "step 35100: train loss 1.5061, val loss 1.7007\n",
      "step 35200: train loss 1.5177, val loss 1.6926\n",
      "step 35300: train loss 1.5067, val loss 1.7089\n",
      "step 35400: train loss 1.5195, val loss 1.6997\n",
      "step 35500: train loss 1.5057, val loss 1.6912\n",
      "step 35600: train loss 1.5075, val loss 1.7058\n",
      "step 35700: train loss 1.5028, val loss 1.7046\n",
      "step 35800: train loss 1.5136, val loss 1.6898\n",
      "step 35900: train loss 1.5179, val loss 1.6976\n",
      "step 36000: train loss 1.5083, val loss 1.6982\n",
      "step 36100: train loss 1.5122, val loss 1.7035\n",
      "step 36200: train loss 1.5099, val loss 1.6862\n",
      "step 36300: train loss 1.5033, val loss 1.7037\n",
      "step 36400: train loss 1.5095, val loss 1.7079\n",
      "step 36500: train loss 1.5130, val loss 1.6893\n",
      "step 36600: train loss 1.5107, val loss 1.6942\n",
      "step 36700: train loss 1.5036, val loss 1.6940\n",
      "step 36800: train loss 1.5075, val loss 1.6967\n",
      "step 36900: train loss 1.5110, val loss 1.6942\n",
      "step 37000: train loss 1.5178, val loss 1.6963\n",
      "step 37100: train loss 1.5101, val loss 1.7062\n",
      "step 37200: train loss 1.5086, val loss 1.7083\n",
      "step 37300: train loss 1.5077, val loss 1.7029\n",
      "step 37400: train loss 1.5154, val loss 1.7010\n",
      "step 37500: train loss 1.5147, val loss 1.7083\n",
      "step 37600: train loss 1.5025, val loss 1.7027\n",
      "step 37700: train loss 1.5058, val loss 1.6973\n",
      "step 37800: train loss 1.5044, val loss 1.7229\n",
      "step 37900: train loss 1.5220, val loss 1.7064\n",
      "step 38000: train loss 1.5082, val loss 1.6898\n",
      "step 38100: train loss 1.5081, val loss 1.6883\n",
      "step 38200: train loss 1.5095, val loss 1.6924\n",
      "step 38300: train loss 1.5156, val loss 1.6888\n",
      "step 38400: train loss 1.5071, val loss 1.6985\n",
      "step 38500: train loss 1.5147, val loss 1.7005\n",
      "step 38600: train loss 1.5127, val loss 1.6865\n",
      "step 38700: train loss 1.5130, val loss 1.6918\n",
      "step 38800: train loss 1.5001, val loss 1.6902\n",
      "step 38900: train loss 1.5079, val loss 1.6921\n",
      "step 39000: train loss 1.5048, val loss 1.6860\n",
      "step 39100: train loss 1.5012, val loss 1.6901\n",
      "step 39200: train loss 1.5005, val loss 1.7008\n",
      "step 39300: train loss 1.5055, val loss 1.6860\n",
      "step 39400: train loss 1.5024, val loss 1.6964\n",
      "step 39500: train loss 1.4982, val loss 1.6866\n",
      "step 39600: train loss 1.5038, val loss 1.7020\n",
      "step 39700: train loss 1.4990, val loss 1.6966\n",
      "step 39800: train loss 1.5006, val loss 1.6864\n",
      "step 39900: train loss 1.4953, val loss 1.6986\n",
      "step 40000: train loss 1.5073, val loss 1.6832\n",
      "step 40100: train loss 1.5057, val loss 1.6846\n",
      "step 40200: train loss 1.4977, val loss 1.7076\n",
      "step 40300: train loss 1.5179, val loss 1.7004\n",
      "step 40400: train loss 1.5043, val loss 1.6994\n",
      "step 40500: train loss 1.4989, val loss 1.7044\n",
      "step 40600: train loss 1.5074, val loss 1.7046\n",
      "step 40700: train loss 1.5097, val loss 1.7056\n",
      "step 40800: train loss 1.5111, val loss 1.6926\n",
      "step 40900: train loss 1.5058, val loss 1.7000\n",
      "step 41000: train loss 1.4954, val loss 1.7033\n",
      "step 41100: train loss 1.5154, val loss 1.6952\n",
      "step 41200: train loss 1.4978, val loss 1.6916\n",
      "step 41300: train loss 1.4867, val loss 1.6979\n",
      "step 41400: train loss 1.5010, val loss 1.7065\n",
      "step 41500: train loss 1.5100, val loss 1.7034\n",
      "step 41600: train loss 1.5070, val loss 1.6943\n",
      "step 41700: train loss 1.5087, val loss 1.6920\n",
      "step 41800: train loss 1.4977, val loss 1.6997\n",
      "step 41900: train loss 1.5094, val loss 1.6935\n",
      "step 42000: train loss 1.5041, val loss 1.7043\n",
      "step 42100: train loss 1.4950, val loss 1.6956\n",
      "step 42200: train loss 1.5057, val loss 1.6974\n",
      "step 42300: train loss 1.4981, val loss 1.7034\n",
      "step 42400: train loss 1.4943, val loss 1.6893\n",
      "step 42500: train loss 1.4925, val loss 1.6732\n",
      "step 42600: train loss 1.5009, val loss 1.7031\n",
      "step 42700: train loss 1.4941, val loss 1.6903\n",
      "step 42800: train loss 1.4987, val loss 1.6948\n",
      "step 42900: train loss 1.5060, val loss 1.7032\n",
      "step 43000: train loss 1.4944, val loss 1.7095\n",
      "step 43100: train loss 1.4889, val loss 1.6984\n",
      "step 43200: train loss 1.5039, val loss 1.6961\n",
      "step 43300: train loss 1.5005, val loss 1.7042\n",
      "step 43400: train loss 1.4987, val loss 1.7006\n",
      "step 43500: train loss 1.4992, val loss 1.6864\n",
      "step 43600: train loss 1.4868, val loss 1.6885\n",
      "step 43700: train loss 1.4977, val loss 1.7050\n",
      "step 43800: train loss 1.5052, val loss 1.6975\n",
      "step 43900: train loss 1.5144, val loss 1.6993\n",
      "step 44000: train loss 1.4877, val loss 1.7072\n",
      "step 44100: train loss 1.5021, val loss 1.6769\n",
      "step 44200: train loss 1.5064, val loss 1.6887\n",
      "step 44300: train loss 1.5061, val loss 1.7023\n",
      "step 44400: train loss 1.4932, val loss 1.6859\n",
      "step 44500: train loss 1.4910, val loss 1.6762\n",
      "step 44600: train loss 1.4922, val loss 1.6927\n",
      "step 44700: train loss 1.5024, val loss 1.6936\n",
      "step 44800: train loss 1.5019, val loss 1.6923\n",
      "step 44900: train loss 1.4911, val loss 1.6821\n",
      "step 45000: train loss 1.4936, val loss 1.6824\n",
      "step 45100: train loss 1.4956, val loss 1.6929\n",
      "step 45200: train loss 1.5043, val loss 1.6972\n",
      "step 45300: train loss 1.5003, val loss 1.7002\n",
      "step 45400: train loss 1.4930, val loss 1.6925\n",
      "step 45500: train loss 1.4915, val loss 1.7005\n",
      "step 45600: train loss 1.5003, val loss 1.7004\n",
      "step 45700: train loss 1.5008, val loss 1.6944\n",
      "step 45800: train loss 1.4928, val loss 1.6916\n",
      "step 45900: train loss 1.4950, val loss 1.6935\n",
      "step 46000: train loss 1.4928, val loss 1.6976\n",
      "step 46100: train loss 1.4926, val loss 1.6879\n",
      "step 46200: train loss 1.5034, val loss 1.6850\n",
      "step 46300: train loss 1.4983, val loss 1.6790\n",
      "step 46400: train loss 1.4960, val loss 1.7085\n",
      "step 46500: train loss 1.4983, val loss 1.6999\n",
      "step 46600: train loss 1.5000, val loss 1.6959\n",
      "step 46700: train loss 1.4865, val loss 1.6958\n",
      "step 46800: train loss 1.4977, val loss 1.6865\n",
      "step 46900: train loss 1.4946, val loss 1.6887\n",
      "step 47000: train loss 1.4901, val loss 1.6916\n",
      "step 47100: train loss 1.4913, val loss 1.6895\n",
      "step 47200: train loss 1.5009, val loss 1.6898\n",
      "step 47300: train loss 1.4924, val loss 1.6741\n",
      "step 47400: train loss 1.4975, val loss 1.6922\n",
      "step 47500: train loss 1.4942, val loss 1.6941\n",
      "step 47600: train loss 1.4959, val loss 1.6901\n",
      "step 47700: train loss 1.4833, val loss 1.6876\n",
      "step 47800: train loss 1.4948, val loss 1.7040\n",
      "step 47900: train loss 1.5037, val loss 1.7005\n",
      "step 48000: train loss 1.4928, val loss 1.6957\n",
      "step 48100: train loss 1.5005, val loss 1.6858\n",
      "step 48200: train loss 1.4973, val loss 1.6969\n",
      "step 48300: train loss 1.4895, val loss 1.6910\n",
      "step 48400: train loss 1.4900, val loss 1.6910\n",
      "step 48500: train loss 1.4928, val loss 1.6903\n",
      "step 48600: train loss 1.4887, val loss 1.6845\n",
      "step 48700: train loss 1.4809, val loss 1.6905\n",
      "step 48800: train loss 1.4923, val loss 1.6979\n",
      "step 48900: train loss 1.4928, val loss 1.7090\n",
      "step 49000: train loss 1.4912, val loss 1.7040\n",
      "step 49100: train loss 1.4936, val loss 1.6941\n",
      "step 49200: train loss 1.4904, val loss 1.7154\n",
      "step 49300: train loss 1.5003, val loss 1.7019\n",
      "step 49400: train loss 1.4900, val loss 1.6808\n",
      "step 49500: train loss 1.4913, val loss 1.6710\n",
      "step 49600: train loss 1.4890, val loss 1.7104\n",
      "step 49700: train loss 1.4818, val loss 1.7175\n",
      "step 49800: train loss 1.4881, val loss 1.6869\n",
      "step 49900: train loss 1.4816, val loss 1.6894\n",
      "step 49999: train loss 1.4884, val loss 1.6971\n",
      "Loss results for res1 dropout=0.0 saved to ./loss_time/res1\\res1_dropout_0.0_losses.json\n",
      "Training time for res1 dropout=0.0 saved to ./loss_time/res1\\res1_dropout_0.0_training_time.json\n",
      "Training model with dropout=0.025\n",
      "step 0: train loss 4.2682, val loss 4.2754\n",
      "step 100: train loss 2.6653, val loss 2.6658\n",
      "step 200: train loss 2.5224, val loss 2.5287\n",
      "step 300: train loss 2.4619, val loss 2.4461\n",
      "step 400: train loss 2.4007, val loss 2.4051\n",
      "step 500: train loss 2.3598, val loss 2.3611\n",
      "step 600: train loss 2.3294, val loss 2.3446\n",
      "step 700: train loss 2.2914, val loss 2.3023\n",
      "step 800: train loss 2.2553, val loss 2.2653\n",
      "step 900: train loss 2.2225, val loss 2.2449\n",
      "step 1000: train loss 2.1957, val loss 2.2163\n",
      "step 1100: train loss 2.1760, val loss 2.1949\n",
      "step 1200: train loss 2.1473, val loss 2.1658\n",
      "step 1300: train loss 2.1276, val loss 2.1664\n",
      "step 1400: train loss 2.1110, val loss 2.1404\n",
      "step 1500: train loss 2.0740, val loss 2.1091\n",
      "step 1600: train loss 2.0770, val loss 2.1067\n",
      "step 1700: train loss 2.0556, val loss 2.0906\n",
      "step 1800: train loss 2.0384, val loss 2.0772\n",
      "step 1900: train loss 2.0103, val loss 2.0569\n",
      "step 2000: train loss 2.0124, val loss 2.0656\n",
      "step 2100: train loss 1.9889, val loss 2.0386\n",
      "step 2200: train loss 1.9694, val loss 2.0415\n",
      "step 2300: train loss 1.9742, val loss 2.0341\n",
      "step 2400: train loss 1.9541, val loss 2.0209\n",
      "step 2500: train loss 1.9511, val loss 2.0204\n",
      "step 2600: train loss 1.9368, val loss 2.0131\n",
      "step 2700: train loss 1.9273, val loss 2.0096\n",
      "step 2800: train loss 1.9219, val loss 2.0047\n",
      "step 2900: train loss 1.9226, val loss 1.9859\n",
      "step 3000: train loss 1.9078, val loss 1.9872\n",
      "step 3100: train loss 1.8924, val loss 1.9826\n",
      "step 3200: train loss 1.8858, val loss 1.9771\n",
      "step 3300: train loss 1.8783, val loss 1.9668\n",
      "step 3400: train loss 1.8754, val loss 1.9671\n",
      "step 3500: train loss 1.8604, val loss 1.9475\n",
      "step 3600: train loss 1.8571, val loss 1.9424\n",
      "step 3700: train loss 1.8682, val loss 1.9521\n",
      "step 3800: train loss 1.8549, val loss 1.9557\n",
      "step 3900: train loss 1.8519, val loss 1.9449\n",
      "step 4000: train loss 1.8253, val loss 1.9410\n",
      "step 4100: train loss 1.8333, val loss 1.9503\n",
      "step 4200: train loss 1.8282, val loss 1.9357\n",
      "step 4300: train loss 1.8277, val loss 1.9271\n",
      "step 4400: train loss 1.8074, val loss 1.9283\n",
      "step 4500: train loss 1.8175, val loss 1.9354\n",
      "step 4600: train loss 1.8052, val loss 1.9309\n",
      "step 4700: train loss 1.7970, val loss 1.9210\n",
      "step 4800: train loss 1.8137, val loss 1.9264\n",
      "step 4900: train loss 1.7856, val loss 1.9304\n",
      "step 5000: train loss 1.8022, val loss 1.9170\n",
      "step 5100: train loss 1.7929, val loss 1.9320\n",
      "step 5200: train loss 1.7826, val loss 1.9121\n",
      "step 5300: train loss 1.7796, val loss 1.9041\n",
      "step 5400: train loss 1.7843, val loss 1.9175\n",
      "step 5500: train loss 1.7743, val loss 1.9013\n",
      "step 5600: train loss 1.7842, val loss 1.9045\n",
      "step 5700: train loss 1.7634, val loss 1.9105\n",
      "step 5800: train loss 1.7558, val loss 1.8835\n",
      "step 5900: train loss 1.7637, val loss 1.8845\n",
      "step 6000: train loss 1.7591, val loss 1.8920\n",
      "step 6100: train loss 1.7591, val loss 1.8926\n",
      "step 6200: train loss 1.7592, val loss 1.8857\n",
      "step 6300: train loss 1.7465, val loss 1.8627\n",
      "step 6400: train loss 1.7439, val loss 1.8850\n",
      "step 6500: train loss 1.7421, val loss 1.8747\n",
      "step 6600: train loss 1.7312, val loss 1.8689\n",
      "step 6700: train loss 1.7398, val loss 1.8698\n",
      "step 6800: train loss 1.7398, val loss 1.8603\n",
      "step 6900: train loss 1.7378, val loss 1.8678\n",
      "step 7000: train loss 1.7404, val loss 1.8654\n",
      "step 7100: train loss 1.7327, val loss 1.8660\n",
      "step 7200: train loss 1.7310, val loss 1.8704\n",
      "step 7300: train loss 1.7299, val loss 1.8605\n",
      "step 7400: train loss 1.7307, val loss 1.8669\n",
      "step 7500: train loss 1.7076, val loss 1.8539\n",
      "step 7600: train loss 1.7094, val loss 1.8565\n",
      "step 7700: train loss 1.7120, val loss 1.8476\n",
      "step 7800: train loss 1.7123, val loss 1.8487\n",
      "step 7900: train loss 1.7124, val loss 1.8581\n",
      "step 8000: train loss 1.7148, val loss 1.8509\n",
      "step 8100: train loss 1.7141, val loss 1.8497\n",
      "step 8200: train loss 1.7088, val loss 1.8499\n",
      "step 8300: train loss 1.6985, val loss 1.8383\n",
      "step 8400: train loss 1.7003, val loss 1.8417\n",
      "step 8500: train loss 1.7108, val loss 1.8482\n",
      "step 8600: train loss 1.6984, val loss 1.8486\n",
      "step 8700: train loss 1.7032, val loss 1.8485\n",
      "step 8800: train loss 1.6908, val loss 1.8373\n",
      "step 8900: train loss 1.7071, val loss 1.8357\n",
      "step 9000: train loss 1.6926, val loss 1.8195\n",
      "step 9100: train loss 1.6886, val loss 1.8259\n",
      "step 9200: train loss 1.6881, val loss 1.8359\n",
      "step 9300: train loss 1.6933, val loss 1.8192\n",
      "step 9400: train loss 1.6986, val loss 1.8453\n",
      "step 9500: train loss 1.6911, val loss 1.8263\n",
      "step 9600: train loss 1.6892, val loss 1.8296\n",
      "step 9700: train loss 1.6906, val loss 1.8319\n",
      "step 9800: train loss 1.6772, val loss 1.8421\n",
      "step 9900: train loss 1.6673, val loss 1.8310\n",
      "step 10000: train loss 1.6813, val loss 1.8271\n",
      "step 10100: train loss 1.6699, val loss 1.8119\n",
      "step 10200: train loss 1.6668, val loss 1.8259\n",
      "step 10300: train loss 1.6779, val loss 1.8142\n",
      "step 10400: train loss 1.6681, val loss 1.8336\n",
      "step 10500: train loss 1.6715, val loss 1.8135\n",
      "step 10600: train loss 1.6734, val loss 1.8257\n",
      "step 10700: train loss 1.6620, val loss 1.8096\n",
      "step 10800: train loss 1.6758, val loss 1.8151\n",
      "step 10900: train loss 1.6626, val loss 1.8280\n",
      "step 11000: train loss 1.6679, val loss 1.8255\n",
      "step 11100: train loss 1.6773, val loss 1.8118\n",
      "step 11200: train loss 1.6575, val loss 1.8111\n",
      "step 11300: train loss 1.6622, val loss 1.8225\n",
      "step 11400: train loss 1.6461, val loss 1.8078\n",
      "step 11500: train loss 1.6520, val loss 1.8180\n",
      "step 11600: train loss 1.6536, val loss 1.8111\n",
      "step 11700: train loss 1.6596, val loss 1.8079\n",
      "step 11800: train loss 1.6620, val loss 1.7994\n",
      "step 11900: train loss 1.6626, val loss 1.8123\n",
      "step 12000: train loss 1.6485, val loss 1.8142\n",
      "step 12100: train loss 1.6607, val loss 1.7806\n",
      "step 12200: train loss 1.6485, val loss 1.7964\n",
      "step 12300: train loss 1.6507, val loss 1.7967\n",
      "step 12400: train loss 1.6465, val loss 1.8016\n",
      "step 12500: train loss 1.6471, val loss 1.7979\n",
      "step 12600: train loss 1.6578, val loss 1.7948\n",
      "step 12700: train loss 1.6420, val loss 1.8100\n",
      "step 12800: train loss 1.6474, val loss 1.7947\n",
      "step 12900: train loss 1.6461, val loss 1.8045\n",
      "step 13000: train loss 1.6396, val loss 1.8045\n",
      "step 13100: train loss 1.6492, val loss 1.7922\n",
      "step 13200: train loss 1.6460, val loss 1.7965\n",
      "step 13300: train loss 1.6355, val loss 1.7894\n",
      "step 13400: train loss 1.6451, val loss 1.7965\n",
      "step 13500: train loss 1.6411, val loss 1.8075\n",
      "step 13600: train loss 1.6388, val loss 1.7831\n",
      "step 13700: train loss 1.6306, val loss 1.7902\n",
      "step 13800: train loss 1.6352, val loss 1.8037\n",
      "step 13900: train loss 1.6254, val loss 1.7914\n",
      "step 14000: train loss 1.6428, val loss 1.7794\n",
      "step 14100: train loss 1.6357, val loss 1.7858\n",
      "step 14200: train loss 1.6326, val loss 1.7900\n",
      "step 14300: train loss 1.6334, val loss 1.7838\n",
      "step 14400: train loss 1.6245, val loss 1.7851\n",
      "step 14500: train loss 1.6253, val loss 1.7834\n",
      "step 14600: train loss 1.6391, val loss 1.7950\n",
      "step 14700: train loss 1.6299, val loss 1.7895\n",
      "step 14800: train loss 1.6275, val loss 1.7842\n",
      "step 14900: train loss 1.6278, val loss 1.7829\n",
      "step 15000: train loss 1.6247, val loss 1.7828\n",
      "step 15100: train loss 1.6259, val loss 1.7853\n",
      "step 15200: train loss 1.6209, val loss 1.7892\n",
      "step 15300: train loss 1.6158, val loss 1.7891\n",
      "step 15400: train loss 1.6211, val loss 1.7810\n",
      "step 15500: train loss 1.6268, val loss 1.7908\n",
      "step 15600: train loss 1.6235, val loss 1.7836\n",
      "step 15700: train loss 1.6203, val loss 1.7894\n",
      "step 15800: train loss 1.6085, val loss 1.7809\n",
      "step 15900: train loss 1.6119, val loss 1.7699\n",
      "step 16000: train loss 1.6200, val loss 1.7869\n",
      "step 16100: train loss 1.6127, val loss 1.7682\n",
      "step 16200: train loss 1.6301, val loss 1.7874\n",
      "step 16300: train loss 1.6137, val loss 1.7776\n",
      "step 16400: train loss 1.6102, val loss 1.7777\n",
      "step 16500: train loss 1.6066, val loss 1.7688\n",
      "step 16600: train loss 1.5998, val loss 1.7693\n",
      "step 16700: train loss 1.6144, val loss 1.7762\n",
      "step 16800: train loss 1.6135, val loss 1.7889\n",
      "step 16900: train loss 1.6130, val loss 1.7614\n",
      "step 17000: train loss 1.6108, val loss 1.7772\n",
      "step 17100: train loss 1.6180, val loss 1.7960\n",
      "step 17200: train loss 1.6040, val loss 1.7793\n",
      "step 17300: train loss 1.6122, val loss 1.7827\n",
      "step 17400: train loss 1.6069, val loss 1.7708\n",
      "step 17500: train loss 1.6001, val loss 1.7735\n",
      "step 17600: train loss 1.6105, val loss 1.7763\n",
      "step 17700: train loss 1.6106, val loss 1.7742\n",
      "step 17800: train loss 1.6029, val loss 1.7658\n",
      "step 17900: train loss 1.6059, val loss 1.7620\n",
      "step 18000: train loss 1.6045, val loss 1.7683\n",
      "step 18100: train loss 1.5889, val loss 1.7694\n",
      "step 18200: train loss 1.6141, val loss 1.7703\n",
      "step 18300: train loss 1.6007, val loss 1.7556\n",
      "step 18400: train loss 1.5971, val loss 1.7590\n",
      "step 18500: train loss 1.5899, val loss 1.7610\n",
      "step 18600: train loss 1.6001, val loss 1.7682\n",
      "step 18700: train loss 1.5974, val loss 1.7502\n",
      "step 18800: train loss 1.6003, val loss 1.7554\n",
      "step 18900: train loss 1.5895, val loss 1.7541\n",
      "step 19000: train loss 1.5981, val loss 1.7627\n",
      "step 19100: train loss 1.5890, val loss 1.7673\n",
      "step 19200: train loss 1.5943, val loss 1.7612\n",
      "step 19300: train loss 1.5938, val loss 1.7381\n",
      "step 19400: train loss 1.6000, val loss 1.7536\n",
      "step 19500: train loss 1.5958, val loss 1.7525\n",
      "step 19600: train loss 1.5920, val loss 1.7554\n",
      "step 19700: train loss 1.5938, val loss 1.7722\n",
      "step 19800: train loss 1.5892, val loss 1.7680\n",
      "step 19900: train loss 1.5995, val loss 1.7515\n",
      "step 20000: train loss 1.5845, val loss 1.7643\n",
      "step 20100: train loss 1.5850, val loss 1.7502\n",
      "step 20200: train loss 1.5864, val loss 1.7527\n",
      "step 20300: train loss 1.5851, val loss 1.7505\n",
      "step 20400: train loss 1.5946, val loss 1.7485\n",
      "step 20500: train loss 1.5848, val loss 1.7454\n",
      "step 20600: train loss 1.5832, val loss 1.7639\n",
      "step 20700: train loss 1.5863, val loss 1.7584\n",
      "step 20800: train loss 1.5879, val loss 1.7623\n",
      "step 20900: train loss 1.5794, val loss 1.7556\n",
      "step 21000: train loss 1.5827, val loss 1.7536\n",
      "step 21100: train loss 1.5861, val loss 1.7616\n",
      "step 21200: train loss 1.5814, val loss 1.7525\n",
      "step 21300: train loss 1.5911, val loss 1.7500\n",
      "step 21400: train loss 1.5863, val loss 1.7469\n",
      "step 21500: train loss 1.5891, val loss 1.7489\n",
      "step 21600: train loss 1.5919, val loss 1.7545\n",
      "step 21700: train loss 1.5878, val loss 1.7594\n",
      "step 21800: train loss 1.5762, val loss 1.7544\n",
      "step 21900: train loss 1.5773, val loss 1.7470\n",
      "step 22000: train loss 1.5824, val loss 1.7465\n",
      "step 22100: train loss 1.5863, val loss 1.7476\n",
      "step 22200: train loss 1.5885, val loss 1.7381\n",
      "step 22300: train loss 1.5745, val loss 1.7401\n",
      "step 22400: train loss 1.5865, val loss 1.7405\n",
      "step 22500: train loss 1.5724, val loss 1.7477\n",
      "step 22600: train loss 1.5755, val loss 1.7403\n",
      "step 22700: train loss 1.5866, val loss 1.7310\n",
      "step 22800: train loss 1.5762, val loss 1.7551\n",
      "step 22900: train loss 1.5725, val loss 1.7602\n",
      "step 23000: train loss 1.5808, val loss 1.7578\n",
      "step 23100: train loss 1.5723, val loss 1.7475\n",
      "step 23200: train loss 1.5748, val loss 1.7428\n",
      "step 23300: train loss 1.5746, val loss 1.7554\n",
      "step 23400: train loss 1.5891, val loss 1.7493\n",
      "step 23500: train loss 1.5763, val loss 1.7534\n",
      "step 23600: train loss 1.5715, val loss 1.7438\n",
      "step 23700: train loss 1.5682, val loss 1.7554\n",
      "step 23800: train loss 1.5704, val loss 1.7694\n",
      "step 23900: train loss 1.5723, val loss 1.7531\n",
      "step 24000: train loss 1.5713, val loss 1.7571\n",
      "step 24100: train loss 1.5782, val loss 1.7375\n",
      "step 24200: train loss 1.5739, val loss 1.7521\n",
      "step 24300: train loss 1.5663, val loss 1.7331\n",
      "step 24400: train loss 1.5664, val loss 1.7582\n",
      "step 24500: train loss 1.5698, val loss 1.7467\n",
      "step 24600: train loss 1.5717, val loss 1.7459\n",
      "step 24700: train loss 1.5703, val loss 1.7405\n",
      "step 24800: train loss 1.5740, val loss 1.7383\n",
      "step 24900: train loss 1.5645, val loss 1.7400\n",
      "step 25000: train loss 1.5726, val loss 1.7323\n",
      "step 25100: train loss 1.5589, val loss 1.7541\n",
      "step 25200: train loss 1.5645, val loss 1.7414\n",
      "step 25300: train loss 1.5734, val loss 1.7411\n",
      "step 25400: train loss 1.5751, val loss 1.7345\n",
      "step 25500: train loss 1.5652, val loss 1.7357\n",
      "step 25600: train loss 1.5653, val loss 1.7470\n",
      "step 25700: train loss 1.5706, val loss 1.7354\n",
      "step 25800: train loss 1.5591, val loss 1.7308\n",
      "step 25900: train loss 1.5663, val loss 1.7356\n",
      "step 26000: train loss 1.5633, val loss 1.7372\n",
      "step 26100: train loss 1.5614, val loss 1.7310\n",
      "step 26200: train loss 1.5679, val loss 1.7460\n",
      "step 26300: train loss 1.5699, val loss 1.7473\n",
      "step 26400: train loss 1.5667, val loss 1.7530\n",
      "step 26500: train loss 1.5669, val loss 1.7278\n",
      "step 26600: train loss 1.5633, val loss 1.7374\n",
      "step 26700: train loss 1.5645, val loss 1.7434\n",
      "step 26800: train loss 1.5636, val loss 1.7422\n",
      "step 26900: train loss 1.5625, val loss 1.7388\n",
      "step 27000: train loss 1.5566, val loss 1.7344\n",
      "step 27100: train loss 1.5469, val loss 1.7327\n",
      "step 27200: train loss 1.5628, val loss 1.7479\n",
      "step 27300: train loss 1.5604, val loss 1.7460\n",
      "step 27400: train loss 1.5625, val loss 1.7270\n",
      "step 27500: train loss 1.5486, val loss 1.7401\n",
      "step 27600: train loss 1.5525, val loss 1.7243\n",
      "step 27700: train loss 1.5670, val loss 1.7238\n",
      "step 27800: train loss 1.5561, val loss 1.7241\n",
      "step 27900: train loss 1.5686, val loss 1.7293\n",
      "step 28000: train loss 1.5687, val loss 1.7292\n",
      "step 28100: train loss 1.5631, val loss 1.7351\n",
      "step 28200: train loss 1.5576, val loss 1.7233\n",
      "step 28300: train loss 1.5612, val loss 1.7211\n",
      "step 28400: train loss 1.5621, val loss 1.7444\n",
      "step 28500: train loss 1.5588, val loss 1.7201\n",
      "step 28600: train loss 1.5531, val loss 1.7174\n",
      "step 28700: train loss 1.5601, val loss 1.7171\n",
      "step 28800: train loss 1.5525, val loss 1.7338\n",
      "step 28900: train loss 1.5604, val loss 1.7303\n",
      "step 29000: train loss 1.5611, val loss 1.7269\n",
      "step 29100: train loss 1.5427, val loss 1.7319\n",
      "step 29200: train loss 1.5579, val loss 1.7279\n",
      "step 29300: train loss 1.5570, val loss 1.7384\n",
      "step 29400: train loss 1.5441, val loss 1.7227\n",
      "step 29500: train loss 1.5552, val loss 1.7315\n",
      "step 29600: train loss 1.5529, val loss 1.7271\n",
      "step 29700: train loss 1.5555, val loss 1.7222\n",
      "step 29800: train loss 1.5622, val loss 1.7377\n",
      "step 29900: train loss 1.5578, val loss 1.7367\n",
      "step 30000: train loss 1.5595, val loss 1.7379\n",
      "step 30100: train loss 1.5517, val loss 1.7518\n",
      "step 30200: train loss 1.5479, val loss 1.7419\n",
      "step 30300: train loss 1.5482, val loss 1.7330\n",
      "step 30400: train loss 1.5506, val loss 1.7179\n",
      "step 30500: train loss 1.5521, val loss 1.7349\n",
      "step 30600: train loss 1.5586, val loss 1.7231\n",
      "step 30700: train loss 1.5398, val loss 1.7237\n",
      "step 30800: train loss 1.5443, val loss 1.7261\n",
      "step 30900: train loss 1.5551, val loss 1.7236\n",
      "step 31000: train loss 1.5597, val loss 1.7341\n",
      "step 31100: train loss 1.5541, val loss 1.7275\n",
      "step 31200: train loss 1.5447, val loss 1.7234\n",
      "step 31300: train loss 1.5434, val loss 1.7269\n",
      "step 31400: train loss 1.5485, val loss 1.7246\n",
      "step 31500: train loss 1.5436, val loss 1.7160\n",
      "step 31600: train loss 1.5509, val loss 1.7255\n",
      "step 31700: train loss 1.5418, val loss 1.7334\n",
      "step 31800: train loss 1.5463, val loss 1.7174\n",
      "step 31900: train loss 1.5479, val loss 1.7223\n",
      "step 32000: train loss 1.5447, val loss 1.7147\n",
      "step 32100: train loss 1.5511, val loss 1.7242\n",
      "step 32200: train loss 1.5459, val loss 1.7211\n",
      "step 32300: train loss 1.5469, val loss 1.6994\n",
      "step 32400: train loss 1.5422, val loss 1.7310\n",
      "step 32500: train loss 1.5471, val loss 1.7142\n",
      "step 32600: train loss 1.5490, val loss 1.7225\n",
      "step 32700: train loss 1.5444, val loss 1.7179\n",
      "step 32800: train loss 1.5445, val loss 1.7176\n",
      "step 32900: train loss 1.5422, val loss 1.7185\n",
      "step 33000: train loss 1.5406, val loss 1.7224\n",
      "step 33100: train loss 1.5362, val loss 1.7174\n",
      "step 33200: train loss 1.5321, val loss 1.7076\n",
      "step 33300: train loss 1.5402, val loss 1.7110\n",
      "step 33400: train loss 1.5448, val loss 1.7124\n",
      "step 33500: train loss 1.5453, val loss 1.7274\n",
      "step 33600: train loss 1.5387, val loss 1.7172\n",
      "step 33700: train loss 1.5504, val loss 1.7319\n",
      "step 33800: train loss 1.5482, val loss 1.7160\n",
      "step 33900: train loss 1.5508, val loss 1.7176\n",
      "step 34000: train loss 1.5455, val loss 1.7156\n",
      "step 34100: train loss 1.5396, val loss 1.7184\n",
      "step 34200: train loss 1.5318, val loss 1.7272\n",
      "step 34300: train loss 1.5363, val loss 1.7218\n",
      "step 34400: train loss 1.5490, val loss 1.7133\n",
      "step 34500: train loss 1.5386, val loss 1.7281\n",
      "step 34600: train loss 1.5485, val loss 1.7161\n",
      "step 34700: train loss 1.5313, val loss 1.7208\n",
      "step 34800: train loss 1.5412, val loss 1.7119\n",
      "step 34900: train loss 1.5289, val loss 1.7043\n",
      "step 35000: train loss 1.5402, val loss 1.7232\n",
      "step 35100: train loss 1.5467, val loss 1.7125\n",
      "step 35200: train loss 1.5332, val loss 1.7071\n",
      "step 35300: train loss 1.5346, val loss 1.7153\n",
      "step 35400: train loss 1.5398, val loss 1.7073\n",
      "step 35500: train loss 1.5448, val loss 1.7211\n",
      "step 35600: train loss 1.5413, val loss 1.7104\n",
      "step 35700: train loss 1.5387, val loss 1.7285\n",
      "step 35800: train loss 1.5410, val loss 1.7219\n",
      "step 35900: train loss 1.5432, val loss 1.7237\n",
      "step 36000: train loss 1.5279, val loss 1.7088\n",
      "step 36100: train loss 1.5326, val loss 1.7193\n",
      "step 36200: train loss 1.5279, val loss 1.7044\n",
      "step 36300: train loss 1.5252, val loss 1.7127\n",
      "step 36400: train loss 1.5396, val loss 1.7183\n",
      "step 36500: train loss 1.5390, val loss 1.7036\n",
      "step 36600: train loss 1.5385, val loss 1.7153\n",
      "step 36700: train loss 1.5356, val loss 1.7122\n",
      "step 36800: train loss 1.5341, val loss 1.7116\n",
      "step 36900: train loss 1.5387, val loss 1.7184\n",
      "step 37000: train loss 1.5427, val loss 1.7103\n",
      "step 37100: train loss 1.5359, val loss 1.7031\n",
      "step 37200: train loss 1.5362, val loss 1.7170\n",
      "step 37300: train loss 1.5417, val loss 1.7091\n",
      "step 37400: train loss 1.5290, val loss 1.6994\n",
      "step 37500: train loss 1.5314, val loss 1.7013\n",
      "step 37600: train loss 1.5222, val loss 1.7066\n",
      "step 37700: train loss 1.5276, val loss 1.7169\n",
      "step 37800: train loss 1.5292, val loss 1.7173\n",
      "step 37900: train loss 1.5376, val loss 1.7074\n",
      "step 38000: train loss 1.5189, val loss 1.7169\n",
      "step 38100: train loss 1.5377, val loss 1.7196\n",
      "step 38200: train loss 1.5356, val loss 1.7024\n",
      "step 38300: train loss 1.5337, val loss 1.7230\n",
      "step 38400: train loss 1.5345, val loss 1.7193\n",
      "step 38500: train loss 1.5368, val loss 1.7146\n",
      "step 38600: train loss 1.5371, val loss 1.7047\n",
      "step 38700: train loss 1.5280, val loss 1.7011\n",
      "step 38800: train loss 1.5380, val loss 1.7005\n",
      "step 38900: train loss 1.5348, val loss 1.7222\n",
      "step 39000: train loss 1.5311, val loss 1.7164\n",
      "step 39100: train loss 1.5264, val loss 1.7055\n",
      "step 39200: train loss 1.5307, val loss 1.7056\n",
      "step 39300: train loss 1.5234, val loss 1.7073\n",
      "step 39400: train loss 1.5255, val loss 1.6981\n",
      "step 39500: train loss 1.5326, val loss 1.6896\n",
      "step 39600: train loss 1.5296, val loss 1.7067\n",
      "step 39700: train loss 1.5294, val loss 1.6986\n",
      "step 39800: train loss 1.5329, val loss 1.7035\n",
      "step 39900: train loss 1.5367, val loss 1.7058\n",
      "step 40000: train loss 1.5291, val loss 1.7110\n",
      "step 40100: train loss 1.5428, val loss 1.7187\n",
      "step 40200: train loss 1.5313, val loss 1.7121\n",
      "step 40300: train loss 1.5288, val loss 1.7145\n",
      "step 40400: train loss 1.5266, val loss 1.7059\n",
      "step 40500: train loss 1.5291, val loss 1.7019\n",
      "step 40600: train loss 1.5244, val loss 1.7007\n",
      "step 40700: train loss 1.5347, val loss 1.6984\n",
      "step 40800: train loss 1.5193, val loss 1.6928\n",
      "step 40900: train loss 1.5298, val loss 1.7190\n",
      "step 41000: train loss 1.5295, val loss 1.7049\n",
      "step 41100: train loss 1.5314, val loss 1.7088\n",
      "step 41200: train loss 1.5199, val loss 1.7088\n",
      "step 41300: train loss 1.5296, val loss 1.6935\n",
      "step 41400: train loss 1.5256, val loss 1.7048\n",
      "step 41500: train loss 1.5276, val loss 1.6923\n",
      "step 41600: train loss 1.5247, val loss 1.7084\n",
      "step 41700: train loss 1.5282, val loss 1.7106\n",
      "step 41800: train loss 1.5191, val loss 1.7079\n",
      "step 41900: train loss 1.5317, val loss 1.7050\n",
      "step 42000: train loss 1.5189, val loss 1.7150\n",
      "step 42100: train loss 1.5283, val loss 1.6910\n",
      "step 42200: train loss 1.5292, val loss 1.7017\n",
      "step 42300: train loss 1.5285, val loss 1.7094\n",
      "step 42400: train loss 1.5224, val loss 1.7121\n",
      "step 42500: train loss 1.5217, val loss 1.7144\n",
      "step 42600: train loss 1.5263, val loss 1.7032\n",
      "step 42700: train loss 1.5197, val loss 1.7045\n",
      "step 42800: train loss 1.5158, val loss 1.7015\n",
      "step 42900: train loss 1.5233, val loss 1.7034\n",
      "step 43000: train loss 1.5253, val loss 1.7093\n",
      "step 43100: train loss 1.5194, val loss 1.7196\n",
      "step 43200: train loss 1.5194, val loss 1.7108\n",
      "step 43300: train loss 1.5147, val loss 1.6951\n",
      "step 43400: train loss 1.5292, val loss 1.7074\n",
      "step 43500: train loss 1.5241, val loss 1.7166\n",
      "step 43600: train loss 1.5222, val loss 1.7034\n",
      "step 43700: train loss 1.5337, val loss 1.7109\n",
      "step 43800: train loss 1.5140, val loss 1.6978\n",
      "step 43900: train loss 1.5270, val loss 1.7094\n",
      "step 44000: train loss 1.5185, val loss 1.7011\n",
      "step 44100: train loss 1.5210, val loss 1.7173\n",
      "step 44200: train loss 1.5347, val loss 1.7201\n",
      "step 44300: train loss 1.5084, val loss 1.6940\n",
      "step 44400: train loss 1.5192, val loss 1.7132\n",
      "step 44500: train loss 1.5227, val loss 1.6969\n",
      "step 44600: train loss 1.5290, val loss 1.7029\n",
      "step 44700: train loss 1.5114, val loss 1.7039\n",
      "step 44800: train loss 1.5180, val loss 1.6971\n",
      "step 44900: train loss 1.5238, val loss 1.7073\n",
      "step 45000: train loss 1.5174, val loss 1.7047\n",
      "step 45100: train loss 1.5228, val loss 1.6950\n",
      "step 45200: train loss 1.5188, val loss 1.7028\n",
      "step 45300: train loss 1.5190, val loss 1.6895\n",
      "step 45400: train loss 1.5233, val loss 1.6886\n",
      "step 45500: train loss 1.5222, val loss 1.6850\n",
      "step 45600: train loss 1.5211, val loss 1.7015\n",
      "step 45700: train loss 1.5241, val loss 1.7112\n",
      "step 45800: train loss 1.5052, val loss 1.6993\n",
      "step 45900: train loss 1.5038, val loss 1.6891\n",
      "step 46000: train loss 1.5240, val loss 1.6835\n",
      "step 46100: train loss 1.5243, val loss 1.6989\n",
      "step 46200: train loss 1.5161, val loss 1.6860\n",
      "step 46300: train loss 1.5206, val loss 1.6966\n",
      "step 46400: train loss 1.5160, val loss 1.6948\n",
      "step 46500: train loss 1.5148, val loss 1.7027\n",
      "step 46600: train loss 1.5206, val loss 1.7071\n",
      "step 46700: train loss 1.5202, val loss 1.6978\n",
      "step 46800: train loss 1.5155, val loss 1.6960\n",
      "step 46900: train loss 1.5162, val loss 1.7056\n",
      "step 47000: train loss 1.5067, val loss 1.6995\n",
      "step 47100: train loss 1.5211, val loss 1.7068\n",
      "step 47200: train loss 1.5185, val loss 1.7035\n",
      "step 47300: train loss 1.5262, val loss 1.7044\n",
      "step 47400: train loss 1.5140, val loss 1.6909\n",
      "step 47500: train loss 1.5156, val loss 1.6972\n",
      "step 47600: train loss 1.5207, val loss 1.6930\n",
      "step 47700: train loss 1.5179, val loss 1.6907\n",
      "step 47800: train loss 1.5181, val loss 1.6936\n",
      "step 47900: train loss 1.5197, val loss 1.7037\n",
      "step 48000: train loss 1.5133, val loss 1.7121\n",
      "step 48100: train loss 1.5129, val loss 1.6896\n",
      "step 48200: train loss 1.5224, val loss 1.7016\n",
      "step 48300: train loss 1.5177, val loss 1.6870\n",
      "step 48400: train loss 1.5188, val loss 1.6918\n",
      "step 48500: train loss 1.5157, val loss 1.6913\n",
      "step 48600: train loss 1.5153, val loss 1.7000\n",
      "step 48700: train loss 1.5162, val loss 1.6836\n",
      "step 48800: train loss 1.5153, val loss 1.6819\n",
      "step 48900: train loss 1.5123, val loss 1.7115\n",
      "step 49000: train loss 1.5015, val loss 1.6970\n",
      "step 49100: train loss 1.5085, val loss 1.6941\n",
      "step 49200: train loss 1.5130, val loss 1.7115\n",
      "step 49300: train loss 1.5024, val loss 1.6920\n",
      "step 49400: train loss 1.5033, val loss 1.7003\n",
      "step 49500: train loss 1.5174, val loss 1.6893\n",
      "step 49600: train loss 1.5174, val loss 1.7155\n",
      "step 49700: train loss 1.5168, val loss 1.6869\n",
      "step 49800: train loss 1.5190, val loss 1.6943\n",
      "step 49900: train loss 1.5100, val loss 1.6956\n",
      "step 49999: train loss 1.5137, val loss 1.6911\n",
      "Loss results for res1 dropout=0.025 saved to ./loss_time/res1\\res1_dropout_0.025_losses.json\n",
      "Training time for res1 dropout=0.025 saved to ./loss_time/res1\\res1_dropout_0.025_training_time.json\n",
      "Training model with dropout=0.2\n",
      "step 0: train loss 4.2743, val loss 4.2824\n",
      "step 100: train loss 3.0065, val loss 3.0363\n",
      "step 200: train loss 2.6478, val loss 2.6519\n",
      "step 300: train loss 2.5734, val loss 2.5740\n",
      "step 400: train loss 2.5360, val loss 2.5268\n",
      "step 500: train loss 2.4961, val loss 2.4828\n",
      "step 600: train loss 2.4693, val loss 2.4627\n",
      "step 700: train loss 2.4475, val loss 2.4479\n",
      "step 800: train loss 2.4139, val loss 2.4168\n",
      "step 900: train loss 2.4074, val loss 2.3958\n",
      "step 1000: train loss 2.3745, val loss 2.3769\n",
      "step 1100: train loss 2.3647, val loss 2.3657\n",
      "step 1200: train loss 2.3507, val loss 2.3452\n",
      "step 1300: train loss 2.3360, val loss 2.3327\n",
      "step 1400: train loss 2.3195, val loss 2.3168\n",
      "step 1500: train loss 2.3132, val loss 2.3127\n",
      "step 1600: train loss 2.2856, val loss 2.2985\n",
      "step 1700: train loss 2.2852, val loss 2.2878\n",
      "step 1800: train loss 2.2810, val loss 2.2895\n",
      "step 1900: train loss 2.2690, val loss 2.2738\n",
      "step 2000: train loss 2.2612, val loss 2.2616\n",
      "step 2100: train loss 2.2421, val loss 2.2531\n",
      "step 2200: train loss 2.2318, val loss 2.2454\n",
      "step 2300: train loss 2.2250, val loss 2.2310\n",
      "step 2400: train loss 2.2074, val loss 2.2334\n",
      "step 2500: train loss 2.2127, val loss 2.2068\n",
      "step 2600: train loss 2.2093, val loss 2.2266\n",
      "step 2700: train loss 2.1939, val loss 2.2127\n",
      "step 2800: train loss 2.1819, val loss 2.1938\n",
      "step 2900: train loss 2.1709, val loss 2.1848\n",
      "step 3000: train loss 2.1812, val loss 2.1863\n",
      "step 3100: train loss 2.1702, val loss 2.1870\n",
      "step 3200: train loss 2.1545, val loss 2.1756\n",
      "step 3300: train loss 2.1581, val loss 2.1657\n",
      "step 3400: train loss 2.1482, val loss 2.1620\n",
      "step 3500: train loss 2.1332, val loss 2.1540\n",
      "step 3600: train loss 2.1456, val loss 2.1687\n",
      "step 3700: train loss 2.1237, val loss 2.1483\n",
      "step 3800: train loss 2.1272, val loss 2.1474\n",
      "step 3900: train loss 2.1249, val loss 2.1546\n",
      "step 4000: train loss 2.1106, val loss 2.1362\n",
      "step 4100: train loss 2.1053, val loss 2.1236\n",
      "step 4200: train loss 2.1013, val loss 2.1246\n",
      "step 4300: train loss 2.1016, val loss 2.1435\n",
      "step 4400: train loss 2.0897, val loss 2.1149\n",
      "step 4500: train loss 2.0999, val loss 2.1279\n",
      "step 4600: train loss 2.0845, val loss 2.1276\n",
      "step 4700: train loss 2.0801, val loss 2.1120\n",
      "step 4800: train loss 2.0724, val loss 2.1004\n",
      "step 4900: train loss 2.0718, val loss 2.0967\n",
      "step 5000: train loss 2.0616, val loss 2.1019\n",
      "step 5100: train loss 2.0663, val loss 2.1023\n",
      "step 5200: train loss 2.0549, val loss 2.0858\n",
      "step 5300: train loss 2.0621, val loss 2.1157\n",
      "step 5400: train loss 2.0521, val loss 2.0998\n",
      "step 5500: train loss 2.0496, val loss 2.0855\n",
      "step 5600: train loss 2.0472, val loss 2.0828\n",
      "step 5700: train loss 2.0396, val loss 2.0736\n",
      "step 5800: train loss 2.0355, val loss 2.0784\n",
      "step 5900: train loss 2.0395, val loss 2.0701\n",
      "step 6000: train loss 2.0251, val loss 2.0851\n",
      "step 6100: train loss 2.0444, val loss 2.0795\n",
      "step 6200: train loss 2.0284, val loss 2.0786\n",
      "step 6300: train loss 2.0269, val loss 2.0712\n",
      "step 6400: train loss 2.0175, val loss 2.0738\n",
      "step 6500: train loss 2.0142, val loss 2.0625\n",
      "step 6600: train loss 2.0022, val loss 2.0611\n",
      "step 6700: train loss 2.0092, val loss 2.0603\n",
      "step 6800: train loss 2.0062, val loss 2.0644\n",
      "step 6900: train loss 2.0078, val loss 2.0596\n",
      "step 7000: train loss 2.0063, val loss 2.0570\n",
      "step 7100: train loss 1.9919, val loss 2.0566\n",
      "step 7200: train loss 1.9977, val loss 2.0480\n",
      "step 7300: train loss 1.9911, val loss 2.0426\n",
      "step 7400: train loss 1.9906, val loss 2.0322\n",
      "step 7500: train loss 1.9782, val loss 2.0345\n",
      "step 7600: train loss 1.9815, val loss 2.0421\n",
      "step 7700: train loss 1.9739, val loss 2.0423\n",
      "step 7800: train loss 1.9820, val loss 2.0362\n",
      "step 7900: train loss 1.9826, val loss 2.0332\n",
      "step 8000: train loss 1.9671, val loss 2.0334\n",
      "step 8100: train loss 1.9723, val loss 2.0308\n",
      "step 8200: train loss 1.9570, val loss 2.0286\n",
      "step 8300: train loss 1.9611, val loss 2.0246\n",
      "step 8400: train loss 1.9561, val loss 2.0228\n",
      "step 8500: train loss 1.9576, val loss 2.0209\n",
      "step 8600: train loss 1.9597, val loss 2.0298\n",
      "step 8700: train loss 1.9683, val loss 2.0255\n",
      "step 8800: train loss 1.9542, val loss 2.0239\n",
      "step 8900: train loss 1.9512, val loss 2.0137\n",
      "step 9000: train loss 1.9451, val loss 2.0017\n",
      "step 9100: train loss 1.9501, val loss 2.0216\n",
      "step 9200: train loss 1.9547, val loss 2.0314\n",
      "step 9300: train loss 1.9379, val loss 2.0034\n",
      "step 9400: train loss 1.9337, val loss 2.0053\n",
      "step 9500: train loss 1.9365, val loss 2.0103\n",
      "step 9600: train loss 1.9353, val loss 2.0116\n",
      "step 9700: train loss 1.9288, val loss 2.0056\n",
      "step 9800: train loss 1.9226, val loss 2.0062\n",
      "step 9900: train loss 1.9201, val loss 1.9971\n",
      "step 10000: train loss 1.9146, val loss 1.9994\n",
      "step 10100: train loss 1.9204, val loss 1.9862\n",
      "step 10200: train loss 1.9158, val loss 2.0026\n",
      "step 10300: train loss 1.9143, val loss 2.0039\n",
      "step 10400: train loss 1.9006, val loss 2.0050\n",
      "step 10500: train loss 1.9123, val loss 2.0016\n",
      "step 10600: train loss 1.9050, val loss 1.9905\n",
      "step 10700: train loss 1.9011, val loss 1.9892\n",
      "step 10800: train loss 1.8967, val loss 1.9824\n",
      "step 10900: train loss 1.9047, val loss 1.9945\n",
      "step 11000: train loss 1.9105, val loss 1.9936\n",
      "step 11100: train loss 1.9036, val loss 1.9903\n",
      "step 11200: train loss 1.9020, val loss 1.9787\n",
      "step 11300: train loss 1.8916, val loss 1.9812\n",
      "step 11400: train loss 1.8828, val loss 1.9837\n",
      "step 11500: train loss 1.9013, val loss 1.9842\n",
      "step 11600: train loss 1.8859, val loss 1.9739\n",
      "step 11700: train loss 1.8999, val loss 1.9840\n",
      "step 11800: train loss 1.8874, val loss 1.9811\n",
      "step 11900: train loss 1.8818, val loss 1.9671\n",
      "step 12000: train loss 1.8829, val loss 1.9767\n",
      "step 12100: train loss 1.8846, val loss 1.9697\n",
      "step 12200: train loss 1.8787, val loss 1.9849\n",
      "step 12300: train loss 1.8838, val loss 1.9823\n",
      "step 12400: train loss 1.8823, val loss 1.9639\n",
      "step 12500: train loss 1.8825, val loss 1.9673\n",
      "step 12600: train loss 1.8724, val loss 1.9745\n",
      "step 12700: train loss 1.8812, val loss 1.9706\n",
      "step 12800: train loss 1.8785, val loss 1.9759\n",
      "step 12900: train loss 1.8752, val loss 1.9673\n",
      "step 13000: train loss 1.8685, val loss 1.9596\n",
      "step 13100: train loss 1.8669, val loss 1.9762\n",
      "step 13200: train loss 1.8725, val loss 1.9533\n",
      "step 13300: train loss 1.8636, val loss 1.9701\n",
      "step 13400: train loss 1.8626, val loss 1.9680\n",
      "step 13500: train loss 1.8629, val loss 1.9526\n",
      "step 13600: train loss 1.8625, val loss 1.9590\n",
      "step 13700: train loss 1.8588, val loss 1.9592\n",
      "step 13800: train loss 1.8560, val loss 1.9659\n",
      "step 13900: train loss 1.8490, val loss 1.9676\n",
      "step 14000: train loss 1.8512, val loss 1.9656\n",
      "step 14100: train loss 1.8586, val loss 1.9532\n",
      "step 14200: train loss 1.8399, val loss 1.9649\n",
      "step 14300: train loss 1.8504, val loss 1.9645\n",
      "step 14400: train loss 1.8521, val loss 1.9799\n",
      "step 14500: train loss 1.8445, val loss 1.9603\n",
      "step 14600: train loss 1.8359, val loss 1.9524\n",
      "step 14700: train loss 1.8463, val loss 1.9570\n",
      "step 14800: train loss 1.8440, val loss 1.9449\n",
      "step 14900: train loss 1.8395, val loss 1.9411\n",
      "step 15000: train loss 1.8337, val loss 1.9562\n",
      "step 15100: train loss 1.8359, val loss 1.9485\n",
      "step 15200: train loss 1.8452, val loss 1.9465\n",
      "step 15300: train loss 1.8330, val loss 1.9491\n",
      "step 15400: train loss 1.8372, val loss 1.9514\n",
      "step 15500: train loss 1.8480, val loss 1.9503\n",
      "step 15600: train loss 1.8330, val loss 1.9488\n",
      "step 15700: train loss 1.8308, val loss 1.9385\n",
      "step 15800: train loss 1.8299, val loss 1.9334\n",
      "step 15900: train loss 1.8327, val loss 1.9549\n",
      "step 16000: train loss 1.8349, val loss 1.9414\n",
      "step 16100: train loss 1.8214, val loss 1.9374\n",
      "step 16200: train loss 1.8186, val loss 1.9477\n",
      "step 16300: train loss 1.8318, val loss 1.9486\n",
      "step 16400: train loss 1.8075, val loss 1.9488\n",
      "step 16500: train loss 1.8130, val loss 1.9296\n",
      "step 16600: train loss 1.8161, val loss 1.9296\n",
      "step 16700: train loss 1.8250, val loss 1.9403\n",
      "step 16800: train loss 1.8155, val loss 1.9414\n",
      "step 16900: train loss 1.8236, val loss 1.9300\n",
      "step 17000: train loss 1.8148, val loss 1.9299\n",
      "step 17100: train loss 1.8202, val loss 1.9423\n",
      "step 17200: train loss 1.8244, val loss 1.9400\n",
      "step 17300: train loss 1.8076, val loss 1.9369\n",
      "step 17400: train loss 1.8093, val loss 1.9282\n",
      "step 17500: train loss 1.8167, val loss 1.9375\n",
      "step 17600: train loss 1.8151, val loss 1.9242\n",
      "step 17700: train loss 1.8089, val loss 1.9528\n",
      "step 17800: train loss 1.8101, val loss 1.9350\n",
      "step 17900: train loss 1.7942, val loss 1.9273\n",
      "step 18000: train loss 1.8130, val loss 1.9269\n",
      "step 18100: train loss 1.8023, val loss 1.9224\n",
      "step 18200: train loss 1.7972, val loss 1.9295\n",
      "step 18300: train loss 1.7971, val loss 1.9256\n",
      "step 18400: train loss 1.8071, val loss 1.9365\n",
      "step 18500: train loss 1.7903, val loss 1.9268\n",
      "step 18600: train loss 1.7960, val loss 1.9284\n",
      "step 18700: train loss 1.8044, val loss 1.9391\n",
      "step 18800: train loss 1.7934, val loss 1.9110\n",
      "step 18900: train loss 1.7911, val loss 1.9225\n",
      "step 19000: train loss 1.7967, val loss 1.9091\n",
      "step 19100: train loss 1.7906, val loss 1.9150\n",
      "step 19200: train loss 1.7893, val loss 1.9306\n",
      "step 19300: train loss 1.7900, val loss 1.9290\n",
      "step 19400: train loss 1.7844, val loss 1.9188\n",
      "step 19500: train loss 1.8000, val loss 1.9322\n",
      "step 19600: train loss 1.7897, val loss 1.9185\n",
      "step 19700: train loss 1.7985, val loss 1.9276\n",
      "step 19800: train loss 1.7886, val loss 1.9232\n",
      "step 19900: train loss 1.7957, val loss 1.9270\n",
      "step 20000: train loss 1.7829, val loss 1.9175\n",
      "step 20100: train loss 1.7948, val loss 1.9264\n",
      "step 20200: train loss 1.7874, val loss 1.9158\n",
      "step 20300: train loss 1.7890, val loss 1.9169\n",
      "step 20400: train loss 1.7766, val loss 1.9282\n",
      "step 20500: train loss 1.7807, val loss 1.9121\n",
      "step 20600: train loss 1.7825, val loss 1.9112\n",
      "step 20700: train loss 1.7745, val loss 1.9183\n",
      "step 20800: train loss 1.7880, val loss 1.9123\n",
      "step 20900: train loss 1.7731, val loss 1.9148\n",
      "step 21000: train loss 1.7698, val loss 1.9242\n",
      "step 21100: train loss 1.7790, val loss 1.9224\n",
      "step 21200: train loss 1.7739, val loss 1.9223\n",
      "step 21300: train loss 1.7783, val loss 1.9165\n",
      "step 21400: train loss 1.7858, val loss 1.9162\n",
      "step 21500: train loss 1.7716, val loss 1.9145\n",
      "step 21600: train loss 1.7668, val loss 1.9103\n",
      "step 21700: train loss 1.7802, val loss 1.9122\n",
      "step 21800: train loss 1.7803, val loss 1.8985\n",
      "step 21900: train loss 1.7720, val loss 1.9227\n",
      "step 22000: train loss 1.7758, val loss 1.9167\n",
      "step 22100: train loss 1.7725, val loss 1.9009\n",
      "step 22200: train loss 1.7628, val loss 1.9099\n",
      "step 22300: train loss 1.7606, val loss 1.9011\n",
      "step 22400: train loss 1.7675, val loss 1.8977\n",
      "step 22500: train loss 1.7680, val loss 1.9181\n",
      "step 22600: train loss 1.7584, val loss 1.8963\n",
      "step 22700: train loss 1.7686, val loss 1.9030\n",
      "step 22800: train loss 1.7661, val loss 1.9021\n",
      "step 22900: train loss 1.7639, val loss 1.9042\n",
      "step 23000: train loss 1.7607, val loss 1.9007\n",
      "step 23100: train loss 1.7624, val loss 1.8971\n",
      "step 23200: train loss 1.7628, val loss 1.9007\n",
      "step 23300: train loss 1.7532, val loss 1.9076\n",
      "step 23400: train loss 1.7655, val loss 1.9062\n",
      "step 23500: train loss 1.7587, val loss 1.8947\n",
      "step 23600: train loss 1.7604, val loss 1.8891\n",
      "step 23700: train loss 1.7691, val loss 1.9039\n",
      "step 23800: train loss 1.7677, val loss 1.8935\n",
      "step 23900: train loss 1.7567, val loss 1.8981\n",
      "step 24000: train loss 1.7556, val loss 1.8927\n",
      "step 24100: train loss 1.7534, val loss 1.8901\n",
      "step 24200: train loss 1.7487, val loss 1.8983\n",
      "step 24300: train loss 1.7482, val loss 1.8960\n",
      "step 24400: train loss 1.7550, val loss 1.8951\n",
      "step 24500: train loss 1.7424, val loss 1.8968\n",
      "step 24600: train loss 1.7494, val loss 1.8930\n",
      "step 24700: train loss 1.7587, val loss 1.8788\n",
      "step 24800: train loss 1.7346, val loss 1.8927\n",
      "step 24900: train loss 1.7562, val loss 1.8978\n",
      "step 25000: train loss 1.7520, val loss 1.8893\n",
      "step 25100: train loss 1.7432, val loss 1.9032\n",
      "step 25200: train loss 1.7489, val loss 1.8956\n",
      "step 25300: train loss 1.7554, val loss 1.8926\n",
      "step 25400: train loss 1.7581, val loss 1.9016\n",
      "step 25500: train loss 1.7438, val loss 1.8990\n",
      "step 25600: train loss 1.7362, val loss 1.8903\n",
      "step 25700: train loss 1.7497, val loss 1.8868\n",
      "step 25800: train loss 1.7422, val loss 1.8957\n",
      "step 25900: train loss 1.7489, val loss 1.9067\n",
      "step 26000: train loss 1.7349, val loss 1.8966\n",
      "step 26100: train loss 1.7356, val loss 1.8995\n",
      "step 26200: train loss 1.7360, val loss 1.8826\n",
      "step 26300: train loss 1.7397, val loss 1.8966\n",
      "step 26400: train loss 1.7419, val loss 1.8835\n",
      "step 26500: train loss 1.7487, val loss 1.8996\n",
      "step 26600: train loss 1.7467, val loss 1.8973\n",
      "step 26700: train loss 1.7366, val loss 1.8962\n",
      "step 26800: train loss 1.7394, val loss 1.8803\n",
      "step 26900: train loss 1.7356, val loss 1.8914\n",
      "step 27000: train loss 1.7415, val loss 1.8904\n",
      "step 27100: train loss 1.7379, val loss 1.8911\n",
      "step 27200: train loss 1.7383, val loss 1.8747\n",
      "step 27300: train loss 1.7250, val loss 1.8900\n",
      "step 27400: train loss 1.7407, val loss 1.8793\n",
      "step 27500: train loss 1.7371, val loss 1.9005\n",
      "step 27600: train loss 1.7399, val loss 1.8989\n",
      "step 27700: train loss 1.7278, val loss 1.8826\n",
      "step 27800: train loss 1.7322, val loss 1.8902\n",
      "step 27900: train loss 1.7396, val loss 1.8893\n",
      "step 28000: train loss 1.7237, val loss 1.8713\n",
      "step 28100: train loss 1.7219, val loss 1.8809\n",
      "step 28200: train loss 1.7290, val loss 1.8662\n",
      "step 28300: train loss 1.7242, val loss 1.8899\n",
      "step 28400: train loss 1.7404, val loss 1.8914\n",
      "step 28500: train loss 1.7301, val loss 1.8841\n",
      "step 28600: train loss 1.7250, val loss 1.8688\n",
      "step 28700: train loss 1.7203, val loss 1.8784\n",
      "step 28800: train loss 1.7252, val loss 1.8846\n",
      "step 28900: train loss 1.7268, val loss 1.8771\n",
      "step 29000: train loss 1.7176, val loss 1.8667\n",
      "step 29100: train loss 1.7181, val loss 1.8749\n",
      "step 29200: train loss 1.7288, val loss 1.8759\n",
      "step 29300: train loss 1.7158, val loss 1.8718\n",
      "step 29400: train loss 1.7206, val loss 1.8659\n",
      "step 29500: train loss 1.7151, val loss 1.8863\n",
      "step 29600: train loss 1.7175, val loss 1.8680\n",
      "step 29700: train loss 1.7217, val loss 1.8806\n",
      "step 29800: train loss 1.7210, val loss 1.8757\n",
      "step 29900: train loss 1.7268, val loss 1.8667\n",
      "step 30000: train loss 1.7276, val loss 1.8809\n",
      "step 30100: train loss 1.7312, val loss 1.8843\n",
      "step 30200: train loss 1.7148, val loss 1.8641\n",
      "step 30300: train loss 1.7149, val loss 1.8793\n",
      "step 30400: train loss 1.7192, val loss 1.8700\n",
      "step 30500: train loss 1.7185, val loss 1.8841\n",
      "step 30600: train loss 1.7238, val loss 1.8731\n",
      "step 30700: train loss 1.7182, val loss 1.8745\n",
      "step 30800: train loss 1.7146, val loss 1.8595\n",
      "step 30900: train loss 1.7084, val loss 1.8611\n",
      "step 31000: train loss 1.7174, val loss 1.8752\n",
      "step 31100: train loss 1.7145, val loss 1.8643\n",
      "step 31200: train loss 1.7080, val loss 1.8680\n",
      "step 31300: train loss 1.7182, val loss 1.8689\n",
      "step 31400: train loss 1.7148, val loss 1.8825\n",
      "step 31500: train loss 1.7129, val loss 1.8722\n",
      "step 31600: train loss 1.7226, val loss 1.8759\n",
      "step 31700: train loss 1.7077, val loss 1.8742\n",
      "step 31800: train loss 1.7249, val loss 1.8672\n",
      "step 31900: train loss 1.7035, val loss 1.8707\n",
      "step 32000: train loss 1.7076, val loss 1.8712\n",
      "step 32100: train loss 1.7183, val loss 1.8822\n",
      "step 32200: train loss 1.7141, val loss 1.8631\n",
      "step 32300: train loss 1.7152, val loss 1.8771\n",
      "step 32400: train loss 1.7118, val loss 1.8803\n",
      "step 32500: train loss 1.7047, val loss 1.8807\n",
      "step 32600: train loss 1.7063, val loss 1.8565\n",
      "step 32700: train loss 1.7013, val loss 1.8670\n",
      "step 32800: train loss 1.7052, val loss 1.8634\n",
      "step 32900: train loss 1.7160, val loss 1.8636\n",
      "step 33000: train loss 1.7103, val loss 1.8708\n",
      "step 33100: train loss 1.7105, val loss 1.8534\n",
      "step 33200: train loss 1.7009, val loss 1.8746\n",
      "step 33300: train loss 1.7101, val loss 1.8722\n",
      "step 33400: train loss 1.7147, val loss 1.8762\n",
      "step 33500: train loss 1.7069, val loss 1.8607\n",
      "step 33600: train loss 1.7094, val loss 1.8627\n",
      "step 33700: train loss 1.7072, val loss 1.8594\n",
      "step 33800: train loss 1.7068, val loss 1.8715\n",
      "step 33900: train loss 1.7139, val loss 1.8792\n",
      "step 34000: train loss 1.7024, val loss 1.8634\n",
      "step 34100: train loss 1.7134, val loss 1.8587\n",
      "step 34200: train loss 1.7109, val loss 1.8641\n",
      "step 34300: train loss 1.7028, val loss 1.8638\n",
      "step 34400: train loss 1.7021, val loss 1.8512\n",
      "step 34500: train loss 1.6989, val loss 1.8626\n",
      "step 34600: train loss 1.7094, val loss 1.8669\n",
      "step 34700: train loss 1.7104, val loss 1.8551\n",
      "step 34800: train loss 1.7044, val loss 1.8525\n",
      "step 34900: train loss 1.7043, val loss 1.8551\n",
      "step 35000: train loss 1.7091, val loss 1.8639\n",
      "step 35100: train loss 1.7035, val loss 1.8663\n",
      "step 35200: train loss 1.7019, val loss 1.8544\n",
      "step 35300: train loss 1.7111, val loss 1.8409\n",
      "step 35400: train loss 1.7021, val loss 1.8632\n",
      "step 35500: train loss 1.7085, val loss 1.8607\n",
      "step 35600: train loss 1.7009, val loss 1.8537\n",
      "step 35700: train loss 1.7017, val loss 1.8710\n",
      "step 35800: train loss 1.6967, val loss 1.8554\n",
      "step 35900: train loss 1.6899, val loss 1.8447\n",
      "step 36000: train loss 1.6942, val loss 1.8435\n",
      "step 36100: train loss 1.6989, val loss 1.8622\n",
      "step 36200: train loss 1.6922, val loss 1.8608\n",
      "step 36300: train loss 1.6909, val loss 1.8570\n",
      "step 36400: train loss 1.6945, val loss 1.8533\n",
      "step 36500: train loss 1.6855, val loss 1.8538\n",
      "step 36600: train loss 1.6986, val loss 1.8480\n",
      "step 36700: train loss 1.6936, val loss 1.8455\n",
      "step 36800: train loss 1.6859, val loss 1.8549\n",
      "step 36900: train loss 1.6987, val loss 1.8464\n",
      "step 37000: train loss 1.6957, val loss 1.8515\n",
      "step 37100: train loss 1.6902, val loss 1.8518\n",
      "step 37200: train loss 1.6900, val loss 1.8503\n",
      "step 37300: train loss 1.6767, val loss 1.8498\n",
      "step 37400: train loss 1.6922, val loss 1.8494\n",
      "step 37500: train loss 1.6907, val loss 1.8537\n",
      "step 37600: train loss 1.6880, val loss 1.8403\n",
      "step 37700: train loss 1.6908, val loss 1.8647\n",
      "step 37800: train loss 1.6875, val loss 1.8465\n",
      "step 37900: train loss 1.6868, val loss 1.8608\n",
      "step 38000: train loss 1.6897, val loss 1.8377\n",
      "step 38100: train loss 1.6900, val loss 1.8599\n",
      "step 38200: train loss 1.6928, val loss 1.8496\n",
      "step 38300: train loss 1.7023, val loss 1.8537\n",
      "step 38400: train loss 1.6890, val loss 1.8561\n",
      "step 38500: train loss 1.6876, val loss 1.8533\n",
      "step 38600: train loss 1.6920, val loss 1.8658\n",
      "step 38700: train loss 1.6863, val loss 1.8527\n",
      "step 38800: train loss 1.6849, val loss 1.8516\n",
      "step 38900: train loss 1.6787, val loss 1.8520\n",
      "step 39000: train loss 1.6886, val loss 1.8648\n",
      "step 39100: train loss 1.6850, val loss 1.8588\n",
      "step 39200: train loss 1.6936, val loss 1.8505\n",
      "step 39300: train loss 1.6753, val loss 1.8562\n",
      "step 39400: train loss 1.6852, val loss 1.8484\n",
      "step 39500: train loss 1.6830, val loss 1.8414\n",
      "step 39600: train loss 1.6771, val loss 1.8316\n",
      "step 39700: train loss 1.6802, val loss 1.8334\n",
      "step 39800: train loss 1.6772, val loss 1.8378\n",
      "step 39900: train loss 1.6843, val loss 1.8329\n",
      "step 40000: train loss 1.6672, val loss 1.8261\n",
      "step 40100: train loss 1.6792, val loss 1.8279\n",
      "step 40200: train loss 1.6757, val loss 1.8342\n",
      "step 40300: train loss 1.6869, val loss 1.8642\n",
      "step 40400: train loss 1.6879, val loss 1.8553\n",
      "step 40500: train loss 1.6841, val loss 1.8438\n",
      "step 40600: train loss 1.6691, val loss 1.8555\n",
      "step 40700: train loss 1.6745, val loss 1.8571\n",
      "step 40800: train loss 1.6792, val loss 1.8677\n",
      "step 40900: train loss 1.6725, val loss 1.8479\n",
      "step 41000: train loss 1.6886, val loss 1.8391\n",
      "step 41100: train loss 1.6794, val loss 1.8452\n",
      "step 41200: train loss 1.6792, val loss 1.8345\n",
      "step 41300: train loss 1.6801, val loss 1.8504\n",
      "step 41400: train loss 1.6708, val loss 1.8597\n",
      "step 41500: train loss 1.6712, val loss 1.8350\n",
      "step 41600: train loss 1.6810, val loss 1.8482\n",
      "step 41700: train loss 1.6732, val loss 1.8339\n",
      "step 41800: train loss 1.6789, val loss 1.8469\n",
      "step 41900: train loss 1.6791, val loss 1.8416\n",
      "step 42000: train loss 1.6671, val loss 1.8418\n",
      "step 42100: train loss 1.6742, val loss 1.8443\n",
      "step 42200: train loss 1.6691, val loss 1.8537\n",
      "step 42300: train loss 1.6755, val loss 1.8348\n",
      "step 42400: train loss 1.6696, val loss 1.8395\n",
      "step 42500: train loss 1.6622, val loss 1.8343\n",
      "step 42600: train loss 1.6717, val loss 1.8423\n",
      "step 42700: train loss 1.6831, val loss 1.8445\n",
      "step 42800: train loss 1.6696, val loss 1.8353\n",
      "step 42900: train loss 1.6676, val loss 1.8372\n",
      "step 43000: train loss 1.6743, val loss 1.8487\n",
      "step 43100: train loss 1.6720, val loss 1.8340\n",
      "step 43200: train loss 1.6780, val loss 1.8199\n",
      "step 43300: train loss 1.6835, val loss 1.8343\n",
      "step 43400: train loss 1.6648, val loss 1.8362\n",
      "step 43500: train loss 1.6715, val loss 1.8507\n",
      "step 43600: train loss 1.6781, val loss 1.8278\n",
      "step 43700: train loss 1.6733, val loss 1.8482\n",
      "step 43800: train loss 1.6720, val loss 1.8293\n",
      "step 43900: train loss 1.6675, val loss 1.8281\n",
      "step 44000: train loss 1.6760, val loss 1.8355\n",
      "step 44100: train loss 1.6742, val loss 1.8380\n",
      "step 44200: train loss 1.6698, val loss 1.8436\n",
      "step 44300: train loss 1.6789, val loss 1.8524\n",
      "step 44400: train loss 1.6739, val loss 1.8311\n",
      "step 44500: train loss 1.6695, val loss 1.8291\n",
      "step 44600: train loss 1.6533, val loss 1.8333\n",
      "step 44700: train loss 1.6699, val loss 1.8565\n",
      "step 44800: train loss 1.6660, val loss 1.8309\n",
      "step 44900: train loss 1.6749, val loss 1.8381\n",
      "step 45000: train loss 1.6691, val loss 1.8406\n",
      "step 45100: train loss 1.6584, val loss 1.8436\n",
      "step 45200: train loss 1.6558, val loss 1.8392\n",
      "step 45300: train loss 1.6678, val loss 1.8478\n",
      "step 45400: train loss 1.6754, val loss 1.8340\n",
      "step 45500: train loss 1.6681, val loss 1.8470\n",
      "step 45600: train loss 1.6649, val loss 1.8467\n",
      "step 45700: train loss 1.6708, val loss 1.8418\n",
      "step 45800: train loss 1.6679, val loss 1.8493\n",
      "step 45900: train loss 1.6715, val loss 1.8263\n",
      "step 46000: train loss 1.6650, val loss 1.8404\n",
      "step 46100: train loss 1.6533, val loss 1.8228\n",
      "step 46200: train loss 1.6741, val loss 1.8565\n",
      "step 46300: train loss 1.6671, val loss 1.8228\n",
      "step 46400: train loss 1.6559, val loss 1.8231\n",
      "step 46500: train loss 1.6589, val loss 1.8236\n",
      "step 46600: train loss 1.6571, val loss 1.8272\n",
      "step 46700: train loss 1.6572, val loss 1.8301\n",
      "step 46800: train loss 1.6541, val loss 1.8298\n",
      "step 46900: train loss 1.6683, val loss 1.8433\n",
      "step 47000: train loss 1.6633, val loss 1.8368\n",
      "step 47100: train loss 1.6630, val loss 1.8277\n",
      "step 47200: train loss 1.6665, val loss 1.8247\n",
      "step 47300: train loss 1.6616, val loss 1.8265\n",
      "step 47400: train loss 1.6679, val loss 1.8156\n",
      "step 47500: train loss 1.6596, val loss 1.8208\n",
      "step 47600: train loss 1.6570, val loss 1.8351\n",
      "step 47700: train loss 1.6611, val loss 1.8280\n",
      "step 47800: train loss 1.6574, val loss 1.8237\n",
      "step 47900: train loss 1.6576, val loss 1.8254\n",
      "step 48000: train loss 1.6622, val loss 1.8374\n",
      "step 48100: train loss 1.6672, val loss 1.8329\n",
      "step 48200: train loss 1.6610, val loss 1.8527\n",
      "step 48300: train loss 1.6566, val loss 1.8329\n",
      "step 48400: train loss 1.6669, val loss 1.8263\n",
      "step 48500: train loss 1.6688, val loss 1.8256\n",
      "step 48600: train loss 1.6589, val loss 1.8319\n",
      "step 48700: train loss 1.6614, val loss 1.8118\n",
      "step 48800: train loss 1.6488, val loss 1.8272\n",
      "step 48900: train loss 1.6523, val loss 1.8236\n",
      "step 49000: train loss 1.6579, val loss 1.8176\n",
      "step 49100: train loss 1.6612, val loss 1.8227\n",
      "step 49200: train loss 1.6660, val loss 1.8181\n",
      "step 49300: train loss 1.6564, val loss 1.8267\n",
      "step 49400: train loss 1.6522, val loss 1.8164\n",
      "step 49500: train loss 1.6612, val loss 1.8354\n",
      "step 49600: train loss 1.6598, val loss 1.8364\n",
      "step 49700: train loss 1.6574, val loss 1.8322\n",
      "step 49800: train loss 1.6607, val loss 1.8284\n",
      "step 49900: train loss 1.6595, val loss 1.8190\n",
      "step 49999: train loss 1.6597, val loss 1.8267\n",
      "Loss results for res1 dropout=0.2 saved to ./loss_time/res1\\res1_dropout_0.2_losses.json\n",
      "Training time for res1 dropout=0.2 saved to ./loss_time/res1\\res1_dropout_0.2_training_time.json\n",
      "Training model with dropout=0.5\n",
      "step 0: train loss 4.2928, val loss 4.2988\n",
      "step 100: train loss 3.3296, val loss 3.3592\n",
      "step 200: train loss 3.3176, val loss 3.3489\n",
      "step 300: train loss 3.3071, val loss 3.3520\n",
      "step 400: train loss 3.3118, val loss 3.3508\n",
      "step 500: train loss 3.3083, val loss 3.3470\n",
      "step 600: train loss 3.2246, val loss 3.2706\n",
      "step 700: train loss 3.1444, val loss 3.1606\n",
      "step 800: train loss 2.8861, val loss 2.9120\n",
      "step 900: train loss 2.8078, val loss 2.8128\n",
      "step 1000: train loss 2.7487, val loss 2.7470\n",
      "step 1100: train loss 2.7148, val loss 2.7122\n",
      "step 1200: train loss 2.6668, val loss 2.6741\n",
      "step 1300: train loss 2.6385, val loss 2.6299\n",
      "step 1400: train loss 2.6265, val loss 2.6187\n",
      "step 1500: train loss 2.6140, val loss 2.6032\n",
      "step 1600: train loss 2.6010, val loss 2.5865\n",
      "step 1700: train loss 2.5929, val loss 2.5917\n",
      "step 1800: train loss 2.5772, val loss 2.5676\n",
      "step 1900: train loss 2.5588, val loss 2.5564\n",
      "step 2000: train loss 2.5447, val loss 2.5429\n",
      "step 2100: train loss 2.5214, val loss 2.5299\n",
      "step 2200: train loss 2.5189, val loss 2.5078\n",
      "step 2300: train loss 2.4936, val loss 2.4870\n",
      "step 2400: train loss 2.4991, val loss 2.4898\n",
      "step 2500: train loss 2.4789, val loss 2.4805\n",
      "step 2600: train loss 2.4613, val loss 2.4667\n",
      "step 2700: train loss 2.4648, val loss 2.4633\n",
      "step 2800: train loss 2.4500, val loss 2.4442\n",
      "step 2900: train loss 2.4471, val loss 2.4429\n",
      "step 3000: train loss 2.4568, val loss 2.4497\n",
      "step 3100: train loss 2.4373, val loss 2.4253\n",
      "step 3200: train loss 2.4376, val loss 2.4259\n",
      "step 3300: train loss 2.4278, val loss 2.4277\n",
      "step 3400: train loss 2.4294, val loss 2.4190\n",
      "step 3500: train loss 2.4135, val loss 2.4091\n",
      "step 3600: train loss 2.4144, val loss 2.4052\n",
      "step 3700: train loss 2.4185, val loss 2.4045\n",
      "step 3800: train loss 2.4015, val loss 2.3892\n",
      "step 3900: train loss 2.3994, val loss 2.3993\n",
      "step 4000: train loss 2.3966, val loss 2.3898\n",
      "step 4100: train loss 2.3860, val loss 2.3842\n",
      "step 4200: train loss 2.3747, val loss 2.3679\n",
      "step 4300: train loss 2.3871, val loss 2.3803\n",
      "step 4400: train loss 2.3740, val loss 2.3756\n",
      "step 4500: train loss 2.3857, val loss 2.3754\n",
      "step 4600: train loss 2.3896, val loss 2.3736\n",
      "step 4700: train loss 2.3693, val loss 2.3625\n",
      "step 4800: train loss 2.3624, val loss 2.3653\n",
      "step 4900: train loss 2.3690, val loss 2.3699\n",
      "step 5000: train loss 2.3509, val loss 2.3430\n",
      "step 5100: train loss 2.3543, val loss 2.3549\n",
      "step 5200: train loss 2.3480, val loss 2.3476\n",
      "step 5300: train loss 2.3378, val loss 2.3394\n",
      "step 5400: train loss 2.3511, val loss 2.3533\n",
      "step 5500: train loss 2.3448, val loss 2.3457\n",
      "step 5600: train loss 2.3357, val loss 2.3318\n",
      "step 5700: train loss 2.3350, val loss 2.3319\n",
      "step 5800: train loss 2.3306, val loss 2.3299\n",
      "step 5900: train loss 2.3387, val loss 2.3362\n",
      "step 6000: train loss 2.3248, val loss 2.3202\n",
      "step 6100: train loss 2.3236, val loss 2.3283\n",
      "step 6200: train loss 2.3106, val loss 2.3125\n",
      "step 6300: train loss 2.3173, val loss 2.3085\n",
      "step 6400: train loss 2.3136, val loss 2.3081\n",
      "step 6500: train loss 2.3111, val loss 2.3147\n",
      "step 6600: train loss 2.3048, val loss 2.3182\n",
      "step 6700: train loss 2.2982, val loss 2.3001\n",
      "step 6800: train loss 2.3028, val loss 2.2972\n",
      "step 6900: train loss 2.2901, val loss 2.2880\n",
      "step 7000: train loss 2.2910, val loss 2.2989\n",
      "step 7100: train loss 2.2897, val loss 2.3014\n",
      "step 7200: train loss 2.2893, val loss 2.2796\n",
      "step 7300: train loss 2.2857, val loss 2.2856\n",
      "step 7400: train loss 2.2901, val loss 2.2831\n",
      "step 7500: train loss 2.2749, val loss 2.2756\n",
      "step 7600: train loss 2.2791, val loss 2.2856\n",
      "step 7700: train loss 2.2704, val loss 2.2604\n",
      "step 7800: train loss 2.2626, val loss 2.2714\n",
      "step 7900: train loss 2.2537, val loss 2.2586\n",
      "step 8000: train loss 2.2525, val loss 2.2714\n",
      "step 8100: train loss 2.2504, val loss 2.2547\n",
      "step 8200: train loss 2.2604, val loss 2.2597\n",
      "step 8300: train loss 2.2498, val loss 2.2531\n",
      "step 8400: train loss 2.2391, val loss 2.2514\n",
      "step 8500: train loss 2.2513, val loss 2.2548\n",
      "step 8600: train loss 2.2424, val loss 2.2596\n",
      "step 8700: train loss 2.2354, val loss 2.2454\n",
      "step 8800: train loss 2.2353, val loss 2.2342\n",
      "step 8900: train loss 2.2229, val loss 2.2350\n",
      "step 9000: train loss 2.2292, val loss 2.2402\n",
      "step 9100: train loss 2.2249, val loss 2.2323\n",
      "step 9200: train loss 2.2231, val loss 2.2175\n",
      "step 9300: train loss 2.2229, val loss 2.2413\n",
      "step 9400: train loss 2.2201, val loss 2.2319\n",
      "step 9500: train loss 2.2200, val loss 2.2259\n",
      "step 9600: train loss 2.2094, val loss 2.2240\n",
      "step 9700: train loss 2.2035, val loss 2.2239\n",
      "step 9800: train loss 2.2145, val loss 2.2180\n",
      "step 9900: train loss 2.2083, val loss 2.2144\n",
      "step 10000: train loss 2.2005, val loss 2.2076\n",
      "step 10100: train loss 2.2047, val loss 2.2237\n",
      "step 10200: train loss 2.2022, val loss 2.2153\n",
      "step 10300: train loss 2.2031, val loss 2.2085\n",
      "step 10400: train loss 2.1956, val loss 2.1999\n",
      "step 10500: train loss 2.1913, val loss 2.2088\n",
      "step 10600: train loss 2.1948, val loss 2.2164\n",
      "step 10700: train loss 2.1828, val loss 2.2028\n",
      "step 10800: train loss 2.1820, val loss 2.2081\n",
      "step 10900: train loss 2.1768, val loss 2.1984\n",
      "step 11000: train loss 2.1806, val loss 2.1971\n",
      "step 11100: train loss 2.1760, val loss 2.1955\n",
      "step 11200: train loss 2.1724, val loss 2.1952\n",
      "step 11300: train loss 2.1816, val loss 2.1924\n",
      "step 11400: train loss 2.1755, val loss 2.1985\n",
      "step 11500: train loss 2.1729, val loss 2.1942\n",
      "step 11600: train loss 2.1834, val loss 2.1876\n",
      "step 11700: train loss 2.1632, val loss 2.1876\n",
      "step 11800: train loss 2.1722, val loss 2.1914\n",
      "step 11900: train loss 2.1703, val loss 2.1861\n",
      "step 12000: train loss 2.1571, val loss 2.1796\n",
      "step 12100: train loss 2.1655, val loss 2.1695\n",
      "step 12200: train loss 2.1606, val loss 2.1736\n",
      "step 12300: train loss 2.1642, val loss 2.1854\n",
      "step 12400: train loss 2.1597, val loss 2.1841\n",
      "step 12500: train loss 2.1451, val loss 2.1725\n",
      "step 12600: train loss 2.1634, val loss 2.1817\n",
      "step 12700: train loss 2.1521, val loss 2.1709\n",
      "step 12800: train loss 2.1529, val loss 2.1644\n",
      "step 12900: train loss 2.1424, val loss 2.1731\n",
      "step 13000: train loss 2.1449, val loss 2.1698\n",
      "step 13100: train loss 2.1444, val loss 2.1625\n",
      "step 13200: train loss 2.1464, val loss 2.1614\n",
      "step 13300: train loss 2.1491, val loss 2.1626\n",
      "step 13400: train loss 2.1403, val loss 2.1627\n",
      "step 13500: train loss 2.1300, val loss 2.1618\n",
      "step 13600: train loss 2.1273, val loss 2.1558\n",
      "step 13700: train loss 2.1305, val loss 2.1453\n",
      "step 13800: train loss 2.1379, val loss 2.1552\n",
      "step 13900: train loss 2.1365, val loss 2.1687\n",
      "step 14000: train loss 2.1282, val loss 2.1538\n",
      "step 14100: train loss 2.1294, val loss 2.1502\n",
      "step 14200: train loss 2.1261, val loss 2.1516\n",
      "step 14300: train loss 2.1216, val loss 2.1526\n",
      "step 14400: train loss 2.1235, val loss 2.1549\n",
      "step 14500: train loss 2.1200, val loss 2.1488\n",
      "step 14600: train loss 2.1247, val loss 2.1521\n",
      "step 14700: train loss 2.1226, val loss 2.1416\n",
      "step 14800: train loss 2.1110, val loss 2.1313\n",
      "step 14900: train loss 2.1186, val loss 2.1441\n",
      "step 15000: train loss 2.1115, val loss 2.1385\n",
      "step 15100: train loss 2.1183, val loss 2.1400\n",
      "step 15200: train loss 2.1089, val loss 2.1478\n",
      "step 15300: train loss 2.1161, val loss 2.1384\n",
      "step 15400: train loss 2.1168, val loss 2.1433\n",
      "step 15500: train loss 2.1094, val loss 2.1413\n",
      "step 15600: train loss 2.1046, val loss 2.1348\n",
      "step 15700: train loss 2.1119, val loss 2.1401\n",
      "step 15800: train loss 2.0973, val loss 2.1314\n",
      "step 15900: train loss 2.1019, val loss 2.1296\n",
      "step 16000: train loss 2.1061, val loss 2.1379\n",
      "step 16100: train loss 2.1068, val loss 2.1137\n",
      "step 16200: train loss 2.0992, val loss 2.1306\n",
      "step 16300: train loss 2.1078, val loss 2.1406\n",
      "step 16400: train loss 2.1029, val loss 2.1224\n",
      "step 16500: train loss 2.0916, val loss 2.1256\n",
      "step 16600: train loss 2.0909, val loss 2.1331\n",
      "step 16700: train loss 2.0897, val loss 2.1205\n",
      "step 16800: train loss 2.0947, val loss 2.1210\n",
      "step 16900: train loss 2.0904, val loss 2.1352\n",
      "step 17000: train loss 2.0815, val loss 2.1112\n",
      "step 17100: train loss 2.0972, val loss 2.1265\n",
      "step 17200: train loss 2.0826, val loss 2.1228\n",
      "step 17300: train loss 2.0857, val loss 2.1220\n",
      "step 17400: train loss 2.0788, val loss 2.1239\n",
      "step 17500: train loss 2.0745, val loss 2.1189\n",
      "step 17600: train loss 2.0827, val loss 2.1201\n",
      "step 17700: train loss 2.0885, val loss 2.1219\n",
      "step 17800: train loss 2.0718, val loss 2.1108\n",
      "step 17900: train loss 2.0788, val loss 2.1126\n",
      "step 18000: train loss 2.0708, val loss 2.1086\n",
      "step 18100: train loss 2.0774, val loss 2.1084\n",
      "step 18200: train loss 2.0752, val loss 2.1104\n",
      "step 18300: train loss 2.0835, val loss 2.1297\n",
      "step 18400: train loss 2.0731, val loss 2.1105\n",
      "step 18500: train loss 2.0712, val loss 2.1035\n",
      "step 18600: train loss 2.0613, val loss 2.1075\n",
      "step 18700: train loss 2.0678, val loss 2.1073\n",
      "step 18800: train loss 2.0717, val loss 2.1046\n",
      "step 18900: train loss 2.0604, val loss 2.1062\n",
      "step 19000: train loss 2.0704, val loss 2.1213\n",
      "step 19100: train loss 2.0783, val loss 2.1063\n",
      "step 19200: train loss 2.0679, val loss 2.0988\n",
      "step 19300: train loss 2.0650, val loss 2.1033\n",
      "step 19400: train loss 2.0562, val loss 2.1011\n",
      "step 19500: train loss 2.0648, val loss 2.0950\n",
      "step 19600: train loss 2.0673, val loss 2.0997\n",
      "step 19700: train loss 2.0577, val loss 2.0973\n",
      "step 19800: train loss 2.0565, val loss 2.1026\n",
      "step 19900: train loss 2.0617, val loss 2.0959\n",
      "step 20000: train loss 2.0538, val loss 2.0965\n",
      "step 20100: train loss 2.0541, val loss 2.0920\n",
      "step 20200: train loss 2.0613, val loss 2.0923\n",
      "step 20300: train loss 2.0552, val loss 2.1023\n",
      "step 20400: train loss 2.0568, val loss 2.0958\n",
      "step 20500: train loss 2.0498, val loss 2.0900\n",
      "step 20600: train loss 2.0523, val loss 2.1022\n",
      "step 20700: train loss 2.0505, val loss 2.1004\n",
      "step 20800: train loss 2.0567, val loss 2.1000\n",
      "step 20900: train loss 2.0528, val loss 2.1001\n",
      "step 21000: train loss 2.0636, val loss 2.1143\n",
      "step 21100: train loss 2.0466, val loss 2.1069\n",
      "step 21200: train loss 2.0485, val loss 2.0990\n",
      "step 21300: train loss 2.0461, val loss 2.1042\n",
      "step 21400: train loss 2.0368, val loss 2.0974\n",
      "step 21500: train loss 2.0387, val loss 2.0966\n",
      "step 21600: train loss 2.0418, val loss 2.0956\n",
      "step 21700: train loss 2.0366, val loss 2.0874\n",
      "step 21800: train loss 2.0428, val loss 2.0955\n",
      "step 21900: train loss 2.0413, val loss 2.0968\n",
      "step 22000: train loss 2.0401, val loss 2.0850\n",
      "step 22100: train loss 2.0347, val loss 2.0882\n",
      "step 22200: train loss 2.0420, val loss 2.0939\n",
      "step 22300: train loss 2.0337, val loss 2.0941\n",
      "step 22400: train loss 2.0407, val loss 2.0878\n",
      "step 22500: train loss 2.0437, val loss 2.0971\n",
      "step 22600: train loss 2.0390, val loss 2.0869\n",
      "step 22700: train loss 2.0269, val loss 2.0881\n",
      "step 22800: train loss 2.0370, val loss 2.0905\n",
      "step 22900: train loss 2.0313, val loss 2.0885\n",
      "step 23000: train loss 2.0200, val loss 2.0874\n",
      "step 23100: train loss 2.0224, val loss 2.0774\n",
      "step 23200: train loss 2.0215, val loss 2.0768\n",
      "step 23300: train loss 2.0306, val loss 2.0727\n",
      "step 23400: train loss 2.0253, val loss 2.0809\n",
      "step 23500: train loss 2.0271, val loss 2.0789\n",
      "step 23600: train loss 2.0138, val loss 2.0834\n",
      "step 23700: train loss 2.0274, val loss 2.0768\n",
      "step 23800: train loss 2.0325, val loss 2.0908\n",
      "step 23900: train loss 2.0211, val loss 2.0748\n",
      "step 24000: train loss 2.0231, val loss 2.0782\n",
      "step 24100: train loss 2.0229, val loss 2.0917\n",
      "step 24200: train loss 2.0318, val loss 2.0933\n",
      "step 24300: train loss 2.0161, val loss 2.0861\n",
      "step 24400: train loss 2.0248, val loss 2.0752\n",
      "step 24500: train loss 2.0106, val loss 2.0729\n",
      "step 24600: train loss 2.0195, val loss 2.0904\n",
      "step 24700: train loss 2.0143, val loss 2.0773\n",
      "step 24800: train loss 2.0130, val loss 2.0722\n",
      "step 24900: train loss 2.0081, val loss 2.0759\n",
      "step 25000: train loss 2.0125, val loss 2.0705\n",
      "step 25100: train loss 2.0089, val loss 2.0878\n",
      "step 25200: train loss 2.0174, val loss 2.0811\n",
      "step 25300: train loss 2.0076, val loss 2.0814\n",
      "step 25400: train loss 2.0098, val loss 2.0746\n",
      "step 25500: train loss 2.0105, val loss 2.0832\n",
      "step 25600: train loss 2.0059, val loss 2.0687\n",
      "step 25700: train loss 2.0020, val loss 2.0658\n",
      "step 25800: train loss 2.0198, val loss 2.0577\n",
      "step 25900: train loss 2.0037, val loss 2.0732\n",
      "step 26000: train loss 2.0158, val loss 2.0677\n",
      "step 26100: train loss 2.0046, val loss 2.0732\n",
      "step 26200: train loss 1.9963, val loss 2.0567\n",
      "step 26300: train loss 2.0103, val loss 2.0810\n",
      "step 26400: train loss 2.0079, val loss 2.0844\n",
      "step 26500: train loss 2.0042, val loss 2.0647\n",
      "step 26600: train loss 2.0044, val loss 2.0690\n",
      "step 26700: train loss 2.0030, val loss 2.0678\n",
      "step 26800: train loss 2.0099, val loss 2.0662\n",
      "step 26900: train loss 1.9981, val loss 2.0786\n",
      "step 27000: train loss 2.0072, val loss 2.0570\n",
      "step 27100: train loss 2.0073, val loss 2.0621\n",
      "step 27200: train loss 2.0001, val loss 2.0723\n",
      "step 27300: train loss 1.9861, val loss 2.0573\n",
      "step 27400: train loss 1.9983, val loss 2.0780\n",
      "step 27500: train loss 1.9943, val loss 2.0556\n",
      "step 27600: train loss 1.9852, val loss 2.0605\n",
      "step 27700: train loss 1.9934, val loss 2.0565\n",
      "step 27800: train loss 1.9916, val loss 2.0712\n",
      "step 27900: train loss 2.0006, val loss 2.0641\n",
      "step 28000: train loss 2.0036, val loss 2.0601\n",
      "step 28100: train loss 1.9919, val loss 2.0696\n",
      "step 28200: train loss 1.9861, val loss 2.0685\n",
      "step 28300: train loss 1.9906, val loss 2.0575\n",
      "step 28400: train loss 1.9910, val loss 2.0582\n",
      "step 28500: train loss 1.9896, val loss 2.0623\n",
      "step 28600: train loss 1.9984, val loss 2.0621\n",
      "step 28700: train loss 1.9851, val loss 2.0503\n",
      "step 28800: train loss 1.9859, val loss 2.0616\n",
      "step 28900: train loss 1.9861, val loss 2.0578\n",
      "step 29000: train loss 1.9801, val loss 2.0722\n",
      "step 29100: train loss 1.9885, val loss 2.0602\n",
      "step 29200: train loss 1.9869, val loss 2.0640\n",
      "step 29300: train loss 1.9829, val loss 2.0520\n",
      "step 29400: train loss 1.9731, val loss 2.0543\n",
      "step 29500: train loss 1.9850, val loss 2.0528\n",
      "step 29600: train loss 1.9702, val loss 2.0532\n",
      "step 29700: train loss 1.9838, val loss 2.0550\n",
      "step 29800: train loss 1.9876, val loss 2.0520\n",
      "step 29900: train loss 1.9786, val loss 2.0564\n",
      "step 30000: train loss 1.9722, val loss 2.0483\n",
      "step 30100: train loss 1.9753, val loss 2.0554\n",
      "step 30200: train loss 1.9735, val loss 2.0655\n",
      "step 30300: train loss 1.9866, val loss 2.0422\n",
      "step 30400: train loss 1.9651, val loss 2.0440\n",
      "step 30500: train loss 1.9752, val loss 2.0443\n",
      "step 30600: train loss 1.9803, val loss 2.0440\n",
      "step 30700: train loss 1.9753, val loss 2.0617\n",
      "step 30800: train loss 1.9793, val loss 2.0582\n",
      "step 30900: train loss 1.9728, val loss 2.0528\n",
      "step 31000: train loss 1.9705, val loss 2.0391\n",
      "step 31100: train loss 1.9618, val loss 2.0415\n",
      "step 31200: train loss 1.9776, val loss 2.0670\n",
      "step 31300: train loss 1.9702, val loss 2.0459\n",
      "step 31400: train loss 1.9592, val loss 2.0443\n",
      "step 31500: train loss 1.9725, val loss 2.0505\n",
      "step 31600: train loss 1.9537, val loss 2.0490\n",
      "step 31700: train loss 1.9592, val loss 2.0462\n",
      "step 31800: train loss 1.9630, val loss 2.0395\n",
      "step 31900: train loss 1.9637, val loss 2.0517\n",
      "step 32000: train loss 1.9629, val loss 2.0480\n",
      "step 32100: train loss 1.9645, val loss 2.0414\n",
      "step 32200: train loss 1.9656, val loss 2.0386\n",
      "step 32300: train loss 1.9539, val loss 2.0435\n",
      "step 32400: train loss 1.9584, val loss 2.0358\n",
      "step 32500: train loss 1.9524, val loss 2.0329\n",
      "step 32600: train loss 1.9600, val loss 2.0479\n",
      "step 32700: train loss 1.9506, val loss 2.0406\n",
      "step 32800: train loss 1.9549, val loss 2.0291\n",
      "step 32900: train loss 1.9574, val loss 2.0396\n",
      "step 33000: train loss 1.9637, val loss 2.0517\n",
      "step 33100: train loss 1.9561, val loss 2.0283\n",
      "step 33200: train loss 1.9501, val loss 2.0339\n",
      "step 33300: train loss 1.9537, val loss 2.0473\n",
      "step 33400: train loss 1.9546, val loss 2.0418\n",
      "step 33500: train loss 1.9535, val loss 2.0418\n",
      "step 33600: train loss 1.9511, val loss 2.0325\n",
      "step 33700: train loss 1.9387, val loss 2.0355\n",
      "step 33800: train loss 1.9617, val loss 2.0357\n",
      "step 33900: train loss 1.9413, val loss 2.0301\n",
      "step 34000: train loss 1.9382, val loss 2.0385\n",
      "step 34100: train loss 1.9424, val loss 2.0357\n",
      "step 34200: train loss 1.9490, val loss 2.0272\n",
      "step 34300: train loss 1.9422, val loss 2.0410\n",
      "step 34400: train loss 1.9524, val loss 2.0363\n",
      "step 34500: train loss 1.9489, val loss 2.0291\n",
      "step 34600: train loss 1.9463, val loss 2.0275\n",
      "step 34700: train loss 1.9475, val loss 2.0189\n",
      "step 34800: train loss 1.9451, val loss 2.0228\n",
      "step 34900: train loss 1.9421, val loss 2.0346\n",
      "step 35000: train loss 1.9387, val loss 2.0287\n",
      "step 35100: train loss 1.9531, val loss 2.0267\n",
      "step 35200: train loss 1.9414, val loss 2.0425\n",
      "step 35300: train loss 1.9424, val loss 2.0363\n",
      "step 35400: train loss 1.9382, val loss 2.0232\n",
      "step 35500: train loss 1.9328, val loss 2.0376\n",
      "step 35600: train loss 1.9399, val loss 2.0285\n",
      "step 35700: train loss 1.9435, val loss 2.0395\n",
      "step 35800: train loss 1.9380, val loss 2.0344\n",
      "step 35900: train loss 1.9464, val loss 2.0292\n",
      "step 36000: train loss 1.9432, val loss 2.0413\n",
      "step 36100: train loss 1.9418, val loss 2.0308\n",
      "step 36200: train loss 1.9319, val loss 2.0266\n",
      "step 36300: train loss 1.9243, val loss 2.0345\n",
      "step 36400: train loss 1.9341, val loss 2.0197\n",
      "step 36500: train loss 1.9310, val loss 2.0250\n",
      "step 36600: train loss 1.9367, val loss 2.0267\n",
      "step 36700: train loss 1.9295, val loss 2.0266\n",
      "step 36800: train loss 1.9381, val loss 2.0227\n",
      "step 36900: train loss 1.9353, val loss 2.0245\n",
      "step 37000: train loss 1.9247, val loss 2.0194\n",
      "step 37100: train loss 1.9307, val loss 2.0231\n",
      "step 37200: train loss 1.9256, val loss 2.0277\n",
      "step 37300: train loss 1.9304, val loss 2.0281\n",
      "step 37400: train loss 1.9321, val loss 2.0065\n",
      "step 37500: train loss 1.9300, val loss 2.0199\n",
      "step 37600: train loss 1.9315, val loss 2.0134\n",
      "step 37700: train loss 1.9301, val loss 2.0164\n",
      "step 37800: train loss 1.9246, val loss 2.0155\n",
      "step 37900: train loss 1.9259, val loss 2.0129\n",
      "step 38000: train loss 1.9286, val loss 2.0175\n",
      "step 38100: train loss 1.9268, val loss 2.0172\n",
      "step 38200: train loss 1.9322, val loss 2.0186\n",
      "step 38300: train loss 1.9287, val loss 2.0240\n",
      "step 38400: train loss 1.9292, val loss 2.0151\n",
      "step 38500: train loss 1.9242, val loss 2.0153\n",
      "step 38600: train loss 1.9208, val loss 2.0121\n",
      "step 38700: train loss 1.9274, val loss 2.0209\n",
      "step 38800: train loss 1.9295, val loss 2.0208\n",
      "step 38900: train loss 1.9240, val loss 2.0100\n",
      "step 39000: train loss 1.9145, val loss 2.0256\n",
      "step 39100: train loss 1.9249, val loss 2.0093\n",
      "step 39200: train loss 1.9260, val loss 2.0142\n",
      "step 39300: train loss 1.9242, val loss 2.0245\n",
      "step 39400: train loss 1.9173, val loss 2.0091\n",
      "step 39500: train loss 1.9216, val loss 2.0140\n",
      "step 39600: train loss 1.9239, val loss 2.0145\n",
      "step 39700: train loss 1.9119, val loss 2.0162\n",
      "step 39800: train loss 1.9101, val loss 2.0210\n",
      "step 39900: train loss 1.9121, val loss 2.0092\n",
      "step 40000: train loss 1.9127, val loss 2.0028\n",
      "step 40100: train loss 1.9121, val loss 2.0125\n",
      "step 40200: train loss 1.9064, val loss 2.0087\n",
      "step 40300: train loss 1.9108, val loss 2.0105\n",
      "step 40400: train loss 1.9119, val loss 2.0223\n",
      "step 40500: train loss 1.9116, val loss 2.0154\n",
      "step 40600: train loss 1.9166, val loss 2.0079\n",
      "step 40700: train loss 1.9025, val loss 2.0165\n",
      "step 40800: train loss 1.9114, val loss 2.0116\n",
      "step 40900: train loss 1.9157, val loss 2.0007\n",
      "step 41000: train loss 1.9105, val loss 2.0040\n",
      "step 41100: train loss 1.9196, val loss 2.0037\n",
      "step 41200: train loss 1.9002, val loss 2.0063\n",
      "step 41300: train loss 1.9147, val loss 2.0007\n",
      "step 41400: train loss 1.9077, val loss 2.0056\n",
      "step 41500: train loss 1.8994, val loss 2.0109\n",
      "step 41600: train loss 1.9114, val loss 2.0084\n",
      "step 41700: train loss 1.9008, val loss 2.0155\n",
      "step 41800: train loss 1.9122, val loss 2.0142\n",
      "step 41900: train loss 1.9105, val loss 2.0060\n",
      "step 42000: train loss 1.9094, val loss 2.0001\n",
      "step 42100: train loss 1.8967, val loss 2.0106\n",
      "step 42200: train loss 1.9024, val loss 2.0050\n",
      "step 42300: train loss 1.9035, val loss 1.9986\n",
      "step 42400: train loss 1.9061, val loss 2.0090\n",
      "step 42500: train loss 1.9012, val loss 2.0081\n",
      "step 42600: train loss 1.9141, val loss 2.0060\n",
      "step 42700: train loss 1.9056, val loss 1.9982\n",
      "step 42800: train loss 1.9020, val loss 2.0001\n",
      "step 42900: train loss 1.9042, val loss 1.9964\n",
      "step 43000: train loss 1.9001, val loss 2.0015\n",
      "step 43100: train loss 1.8999, val loss 1.9941\n",
      "step 43200: train loss 1.9061, val loss 2.0104\n",
      "step 43300: train loss 1.9055, val loss 1.9940\n",
      "step 43400: train loss 1.8950, val loss 1.9877\n",
      "step 43500: train loss 1.9042, val loss 2.0007\n",
      "step 43600: train loss 1.9092, val loss 1.9935\n",
      "step 43700: train loss 1.9037, val loss 2.0062\n",
      "step 43800: train loss 1.8927, val loss 1.9963\n",
      "step 43900: train loss 1.8885, val loss 2.0181\n",
      "step 44000: train loss 1.9018, val loss 2.0006\n",
      "step 44100: train loss 1.8989, val loss 2.0035\n",
      "step 44200: train loss 1.8942, val loss 1.9942\n",
      "step 44300: train loss 1.8931, val loss 1.9826\n",
      "step 44400: train loss 1.9000, val loss 2.0147\n",
      "step 44500: train loss 1.8988, val loss 2.0042\n",
      "step 44600: train loss 1.8934, val loss 1.9965\n",
      "step 44700: train loss 1.8983, val loss 2.0015\n",
      "step 44800: train loss 1.8928, val loss 2.0017\n",
      "step 44900: train loss 1.8989, val loss 2.0049\n",
      "step 45000: train loss 1.8947, val loss 1.9988\n",
      "step 45100: train loss 1.8920, val loss 1.9928\n",
      "step 45200: train loss 1.8968, val loss 1.9931\n",
      "step 45300: train loss 1.9006, val loss 1.9927\n",
      "step 45400: train loss 1.8981, val loss 1.9939\n",
      "step 45500: train loss 1.8944, val loss 2.0037\n",
      "step 45600: train loss 1.8948, val loss 1.9861\n",
      "step 45700: train loss 1.8948, val loss 2.0049\n",
      "step 45800: train loss 1.8847, val loss 2.0014\n",
      "step 45900: train loss 1.8866, val loss 2.0039\n",
      "step 46000: train loss 1.8938, val loss 1.9969\n",
      "step 46100: train loss 1.8822, val loss 1.9898\n",
      "step 46200: train loss 1.8896, val loss 1.9876\n",
      "step 46300: train loss 1.9057, val loss 2.0055\n",
      "step 46400: train loss 1.8823, val loss 1.9948\n",
      "step 46500: train loss 1.8919, val loss 1.9923\n",
      "step 46600: train loss 1.8907, val loss 2.0077\n",
      "step 46700: train loss 1.8931, val loss 1.9930\n",
      "step 46800: train loss 1.8861, val loss 1.9975\n",
      "step 46900: train loss 1.8946, val loss 2.0111\n",
      "step 47000: train loss 1.8831, val loss 1.9937\n",
      "step 47100: train loss 1.8781, val loss 1.9787\n",
      "step 47200: train loss 1.8980, val loss 1.9951\n",
      "step 47300: train loss 1.8880, val loss 1.9955\n",
      "step 47400: train loss 1.9021, val loss 1.9993\n",
      "step 47500: train loss 1.8854, val loss 1.9933\n",
      "step 47600: train loss 1.8853, val loss 1.9933\n",
      "step 47700: train loss 1.8793, val loss 1.9966\n",
      "step 47800: train loss 1.8797, val loss 1.9848\n",
      "step 47900: train loss 1.8878, val loss 1.9941\n",
      "step 48000: train loss 1.8776, val loss 1.9975\n",
      "step 48100: train loss 1.8843, val loss 1.9926\n",
      "step 48200: train loss 1.8855, val loss 1.9882\n",
      "step 48300: train loss 1.8801, val loss 1.9901\n",
      "step 48400: train loss 1.8927, val loss 1.9973\n",
      "step 48500: train loss 1.8802, val loss 1.9914\n",
      "step 48600: train loss 1.8752, val loss 1.9904\n",
      "step 48700: train loss 1.8859, val loss 1.9845\n",
      "step 48800: train loss 1.8794, val loss 2.0000\n",
      "step 48900: train loss 1.8675, val loss 1.9905\n",
      "step 49000: train loss 1.8710, val loss 1.9867\n",
      "step 49100: train loss 1.8756, val loss 1.9900\n",
      "step 49200: train loss 1.8825, val loss 1.9867\n",
      "step 49300: train loss 1.8798, val loss 1.9887\n",
      "step 49400: train loss 1.8889, val loss 2.0002\n",
      "step 49500: train loss 1.8772, val loss 1.9843\n",
      "step 49600: train loss 1.8830, val loss 1.9944\n",
      "step 49700: train loss 1.8772, val loss 1.9917\n",
      "step 49800: train loss 1.8712, val loss 1.9943\n",
      "step 49900: train loss 1.8674, val loss 1.9863\n",
      "step 49999: train loss 1.8709, val loss 1.9823\n",
      "Loss results for res1 dropout=0.5 saved to ./loss_time/res1\\res1_dropout_0.5_losses.json\n",
      "Training time for res1 dropout=0.5 saved to ./loss_time/res1\\res1_dropout_0.5_training_time.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "res1_dropout_values = [0.0, 0.025, 0.2, 0.5]\n",
    "\n",
    "for dropout in res1_dropout_values:\n",
    "    print(f\"Training model with dropout={dropout}\")\n",
    "\n",
    "    # Start timing the training process\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Instantiate the model with the current dropout value\n",
    "    model = BigramLanguageModel(res1_dropout=dropout).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Lists to store the loss values for the current dropout experiment\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "        # Evaluate the loss on training and validation sets at certain intervals\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss()  # Ensure this function uses the model from the current loop\n",
    "            train_losses.append(losses['train'].item())\n",
    "            val_losses.append(losses['val'].item())\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        \n",
    "        # Sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "        \n",
    "        # Calculate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # End timing the training process\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time  # Calculate the total training time\n",
    "\n",
    "    # Results for losses\n",
    "    loss_results = {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "    }\n",
    "\n",
    "    # Result for training time\n",
    "    time_result = {\n",
    "        \"training_time\": training_time,\n",
    "    }\n",
    "\n",
    "    # Define the folder path\n",
    "    folder_path = \"./loss_time/res1\"\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    # Filename and path for losses\n",
    "    loss_filename = f'res1_dropout_{dropout}_losses.json'\n",
    "    loss_file_path = os.path.join(folder_path, loss_filename)\n",
    "    \n",
    "    # Write the loss results to the file\n",
    "    with open(loss_file_path, 'w') as f:\n",
    "        json.dump(loss_results, f)\n",
    "    \n",
    "    print(f\"Loss results for res1 dropout={dropout} saved to {loss_file_path}\")\n",
    "    \n",
    "    # Filename and path for training time\n",
    "    time_filename = f'res1_dropout_{dropout}_training_time.json'\n",
    "    time_file_path = os.path.join(folder_path, time_filename)\n",
    "    \n",
    "    # Write the training time to the file\n",
    "    with open(time_file_path, 'w') as f:\n",
    "        json.dump(time_result, f)\n",
    "    \n",
    "    print(f\"Training time for res1 dropout={dropout} saved to {time_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization saved to ./visualization/res1_loss_curves.png\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Specify the folder path where the loss JSON files are stored\n",
    "folder_path = \"./loss_time/res1\"\n",
    "# Specify the path to save the visualization result\n",
    "visualization_save_path = \"./visualization/res1_loss_curves.png\"\n",
    "\n",
    "# Use a pattern or a specific part of the file name to identify loss files\n",
    "# Assuming loss files contain '_losses' in their filenames\n",
    "loss_files = [f for f in os.listdir(folder_path) if f.endswith('_losses.json')]\n",
    "\n",
    "# Visualize the loss curves for each dropout configuration\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for file in loss_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    \n",
    "    # Read the loss values from the JSON file\n",
    "    with open(file_path, 'r') as f:\n",
    "        losses = json.load(f)\n",
    "    \n",
    "    # Plot the validation loss curves\n",
    "    # Extracting the dropout value from the filename for labeling\n",
    "    dropout_label = file.split('_')[2]  # Assumes format 'attn_dropout_{value}_losses.json'\n",
    "    plt.plot(losses['val_losses'], label=f\"Validation Loss Dropout {dropout_label}\")\n",
    "\n",
    "plt.title(\"Loss Curves for Different Residual_1 Dropout Configurations\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the image\n",
    "plt.savefig(visualization_save_path)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Visualization saved to {visualization_save_path}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
