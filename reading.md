# Reading for Deep Delight

Deep is a reference to Deep Reinforcement Learning, the technology behind AI advancements today. 
Delight, or precisely DLITE, is a new information measure (loss function) that Weimao Ke developed for information representation and learning. 

## DLITE

DLITE Theory paper: 
https://arxiv.org/abs/2002.07888

DLITE for Big Data and IR: 
https://ieeexplore.ieee.org/document/10020937
Video presentation: https://www.youtube.com/watch?v=qQXCgmX8sOk



## Language Models for IR

MRS Book Chapter 12: Language Models
https://nlp.stanford.edu/IR-book/html/htmledition/language-models-for-information-retrieval-1.html

Elastic Language Models
https://www.elastic.co/blog/language-models-in-elasticsearch

## Linear & Non-linear Classification (w/ NN)

Linear classification (w/ single perceptron): 
https://youtu.be/kuNQViqLw40

Non-linear classification (w/ NN): 
https://youtu.be/P1RAjm6di20

## Reinforcement Learning

* Weimao Ke's Tutorial: http://keensee.com/pdp/research/rl_taxi.html
* Video on the above tutorial: https://youtu.be/QUPpKgXJd5M

## Deep Reinforcement Learning

A gentle introduction to Deep Reinforcement Learning: 
https://towardsdatascience.com/drl-01-a-gentle-introduction-to-deep-reinforcement-learning-405b79866bf4

which is part of the Deep Reinforcement Learnign series here: 
https://torres.ai/deep-reinforcement-learning-explained-series/

## Transformer for Language Models (GPT)

Transformer explained (on Attention is All You Need paper): 
https://towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634

Andrej Karpathy's Let's Build GPT: 
https://www.youtube.com/watch?v=kCc8FmEb1nY

Karpathy blog: 
https://karpathy.github.io/


