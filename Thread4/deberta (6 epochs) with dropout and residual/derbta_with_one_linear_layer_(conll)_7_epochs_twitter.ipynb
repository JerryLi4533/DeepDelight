{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEBdvK_nGr3w",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import classification_report\n",
        "import random ,json\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, Dataset\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hskApG2OGr3x"
      },
      "source": [
        "## Setting Basic Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlnriKCUGr3y",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    batch_size = 4\n",
        "    epochs = 7\n",
        "    lr = 1e-5\n",
        "    seed = 123\n",
        "\n",
        "\n",
        "    # Can only have one True\n",
        "    # only_use_standard_linear_layer=True, use linear layer\n",
        "    only_use_standard_linear_layer = True\n",
        "    # Setting the linear layer number\n",
        "    linear_layer_num = 3\n",
        "    linear_layer = [512, 512, 512]\n",
        "\n",
        "\n",
        "    # if only_use_standard_linear_layer is False, only_use_dropout is True, it means just use dropout.\n",
        "    only_use_dropout = False\n",
        "    dropout_prob = [0.05, 0, 0.05]\n",
        "\n",
        "\n",
        "    # if only_use_standard_linear_layer is False, only_use_residual = True, just use resdiual\n",
        "    only_use_residual = False\n",
        "    # if only_use_standard_linear_layer is False, only_use_residual_and_dropout = True, use residual and dropout together.\n",
        "    only_use_residual_and_dropout = False\n",
        "\n",
        "\n",
        "\n",
        "    assert sum([1 if only_use_standard_linear_layer else 0,\n",
        "                1 if only_use_dropout else 0,\n",
        "                1 if only_use_residual else 0,\n",
        "                1 if only_use_residual_and_dropout else 0]) == 1\n",
        "\n",
        "\n",
        "\n",
        "    # if lstm layer is 0, then not using the lstm layer. Setting the lstm layer based on layers number required\n",
        "    lstm_layer_num = 0\n",
        "    bi_lstm=True\n",
        "\n",
        "\n",
        "    # Internet resource; download from Internet\n",
        "    # model_name = \"microsoft/deberta-v3-base\"\n",
        "\n",
        "    model_name = \"microsoft/deberta-base\"\n",
        "    # model_name = \"deberta-base\"\n",
        "\n",
        "    hidden_size=768\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_data_name = \"broad_twitter_corpus\" # conll2003,  ner_datasetreference\n",
        "\n",
        "    @classmethod\n",
        "    def describe(cls):\n",
        "        parm = {\"train_data_name\": cls.train_data_name,\n",
        "                \"encoder_name\": cls.model_name,\n",
        "                \"batch_size\": cls.batch_size,\n",
        "                \"epochs\": cls.epochs,\n",
        "                \"lr\": cls.lr,\n",
        "                \"seed\": cls.seed,\n",
        "                \"bi_lstm\": cls.bi_lstm,\n",
        "                \"lstm_layer_num\": cls.lstm_layer_num,\n",
        "                \"linear_layer\": cls.linear_layer,\n",
        "                \"linear_layer_num\":cls.linear_layer_num,\n",
        "                \"dropout_prob\": cls.dropout_prob,\n",
        "                \"only_use_standard_linear_layer\": cls.only_use_standard_linear_layer,\n",
        "                \"only_use_dropout\": cls.only_use_dropout,\n",
        "                \"only_use_residual\": cls.only_use_residual,\n",
        "                \"only_use_residual_and_dropout\": cls.only_use_residual_and_dropout}\n",
        "        return json.dumps(parm , ensure_ascii=False, indent=2)\n",
        "\n",
        "random.seed(Config.seed)\n",
        "np.random.seed(Config.seed)\n",
        "torch.manual_seed(Config.seed)\n",
        "torch.cuda.manual_seed_all(Config.seed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQGJgAySrwEr"
      },
      "source": [
        "## given configuration result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BObln4RIGr3y",
        "outputId": "2fe55f29-a8c5-4e2e-e858-00c89bee6028",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"train_data_name\": \"broad_twitter_corpus\",\n",
            "  \"encoder_name\": \"deberta-base\",\n",
            "  \"batch_size\": 4,\n",
            "  \"epochs\": 7,\n",
            "  \"lr\": 1e-05,\n",
            "  \"seed\": 123,\n",
            "  \"bi_lstm\": true,\n",
            "  \"lstm_layer_num\": 0,\n",
            "  \"linear_layer\": [\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"linear_layer_num\": 3,\n",
            "  \"dropout_prob\": [\n",
            "    0.05,\n",
            "    0,\n",
            "    0.05\n",
            "  ],\n",
            "  \"only_use_standard_linear_layer\": true,\n",
            "  \"only_use_dropout\": false,\n",
            "  \"only_use_residual\": false,\n",
            "  \"only_use_residual_and_dropout\": false\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(Config.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9ig_SZXGr3z"
      },
      "source": [
        "## Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMvdJcT17aD9"
      },
      "outputs": [],
      "source": [
        "def read_broad_twitter_corpus(data_dir):\n",
        "    ret, sample  = [], []\n",
        "    for file in sorted(os.listdir(data_dir)):\n",
        "        if file.endswith(\".conll\"):\n",
        "            file = os.path.join(data_dir, file)\n",
        "            for idx,line in  enumerate(open(file, \"r\", encoding=\"utf-8\")):\n",
        "                line = line.strip()\n",
        "                if line == \"\":\n",
        "                    if len(sample) > 0 :\n",
        "                        ret.append({\"word\": [i[0] for i in sample], \"tag\": [i[1] for i in sample]})\n",
        "                    sample = []\n",
        "                else:\n",
        "                    tokens = line.split(\"\\t\")\n",
        "                    if len(tokens) != 2:\n",
        "                        continue\n",
        "                    else:\n",
        "                        sample.append(tokens)\n",
        "            if len(sample) > 0:\n",
        "                ret.append({\"word\": [i[0] for i in sample], \"tag\": [i[1] for i in sample]})\n",
        "\n",
        "    return pd.DataFrame(ret)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGZ_d-HrGr3z",
        "outputId": "8f88faf1-b500-4705-8902-7d88c9574f03",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5338, 2) (2000, 2) (2001, 2)\n"
          ]
        }
      ],
      "source": [
        "def read_conll2003(file_path):\n",
        "    data = []\n",
        "    sample = []\n",
        "    for idx, line in enumerate(open(file_path)):\n",
        "        if idx == 0:\n",
        "            continue\n",
        "        line = line.strip()\n",
        "        if line == \"\":\n",
        "            if len(sample) != 0:\n",
        "                data.append(sample)\n",
        "            sample = []\n",
        "        else:\n",
        "            line = line.split()\n",
        "            assert len(line) == 4\n",
        "            sample.append([line[0], line[-1]])\n",
        "    if len(sample) != 0:\n",
        "        data.append(sample)\n",
        "    data = [{\"word\": [i[0] for i in sample], \"tag\": [i[1] for i in sample]} for sample in data]\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Setting the chosen dataset, conll2003 or ner_datasetreference.\n",
        "if Config.train_data_name == \"conll2003\":\n",
        "    train_path = os.path.join(Config.train_data_name, 'train.txt')\n",
        "    dev_path = os.path.join(Config.train_data_name, 'valid.txt')\n",
        "    test_path = os.path.join(Config.train_data_name, 'test.txt')\n",
        "    train_df = read_conll2003(train_path)\n",
        "    valid_df = read_conll2003(dev_path)\n",
        "    test_df = read_conll2003(test_path)\n",
        "    print(train_df.shape, valid_df.shape, test_df.shape)\n",
        "elif Config.train_data_name == \"ner_datasetreference\":\n",
        "    df = pd.read_csv(\"ner_datasetreference.csv\", encoding='iso-8859-1')\n",
        "    data = []\n",
        "    word, tag = [], []\n",
        "    for i,j,k in zip(df['Sentence #'], df['Word'], df['Tag']):\n",
        "        if not pd.isnull(i):\n",
        "            assert i.startswith('Sentence')\n",
        "            if len(word) > 0:\n",
        "                data.append({\"word\":word, \"tag\":tag})\n",
        "            word, tag = [], []\n",
        "        if isinstance(j, str) and isinstance(k, str):\n",
        "            # remove 'art', 'eve', 'nat' label for better macro results\n",
        "            if any( t in k for t in ['art', 'eve', 'nat']):\n",
        "                continue\n",
        "            word.append(j)\n",
        "            tag.append(k)\n",
        "    if len(word) > 0:\n",
        "        data.append({\"word\":word, \"tag\":tag})\n",
        "        word, tag = [], []\n",
        "    print(data[0], data[-1])\n",
        "    df = pd.DataFrame(data)\n",
        "    train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
        "    valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "    print(df.shape, train_df.shape, valid_df.shape, test_df.shape)\n",
        "elif Config.train_data_name == 'broad_twitter_corpus':\n",
        "    train_df = read_broad_twitter_corpus(os.path.join(Config.train_data_name, 'train'))\n",
        "    valid_df = read_broad_twitter_corpus(os.path.join(Config.train_data_name, 'dev'))\n",
        "    test_df = read_broad_twitter_corpus(os.path.join(Config.train_data_name, 'test'))\n",
        "    print(train_df.shape, valid_df.shape, test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "BYJxdEf9Gr30",
        "outputId": "b51e9033-9871-405a-c988-fe46de89690d",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[I, hate, the, words, chunder, ,, vomit, and, ...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[♥, ., ., ), ), (, ♫, ., (, ړײ, ), ♫, ., ♥, .,...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Alesan, kenapa, mlm, kita, lbh, srg, galau, P...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Complete, Tosca, on, the, tube, http://t.co/O...</td>\n",
              "      <td>[O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Think, you, call, that, smash, and, grab, ., ...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5333</th>\n",
              "      <td>[NASA, discusses, scrapped, space, walk, ,, li...</td>\n",
              "      <td>[B-ORG, O, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5334</th>\n",
              "      <td>[Watch, :, Attorney, General, Eric, Holder, 's...</td>\n",
              "      <td>[O, O, O, O, B-PER, I-PER, O, B-ORG, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5335</th>\n",
              "      <td>[Filibuster, fight, gears, up, in, Senate, ., ...</td>\n",
              "      <td>[O, O, O, O, O, B-ORG, O, O, O, O, O, O, O, O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5336</th>\n",
              "      <td>[Watch, live, :, Senator, Gillibrand, newser, ...</td>\n",
              "      <td>[O, O, O, B-PER, I-PER, O, O, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5337</th>\n",
              "      <td>[Watch, Live, :, President, Obama, ,, George, ...</td>\n",
              "      <td>[O, O, O, B-PER, I-PER, O, B-PER, I-PER, I-PER...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5338 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   word  \\\n",
              "0     [I, hate, the, words, chunder, ,, vomit, and, ...   \n",
              "1     [♥, ., ., ), ), (, ♫, ., (, ړײ, ), ♫, ., ♥, .,...   \n",
              "2     [Alesan, kenapa, mlm, kita, lbh, srg, galau, P...   \n",
              "3     [Complete, Tosca, on, the, tube, http://t.co/O...   \n",
              "4     [Think, you, call, that, smash, and, grab, ., ...   \n",
              "...                                                 ...   \n",
              "5333  [NASA, discusses, scrapped, space, walk, ,, li...   \n",
              "5334  [Watch, :, Attorney, General, Eric, Holder, 's...   \n",
              "5335  [Filibuster, fight, gears, up, in, Senate, ., ...   \n",
              "5336  [Watch, live, :, Senator, Gillibrand, newser, ...   \n",
              "5337  [Watch, Live, :, President, Obama, ,, George, ...   \n",
              "\n",
              "                                                    tag  \n",
              "0                  [O, O, O, O, O, O, O, O, O, O, O, O]  \n",
              "1     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "3                                    [O, O, O, O, O, O]  \n",
              "4     [O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O,...  \n",
              "...                                                 ...  \n",
              "5333                       [B-ORG, O, O, O, O, O, O, O]  \n",
              "5334         [O, O, O, O, B-PER, I-PER, O, B-ORG, O, O]  \n",
              "5335  [O, O, O, O, O, B-ORG, O, O, O, O, O, O, O, O,...  \n",
              "5336    [O, O, O, B-PER, I-PER, O, O, O, O, O, O, O, O]  \n",
              "5337  [O, O, O, B-PER, I-PER, O, B-PER, I-PER, I-PER...  \n",
              "\n",
              "[5338 rows x 2 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "qNmakWvLGr30",
        "outputId": "4049fe36-b3ff-4245-ea86-c4b867c1d9d3",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[@, colgo, hey, ,, congrats, to, you, and, the...</td>\n",
              "      <td>[B-PER, B-PER, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[This, morning, I, met, with, Senators, Inabo,...</td>\n",
              "      <td>[O, O, O, O, O, O, B-PER, O, B-PER, O, B-LOC, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Enjoying, the, Chinese, Assoc, of, Vic, annua...</td>\n",
              "      <td>[O, O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Best, wishes, to, Kevin, ,, Therese, &amp;, their...</td>\n",
              "      <td>[O, O, O, B-PER, O, B-PER, O, O, O, O, O, O, O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Looking, forward, to, Restaurant, &amp;, Catering...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>[Talks, @, Pulitzer, :, Amy, Toensing, and, Je...</td>\n",
              "      <td>[O, O, B-ORG, O, B-PER, I-PER, O, B-PER, I-PER...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>[Introducing, new, creative, tools, to, edit, ...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, B-ORG, O, O, O, O, O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>[@, juliaduin, Great, ., Please, email, to, ae...</td>\n",
              "      <td>[B-PER, B-PER, O, O, O, O, O, B-PER, B-ORG, B-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>[@, jimwaterson, steak, bake, ?]</td>\n",
              "      <td>[B-PER, B-PER, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000</th>\n",
              "      <td>[Here, 's, what, you, can, expect, from, @, bi...</td>\n",
              "      <td>[O, O, O, O, O, O, O, B-PER, B-PER, O, O, O, O...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2001 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   word  \\\n",
              "0     [@, colgo, hey, ,, congrats, to, you, and, the...   \n",
              "1     [This, morning, I, met, with, Senators, Inabo,...   \n",
              "2     [Enjoying, the, Chinese, Assoc, of, Vic, annua...   \n",
              "3     [Best, wishes, to, Kevin, ,, Therese, &, their...   \n",
              "4     [Looking, forward, to, Restaurant, &, Catering...   \n",
              "...                                                 ...   \n",
              "1996  [Talks, @, Pulitzer, :, Amy, Toensing, and, Je...   \n",
              "1997  [Introducing, new, creative, tools, to, edit, ...   \n",
              "1998  [@, juliaduin, Great, ., Please, email, to, ae...   \n",
              "1999                   [@, jimwaterson, steak, bake, ?]   \n",
              "2000  [Here, 's, what, you, can, expect, from, @, bi...   \n",
              "\n",
              "                                                    tag  \n",
              "0     [B-PER, B-PER, O, O, O, O, O, O, O, O, O, O, O...  \n",
              "1     [O, O, O, O, O, O, B-PER, O, B-PER, O, B-LOC, ...  \n",
              "2     [O, O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O,...  \n",
              "3     [O, O, O, B-PER, O, B-PER, O, O, O, O, O, O, O...  \n",
              "4     [O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O,...  \n",
              "...                                                 ...  \n",
              "1996  [O, O, B-ORG, O, B-PER, I-PER, O, B-PER, I-PER...  \n",
              "1997  [O, O, O, O, O, O, O, O, B-ORG, O, O, O, O, O,...  \n",
              "1998  [B-PER, B-PER, O, O, O, O, O, B-PER, B-ORG, B-...  \n",
              "1999                            [B-PER, B-PER, O, O, O]  \n",
              "2000  [O, O, O, O, O, O, O, B-PER, B-PER, O, O, O, O...  \n",
              "\n",
              "[2001 rows x 2 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "wdv02qIvGr30",
        "outputId": "c3ba35fc-fb49-4151-ac93-ee5e8e574ebf",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Bbl]</td>\n",
              "      <td>[O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[@, Theory_Versa, be, 10, mins]</td>\n",
              "      <td>[B-PER, B-PER, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[This, Video, of, Yasiel, Puig, Losing, His, M...</td>\n",
              "      <td>[O, O, O, B-PER, I-PER, O, O, O, O, O, O, O, O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[@, ARICkologyyy, No, worries]</td>\n",
              "      <td>[B-ORG, B-ORG, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[@, abdielss, iya, ganteng, ,, king, kong, is,...</td>\n",
              "      <td>[B-PER, B-PER, O, O, O, B-PER, I-PER, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>[@, fyBTSVee_, okay, :), anyway, ,, if, i, cha...</td>\n",
              "      <td>[B-PER, B-PER, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>[RT, @, MOVIEMEMORlES, :, #OneOfMyFavoriteMovi...</td>\n",
              "      <td>[O, B-PER, B-PER, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>[Provide, you, 165, +, real, and, permanent, l...</td>\n",
              "      <td>[O, O, O, O, O, O, O, B-ORG, O, O, O, O, O, O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>[Kermit, &amp;, Steve, Martin, ,, la, nuova, rainb...</td>\n",
              "      <td>[B-PER, O, B-PER, I-PER, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>[Wind, 0, km, /, h, S, ., Barometer, 1023, ., ...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   word  \\\n",
              "0                                                 [Bbl]   \n",
              "1                       [@, Theory_Versa, be, 10, mins]   \n",
              "2     [This, Video, of, Yasiel, Puig, Losing, His, M...   \n",
              "3                        [@, ARICkologyyy, No, worries]   \n",
              "4     [@, abdielss, iya, ganteng, ,, king, kong, is,...   \n",
              "...                                                 ...   \n",
              "1995  [@, fyBTSVee_, okay, :), anyway, ,, if, i, cha...   \n",
              "1996  [RT, @, MOVIEMEMORlES, :, #OneOfMyFavoriteMovi...   \n",
              "1997  [Provide, you, 165, +, real, and, permanent, l...   \n",
              "1998  [Kermit, &, Steve, Martin, ,, la, nuova, rainb...   \n",
              "1999  [Wind, 0, km, /, h, S, ., Barometer, 1023, ., ...   \n",
              "\n",
              "                                                    tag  \n",
              "0                                                   [O]  \n",
              "1                               [B-PER, B-PER, O, O, O]  \n",
              "2     [O, O, O, B-PER, I-PER, O, O, O, O, O, O, O, O...  \n",
              "3                                  [B-ORG, B-ORG, O, O]  \n",
              "4           [B-PER, B-PER, O, O, O, B-PER, I-PER, O, O]  \n",
              "...                                                 ...  \n",
              "1995  [B-PER, B-PER, O, O, O, O, O, O, O, O, O, O, O...  \n",
              "1996                   [O, B-PER, B-PER, O, O, O, O, O]  \n",
              "1997  [O, O, O, O, O, O, O, B-ORG, O, O, O, O, O, O,...  \n",
              "1998         [B-PER, O, B-PER, I-PER, O, O, O, O, O, O]  \n",
              "1999  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "\n",
              "[2000 rows x 2 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5vlJtg-Gr31",
        "outputId": "3345c39f-1db4-4152-ac8e-54383f078eec",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ner category ['LOC', 'ORG', 'PER'] .\n",
            "\n",
            "label list ['O', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG', 'B-PER', 'I-PER'] .\n",
            "\n",
            "label2id {'O': 0, 'B-LOC': 1, 'I-LOC': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-PER': 5, 'I-PER': 6} .\n",
            "\n",
            "id2label {0: 'O', 1: 'B-LOC', 2: 'I-LOC', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-PER', 6: 'I-PER'}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def collect_label(df_list):\n",
        "    ret = set()\n",
        "    for df in df_list:\n",
        "        for labels in df['tag']:\n",
        "            for l in labels:\n",
        "                if l == \"O\":\n",
        "                    continue\n",
        "                assert l.startswith(\"B-\") or l.startswith(\"I-\")\n",
        "                ret.add(l[2:])\n",
        "    return sorted(list(ret))\n",
        "\n",
        "ner_category = collect_label([train_df, valid_df, test_df])\n",
        "label_list = []\n",
        "for l in ner_category:\n",
        "    label_list.append(\"B-\" + l)\n",
        "    label_list.append(\"I-\" + l)\n",
        "label_list = ['O'] + label_list\n",
        "label2id = dict([(v, idx) for idx, v in enumerate(label_list)])\n",
        "id2label = dict([(idx, v) for idx, v in enumerate(label_list)])\n",
        "print(f\"ner category {ner_category} .\\n\\nlabel list {label_list} .\\n\\nlabel2id {label2id} .\\n\\nid2label {id2label}\\n\\n\")\n",
        "label_list = label_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLLN7xMQGr31"
      },
      "source": [
        "## Import Reberta Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290,
          "referenced_widgets": [
            "7256149fd04948eca3dcf636f7d59a55",
            "45f8cb11996d4468901fe7d53a22c2ae",
            "4273ade980794722bb1606f017cf83bb",
            "2e489ff248c34ce3a93ca757f09f9d7b",
            "2b64e8d570634f34bf325232ff1508be",
            "56521d7163f84a188d0a2b6e62fd4c23",
            "3f5be85c0efe42f0bd83cd9c9bb531b9",
            "1a7b93126ec644518f34658935d7e809",
            "55fa61fb78574c6fb1adb9d9612c459a",
            "f9552ca48ea94ef591bfef6cc0e16d4b",
            "f867a7e05f0f4dc99bef3ef27e839837",
            "c0fc05759e1448409fdf1257ca0477e3",
            "d39b0e5a8dad4098a1a957d3b4f9cf7d",
            "053603035cfc49bda58394a1430fa2b7",
            "bc455f2a4c824381bea1f814cde5edc0",
            "2eaa5c86ef7d44208b6ef72cf3a92686",
            "27ac0084829a4b668009fc1774abb049",
            "ef633886efaf478e9c4002e949086579",
            "d4172a659d1c4a22b6196023c523bd7a",
            "48c239d10b444ff6a0b3df02f23987cb",
            "05cb78e546ec4193b9dca2a05313e696",
            "b583c35bff8a46a89b8d223afa4dfd92",
            "48e7cdf29cf14346977b92411cf5684c",
            "55af4e76445d4b089643ad643182b13c",
            "6db4caa38b064dfdb3e2a0fba7758fde",
            "fa01c3d6c221462fa0f1fa44c658d668",
            "3fd2877370ff43a293f8c7d837e594a6",
            "2bd0f9d0f94a422f8e9bbca727eab857",
            "2725dff4e9e5420f8797650cdb17ab56",
            "3d0fe1de2a4640eeb06d62d0dbab2e7b",
            "da51306992fc46c081d060334dc025ce",
            "33b98c115c43494da099f16791ca439d",
            "35035a00517d40a491e1b234ab759b48",
            "9c9077f56a7a4bec9d818d958d067c66",
            "2e1b8bac73c840ed9abb1cfc6032e341",
            "13fdabcd3ec1414fbb91917499bf067a",
            "3652cfa75ee349ac9900daa5a23e749a",
            "dc4f542c9c72483aaa3ddb19b1d15a69",
            "293233eb8ec24b80a72f1d4a57db2566",
            "c3e7a4da2dbd4bfb9db9c5d8b05910c6",
            "30682e8e0cbb4d11ae69f30b81d52675",
            "b10d7e8a23cd4d8491c8eceea018e698",
            "97693e173fd8439982e698c40f06e7ac",
            "5d6797166255429a9a9a5d6d8e344519"
          ]
        },
        "id": "Sy1aItuvGr31",
        "outputId": "a88c5a5f-5856-433c-88d0-3de3840de3a7",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(Config.model_name, add_prefix_space=True)\n",
        "print(tokenizer.is_fast)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvXSJptjGr31"
      },
      "source": [
        "## tokenize and build Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G49tIcsZGr31",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def align(tag, word_ids):\n",
        "    aligned_tag = []\n",
        "    i = 0\n",
        "    while i < len(word_ids):\n",
        "        if word_ids[i] is None:\n",
        "            aligned_tag.append(None)\n",
        "            i += 1\n",
        "        elif tag[word_ids[i]] == \"O\":\n",
        "            aligned_tag.append(tag[word_ids[i]])\n",
        "            i += 1\n",
        "        elif tag[word_ids[i]].startswith(\"B-\"):\n",
        "            n = 0\n",
        "            while (i+n) < len(word_ids) and word_ids[i]  == word_ids[i+n]:\n",
        "                n += 1\n",
        "            aligned_tag.append(tag[word_ids[i]])\n",
        "            if n > 1:\n",
        "                aligned_tag.extend([\"I-\" + tag[word_ids[i]][2:] ] * (n-1))\n",
        "            i = i + n\n",
        "        else:\n",
        "            aligned_tag.append(tag[word_ids[i]])\n",
        "            i += 1\n",
        "    return aligned_tag\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LldB9JC9Gr31",
        "outputId": "282f340c-211f-44c4-bfc7-8af712fe54b9",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I', '1996-08-22', '1996-08-22', 'I'] ['O', 'B-LOC', 'B-ORG', 'O']\n",
            "   tokens   tags  word-index\n",
            "0   [CLS]   None         NaN\n",
            "1      ĠI      O         0.0\n",
            "2   Ġ1996  B-LOC         1.0\n",
            "3       -  I-LOC         1.0\n",
            "4      08  I-LOC         1.0\n",
            "5       -  I-LOC         1.0\n",
            "6      22  I-LOC         1.0\n",
            "7   Ġ1996  B-ORG         2.0\n",
            "8       -  I-ORG         2.0\n",
            "9      08  I-ORG         2.0\n",
            "10      -  I-ORG         2.0\n",
            "11     22  I-ORG         2.0\n",
            "12     ĠI      O         3.0\n",
            "13  [SEP]   None         NaN\n"
          ]
        }
      ],
      "source": [
        "#words = train_df.iloc[2][\"word\"]\n",
        "#tag = train_df.iloc[2][\"label\"]\n",
        "words = ['I', '1996-08-22', '1996-08-22', 'I']\n",
        "tag = [\"O\", \"B-LOC\", \"B-ORG\", \"O\"]\n",
        "print(words, tag)\n",
        "s = tokenizer(words, truncation=True, is_split_into_words=True)\n",
        "word_ids = s.word_ids()\n",
        "# align tokens and words\n",
        "tokens = tokenizer.convert_ids_to_tokens(s['input_ids'])\n",
        "tags = align(tag, s.word_ids())\n",
        "print(pd.DataFrame(list(zip(tokens, tags, word_ids)), columns=['tokens', 'tags', 'word-index']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3i02DNiGr31",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def preprocess(x):\n",
        "    word = x['word']\n",
        "    r = tokenizer(word, truncation=True, is_split_into_words=True)\n",
        "    word_ids = r.word_ids()\n",
        "    tokens = tokenizer.convert_ids_to_tokens(r['input_ids'])\n",
        "    align_label = align(x['tag'], word_ids)\n",
        "    return tokens, align_label, r['input_ids'], [label2id[i] if i is not None else -100  for i in align_label], word_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmnu0LUoGr31",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_df[['token', 'label', 'id', 'label_id', 'word_ids']] = train_df.apply(lambda x: pd.Series(preprocess(x)), axis=1)\n",
        "valid_df[['token', 'label', 'id', 'label_id', 'word_ids']] = valid_df.apply(lambda x: pd.Series(preprocess(x)), axis=1)\n",
        "test_df[['token', 'label', 'id', 'label_id', 'word_ids']] = test_df.apply(lambda x: pd.Series(preprocess(x)), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "id": "LHAT0xb_Gr32",
        "outputId": "a4e38560-e416-40ce-daec-9ee1e6deb9ef",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "      <th>token</th>\n",
              "      <th>label</th>\n",
              "      <th>id</th>\n",
              "      <th>label_id</th>\n",
              "      <th>word_ids</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[@, colgo, hey, ,, congrats, to, you, and, the...</td>\n",
              "      <td>[B-PER, B-PER, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
              "      <td>[[CLS], Ġ@, Ġcol, go, Ġhey, Ġ,, Ġcongr, ats, Ġ...</td>\n",
              "      <td>[None, B-PER, B-PER, I-PER, O, O, O, O, O, O, ...</td>\n",
              "      <td>[1, 787, 11311, 2977, 17232, 2156, 41645, 2923...</td>\n",
              "      <td>[-100, 5, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[None, 0, 1, 1, 2, 3, 4, 4, 5, 6, 7, 8, 9, 10,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[This, morning, I, met, with, Senators, Inabo,...</td>\n",
              "      <td>[O, O, O, O, O, O, B-PER, O, B-PER, O, B-LOC, ...</td>\n",
              "      <td>[[CLS], ĠThis, Ġmorning, ĠI, Ġmet, Ġwith, ĠSen...</td>\n",
              "      <td>[None, O, O, O, O, O, O, B-PER, I-PER, O, B-PE...</td>\n",
              "      <td>[1, 152, 662, 38, 1145, 19, 10421, 96, 21763, ...</td>\n",
              "      <td>[-100, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 0, 1, 2, ...</td>\n",
              "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10, 10...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Enjoying, the, Chinese, Assoc, of, Vic, annua...</td>\n",
              "      <td>[O, O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O,...</td>\n",
              "      <td>[[CLS], ĠEnjoy, ing, Ġthe, ĠChinese, ĠAss, oc,...</td>\n",
              "      <td>[None, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-...</td>\n",
              "      <td>[1, 16013, 154, 5, 1111, 6331, 1975, 9, 13708,...</td>\n",
              "      <td>[-100, 0, 0, 0, 3, 4, 4, 4, 4, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[None, 0, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Best, wishes, to, Kevin, ,, Therese, &amp;, their...</td>\n",
              "      <td>[O, O, O, B-PER, O, B-PER, O, O, O, O, O, O, O...</td>\n",
              "      <td>[[CLS], ĠBest, Ġwishes, Ġto, ĠKevin, Ġ,, ĠTher...</td>\n",
              "      <td>[None, O, O, O, B-PER, O, B-PER, I-PER, O, O, ...</td>\n",
              "      <td>[1, 2700, 8605, 7, 2363, 2156, 345, 1090, 359,...</td>\n",
              "      <td>[-100, 0, 0, 0, 5, 0, 5, 6, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[None, 0, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10, 11...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Looking, forward, to, Restaurant, &amp;, Catering...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O,...</td>\n",
              "      <td>[[CLS], ĠLooking, Ġforward, Ġto, ĠRestaurant, ...</td>\n",
              "      <td>[None, O, O, O, O, O, O, O, O, O, O, B-LOC, I-...</td>\n",
              "      <td>[1, 7817, 556, 7, 11561, 359, 20322, 154, 4229...</td>\n",
              "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, ...</td>\n",
              "      <td>[None, 0, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 9, 10,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>[Talks, @, Pulitzer, :, Amy, Toensing, and, Je...</td>\n",
              "      <td>[O, O, B-ORG, O, B-PER, I-PER, O, B-PER, I-PER...</td>\n",
              "      <td>[[CLS], ĠTalks, Ġ@, ĠPulitzer, Ġ:, ĠAmy, ĠTo, ...</td>\n",
              "      <td>[None, O, O, B-ORG, O, B-PER, I-PER, I-PER, O,...</td>\n",
              "      <td>[1, 22630, 787, 26205, 4832, 6918, 598, 21591,...</td>\n",
              "      <td>[-100, 0, 0, 3, 0, 5, 6, 6, 0, 5, 6, 6, 0, 0, ...</td>\n",
              "      <td>[None, 0, 1, 2, 3, 4, 5, 5, 6, 7, 8, 8, 9, 10,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>[Introducing, new, creative, tools, to, edit, ...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, B-ORG, O, O, O, O, O,...</td>\n",
              "      <td>[[CLS], ĠIntrodu, cing, Ġnew, Ġcreative, Ġtool...</td>\n",
              "      <td>[None, O, O, O, O, O, O, O, O, O, B-ORG, O, O,...</td>\n",
              "      <td>[1, 32687, 11162, 92, 3904, 3270, 7, 17668, 23...</td>\n",
              "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, ...</td>\n",
              "      <td>[None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>[@, juliaduin, Great, ., Please, email, to, ae...</td>\n",
              "      <td>[B-PER, B-PER, O, O, O, O, O, B-PER, B-ORG, B-...</td>\n",
              "      <td>[[CLS], Ġ@, Ġj, uli, ad, uin, ĠGreat, Ġ., ĠPle...</td>\n",
              "      <td>[None, B-PER, B-PER, I-PER, I-PER, I-PER, O, O...</td>\n",
              "      <td>[1, 787, 1236, 18425, 625, 39808, 2860, 479, 3...</td>\n",
              "      <td>[-100, 5, 5, 6, 6, 6, 0, 0, 0, 0, 0, 5, 6, 6, ...</td>\n",
              "      <td>[None, 0, 1, 1, 1, 1, 2, 3, 4, 5, 6, 7, 7, 7, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>[@, jimwaterson, steak, bake, ?]</td>\n",
              "      <td>[B-PER, B-PER, O, O, O]</td>\n",
              "      <td>[[CLS], Ġ@, Ġj, im, wat, erson, Ġsteak, Ġbake,...</td>\n",
              "      <td>[None, B-PER, B-PER, I-PER, I-PER, I-PER, O, O...</td>\n",
              "      <td>[1, 787, 1236, 757, 24749, 4277, 19464, 24870,...</td>\n",
              "      <td>[-100, 5, 5, 6, 6, 6, 0, 0, 0, -100]</td>\n",
              "      <td>[None, 0, 1, 1, 1, 1, 2, 3, 4, None]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000</th>\n",
              "      <td>[Here, 's, what, you, can, expect, from, @, bi...</td>\n",
              "      <td>[O, O, O, O, O, O, O, B-PER, B-PER, O, O, O, O...</td>\n",
              "      <td>[[CLS], ĠHere, Ġ', s, Ġwhat, Ġyou, Ġcan, Ġexpe...</td>\n",
              "      <td>[None, O, O, O, O, O, O, O, O, B-PER, B-PER, I...</td>\n",
              "      <td>[1, 1398, 128, 29, 99, 47, 64, 1057, 31, 787, ...</td>\n",
              "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 6, 6, 6, ...</td>\n",
              "      <td>[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 8, 8, 8, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2001 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   word  \\\n",
              "0     [@, colgo, hey, ,, congrats, to, you, and, the...   \n",
              "1     [This, morning, I, met, with, Senators, Inabo,...   \n",
              "2     [Enjoying, the, Chinese, Assoc, of, Vic, annua...   \n",
              "3     [Best, wishes, to, Kevin, ,, Therese, &, their...   \n",
              "4     [Looking, forward, to, Restaurant, &, Catering...   \n",
              "...                                                 ...   \n",
              "1996  [Talks, @, Pulitzer, :, Amy, Toensing, and, Je...   \n",
              "1997  [Introducing, new, creative, tools, to, edit, ...   \n",
              "1998  [@, juliaduin, Great, ., Please, email, to, ae...   \n",
              "1999                   [@, jimwaterson, steak, bake, ?]   \n",
              "2000  [Here, 's, what, you, can, expect, from, @, bi...   \n",
              "\n",
              "                                                    tag  \\\n",
              "0     [B-PER, B-PER, O, O, O, O, O, O, O, O, O, O, O...   \n",
              "1     [O, O, O, O, O, O, B-PER, O, B-PER, O, B-LOC, ...   \n",
              "2     [O, O, B-ORG, I-ORG, I-ORG, I-ORG, O, O, O, O,...   \n",
              "3     [O, O, O, B-PER, O, B-PER, O, O, O, O, O, O, O...   \n",
              "4     [O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O,...   \n",
              "...                                                 ...   \n",
              "1996  [O, O, B-ORG, O, B-PER, I-PER, O, B-PER, I-PER...   \n",
              "1997  [O, O, O, O, O, O, O, O, B-ORG, O, O, O, O, O,...   \n",
              "1998  [B-PER, B-PER, O, O, O, O, O, B-PER, B-ORG, B-...   \n",
              "1999                            [B-PER, B-PER, O, O, O]   \n",
              "2000  [O, O, O, O, O, O, O, B-PER, B-PER, O, O, O, O...   \n",
              "\n",
              "                                                  token  \\\n",
              "0     [[CLS], Ġ@, Ġcol, go, Ġhey, Ġ,, Ġcongr, ats, Ġ...   \n",
              "1     [[CLS], ĠThis, Ġmorning, ĠI, Ġmet, Ġwith, ĠSen...   \n",
              "2     [[CLS], ĠEnjoy, ing, Ġthe, ĠChinese, ĠAss, oc,...   \n",
              "3     [[CLS], ĠBest, Ġwishes, Ġto, ĠKevin, Ġ,, ĠTher...   \n",
              "4     [[CLS], ĠLooking, Ġforward, Ġto, ĠRestaurant, ...   \n",
              "...                                                 ...   \n",
              "1996  [[CLS], ĠTalks, Ġ@, ĠPulitzer, Ġ:, ĠAmy, ĠTo, ...   \n",
              "1997  [[CLS], ĠIntrodu, cing, Ġnew, Ġcreative, Ġtool...   \n",
              "1998  [[CLS], Ġ@, Ġj, uli, ad, uin, ĠGreat, Ġ., ĠPle...   \n",
              "1999  [[CLS], Ġ@, Ġj, im, wat, erson, Ġsteak, Ġbake,...   \n",
              "2000  [[CLS], ĠHere, Ġ', s, Ġwhat, Ġyou, Ġcan, Ġexpe...   \n",
              "\n",
              "                                                  label  \\\n",
              "0     [None, B-PER, B-PER, I-PER, O, O, O, O, O, O, ...   \n",
              "1     [None, O, O, O, O, O, O, B-PER, I-PER, O, B-PE...   \n",
              "2     [None, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-...   \n",
              "3     [None, O, O, O, B-PER, O, B-PER, I-PER, O, O, ...   \n",
              "4     [None, O, O, O, O, O, O, O, O, O, O, B-LOC, I-...   \n",
              "...                                                 ...   \n",
              "1996  [None, O, O, B-ORG, O, B-PER, I-PER, I-PER, O,...   \n",
              "1997  [None, O, O, O, O, O, O, O, O, O, B-ORG, O, O,...   \n",
              "1998  [None, B-PER, B-PER, I-PER, I-PER, I-PER, O, O...   \n",
              "1999  [None, B-PER, B-PER, I-PER, I-PER, I-PER, O, O...   \n",
              "2000  [None, O, O, O, O, O, O, O, O, B-PER, B-PER, I...   \n",
              "\n",
              "                                                     id  \\\n",
              "0     [1, 787, 11311, 2977, 17232, 2156, 41645, 2923...   \n",
              "1     [1, 152, 662, 38, 1145, 19, 10421, 96, 21763, ...   \n",
              "2     [1, 16013, 154, 5, 1111, 6331, 1975, 9, 13708,...   \n",
              "3     [1, 2700, 8605, 7, 2363, 2156, 345, 1090, 359,...   \n",
              "4     [1, 7817, 556, 7, 11561, 359, 20322, 154, 4229...   \n",
              "...                                                 ...   \n",
              "1996  [1, 22630, 787, 26205, 4832, 6918, 598, 21591,...   \n",
              "1997  [1, 32687, 11162, 92, 3904, 3270, 7, 17668, 23...   \n",
              "1998  [1, 787, 1236, 18425, 625, 39808, 2860, 479, 3...   \n",
              "1999  [1, 787, 1236, 757, 24749, 4277, 19464, 24870,...   \n",
              "2000  [1, 1398, 128, 29, 99, 47, 64, 1057, 31, 787, ...   \n",
              "\n",
              "                                               label_id  \\\n",
              "0     [-100, 5, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "1     [-100, 0, 0, 0, 0, 0, 0, 5, 6, 0, 5, 0, 1, 2, ...   \n",
              "2     [-100, 0, 0, 0, 3, 4, 4, 4, 4, 0, 0, 0, 0, 0, ...   \n",
              "3     [-100, 0, 0, 0, 5, 0, 5, 6, 0, 0, 0, 0, 0, 0, ...   \n",
              "4     [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, ...   \n",
              "...                                                 ...   \n",
              "1996  [-100, 0, 0, 3, 0, 5, 6, 6, 0, 5, 6, 6, 0, 0, ...   \n",
              "1997  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, ...   \n",
              "1998  [-100, 5, 5, 6, 6, 6, 0, 0, 0, 0, 0, 5, 6, 6, ...   \n",
              "1999               [-100, 5, 5, 6, 6, 6, 0, 0, 0, -100]   \n",
              "2000  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 6, 6, 6, ...   \n",
              "\n",
              "                                               word_ids  \n",
              "0     [None, 0, 1, 1, 2, 3, 4, 4, 5, 6, 7, 8, 9, 10,...  \n",
              "1     [None, 0, 1, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10, 10...  \n",
              "2     [None, 0, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10,...  \n",
              "3     [None, 0, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10, 11...  \n",
              "4     [None, 0, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 9, 10,...  \n",
              "...                                                 ...  \n",
              "1996  [None, 0, 1, 2, 3, 4, 5, 5, 6, 7, 8, 8, 9, 10,...  \n",
              "1997  [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10...  \n",
              "1998  [None, 0, 1, 1, 1, 1, 2, 3, 4, 5, 6, 7, 7, 7, ...  \n",
              "1999               [None, 0, 1, 1, 1, 1, 2, 3, 4, None]  \n",
              "2000  [None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 8, 8, 8, ...  \n",
              "\n",
              "[2001 rows x 7 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWRSGLs4Gr32"
      },
      "source": [
        "## Building Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNHgo_2cGr32",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "class NerDataset(Dataset):\n",
        "    def __init__(self, df, device):\n",
        "        self.data = df.to_dict(orient='records')\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.data[item]\n",
        "\n",
        "    def collate_to_max_length(self, batch):\n",
        "        max_seq_length = max([len(s['id']) for s in batch])\n",
        "        batch = sorted(batch, key=lambda x: -len(x['id']))\n",
        "        seq_length = torch.tensor([len(x['id']) for x in batch])\n",
        "        input_ids = torch.tensor([x[\"id\"] + [0] * (max_seq_length - len(x['id'])) for x in batch]).to(self.device)\n",
        "        labels = torch.tensor([x[\"label_id\"] + [-100] * (max_seq_length - len(x['label_id'])) for x in batch]).to(self.device)\n",
        "        return {\"id\": input_ids, \"label_id\": labels, 'seq_length':seq_length, \"sample\":batch}\n",
        "\n",
        "\n",
        "dataset_train = NerDataset(train_df, Config.device)\n",
        "\n",
        "train_dataloader = DataLoader(dataset_train,\n",
        "                              sampler=RandomSampler(dataset_train),\n",
        "                              batch_size=Config.batch_size,\n",
        "                              drop_last=False,\n",
        "                              collate_fn=dataset_train.collate_to_max_length)\n",
        "\n",
        "\n",
        "\n",
        "dataset_valid = NerDataset(valid_df, Config.device)\n",
        "\n",
        "valid_dataloader = DataLoader(dataset_valid,\n",
        "                              sampler=RandomSampler(dataset_valid),\n",
        "                              batch_size=Config.batch_size,\n",
        "                              drop_last=False,\n",
        "                              collate_fn=dataset_valid.collate_to_max_length)\n",
        "\n",
        "\n",
        "dataset_test = NerDataset(test_df, Config.device)\n",
        "\n",
        "test_dataloader = DataLoader(dataset_test,\n",
        "                              sampler=RandomSampler(dataset_test),\n",
        "                              batch_size=Config.batch_size,\n",
        "                              drop_last=False,\n",
        "                              collate_fn=dataset_test.collate_to_max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5B33GkMGr32"
      },
      "source": [
        "## Building Custom loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp_D66RpGr32",
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "class L1_Loss:\n",
        "    def __init__(self):\n",
        "        self.l1_loss = nn.L1Loss()\n",
        "    def loss(self, target, logit, label_num):\n",
        "\n",
        "        target = target.view(-1)\n",
        "        logit = logit.view(-1, label_num)\n",
        "        mask = target.ne(-100).to(logit.device)\n",
        "        logit = torch.masked_select(logit, mask.unsqueeze(-1).expand_as(logit)).reshape(-1, label_num)\n",
        "        target = torch.masked_select(target, mask)\n",
        "\n",
        "        target = F.one_hot(target, num_classes=label_num)\n",
        "        return self.l1_loss(logit, target.float())\n",
        "\n",
        "\n",
        "class L2_Loss:\n",
        "    def __init__(self):\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "    def loss(self, target, logit,label_num):\n",
        "        target = target.view(-1)\n",
        "        logit = logit.view(-1, label_num)\n",
        "        mask = target.ne(-100).to(logit.device)\n",
        "        logit = torch.masked_select(logit, mask.unsqueeze(-1).expand_as(logit)).reshape(-1, label_num)\n",
        "        target = torch.masked_select(target, mask)\n",
        "\n",
        "        target = F.one_hot(target, num_classes=label_num)\n",
        "        loss = self.mse_loss(logit, target.float())\n",
        "        return loss\n",
        "\n",
        "class CE_Loss:\n",
        "    def __init__(self):\n",
        "        self.ce_loss = nn.CrossEntropyLoss(ignore_index=-100, reduce='mean')\n",
        "    def loss(self, target, logit, label_num):\n",
        "        return self.ce_loss(logit.reshape(-1, label_num), target.reshape(-1) )\n",
        "\n",
        "class KLDivergenceLoss:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def loss(self, target, logit, label_num):\n",
        "        target = target.view(-1)\n",
        "        logit = logit.view(-1, label_num)\n",
        "\n",
        "        mask = target.ne(-100).to(logit.device)\n",
        "        logit = torch.masked_select(logit, mask.unsqueeze(-1).expand_as(logit)).reshape(-1, label_num)\n",
        "        target = torch.masked_select(target, mask)\n",
        "\n",
        "        probs = F.softmax(logit, dim=-1)\n",
        "\n",
        "        # One-hot encode the targets to get true probabilities\n",
        "        true_probs = F.one_hot(target, num_classes=label_num).float()\n",
        "\n",
        "        mask_true_probs = true_probs > 0\n",
        "\n",
        "        # Calculate g function for non-zero elements using the mask\n",
        "        kl_values = torch.zeros_like(probs)\n",
        "        kl_values[mask_true_probs] = true_probs[mask_true_probs] * torch.log(true_probs[mask_true_probs]/probs[mask_true_probs])\n",
        "\n",
        "        # Sum over all classes and average over the batch size\n",
        "        loss = kl_values.sum(dim=-1).mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "# DLITE Loss function\n",
        "class DLITELoss:\n",
        "    def __init__(self):\n",
        "        super(DLITELoss, self).__init__()\n",
        "\n",
        "    def loss(self, targets, logits, label_num, epsilon=1e-10):\n",
        "        targets = targets.view(-1)\n",
        "        logits = logits.view(-1, label_num)\n",
        "\n",
        "        mask = targets.ne(-100).to(logits.device)\n",
        "        logits = torch.masked_select(logits, mask.unsqueeze(-1).expand_as(logits)).reshape(-1, label_num)\n",
        "        targets = torch.masked_select(targets, mask)\n",
        "\n",
        "        # Convert logits to probabilities using softmax\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # One-hot encode the targets to get true probabilities\n",
        "        true_probs = F.one_hot(targets, num_classes=probs.size(-1)).float()\n",
        "\n",
        "        # Define the g function\n",
        "        g_values = torch.abs(probs * (1 - torch.log(probs + epsilon)) - true_probs * (1 - torch.log(true_probs + epsilon)))\n",
        "\n",
        "        # Define the delta_h function\n",
        "        delta_h_values = torch.abs(probs**2 * (1 - 2 * torch.log(probs + epsilon)) - true_probs**2 * (1 - 2 * torch.log(true_probs + epsilon))) / (2 * (probs + true_probs))\n",
        "\n",
        "        # Compute DLITE loss for each class\n",
        "        dl_values = g_values - delta_h_values\n",
        "\n",
        "        # Sum over all classes and average over batch size\n",
        "        loss = dl_values.sum(dim=-1).mean()\n",
        "\n",
        "        return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0x9wjATtD7Y"
      },
      "source": [
        "## Adding Custom Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bf_rR5pGr32",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "class LSTMEncoder(nn.Module):\n",
        "    \"\"\"lstm encoder\n",
        "    \"\"\"\n",
        "    def __init__(self, config, hidden_size):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "        self.lstm = torch.nn.LSTM(hidden_size, hidden_size,\n",
        "                                  num_layers=config.lstm_layer_num, bidirectional=config.bi_lstm,\n",
        "                                  batch_first=True)\n",
        "\n",
        "    def forward(self, hidden_state, seq_length):\n",
        "        sequence_output = pack_padded_sequence(hidden_state, seq_length, batch_first=True)\n",
        "        sequence_output, (h_n, c_n) = self.lstm(sequence_output)\n",
        "        sequence_output, _ = pad_packed_sequence(sequence_output, batch_first=True)\n",
        "        return sequence_output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LinearResidualLayer(nn.Module):\n",
        "    def __init__(self, config, hidden_size, output_dim):\n",
        "        super(LinearResidualLayer, self).__init__()\n",
        "        self.config = config\n",
        "        self.linear_layer1 = nn.Linear(in_features=hidden_size, out_features=output_dim)\n",
        "        self.linear_layer2 = nn.Linear(in_features=output_dim, out_features=output_dim)\n",
        "        self.linear_layer3 = nn.Linear(in_features=output_dim, out_features=output_dim)\n",
        "        self.act_func = nn.ReLU()\n",
        "        if not self.config.only_use_residual:\n",
        "            self.dropout1 = nn.Dropout(config.dropout_prob[0])\n",
        "        self.ln_1 = nn.LayerNorm(output_dim)\n",
        "        if not self.config.only_use_residual:\n",
        "            self.dropout2 = nn.Dropout(config.dropout_prob[1])\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.act_func(self.linear_layer1(x))\n",
        "        # x = self.ln_1(x)\n",
        "\n",
        "        # y = self.dropout1(x) + self.dropout1(self.act_func(self.linear_layer2(x) )  )\n",
        "        # y = self.ln_2(y)\n",
        "        # z = self.dropout2(x) + self.dropout1(y) + self.dropout2(self.act_func(self.linear_layer3(y) )  )\n",
        "\n",
        "        x = self.act_func(self.linear_layer1(x))\n",
        "        x = self.ln_1(x)\n",
        "\n",
        "        if self.config.only_use_residual:\n",
        "            y = x + self.act_func(self.linear_layer2(x) )\n",
        "            z = x + y + self.act_func(self.linear_layer3(y) )\n",
        "        elif self.config.only_use_residual_and_dropout:\n",
        "            y = self.dropout1(x) + self.act_func(self.linear_layer2(x) )\n",
        "            z = self.dropout2(x) + self.dropout1(y) + self.act_func(self.linear_layer3(y) )\n",
        "        else:\n",
        "            assert ValueError(\"config error\")\n",
        "        return z\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Ner_Model(nn.Module):\n",
        "    def __init__(self,config, label_num, loss_name):\n",
        "        super(Ner_Model, self).__init__()\n",
        "        self.config = config\n",
        "        # deberat model\n",
        "        self.model = transformers.AutoModel.from_pretrained(config.model_name)\n",
        "\n",
        "        hidden_size = config.hidden_size\n",
        "\n",
        "        # using linear layer\n",
        "        linear_layer = []\n",
        "        if self.config.linear_layer_num  > 0:\n",
        "            # use linear layer\n",
        "            if self.config.only_use_standard_linear_layer:\n",
        "                for out_dim in config.linear_layer[0:config.linear_layer_num]:\n",
        "                    linear_layer.append( nn.Linear(in_features=hidden_size, out_features=out_dim) )\n",
        "                    linear_layer.append( nn.ReLU() )\n",
        "                    hidden_size = out_dim\n",
        "                self.linear_model = nn.Sequential(*linear_layer)\n",
        "            # just use dropout\n",
        "            elif self.config.only_use_dropout:\n",
        "                for i, out_dim in enumerate(config.linear_layer[0:config.linear_layer_num]):\n",
        "                    linear_layer.append( nn.Linear(in_features=hidden_size, out_features=out_dim) )\n",
        "                    linear_layer.append( nn.ReLU() )\n",
        "                    linear_layer.append( nn.Dropout(config.dropout_prob[i]) )\n",
        "                    hidden_size = out_dim\n",
        "                self.linear_model = nn.Sequential(*linear_layer)\n",
        "            else:\n",
        "                # use 3 linear layer for skip 2 dropout\n",
        "                assert config.linear_layer[0] == config.linear_layer[1] == config.linear_layer[2]\n",
        "                assert len(config.dropout_prob) == 2\n",
        "                self.linear_model = LinearResidualLayer(config, hidden_size,config.linear_layer[0])\n",
        "                hidden_size = config.linear_layer[0]\n",
        "\n",
        "        # whether to use lstm layer\n",
        "        if config.lstm_layer_num > 0:\n",
        "            self.lstm = LSTMEncoder(config,hidden_size)\n",
        "\n",
        "        # identify label number\n",
        "        self.label_num = label_num\n",
        "\n",
        "        # whether to use bi-lstm layer\n",
        "        if config.bi_lstm and config.lstm_layer_num > 0:\n",
        "            hidden_size = hidden_size * 2\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_size, label_num)\n",
        "\n",
        "        if loss_name == 'ce':\n",
        "            self.loss_func = CE_Loss()\n",
        "        elif loss_name == 'l1':\n",
        "            self.loss_func = L1_Loss()\n",
        "        elif loss_name == 'l2':\n",
        "            self.loss_func = L2_Loss()\n",
        "        elif loss_name == 'kl':\n",
        "            self.loss_func = KLDivergenceLoss()\n",
        "        elif loss_name == 'dlite':\n",
        "            self.loss_func = DLITELoss()\n",
        "        else:\n",
        "            assert 1==0\n",
        "\n",
        "        print(\"model configuration\")\n",
        "        print(\"%\" * 20)\n",
        "        print(self)\n",
        "        print(\"%\" * 20)\n",
        "\n",
        "    def forward(self, input_ids, seq_length, attention_mask, labels):\n",
        "        output = self.model(input_ids, attention_mask)\n",
        "        sequence_output = output[0]\n",
        "        if self.config.linear_layer_num > 0:\n",
        "            sequence_output = self.linear_model(sequence_output)\n",
        "\n",
        "        if self.config.lstm_layer_num > 0:\n",
        "            sequence_output = self.lstm(sequence_output, seq_length)\n",
        "\n",
        "        logit = self.classifier(sequence_output)\n",
        "        loss = self.loss_func.loss(labels, logit, len(label2id))\n",
        "        return loss, logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8F51tFmwGr32",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Building optimizer\n",
        "def get_optimizer(model, config):\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.1},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "\n",
        "    optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                      betas=(0.9, 0.98),\n",
        "                      lr=config.lr)\n",
        "    return optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uXq2R_yGr32"
      },
      "source": [
        "## Defining the training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDIJoYpWGr33",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate(model, data_loader, mode=\"Validation\"):\n",
        "    ground_truth, predict = [], []\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples = 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(data_loader):\n",
        "            attention_mask = batch[\"id\"].ne(0)\n",
        "            targets = batch['label_id']\n",
        "            loss, logit = model(batch[\"id\"], batch['seq_length'], attention_mask=attention_mask,\n",
        "                                             labels=targets)\n",
        "            eval_loss += loss.cpu().item()\n",
        "            if (step+1) % 100==0:\n",
        "                loss_step = eval_loss / (step+1)\n",
        "                print(f\"{mode} loss per 100 evaluation steps: {loss_step}\")\n",
        "\n",
        "            # compute training accuracy\n",
        "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = logit.view(-1, len(label2id)) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            active_accuracy = flattened_targets.ne(-100) # active accuracy is also of shape (batch_size * seq_len,)\n",
        "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "            eval_preds.extend(predictions.tolist())\n",
        "            eval_labels.extend(targets.tolist())\n",
        "\n",
        "    eval_loss = eval_loss / (step+1)\n",
        "    eval_accuracy = eval_accuracy / (step+1)\n",
        "    eval_labels,eval_preds = [id2label[i] for i in eval_labels], [id2label[i] for i in eval_preds]\n",
        "\n",
        "\n",
        "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(eval_labels, eval_preds, average='micro')\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(eval_labels, eval_preds, average='macro')\n",
        "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(eval_labels, eval_preds,average='weighted')\n",
        "\n",
        "    p_r_f1 = [[round(precision_micro,4), round(recall_micro,4), round(f1_micro,4)],\n",
        "              [round(precision_macro,4), round(recall_macro,4), round(f1_macro,4)],\n",
        "              [round(precision_weighted,4), round(recall_weighted,4), round(f1_weighted,4)]]\n",
        "\n",
        "    p_r_f1 = pd.DataFrame(p_r_f1, columns=['precision', 'recall', 'f1'], index=['micro', 'macro', 'weighted'])\n",
        "\n",
        "    print(f\"{mode} Loss: {eval_loss}\")\n",
        "    print(f\"{mode} Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    p_r_f1_each_label = classification_report(eval_labels, eval_preds)\n",
        "    print(f\"{mode} P-R-F1 for each label: \\n{p_r_f1_each_label}\")\n",
        "    print(f\"{mode} P-R-F1 tor all label: \\n{p_r_f1}\")\n",
        "    print(f\"{mode} steps: {(step+1)}\")\n",
        "    return eval_loss, p_r_f1, p_r_f1_each_label\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMapwwmhGr33"
      },
      "source": [
        "## Running under 5 custom loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzSeh-WWGr33",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "loss_list = ['l1', 'l2', 'ce', 'kl', 'dlite']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "71b8d11b1c2e4c39bab31f14c57a4a04",
            "eeea880059b343af93af6c814e8948be",
            "f89a020d4307401ea8c19b7b77d05cc4",
            "91bb307f4dac4698bf20439d15488ac3",
            "7fb9d257657749218960091e8497f534",
            "dba6a3d3055a43b799dd15ffc51292f4",
            "e6a10ce2ceb24a86af9ecfcfe64c1c89",
            "50287fd7f7174e67904755e2d0a106b9",
            "9db6ced7c2c246b7a179b40866da6b9c",
            "8d9d9027985142718e7cf8a06f58946d",
            "0b8ab7248a0545c09285b395e5e22ecb"
          ]
        },
        "id": "n6U0QqPKGr33",
        "outputId": "d08d3380-eadd-4e97-b867-ba0255cfd4a0",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "loss_name: l1\n",
            "model configuration\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Ner_Model(\n",
            "  (model): DebertaModel(\n",
            "    (embeddings): DebertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "      (LayerNorm): DebertaLayerNorm()\n",
            "      (dropout): StableDropout()\n",
            "    )\n",
            "    (encoder): DebertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x DebertaLayer(\n",
            "          (attention): DebertaAttention(\n",
            "            (self): DisentangledSelfAttention(\n",
            "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
            "              (pos_dropout): StableDropout()\n",
            "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "            (output): DebertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): DebertaLayerNorm()\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "          )\n",
            "          (intermediate): DebertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): DebertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): DebertaLayerNorm()\n",
            "            (dropout): StableDropout()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (rel_embeddings): Embedding(1024, 768)\n",
            "    )\n",
            "  )\n",
            "  (linear_model): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            "  (classifier): Linear(in_features=512, out_features=7, bias=True)\n",
            ")\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.0891808272851631\n",
            "Training loss per 100 training steps: 0.0631014237506315\n",
            "Training loss per 100 training steps: 0.05316328850652401\n",
            "Training loss per 100 training steps: 0.048350856996839865\n",
            "Training loss per 100 training steps: 0.04578374223271385\n",
            "Training loss per 100 training steps: 0.04268587043935743\n",
            "Training loss per 100 training steps: 0.04028569186871339\n",
            "Training loss per 100 training steps: 0.038315572442370466\n",
            "Training loss per 100 training steps: 0.03667015463165525\n",
            "Training loss per 100 training steps: 0.03558047703769989\n",
            "Training loss per 100 training steps: 0.034599217225543474\n",
            "Training loss per 100 training steps: 0.03370058181482212\n",
            "Training loss per 100 training steps: 0.03309074075659737\n",
            "Training loss epoch: 0.032758202100667946\n",
            "Training accuracy epoch: 0.8829974295309042\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.032700107134878635\n",
            "Validation loss per 100 evaluation steps: 0.035239015880506486\n",
            "Validation loss per 100 evaluation steps: 0.0352739704788352\n",
            "Validation loss per 100 evaluation steps: 0.03536975947819883\n",
            "Validation loss per 100 evaluation steps: 0.03592895360244438\n",
            "Validation Loss: 0.03592895360244438\n",
            "Validation Accuracy: 0.8862713407071079\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.00      0.00      0.00       309\n",
            "       B-ORG       0.00      0.00      0.00       736\n",
            "       B-PER       0.52      0.40      0.45      2323\n",
            "       I-LOC       0.00      0.00      0.00       431\n",
            "       I-ORG       0.20      0.03      0.05      1032\n",
            "       I-PER       0.57      0.76      0.65      4121\n",
            "           O       0.94      0.98      0.96     41492\n",
            "\n",
            "    accuracy                           0.89     50444\n",
            "   macro avg       0.32      0.31      0.30     50444\n",
            "weighted avg       0.85      0.89      0.87     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.8864  0.8864  0.8864\n",
            "macro        0.3189  0.3092  0.3015\n",
            "weighted     0.8513  0.8864  0.8658\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.03160438223043457\n",
            "Test loss per 100 evaluation steps: 0.03335338474425953\n",
            "Test loss per 100 evaluation steps: 0.03329517213744111\n",
            "Test loss per 100 evaluation steps: 0.0332409500257927\n",
            "Test loss per 100 evaluation steps: 0.03363787544867955\n",
            "Test Loss: 0.03363012435809782\n",
            "Test Accuracy: 0.8878879389794991\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.00      0.00      0.00       636\n",
            "       B-ORG       0.00      0.00      0.00      1090\n",
            "       B-PER       0.42      0.40      0.41      2650\n",
            "       I-LOC       0.00      0.00      0.00       835\n",
            "       I-ORG       0.30      0.06      0.10      1401\n",
            "       I-PER       0.55      0.84      0.67      4266\n",
            "           O       0.96      0.99      0.97     48594\n",
            "\n",
            "    accuracy                           0.89     59472\n",
            "   macro avg       0.32      0.33      0.31     59472\n",
            "weighted avg       0.85      0.89      0.86     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.8891  0.8891  0.8891\n",
            "macro        0.3201  0.3281  0.3078\n",
            "weighted     0.8496  0.8891  0.8650\n",
            "Test steps: 501\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.02487007139250636\n",
            "Training loss per 100 training steps: 0.024759358405135573\n",
            "Training loss per 100 training steps: 0.024488299208848426\n",
            "Training loss per 100 training steps: 0.02401029511645902\n",
            "Training loss per 100 training steps: 0.023896650241804308\n",
            "Training loss per 100 training steps: 0.0236966643469835\n",
            "Training loss per 100 training steps: 0.02386787308870615\n",
            "Training loss per 100 training steps: 0.02385414907024824\n",
            "Training loss per 100 training steps: 0.023890194675510026\n",
            "Training loss per 100 training steps: 0.023945922780781986\n",
            "Training loss per 100 training steps: 0.023947183651722628\n",
            "Training loss per 100 training steps: 0.023759530953878616\n",
            "Training loss per 100 training steps: 0.023545760660871076\n",
            "Training loss epoch: 0.0235051991240983\n",
            "Training accuracy epoch: 0.902967434113471\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.03849722129292786\n",
            "Validation loss per 100 evaluation steps: 0.037760764309205115\n",
            "Validation loss per 100 evaluation steps: 0.037440799652443575\n",
            "Validation loss per 100 evaluation steps: 0.03708506728813518\n",
            "Validation loss per 100 evaluation steps: 0.03696178404218517\n",
            "Validation Loss: 0.03696178404218517\n",
            "Validation Accuracy: 0.8812612263934384\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.00      0.00      0.00       309\n",
            "       B-ORG       0.00      0.00      0.00       736\n",
            "       B-PER       0.00      0.00      0.00      2323\n",
            "       I-LOC       0.00      0.00      0.00       431\n",
            "       I-ORG       0.00      0.00      0.00      1032\n",
            "       I-PER       0.56      0.83      0.67      4121\n",
            "           O       0.93      0.99      0.96     41492\n",
            "\n",
            "    accuracy                           0.88     50444\n",
            "   macro avg       0.21      0.26      0.23     50444\n",
            "weighted avg       0.81      0.88      0.84     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.8818  0.8818  0.8818\n",
            "macro        0.2124  0.2601  0.2324\n",
            "weighted     0.8075  0.8818  0.8416\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.036748704239726065\n",
            "Test loss per 100 evaluation steps: 0.03657447449862957\n",
            "Test loss per 100 evaluation steps: 0.036109252165382105\n",
            "Test loss per 100 evaluation steps: 0.03541619624884333\n",
            "Test loss per 100 evaluation steps: 0.03564958727033809\n",
            "Test Loss: 0.03558055253698492\n",
            "Test Accuracy: 0.8759049730548124\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.00      0.00      0.00       636\n",
            "       B-ORG       0.00      0.00      0.00      1090\n",
            "       B-PER       0.00      0.00      0.00      2650\n",
            "       I-LOC       0.00      0.00      0.00       835\n",
            "       I-ORG       0.00      0.00      0.00      1401\n",
            "       I-PER       0.48      0.92      0.63      4266\n",
            "           O       0.94      0.99      0.97     48594\n",
            "\n",
            "    accuracy                           0.88     59472\n",
            "   macro avg       0.20      0.27      0.23     59472\n",
            "weighted avg       0.80      0.88      0.83     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.8773  0.8773  0.8773\n",
            "macro        0.2027  0.2735  0.2279\n",
            "weighted     0.8035  0.8773  0.8348\n",
            "Test steps: 501\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.022018511301139368\n",
            "Training loss per 100 training steps: 0.022230130847892726\n",
            "Training loss per 100 training steps: 0.023398409500950946\n",
            "Training loss per 100 training steps: 0.023347915091726464\n",
            "Training loss per 100 training steps: 0.023484816411859354\n",
            "Training loss per 100 training steps: 0.023093798645810844\n",
            "Training loss per 100 training steps: 0.023148274488464397\n",
            "Training loss per 100 training steps: 0.022770409898803336\n",
            "Training loss per 100 training steps: 0.022726199379992776\n",
            "Training loss per 100 training steps: 0.02266506992385257\n",
            "Training loss per 100 training steps: 0.02280939029434442\n",
            "Training loss per 100 training steps: 0.022632097766181688\n",
            "Training loss per 100 training steps: 0.02255025441109095\n",
            "Training loss epoch: 0.02254922485697582\n",
            "Training accuracy epoch: 0.91237151442188\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.028938005634117872\n",
            "Validation loss per 100 evaluation steps: 0.0290184581457288\n",
            "Validation loss per 100 evaluation steps: 0.02853814118774608\n",
            "Validation loss per 100 evaluation steps: 0.02886180379646248\n",
            "Validation loss per 100 evaluation steps: 0.029230169587652198\n",
            "Validation Loss: 0.029230169587652198\n",
            "Validation Accuracy: 0.9179826417973571\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.26      0.56      0.35       309\n",
            "       B-ORG       0.00      0.00      0.00       736\n",
            "       B-PER       0.81      0.70      0.75      2323\n",
            "       I-LOC       0.42      0.20      0.27       431\n",
            "       I-ORG       0.16      0.01      0.01      1032\n",
            "       I-PER       0.70      0.90      0.79      4121\n",
            "           O       0.96      0.98      0.97     41492\n",
            "\n",
            "    accuracy                           0.92     50444\n",
            "   macro avg       0.47      0.48      0.45     50444\n",
            "weighted avg       0.90      0.92      0.90     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9177  0.9177  0.9177\n",
            "macro        0.4733  0.4787  0.4500\n",
            "weighted     0.8957  0.9177  0.9033\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.02733263948466629\n",
            "Test loss per 100 evaluation steps: 0.028753124322975054\n",
            "Test loss per 100 evaluation steps: 0.028467264277860522\n",
            "Test loss per 100 evaluation steps: 0.028214475252316334\n",
            "Test loss per 100 evaluation steps: 0.0283763102167286\n",
            "Test Loss: 0.028352011120162474\n",
            "Test Accuracy: 0.9178313340001879\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.34      0.61      0.43       636\n",
            "       B-ORG       0.00      0.00      0.00      1090\n",
            "       B-PER       0.82      0.68      0.74      2650\n",
            "       I-LOC       0.51      0.28      0.36       835\n",
            "       I-ORG       0.44      0.03      0.05      1401\n",
            "       I-PER       0.68      0.93      0.78      4266\n",
            "           O       0.97      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.92     59472\n",
            "   macro avg       0.54      0.50      0.48     59472\n",
            "weighted avg       0.90      0.92      0.90     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9178  0.9178  0.9178\n",
            "macro        0.5367  0.5018  0.4787\n",
            "weighted     0.8977  0.9178  0.9007\n",
            "Test steps: 501\n",
            "Training epoch: 4\n",
            "Training loss per 100 training steps: 0.018635992904892193\n",
            "Training loss per 100 training steps: 0.0183148356404854\n",
            "Training loss per 100 training steps: 0.018980514943250454\n",
            "Training loss per 100 training steps: 0.0190651511438773\n",
            "Training loss per 100 training steps: 0.01894492228364106\n",
            "Training loss per 100 training steps: 0.01943892158868645\n",
            "Training loss per 100 training steps: 0.019368462112449508\n",
            "Training loss per 100 training steps: 0.019154007255201576\n",
            "Training loss per 100 training steps: 0.01899036636420836\n",
            "Training loss per 100 training steps: 0.01902992900426034\n",
            "Training loss per 100 training steps: 0.018953282807117583\n",
            "Training loss per 100 training steps: 0.018849904397769325\n",
            "Training loss per 100 training steps: 0.018913536118141984\n",
            "Training loss epoch: 0.018991772614899947\n",
            "Training accuracy epoch: 0.920862557395361\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.029010232649161482\n",
            "Validation loss per 100 evaluation steps: 0.028256293061422185\n",
            "Validation loss per 100 evaluation steps: 0.026973186956408123\n",
            "Validation loss per 100 evaluation steps: 0.02671947663242463\n",
            "Validation loss per 100 evaluation steps: 0.02702165105845779\n",
            "Validation Loss: 0.02702165105845779\n",
            "Validation Accuracy: 0.9144328589929009\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.23      0.69      0.34       309\n",
            "       B-ORG       0.00      0.00      0.00       736\n",
            "       B-PER       0.72      0.79      0.75      2323\n",
            "       I-LOC       0.75      0.01      0.01       431\n",
            "       I-ORG       0.00      0.00      0.00      1032\n",
            "       I-PER       0.75      0.92      0.83      4121\n",
            "           O       0.96      0.97      0.96     41492\n",
            "\n",
            "    accuracy                           0.91     50444\n",
            "   macro avg       0.49      0.48      0.41     50444\n",
            "weighted avg       0.89      0.91      0.90     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9133  0.9133  0.9133\n",
            "macro        0.4865  0.4821  0.4144\n",
            "weighted     0.8919  0.9133  0.8980\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.025836793328635393\n",
            "Test loss per 100 evaluation steps: 0.02590644974494353\n",
            "Test loss per 100 evaluation steps: 0.02558133098607262\n",
            "Test loss per 100 evaluation steps: 0.025335666574537754\n",
            "Test loss per 100 evaluation steps: 0.025424536257167347\n",
            "Test Loss: 0.025531752647654508\n",
            "Test Accuracy: 0.9183269055397576\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.26      0.75      0.39       636\n",
            "       B-ORG       0.00      0.00      0.00      1090\n",
            "       B-PER       0.77      0.92      0.84      2650\n",
            "       I-LOC       0.56      0.01      0.01       835\n",
            "       I-ORG       0.00      0.00      0.00      1401\n",
            "       I-PER       0.79      0.94      0.86      4266\n",
            "           O       0.97      0.98      0.97     48594\n",
            "\n",
            "    accuracy                           0.92     59472\n",
            "   macro avg       0.48      0.51      0.44     59472\n",
            "weighted avg       0.89      0.92      0.90     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9193  0.9193  0.9193\n",
            "macro        0.4774  0.5141  0.4382\n",
            "weighted     0.8916  0.9193  0.8996\n",
            "Test steps: 501\n",
            "Training epoch: 5\n",
            "Training loss per 100 training steps: 0.01774839703401085\n",
            "Training loss per 100 training steps: 0.01801445080753183\n",
            "Training loss per 100 training steps: 0.017467687566337795\n",
            "Training loss per 100 training steps: 0.017778780822118277\n",
            "Training loss per 100 training steps: 0.01775490878103301\n",
            "Training loss per 100 training steps: 0.017830145745926226\n",
            "Training loss per 100 training steps: 0.018097177925519646\n",
            "Training loss per 100 training steps: 0.018129472547007027\n",
            "Training loss per 100 training steps: 0.018085563251839226\n",
            "Training loss per 100 training steps: 0.018160855212656316\n",
            "Training loss per 100 training steps: 0.01794467489679598\n",
            "Training loss per 100 training steps: 0.017812998574445373\n",
            "Training loss per 100 training steps: 0.01787783587649196\n",
            "Training loss epoch: 0.01784752849727077\n",
            "Training accuracy epoch: 0.9258351614969887\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.022462389229331164\n",
            "Validation loss per 100 evaluation steps: 0.02271664415835403\n",
            "Validation loss per 100 evaluation steps: 0.02252033956969778\n",
            "Validation loss per 100 evaluation steps: 0.023355942188791234\n",
            "Validation loss per 100 evaluation steps: 0.022836054107989185\n",
            "Validation Loss: 0.022836054107989185\n",
            "Validation Accuracy: 0.9203099737205599\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.22      0.64      0.33       309\n",
            "       B-ORG       0.00      0.00      0.00       736\n",
            "       B-PER       0.82      0.66      0.73      2323\n",
            "       I-LOC       0.50      0.00      0.01       431\n",
            "       I-ORG       0.00      0.00      0.00      1032\n",
            "       I-PER       0.86      0.85      0.86      4121\n",
            "           O       0.94      0.99      0.97     41492\n",
            "\n",
            "    accuracy                           0.92     50444\n",
            "   macro avg       0.48      0.45      0.41     50444\n",
            "weighted avg       0.89      0.92      0.90     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9195  0.9195  0.9195\n",
            "macro        0.4786  0.4503  0.4135\n",
            "weighted     0.8900  0.9195  0.9011\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.021358415007125587\n",
            "Test loss per 100 evaluation steps: 0.02047815651661949\n",
            "Test loss per 100 evaluation steps: 0.02066745645541232\n",
            "Test loss per 100 evaluation steps: 0.020672012949798955\n",
            "Test loss per 100 evaluation steps: 0.021066194881103003\n",
            "Test Loss: 0.02110030309159119\n",
            "Test Accuracy: 0.9205377610419174\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.21      0.75      0.33       636\n",
            "       B-ORG       0.00      0.00      0.00      1090\n",
            "       B-PER       0.83      0.82      0.83      2650\n",
            "       I-LOC       0.30      0.01      0.01       835\n",
            "       I-ORG       0.80      0.00      0.01      1401\n",
            "       I-PER       0.84      0.91      0.87      4266\n",
            "           O       0.96      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.92     59472\n",
            "   macro avg       0.56      0.50      0.43     59472\n",
            "weighted avg       0.91      0.92      0.90     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9202  0.9202  0.9202\n",
            "macro        0.5642  0.4973  0.4330\n",
            "weighted     0.9103  0.9202  0.9021\n",
            "Test steps: 501\n",
            "Training epoch: 6\n",
            "Training loss per 100 training steps: 0.01731696400733199\n",
            "Training loss per 100 training steps: 0.016207979153550697\n",
            "Training loss per 100 training steps: 0.016172047100456742\n",
            "Training loss per 100 training steps: 0.016272683110437357\n",
            "Training loss per 100 training steps: 0.016355271459848154\n",
            "Training loss per 100 training steps: 0.016257385171144657\n",
            "Training loss per 100 training steps: 0.016164253083032754\n",
            "Training loss per 100 training steps: 0.016083391391366605\n",
            "Training loss per 100 training steps: 0.016085340235472863\n",
            "Training loss per 100 training steps: 0.01574663491631509\n",
            "Training loss per 100 training steps: 0.015720904675783295\n",
            "Training loss per 100 training steps: 0.015729472874833544\n",
            "Training loss per 100 training steps: 0.015800267363105937\n",
            "Training loss epoch: 0.01586059314904696\n",
            "Training accuracy epoch: 0.9324182228126698\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.021771033907425588\n",
            "Validation loss per 100 evaluation steps: 0.02215720982552739\n",
            "Validation loss per 100 evaluation steps: 0.02227982476101412\n",
            "Validation loss per 100 evaluation steps: 0.022997464802319883\n",
            "Validation loss per 100 evaluation steps: 0.02254829584516119\n",
            "Validation Loss: 0.02254829584516119\n",
            "Validation Accuracy: 0.9224267944057785\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.51      0.45      0.47       309\n",
            "       B-ORG       0.00      0.00      0.00       736\n",
            "       B-PER       0.82      0.70      0.75      2323\n",
            "       I-LOC       0.00      0.00      0.00       431\n",
            "       I-ORG       0.24      0.12      0.16      1032\n",
            "       I-PER       0.82      0.87      0.85      4121\n",
            "           O       0.95      0.99      0.97     41492\n",
            "\n",
            "    accuracy                           0.92     50444\n",
            "   macro avg       0.48      0.45      0.46     50444\n",
            "weighted avg       0.89      0.92      0.90     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9205  0.9205  0.9205\n",
            "macro        0.4762  0.4460  0.4574\n",
            "weighted     0.8909  0.9205  0.9047\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.020440184333128854\n",
            "Test loss per 100 evaluation steps: 0.020664228546083904\n",
            "Test loss per 100 evaluation steps: 0.020460959513438865\n",
            "Test loss per 100 evaluation steps: 0.01983076724107377\n",
            "Test loss per 100 evaluation steps: 0.02016572301206179\n",
            "Test Loss: 0.020127149925245617\n",
            "Test Accuracy: 0.9277189609904151\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.55      0.48      0.51       636\n",
            "       B-ORG       0.00      0.00      0.00      1090\n",
            "       B-PER       0.81      0.88      0.85      2650\n",
            "       I-LOC       0.00      0.00      0.00       835\n",
            "       I-ORG       0.26      0.27      0.26      1401\n",
            "       I-PER       0.82      0.93      0.87      4266\n",
            "           O       0.97      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.93     59472\n",
            "   macro avg       0.49      0.51      0.50     59472\n",
            "weighted avg       0.90      0.93      0.91     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9274  0.9274  0.9274\n",
            "macro        0.4871  0.5071  0.4960\n",
            "weighted     0.8980  0.9274  0.9123\n",
            "Test steps: 501\n",
            "Training epoch: 7\n",
            "Training loss per 100 training steps: 0.019828955338161904\n",
            "Training loss per 100 training steps: 0.01863400745976833\n",
            "Training loss per 100 training steps: 0.017683292988998196\n",
            "Training loss per 100 training steps: 0.017615737918094964\n",
            "Training loss per 100 training steps: 0.017366868981160223\n",
            "Training loss per 100 training steps: 0.017034964816169427\n",
            "Training loss per 100 training steps: 0.01670452892622312\n",
            "Training loss per 100 training steps: 0.016511461452719232\n",
            "Training loss per 100 training steps: 0.01638718142390846\n",
            "Training loss per 100 training steps: 0.01603955924510956\n",
            "Training loss per 100 training steps: 0.015989549270439468\n",
            "Training loss per 100 training steps: 0.016007576458117304\n",
            "Training loss per 100 training steps: 0.015924997929228434\n",
            "Training loss epoch: 0.015950972310503707\n",
            "Training accuracy epoch: 0.9324841404119639\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.026865696366876364\n",
            "Validation loss per 100 evaluation steps: 0.026844639827613717\n",
            "Validation loss per 100 evaluation steps: 0.02685558544510665\n",
            "Validation loss per 100 evaluation steps: 0.026111173015669918\n",
            "Validation loss per 100 evaluation steps: 0.0261028316100128\n",
            "Validation Loss: 0.0261028316100128\n",
            "Validation Accuracy: 0.8879580295464421\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.00      0.00      0.00       309\n",
            "       B-ORG       0.00      0.00      0.00       736\n",
            "       B-PER       0.92      0.43      0.59      2323\n",
            "       I-LOC       0.09      0.63      0.16       431\n",
            "       I-ORG       0.00      0.00      0.00      1032\n",
            "       I-PER       0.95      0.60      0.74      4121\n",
            "           O       0.94      0.99      0.96     41492\n",
            "\n",
            "    accuracy                           0.89     50444\n",
            "   macro avg       0.41      0.38      0.35     50444\n",
            "weighted avg       0.89      0.89      0.88     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.8875  0.8875  0.8875\n",
            "macro        0.4149  0.3791  0.3498\n",
            "weighted     0.8924  0.8875  0.8804\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.026758768213912845\n",
            "Test loss per 100 evaluation steps: 0.023978975888167043\n",
            "Test loss per 100 evaluation steps: 0.02376425215838632\n",
            "Test loss per 100 evaluation steps: 0.02364854033890879\n",
            "Test loss per 100 evaluation steps: 0.02396269069169648\n",
            "Test Loss: 0.023967106991350844\n",
            "Test Accuracy: 0.8920768475595673\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.00      0.00      0.00       636\n",
            "       B-ORG       0.00      0.00      0.00      1090\n",
            "       B-PER       0.95      0.58      0.72      2650\n",
            "       I-LOC       0.14      0.79      0.24       835\n",
            "       I-ORG       0.00      0.00      0.00      1401\n",
            "       I-PER       0.96      0.67      0.79      4266\n",
            "           O       0.96      0.99      0.97     48594\n",
            "\n",
            "    accuracy                           0.89     59472\n",
            "   macro avg       0.43      0.43      0.39     59472\n",
            "weighted avg       0.89      0.89      0.89     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.8929  0.8929  0.8929\n",
            "macro        0.4301  0.4328  0.3895\n",
            "weighted     0.8943  0.8929  0.8864\n",
            "Test steps: 501\n",
            "====================================================================================================\n",
            "loss_name: l2\n",
            "model configuration\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Ner_Model(\n",
            "  (model): DebertaModel(\n",
            "    (embeddings): DebertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "      (LayerNorm): DebertaLayerNorm()\n",
            "      (dropout): StableDropout()\n",
            "    )\n",
            "    (encoder): DebertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x DebertaLayer(\n",
            "          (attention): DebertaAttention(\n",
            "            (self): DisentangledSelfAttention(\n",
            "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
            "              (pos_dropout): StableDropout()\n",
            "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "            (output): DebertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): DebertaLayerNorm()\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "          )\n",
            "          (intermediate): DebertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): DebertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): DebertaLayerNorm()\n",
            "            (dropout): StableDropout()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (rel_embeddings): Embedding(1024, 768)\n",
            "    )\n",
            "  )\n",
            "  (linear_model): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            "  (classifier): Linear(in_features=512, out_features=7, bias=True)\n",
            ")\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.0418267334648408\n",
            "Training loss per 100 training steps: 0.031305506695644\n",
            "Training loss per 100 training steps: 0.025628630394639914\n",
            "Training loss per 100 training steps: 0.02311084850320185\n",
            "Training loss per 100 training steps: 0.02074476720194798\n",
            "Training loss per 100 training steps: 0.019177865629996328\n",
            "Training loss per 100 training steps: 0.01797623519669287\n",
            "Training loss per 100 training steps: 0.016883494238518326\n",
            "Training loss per 100 training steps: 0.016170888136305923\n",
            "Training loss per 100 training steps: 0.015555368678891682\n",
            "Training loss per 100 training steps: 0.015033095339194088\n",
            "Training loss per 100 training steps: 0.014558563542535315\n",
            "Training loss per 100 training steps: 0.014107508080935357\n",
            "Training loss epoch: 0.013969150277191237\n",
            "Training accuracy epoch: 0.9412843426987072\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.013428540155873634\n",
            "Validation loss per 100 evaluation steps: 0.014433193606964777\n",
            "Validation loss per 100 evaluation steps: 0.01492069862249385\n",
            "Validation loss per 100 evaluation steps: 0.014741334506179555\n",
            "Validation loss per 100 evaluation steps: 0.014668620675918646\n",
            "Validation Loss: 0.014668620675918646\n",
            "Validation Accuracy: 0.9305374960694611\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.82      0.60      0.70       309\n",
            "       B-ORG       0.57      0.57      0.57       736\n",
            "       B-PER       0.93      0.62      0.75      2323\n",
            "       I-LOC       0.66      0.50      0.57       431\n",
            "       I-ORG       0.47      0.52      0.49      1032\n",
            "       I-PER       0.94      0.75      0.83      4121\n",
            "           O       0.95      0.99      0.97     41492\n",
            "\n",
            "    accuracy                           0.93     50444\n",
            "   macro avg       0.76      0.65      0.70     50444\n",
            "weighted avg       0.93      0.93      0.93     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9303  0.9303  0.9303\n",
            "macro        0.7647  0.6509  0.6976\n",
            "weighted     0.9301  0.9303  0.9274\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.01217224241234362\n",
            "Test loss per 100 evaluation steps: 0.012452335316047537\n",
            "Test loss per 100 evaluation steps: 0.011998743848719944\n",
            "Test loss per 100 evaluation steps: 0.012143282585602719\n",
            "Test loss per 100 evaluation steps: 0.012289844277896918\n",
            "Test Loss: 0.012323492397754054\n",
            "Test Accuracy: 0.9438638198866559\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.82      0.65      0.73       636\n",
            "       B-ORG       0.59      0.78      0.67      1090\n",
            "       B-PER       0.95      0.81      0.87      2650\n",
            "       I-LOC       0.78      0.56      0.65       835\n",
            "       I-ORG       0.51      0.69      0.59      1401\n",
            "       I-PER       0.95      0.82      0.88      4266\n",
            "           O       0.97      0.98      0.98     48594\n",
            "\n",
            "    accuracy                           0.94     59472\n",
            "   macro avg       0.80      0.76      0.77     59472\n",
            "weighted avg       0.95      0.94      0.95     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9447  0.9447  0.9447\n",
            "macro        0.7972  0.7562  0.7674\n",
            "weighted     0.9492  0.9447  0.9454\n",
            "Test steps: 501\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.007409976568087586\n",
            "Training loss per 100 training steps: 0.007798526788246818\n",
            "Training loss per 100 training steps: 0.0075546107174886854\n",
            "Training loss per 100 training steps: 0.007702785404890164\n",
            "Training loss per 100 training steps: 0.007567303109375643\n",
            "Training loss per 100 training steps: 0.007610324150737142\n",
            "Training loss per 100 training steps: 0.007533898592435954\n",
            "Training loss per 100 training steps: 0.007410561885158131\n",
            "Training loss per 100 training steps: 0.007488844041310788\n",
            "Training loss per 100 training steps: 0.007607186818295304\n",
            "Training loss per 100 training steps: 0.007524903952888218\n",
            "Training loss per 100 training steps: 0.0074194930685462165\n",
            "Training loss per 100 training steps: 0.007415731856915675\n",
            "Training loss epoch: 0.007356239534719487\n",
            "Training accuracy epoch: 0.9674529461498479\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.012269352849834832\n",
            "Validation loss per 100 evaluation steps: 0.013016068589531643\n",
            "Validation loss per 100 evaluation steps: 0.013847866429835752\n",
            "Validation loss per 100 evaluation steps: 0.014148376867815386\n",
            "Validation loss per 100 evaluation steps: 0.013958987158555829\n",
            "Validation Loss: 0.013958987158555829\n",
            "Validation Accuracy: 0.9377147406696414\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.83      0.61      0.71       309\n",
            "       B-ORG       0.75      0.36      0.49       736\n",
            "       B-PER       0.91      0.64      0.75      2323\n",
            "       I-LOC       0.77      0.47      0.59       431\n",
            "       I-ORG       0.70      0.30      0.42      1032\n",
            "       I-PER       0.92      0.85      0.88      4121\n",
            "           O       0.94      1.00      0.97     41492\n",
            "\n",
            "    accuracy                           0.94     50444\n",
            "   macro avg       0.83      0.60      0.69     50444\n",
            "weighted avg       0.93      0.94      0.93     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9372  0.9372  0.9372\n",
            "macro        0.8319  0.6044  0.6859\n",
            "weighted     0.9314  0.9372  0.9295\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.01125931748529183\n",
            "Test loss per 100 evaluation steps: 0.01119404024781943\n",
            "Test loss per 100 evaluation steps: 0.011394917494368807\n",
            "Test loss per 100 evaluation steps: 0.011300435526277398\n",
            "Test loss per 100 evaluation steps: 0.011026255862318066\n",
            "Test Loss: 0.01103694367170075\n",
            "Test Accuracy: 0.9512438174736422\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.80      0.71      0.75       636\n",
            "       B-ORG       0.76      0.61      0.68      1090\n",
            "       B-PER       0.93      0.81      0.86      2650\n",
            "       I-LOC       0.83      0.59      0.69       835\n",
            "       I-ORG       0.73      0.51      0.60      1401\n",
            "       I-PER       0.94      0.89      0.91      4266\n",
            "           O       0.96      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.95     59472\n",
            "   macro avg       0.85      0.73      0.78     59472\n",
            "weighted avg       0.95      0.95      0.95     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9513  0.9513  0.9513\n",
            "macro        0.8498  0.7297  0.7816\n",
            "weighted     0.9478  0.9513  0.9482\n",
            "Test steps: 501\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.004618238563234627\n",
            "Training loss per 100 training steps: 0.004994847266716534\n",
            "Training loss per 100 training steps: 0.0053054664192071265\n",
            "Training loss per 100 training steps: 0.0056132255463489855\n",
            "Training loss per 100 training steps: 0.005775289537967182\n",
            "Training loss per 100 training steps: 0.005700863196828626\n",
            "Training loss per 100 training steps: 0.005731019619556069\n",
            "Training loss per 100 training steps: 0.005743411881421707\n",
            "Training loss per 100 training steps: 0.005675716084547781\n",
            "Training loss per 100 training steps: 0.005617935911963286\n",
            "Training loss per 100 training steps: 0.0056055189755583835\n",
            "Training loss per 100 training steps: 0.0055593537117601954\n",
            "Training loss per 100 training steps: 0.0055060103331785425\n",
            "Training loss epoch: 0.005498596497775733\n",
            "Training accuracy epoch: 0.976295542908077\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.013586448582136654\n",
            "Validation loss per 100 evaluation steps: 0.013002584241658042\n",
            "Validation loss per 100 evaluation steps: 0.012913243701453515\n",
            "Validation loss per 100 evaluation steps: 0.013251057312245393\n",
            "Validation loss per 100 evaluation steps: 0.013155714282300323\n",
            "Validation Loss: 0.013155714282300323\n",
            "Validation Accuracy: 0.9411712991290854\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.75      0.61      0.67       309\n",
            "       B-ORG       0.71      0.43      0.54       736\n",
            "       B-PER       0.91      0.71      0.80      2323\n",
            "       I-LOC       0.59      0.47      0.52       431\n",
            "       I-ORG       0.61      0.43      0.51      1032\n",
            "       I-PER       0.91      0.87      0.89      4121\n",
            "           O       0.96      0.99      0.97     41492\n",
            "\n",
            "    accuracy                           0.94     50444\n",
            "   macro avg       0.78      0.64      0.70     50444\n",
            "weighted avg       0.94      0.94      0.94     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9406  0.9406  0.9406\n",
            "macro        0.7755  0.6445  0.6993\n",
            "weighted     0.9356  0.9406  0.9364\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.011526366327016148\n",
            "Test loss per 100 evaluation steps: 0.010614547114892047\n",
            "Test loss per 100 evaluation steps: 0.010640964806758954\n",
            "Test loss per 100 evaluation steps: 0.01033507253147036\n",
            "Test loss per 100 evaluation steps: 0.010312404358468485\n",
            "Test Loss: 0.010298208554938506\n",
            "Test Accuracy: 0.955573728002151\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.82      0.71      0.76       636\n",
            "       B-ORG       0.74      0.67      0.71      1090\n",
            "       B-PER       0.93      0.88      0.91      2650\n",
            "       I-LOC       0.81      0.62      0.71       835\n",
            "       I-ORG       0.64      0.61      0.62      1401\n",
            "       I-PER       0.94      0.90      0.92      4266\n",
            "           O       0.97      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.96     59472\n",
            "   macro avg       0.84      0.77      0.80     59472\n",
            "weighted avg       0.95      0.96      0.95     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9562  0.9562  0.9562\n",
            "macro        0.8380  0.7698  0.8011\n",
            "weighted     0.9544  0.9562  0.9549\n",
            "Test steps: 501\n",
            "Training epoch: 4\n",
            "Training loss per 100 training steps: 0.0034922289052337872\n",
            "Training loss per 100 training steps: 0.004006620076725084\n",
            "Training loss per 100 training steps: 0.004037953052550923\n",
            "Training loss per 100 training steps: 0.004090254788552556\n",
            "Training loss per 100 training steps: 0.004309893063255004\n",
            "Training loss per 100 training steps: 0.004347866006743667\n",
            "Training loss per 100 training steps: 0.004366466456574147\n",
            "Training loss per 100 training steps: 0.00433480506639171\n",
            "Training loss per 100 training steps: 0.004368606663233045\n",
            "Training loss per 100 training steps: 0.004313981190025515\n",
            "Training loss per 100 training steps: 0.004361993679088879\n",
            "Training loss per 100 training steps: 0.004305393172318569\n",
            "Training loss per 100 training steps: 0.004302429309848846\n",
            "Training loss epoch: 0.004302475944547885\n",
            "Training accuracy epoch: 0.9813962844022188\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.014601411369512788\n",
            "Validation loss per 100 evaluation steps: 0.014217645834578433\n",
            "Validation loss per 100 evaluation steps: 0.014891909591727502\n",
            "Validation loss per 100 evaluation steps: 0.015284842092769394\n",
            "Validation loss per 100 evaluation steps: 0.01588175853890425\n",
            "Validation Loss: 0.01588175853890425\n",
            "Validation Accuracy: 0.933763639108101\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.83      0.63      0.71       309\n",
            "       B-ORG       0.75      0.36      0.49       736\n",
            "       B-PER       0.83      0.68      0.75      2323\n",
            "       I-LOC       0.73      0.51      0.60       431\n",
            "       I-ORG       0.67      0.32      0.43      1032\n",
            "       I-PER       0.81      0.95      0.87      4121\n",
            "           O       0.96      0.98      0.97     41492\n",
            "\n",
            "    accuracy                           0.94     50444\n",
            "   macro avg       0.80      0.63      0.69     50444\n",
            "weighted avg       0.93      0.94      0.93     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9355  0.9355  0.9355\n",
            "macro        0.7967  0.6322  0.6891\n",
            "weighted     0.9310  0.9355  0.9299\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.012927910034632078\n",
            "Test loss per 100 evaluation steps: 0.012571154481556733\n",
            "Test loss per 100 evaluation steps: 0.012187937591200656\n",
            "Test loss per 100 evaluation steps: 0.012464074646886729\n",
            "Test loss per 100 evaluation steps: 0.012372123322013068\n",
            "Test Loss: 0.012588365280940551\n",
            "Test Accuracy: 0.9474430641477555\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.82      0.69      0.75       636\n",
            "       B-ORG       0.73      0.58      0.65      1090\n",
            "       B-PER       0.87      0.82      0.84      2650\n",
            "       I-LOC       0.79      0.59      0.68       835\n",
            "       I-ORG       0.64      0.51      0.57      1401\n",
            "       I-PER       0.87      0.95      0.91      4266\n",
            "           O       0.97      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.95     59472\n",
            "   macro avg       0.82      0.73      0.77     59472\n",
            "weighted avg       0.95      0.95      0.95     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9481  0.9481  0.9481\n",
            "macro        0.8159  0.7325  0.7686\n",
            "weighted     0.9451  0.9481  0.9458\n",
            "Test steps: 501\n",
            "Training epoch: 5\n",
            "Training loss per 100 training steps: 0.0032647105171236036\n",
            "Training loss per 100 training steps: 0.003043959791502857\n",
            "Training loss per 100 training steps: 0.002860500849674281\n",
            "Training loss per 100 training steps: 0.0028786014858769703\n",
            "Training loss per 100 training steps: 0.002956042431629612\n",
            "Training loss per 100 training steps: 0.0030948034029582533\n",
            "Training loss per 100 training steps: 0.003198499259311315\n",
            "Training loss per 100 training steps: 0.003290731558436164\n",
            "Training loss per 100 training steps: 0.0032516466268942977\n",
            "Training loss per 100 training steps: 0.0033164589846019226\n",
            "Training loss per 100 training steps: 0.0033563982118423818\n",
            "Training loss per 100 training steps: 0.003365955123117601\n",
            "Training loss per 100 training steps: 0.003443986147036552\n",
            "Training loss epoch: 0.003410638526778787\n",
            "Training accuracy epoch: 0.9856065349446582\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.014316641533514484\n",
            "Validation loss per 100 evaluation steps: 0.015195098520471219\n",
            "Validation loss per 100 evaluation steps: 0.01527876341092148\n",
            "Validation loss per 100 evaluation steps: 0.016175721425443042\n",
            "Validation loss per 100 evaluation steps: 0.015699599593273886\n",
            "Validation Loss: 0.015699599593273886\n",
            "Validation Accuracy: 0.9336463305030152\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.76      0.64      0.69       309\n",
            "       B-ORG       0.63      0.48      0.54       736\n",
            "       B-PER       0.92      0.70      0.79      2323\n",
            "       I-LOC       0.59      0.52      0.55       431\n",
            "       I-ORG       0.55      0.42      0.48      1032\n",
            "       I-PER       0.93      0.79      0.86      4121\n",
            "           O       0.95      0.99      0.97     41492\n",
            "\n",
            "    accuracy                           0.93     50444\n",
            "   macro avg       0.76      0.65      0.70     50444\n",
            "weighted avg       0.93      0.93      0.93     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9344  0.9344  0.9344\n",
            "macro        0.7603  0.6489  0.6981\n",
            "weighted     0.9304  0.9344  0.9306\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.011622773157432676\n",
            "Test loss per 100 evaluation steps: 0.011658963914051128\n",
            "Test loss per 100 evaluation steps: 0.01219828421380953\n",
            "Test loss per 100 evaluation steps: 0.012191805629372538\n",
            "Test loss per 100 evaluation steps: 0.011956233705757768\n",
            "Test Loss: 0.011985731836652478\n",
            "Test Accuracy: 0.9507356737134621\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.81      0.73      0.76       636\n",
            "       B-ORG       0.67      0.73      0.70      1090\n",
            "       B-PER       0.95      0.83      0.88      2650\n",
            "       I-LOC       0.78      0.65      0.71       835\n",
            "       I-ORG       0.60      0.65      0.62      1401\n",
            "       I-PER       0.96      0.84      0.90      4266\n",
            "           O       0.97      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.95     59472\n",
            "   macro avg       0.82      0.77      0.79     59472\n",
            "weighted avg       0.95      0.95      0.95     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9515  0.9515  0.9515\n",
            "macro        0.8203  0.7743  0.7948\n",
            "weighted     0.9518  0.9515  0.9510\n",
            "Test steps: 501\n",
            "Training epoch: 6\n",
            "Training loss per 100 training steps: 0.0018503596389564335\n",
            "Training loss per 100 training steps: 0.0024076320388030583\n",
            "Training loss per 100 training steps: 0.0024887647273377903\n",
            "Training loss per 100 training steps: 0.0026285526708124964\n",
            "Training loss per 100 training steps: 0.002689663626977563\n",
            "Training loss per 100 training steps: 0.0027647105472715337\n",
            "Training loss per 100 training steps: 0.002970452434637991\n",
            "Training loss per 100 training steps: 0.0030260443893121193\n",
            "Training loss per 100 training steps: 0.0029820684220370217\n",
            "Training loss per 100 training steps: 0.002929947937998804\n",
            "Training loss per 100 training steps: 0.002860125140022402\n",
            "Training loss per 100 training steps: 0.0028846809475029053\n",
            "Training loss per 100 training steps: 0.002858569175808113\n",
            "Training loss epoch: 0.0028662890330656155\n",
            "Training accuracy epoch: 0.9882809246639314\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.014625161013173056\n",
            "Validation loss per 100 evaluation steps: 0.014105857702961657\n",
            "Validation loss per 100 evaluation steps: 0.014889338426255562\n",
            "Validation loss per 100 evaluation steps: 0.014486719234205338\n",
            "Validation loss per 100 evaluation steps: 0.014361656241952006\n",
            "Validation Loss: 0.014361656241952006\n",
            "Validation Accuracy: 0.9422160856380356\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.83      0.61      0.70       309\n",
            "       B-ORG       0.72      0.38      0.50       736\n",
            "       B-PER       0.89      0.70      0.79      2323\n",
            "       I-LOC       0.75      0.48      0.59       431\n",
            "       I-ORG       0.68      0.33      0.45      1032\n",
            "       I-PER       0.91      0.88      0.89      4121\n",
            "           O       0.95      0.99      0.97     41492\n",
            "\n",
            "    accuracy                           0.94     50444\n",
            "   macro avg       0.82      0.63      0.70     50444\n",
            "weighted avg       0.94      0.94      0.93     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9412  0.9412  0.9412\n",
            "macro        0.8208  0.6254  0.6986\n",
            "weighted     0.9352  0.9412  0.9350\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.013147190186537046\n",
            "Test loss per 100 evaluation steps: 0.011811821174705982\n",
            "Test loss per 100 evaluation steps: 0.01190465197176915\n",
            "Test loss per 100 evaluation steps: 0.012139127406198895\n",
            "Test loss per 100 evaluation steps: 0.01177433495058358\n",
            "Test Loss: 0.011790333206263966\n",
            "Test Accuracy: 0.9521387913069348\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.84      0.67      0.75       636\n",
            "       B-ORG       0.73      0.63      0.68      1090\n",
            "       B-PER       0.92      0.83      0.87      2650\n",
            "       I-LOC       0.80      0.57      0.66       835\n",
            "       I-ORG       0.69      0.55      0.61      1401\n",
            "       I-PER       0.93      0.91      0.92      4266\n",
            "           O       0.97      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.95     59472\n",
            "   macro avg       0.84      0.74      0.78     59472\n",
            "weighted avg       0.95      0.95      0.95     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9523  0.9523  0.9523\n",
            "macro        0.8392  0.7357  0.7812\n",
            "weighted     0.9491  0.9523  0.9499\n",
            "Test steps: 501\n",
            "Training epoch: 7\n",
            "Training loss per 100 training steps: 0.002628000377862918\n",
            "Training loss per 100 training steps: 0.0026979357990603602\n",
            "Training loss per 100 training steps: 0.0025944669053205873\n",
            "Training loss per 100 training steps: 0.0024521286613071423\n",
            "Training loss per 100 training steps: 0.002417376696204883\n",
            "Training loss per 100 training steps: 0.00235963932385251\n",
            "Training loss per 100 training steps: 0.0022687612703521155\n",
            "Training loss per 100 training steps: 0.0022534142989456994\n",
            "Training loss per 100 training steps: 0.002283020814897504\n",
            "Training loss per 100 training steps: 0.002369417998775134\n",
            "Training loss per 100 training steps: 0.00239134306037737\n",
            "Training loss per 100 training steps: 0.002408854928388943\n",
            "Training loss per 100 training steps: 0.002437395160225676\n",
            "Training loss epoch: 0.002420208418505367\n",
            "Training accuracy epoch: 0.9901779271331086\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.017521037892147432\n",
            "Validation loss per 100 evaluation steps: 0.016571097614732935\n",
            "Validation loss per 100 evaluation steps: 0.0166725136224098\n",
            "Validation loss per 100 evaluation steps: 0.015899772106213277\n",
            "Validation loss per 100 evaluation steps: 0.016093719332704495\n",
            "Validation Loss: 0.016093719332704495\n",
            "Validation Accuracy: 0.9350783699225965\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.87      0.58      0.70       309\n",
            "       B-ORG       0.73      0.38      0.50       736\n",
            "       B-PER       0.87      0.68      0.76      2323\n",
            "       I-LOC       0.81      0.47      0.59       431\n",
            "       I-ORG       0.66      0.34      0.45      1032\n",
            "       I-PER       0.89      0.86      0.87      4121\n",
            "           O       0.95      0.99      0.97     41492\n",
            "\n",
            "    accuracy                           0.94     50444\n",
            "   macro avg       0.83      0.61      0.69     50444\n",
            "weighted avg       0.93      0.94      0.93     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9364  0.9364  0.9364\n",
            "macro        0.8260  0.6139  0.6923\n",
            "weighted     0.9304  0.9364  0.9300\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.010956382253243645\n",
            "Test loss per 100 evaluation steps: 0.010851153192688798\n",
            "Test loss per 100 evaluation steps: 0.011227894891308097\n",
            "Test loss per 100 evaluation steps: 0.011672742384917001\n",
            "Test loss per 100 evaluation steps: 0.011902125320892083\n",
            "Test Loss: 0.011879134652890987\n",
            "Test Accuracy: 0.9516817316173186\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.83      0.65      0.73       636\n",
            "       B-ORG       0.74      0.61      0.67      1090\n",
            "       B-PER       0.91      0.85      0.88      2650\n",
            "       I-LOC       0.79      0.55      0.65       835\n",
            "       I-ORG       0.68      0.53      0.60      1401\n",
            "       I-PER       0.92      0.91      0.91      4266\n",
            "           O       0.97      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.95     59472\n",
            "   macro avg       0.83      0.73      0.77     59472\n",
            "weighted avg       0.95      0.95      0.95     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9520  0.9520  0.9520\n",
            "macro        0.8336  0.7285  0.7744\n",
            "weighted     0.9484  0.9520  0.9493\n",
            "Test steps: 501\n",
            "====================================================================================================\n",
            "loss_name: ce\n",
            "model configuration\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Ner_Model(\n",
            "  (model): DebertaModel(\n",
            "    (embeddings): DebertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "      (LayerNorm): DebertaLayerNorm()\n",
            "      (dropout): StableDropout()\n",
            "    )\n",
            "    (encoder): DebertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x DebertaLayer(\n",
            "          (attention): DebertaAttention(\n",
            "            (self): DisentangledSelfAttention(\n",
            "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
            "              (pos_dropout): StableDropout()\n",
            "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "            (output): DebertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): DebertaLayerNorm()\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "          )\n",
            "          (intermediate): DebertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): DebertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): DebertaLayerNorm()\n",
            "            (dropout): StableDropout()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (rel_embeddings): Embedding(1024, 768)\n",
            "    )\n",
            "  )\n",
            "  (linear_model): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            "  (classifier): Linear(in_features=512, out_features=7, bias=True)\n",
            ")\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 1.1302305889129638\n",
            "Training loss per 100 training steps: 0.8430331752449274\n",
            "Training loss per 100 training steps: 0.7093614614009858\n",
            "Training loss per 100 training steps: 0.6106839636247605\n",
            "Training loss per 100 training steps: 0.5470273809898645\n",
            "Training loss per 100 training steps: 0.5026004868761326\n",
            "Training loss per 100 training steps: 0.46723751557325677\n",
            "Training loss per 100 training steps: 0.43724118325975725\n",
            "Training loss per 100 training steps: 0.410187548065765\n",
            "Training loss per 100 training steps: 0.3873611726406962\n",
            "Training loss per 100 training steps: 0.3660968114291741\n",
            "Training loss per 100 training steps: 0.34953560449532234\n",
            "Training loss per 100 training steps: 0.3348568513391253\n",
            "Training loss epoch: 0.32981340224725586\n",
            "Training accuracy epoch: 0.9170110200497971\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.21080349327996373\n",
            "Validation loss per 100 evaluation steps: 0.21584965496789665\n",
            "Validation loss per 100 evaluation steps: 0.2103180087885509\n",
            "Validation loss per 100 evaluation steps: 0.20677532295929268\n",
            "Validation loss per 100 evaluation steps: 0.20738304484263062\n",
            "Validation Loss: 0.20738304484263062\n",
            "Validation Accuracy: 0.939125874009382\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.87      0.50      0.63       309\n",
            "       B-ORG       0.66      0.46      0.54       736\n",
            "       B-PER       0.87      0.88      0.87      2323\n",
            "       I-LOC       0.77      0.27      0.40       431\n",
            "       I-ORG       0.74      0.21      0.33      1032\n",
            "       I-PER       0.83      0.89      0.86      4121\n",
            "           O       0.96      0.99      0.97     41492\n",
            "\n",
            "    accuracy                           0.94     50444\n",
            "   macro avg       0.81      0.60      0.66     50444\n",
            "weighted avg       0.93      0.94      0.93     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9397  0.9397  0.9397\n",
            "macro        0.8142  0.5978  0.6579\n",
            "weighted     0.9346  0.9397  0.9323\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.17551086234860122\n",
            "Test loss per 100 evaluation steps: 0.1773089862242341\n",
            "Test loss per 100 evaluation steps: 0.18689330518245698\n",
            "Test loss per 100 evaluation steps: 0.18732623338233678\n",
            "Test loss per 100 evaluation steps: 0.18384185344912113\n",
            "Test Loss: 0.18348310847753685\n",
            "Test Accuracy: 0.9488319309757496\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.79      0.59      0.68       636\n",
            "       B-ORG       0.65      0.58      0.62      1090\n",
            "       B-PER       0.87      0.94      0.90      2650\n",
            "       I-LOC       0.89      0.41      0.56       835\n",
            "       I-ORG       0.70      0.32      0.44      1401\n",
            "       I-PER       0.87      0.93      0.90      4266\n",
            "           O       0.97      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.95     59472\n",
            "   macro avg       0.82      0.68      0.72     59472\n",
            "weighted avg       0.94      0.95      0.94     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9486  0.9486  0.9486\n",
            "macro        0.8209  0.6796  0.7246\n",
            "weighted     0.9443  0.9486  0.9431\n",
            "Test steps: 501\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.13010309375124052\n",
            "Training loss per 100 training steps: 0.1185056310694199\n",
            "Training loss per 100 training steps: 0.11937869777825351\n",
            "Training loss per 100 training steps: 0.12001030896150042\n",
            "Training loss per 100 training steps: 0.12179840355576016\n",
            "Training loss per 100 training steps: 0.12004856485854058\n",
            "Training loss per 100 training steps: 0.11738073743064888\n",
            "Training loss per 100 training steps: 0.11897441909110057\n",
            "Training loss per 100 training steps: 0.11841438921258991\n",
            "Training loss per 100 training steps: 0.1184202431201702\n",
            "Training loss per 100 training steps: 0.11806097993193279\n",
            "Training loss per 100 training steps: 0.11689610954233406\n",
            "Training loss per 100 training steps: 0.11575870157806023\n",
            "Training loss epoch: 0.1153477783710347\n",
            "Training accuracy epoch: 0.9637918909608966\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.21892934130504726\n",
            "Validation loss per 100 evaluation steps: 0.2087672606948763\n",
            "Validation loss per 100 evaluation steps: 0.21056007692279916\n",
            "Validation loss per 100 evaluation steps: 0.21315653971629217\n",
            "Validation loss per 100 evaluation steps: 0.21016514814458787\n",
            "Validation Loss: 0.21016514814458787\n",
            "Validation Accuracy: 0.9352147940620729\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.81      0.62      0.71       309\n",
            "       B-ORG       0.71      0.43      0.54       736\n",
            "       B-PER       0.89      0.65      0.75      2323\n",
            "       I-LOC       0.71      0.53      0.60       431\n",
            "       I-ORG       0.65      0.37      0.47      1032\n",
            "       I-PER       0.88      0.87      0.87      4121\n",
            "           O       0.95      0.99      0.97     41492\n",
            "\n",
            "    accuracy                           0.94     50444\n",
            "   macro avg       0.80      0.64      0.70     50444\n",
            "weighted avg       0.93      0.94      0.93     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9366  0.9366  0.9366\n",
            "macro        0.8022  0.6371  0.7027\n",
            "weighted     0.9310  0.9366  0.9311\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.18342842023819686\n",
            "Test loss per 100 evaluation steps: 0.16195290920790284\n",
            "Test loss per 100 evaluation steps: 0.17272352032363414\n",
            "Test loss per 100 evaluation steps: 0.17568980714306234\n",
            "Test loss per 100 evaluation steps: 0.1741109355352819\n",
            "Test Loss: 0.18207202494486482\n",
            "Test Accuracy: 0.94706819095557\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.79      0.68      0.73       636\n",
            "       B-ORG       0.69      0.61      0.65      1090\n",
            "       B-PER       0.90      0.83      0.86      2650\n",
            "       I-LOC       0.73      0.59      0.65       835\n",
            "       I-ORG       0.62      0.50      0.55      1401\n",
            "       I-PER       0.90      0.91      0.91      4266\n",
            "           O       0.97      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.95     59472\n",
            "   macro avg       0.80      0.73      0.76     59472\n",
            "weighted avg       0.94      0.95      0.95     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9479  0.9479  0.9479\n",
            "macro        0.8009  0.7292  0.7618\n",
            "weighted     0.9445  0.9479  0.9457\n",
            "Test steps: 501\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.08839536315063015\n",
            "Training loss per 100 training steps: 0.08146152494708075\n",
            "Training loss per 100 training steps: 0.07534439173759892\n",
            "Training loss per 100 training steps: 0.07839511589554604\n",
            "Training loss per 100 training steps: 0.07847329435078428\n",
            "Training loss per 100 training steps: 0.07754139110581794\n",
            "Training loss per 100 training steps: 0.07829611779872461\n",
            "Training loss per 100 training steps: 0.07731787568576692\n",
            "Training loss per 100 training steps: 0.0776536002070255\n",
            "Training loss per 100 training steps: 0.07954511001711945\n",
            "Training loss per 100 training steps: 0.08050272702117747\n",
            "Training loss per 100 training steps: 0.0798389009014257\n",
            "Training loss per 100 training steps: 0.07997049582213199\n",
            "Training loss epoch: 0.08014692423175428\n",
            "Training accuracy epoch: 0.9745307780131882\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.18416230159346014\n",
            "Validation loss per 100 evaluation steps: 0.1843270034645684\n",
            "Validation loss per 100 evaluation steps: 0.17875611234766742\n",
            "Validation loss per 100 evaluation steps: 0.1798607344785705\n",
            "Validation loss per 100 evaluation steps: 0.18068215778609736\n",
            "Validation Loss: 0.18068215778609736\n",
            "Validation Accuracy: 0.9466661569198719\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.81      0.60      0.69       309\n",
            "       B-ORG       0.71      0.47      0.57       736\n",
            "       B-PER       0.91      0.77      0.83      2323\n",
            "       I-LOC       0.70      0.50      0.58       431\n",
            "       I-ORG       0.67      0.42      0.52      1032\n",
            "       I-PER       0.89      0.91      0.90      4121\n",
            "           O       0.96      0.99      0.98     41492\n",
            "\n",
            "    accuracy                           0.95     50444\n",
            "   macro avg       0.81      0.66      0.72     50444\n",
            "weighted avg       0.94      0.95      0.94     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9465  0.9465  0.9465\n",
            "macro        0.8068  0.6648  0.7229\n",
            "weighted     0.9418  0.9465  0.9426\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.1743614161107689\n",
            "Test loss per 100 evaluation steps: 0.17800476164324208\n",
            "Test loss per 100 evaluation steps: 0.16593600193969904\n",
            "Test loss per 100 evaluation steps: 0.16020318201917688\n",
            "Test loss per 100 evaluation steps: 0.16044743812596426\n",
            "Test Loss: 0.16044226452892277\n",
            "Test Accuracy: 0.9506663362789003\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.81      0.69      0.75       636\n",
            "       B-ORG       0.67      0.66      0.67      1090\n",
            "       B-PER       0.93      0.83      0.88      2650\n",
            "       I-LOC       0.79      0.59      0.67       835\n",
            "       I-ORG       0.59      0.54      0.56      1401\n",
            "       I-PER       0.92      0.91      0.91      4266\n",
            "           O       0.97      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.95     59472\n",
            "   macro avg       0.81      0.74      0.77     59472\n",
            "weighted avg       0.95      0.95      0.95     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9505  0.9505  0.9505\n",
            "macro        0.8110  0.7449  0.7746\n",
            "weighted     0.9486  0.9505  0.9491\n",
            "Test steps: 501\n",
            "Training epoch: 4\n",
            "Training loss per 100 training steps: 0.04804821485420689\n",
            "Training loss per 100 training steps: 0.05568061533209402\n",
            "Training loss per 100 training steps: 0.06069677191534235\n",
            "Training loss per 100 training steps: 0.06230112526434823\n",
            "Training loss per 100 training steps: 0.0646578082361957\n",
            "Training loss per 100 training steps: 0.061915993176274546\n",
            "Training loss per 100 training steps: 0.06064084290223296\n",
            "Training loss per 100 training steps: 0.05999762349176308\n",
            "Training loss per 100 training steps: 0.05946243287387511\n",
            "Training loss per 100 training steps: 0.05948694860388059\n",
            "Training loss per 100 training steps: 0.058854008442933925\n",
            "Training loss per 100 training steps: 0.05804694843061346\n",
            "Training loss per 100 training steps: 0.05828246375371236\n",
            "Training loss epoch: 0.058322506892575086\n",
            "Training accuracy epoch: 0.9816059319742642\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.24213733078737276\n",
            "Validation loss per 100 evaluation steps: 0.23845715105097043\n",
            "Validation loss per 100 evaluation steps: 0.22947274288802874\n",
            "Validation loss per 100 evaluation steps: 0.22372159939593986\n",
            "Validation loss per 100 evaluation steps: 0.2238677610246232\n",
            "Validation Loss: 0.2238677610246232\n",
            "Validation Accuracy: 0.9423086060618883\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.72      0.67      0.69       309\n",
            "       B-ORG       0.68      0.52      0.59       736\n",
            "       B-PER       0.93      0.68      0.78      2323\n",
            "       I-LOC       0.57      0.60      0.58       431\n",
            "       I-ORG       0.62      0.44      0.52      1032\n",
            "       I-PER       0.93      0.86      0.89      4121\n",
            "           O       0.96      0.99      0.97     41492\n",
            "\n",
            "    accuracy                           0.94     50444\n",
            "   macro avg       0.77      0.68      0.72     50444\n",
            "weighted avg       0.94      0.94      0.94     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9422  0.9422  0.9422\n",
            "macro        0.7721  0.6791  0.7187\n",
            "weighted     0.9388  0.9422  0.9388\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.21870015006279572\n",
            "Test loss per 100 evaluation steps: 0.20032260126201437\n",
            "Test loss per 100 evaluation steps: 0.1856718732509762\n",
            "Test loss per 100 evaluation steps: 0.18786327990121207\n",
            "Test loss per 100 evaluation steps: 0.19194968289043754\n",
            "Test Loss: 0.19156790997667103\n",
            "Test Accuracy: 0.9492965269545354\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.75      0.74      0.75       636\n",
            "       B-ORG       0.66      0.72      0.69      1090\n",
            "       B-PER       0.95      0.79      0.86      2650\n",
            "       I-LOC       0.67      0.70      0.68       835\n",
            "       I-ORG       0.61      0.62      0.62      1401\n",
            "       I-PER       0.95      0.87      0.91      4266\n",
            "           O       0.97      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.95     59472\n",
            "   macro avg       0.80      0.78      0.78     59472\n",
            "weighted avg       0.95      0.95      0.95     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9502  0.9502  0.9502\n",
            "macro        0.7955  0.7753  0.7840\n",
            "weighted     0.9505  0.9502  0.9498\n",
            "Test steps: 501\n",
            "Training epoch: 5\n",
            "Training loss per 100 training steps: 0.05018526190367993\n",
            "Training loss per 100 training steps: 0.046326080090439065\n",
            "Training loss per 100 training steps: 0.0452427713504585\n",
            "Training loss per 100 training steps: 0.04551127397437085\n",
            "Training loss per 100 training steps: 0.0433307232906518\n",
            "Training loss per 100 training steps: 0.04308802549783043\n",
            "Training loss per 100 training steps: 0.0434982036929536\n",
            "Training loss per 100 training steps: 0.04309954729980745\n",
            "Training loss per 100 training steps: 0.04220360105600169\n",
            "Training loss per 100 training steps: 0.04129897910494765\n",
            "Training loss per 100 training steps: 0.043016766431484776\n",
            "Training loss per 100 training steps: 0.0423131553532221\n",
            "Training loss per 100 training steps: 0.04240699211586616\n",
            "Training loss epoch: 0.04236983369292704\n",
            "Training accuracy epoch: 0.987159595631064\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.2618484434665879\n",
            "Validation loss per 100 evaluation steps: 0.23372901147435188\n",
            "Validation loss per 100 evaluation steps: 0.23399148455258303\n",
            "Validation loss per 100 evaluation steps: 0.24259141552131042\n",
            "Validation loss per 100 evaluation steps: 0.2433692795181414\n",
            "Validation Loss: 0.2433692795181414\n",
            "Validation Accuracy: 0.9437435794411166\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.81      0.64      0.72       309\n",
            "       B-ORG       0.72      0.45      0.56       736\n",
            "       B-PER       0.91      0.78      0.84      2323\n",
            "       I-LOC       0.71      0.60      0.65       431\n",
            "       I-ORG       0.64      0.42      0.50      1032\n",
            "       I-PER       0.93      0.85      0.89      4121\n",
            "           O       0.96      0.99      0.97     41492\n",
            "\n",
            "    accuracy                           0.94     50444\n",
            "   macro avg       0.81      0.67      0.73     50444\n",
            "weighted avg       0.94      0.94      0.94     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9443  0.9443  0.9443\n",
            "macro        0.8111  0.6749  0.7322\n",
            "weighted     0.9397  0.9443  0.9403\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.1632592042174656\n",
            "Test loss per 100 evaluation steps: 0.1878951469209278\n",
            "Test loss per 100 evaluation steps: 0.19024144140266191\n",
            "Test loss per 100 evaluation steps: 0.18800021139351883\n",
            "Test loss per 100 evaluation steps: 0.19131043779221363\n",
            "Test Loss: 0.19121095817094622\n",
            "Test Accuracy: 0.9534332750862089\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.83      0.71      0.77       636\n",
            "       B-ORG       0.69      0.66      0.68      1090\n",
            "       B-PER       0.92      0.89      0.90      2650\n",
            "       I-LOC       0.76      0.65      0.70       835\n",
            "       I-ORG       0.62      0.59      0.61      1401\n",
            "       I-PER       0.93      0.89      0.91      4266\n",
            "           O       0.97      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.95     59472\n",
            "   macro avg       0.82      0.77      0.79     59472\n",
            "weighted avg       0.95      0.95      0.95     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9530  0.9530  0.9530\n",
            "macro        0.8202  0.7686  0.7928\n",
            "weighted     0.9515  0.9530  0.9521\n",
            "Test steps: 501\n",
            "Training epoch: 6\n",
            "Training loss per 100 training steps: 0.03375915873097256\n",
            "Training loss per 100 training steps: 0.03538186969628441\n",
            "Training loss per 100 training steps: 0.031360578457533844\n",
            "Training loss per 100 training steps: 0.03311453060538042\n",
            "Training loss per 100 training steps: 0.03480369565379806\n",
            "Training loss per 100 training steps: 0.0351431157112044\n",
            "Training loss per 100 training steps: 0.03435046246913511\n",
            "Training loss per 100 training steps: 0.03399853581935531\n",
            "Training loss per 100 training steps: 0.03381306458324414\n",
            "Training loss per 100 training steps: 0.03314215243713989\n",
            "Training loss per 100 training steps: 0.033205900152057766\n",
            "Training loss per 100 training steps: 0.03289662285432617\n",
            "Training loss per 100 training steps: 0.03270416884652849\n",
            "Training loss epoch: 0.032561747082223515\n",
            "Training accuracy epoch: 0.9900005824828885\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.24052922086208128\n",
            "Validation loss per 100 evaluation steps: 0.2636072493736174\n",
            "Validation loss per 100 evaluation steps: 0.2766853716617455\n",
            "Validation loss per 100 evaluation steps: 0.2926470040894037\n",
            "Validation loss per 100 evaluation steps: 0.2974500519943176\n",
            "Validation Loss: 0.2974500519943176\n",
            "Validation Accuracy: 0.9421314179420248\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.80      0.60      0.68       309\n",
            "       B-ORG       0.74      0.46      0.57       736\n",
            "       B-PER       0.91      0.70      0.80      2323\n",
            "       I-LOC       0.70      0.51      0.59       431\n",
            "       I-ORG       0.67      0.39      0.49      1032\n",
            "       I-PER       0.91      0.87      0.89      4121\n",
            "           O       0.96      0.99      0.97     41492\n",
            "\n",
            "    accuracy                           0.94     50444\n",
            "   macro avg       0.81      0.65      0.71     50444\n",
            "weighted avg       0.94      0.94      0.94     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9427  0.9427  0.9427\n",
            "macro        0.8127  0.6469  0.7136\n",
            "weighted     0.9377  0.9427  0.9378\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.22901148006902075\n",
            "Test loss per 100 evaluation steps: 0.2370111384926713\n",
            "Test loss per 100 evaluation steps: 0.2298541476367973\n",
            "Test loss per 100 evaluation steps: 0.240031086023082\n",
            "Test loss per 100 evaluation steps: 0.2338555244698655\n",
            "Test Loss: 0.23338964911606006\n",
            "Test Accuracy: 0.953343855179371\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.87      0.69      0.77       636\n",
            "       B-ORG       0.72      0.69      0.70      1090\n",
            "       B-PER       0.93      0.84      0.88      2650\n",
            "       I-LOC       0.81      0.62      0.70       835\n",
            "       I-ORG       0.65      0.59      0.62      1401\n",
            "       I-PER       0.93      0.89      0.91      4266\n",
            "           O       0.97      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.95     59472\n",
            "   macro avg       0.84      0.76      0.80     59472\n",
            "weighted avg       0.95      0.95      0.95     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9532  0.9532  0.9532\n",
            "macro        0.8400  0.7588  0.7955\n",
            "weighted     0.9513  0.9532  0.9517\n",
            "Test steps: 501\n",
            "Training epoch: 7\n",
            "Training loss per 100 training steps: 0.023147679544563288\n",
            "Training loss per 100 training steps: 0.0236807849093384\n",
            "Training loss per 100 training steps: 0.021767399577681014\n",
            "Training loss per 100 training steps: 0.022421700544491614\n",
            "Training loss per 100 training steps: 0.02108642884933215\n",
            "Training loss per 100 training steps: 0.021565519816880017\n",
            "Training loss per 100 training steps: 0.02225113601069357\n",
            "Training loss per 100 training steps: 0.022673773146116218\n",
            "Training loss per 100 training steps: 0.022632961118199294\n",
            "Training loss per 100 training steps: 0.02227570087134518\n",
            "Training loss per 100 training steps: 0.022149006623621972\n",
            "Training loss per 100 training steps: 0.023505297988816287\n",
            "Training loss per 100 training steps: 0.024056499223518657\n",
            "Training loss epoch: 0.02427112534992392\n",
            "Training accuracy epoch: 0.9928397000504835\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.22621984438883375\n",
            "Validation loss per 100 evaluation steps: 0.24565368338560803\n",
            "Validation loss per 100 evaluation steps: 0.2563348526165646\n",
            "Validation loss per 100 evaluation steps: 0.2544247005987199\n",
            "Validation loss per 100 evaluation steps: 0.26614974500109384\n",
            "Validation Loss: 0.26614974500109384\n",
            "Validation Accuracy: 0.9453236084083599\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.81      0.61      0.70       309\n",
            "       B-ORG       0.66      0.53      0.59       736\n",
            "       B-PER       0.92      0.72      0.81      2323\n",
            "       I-LOC       0.72      0.50      0.59       431\n",
            "       I-ORG       0.56      0.49      0.52      1032\n",
            "       I-PER       0.92      0.88      0.90      4121\n",
            "           O       0.96      0.99      0.98     41492\n",
            "\n",
            "    accuracy                           0.95     50444\n",
            "   macro avg       0.79      0.67      0.73     50444\n",
            "weighted avg       0.94      0.95      0.94     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9450  0.9450  0.9450\n",
            "macro        0.7916  0.6748  0.7253\n",
            "weighted     0.9416  0.9450  0.9422\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.19994488748023287\n",
            "Test loss per 100 evaluation steps: 0.20965339363436214\n",
            "Test loss per 100 evaluation steps: 0.21595976487578203\n",
            "Test loss per 100 evaluation steps: 0.2199951109837275\n",
            "Test loss per 100 evaluation steps: 0.22301531391916796\n",
            "Test Loss: 0.22259346478406764\n",
            "Test Accuracy: 0.9506527713295979\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.84      0.67      0.75       636\n",
            "       B-ORG       0.65      0.73      0.69      1090\n",
            "       B-PER       0.94      0.81      0.87      2650\n",
            "       I-LOC       0.79      0.57      0.66       835\n",
            "       I-ORG       0.57      0.69      0.62      1401\n",
            "       I-PER       0.93      0.90      0.92      4266\n",
            "           O       0.98      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.95     59472\n",
            "   macro avg       0.81      0.76      0.78     59472\n",
            "weighted avg       0.95      0.95      0.95     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9510  0.9510  0.9510\n",
            "macro        0.8140  0.7648  0.7835\n",
            "weighted     0.9524  0.9510  0.9509\n",
            "Test steps: 501\n",
            "====================================================================================================\n",
            "loss_name: kl\n",
            "model configuration\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Ner_Model(\n",
            "  (model): DebertaModel(\n",
            "    (embeddings): DebertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "      (LayerNorm): DebertaLayerNorm()\n",
            "      (dropout): StableDropout()\n",
            "    )\n",
            "    (encoder): DebertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x DebertaLayer(\n",
            "          (attention): DebertaAttention(\n",
            "            (self): DisentangledSelfAttention(\n",
            "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
            "              (pos_dropout): StableDropout()\n",
            "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "            (output): DebertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): DebertaLayerNorm()\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "          )\n",
            "          (intermediate): DebertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): DebertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): DebertaLayerNorm()\n",
            "            (dropout): StableDropout()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (rel_embeddings): Embedding(1024, 768)\n",
            "    )\n",
            "  )\n",
            "  (linear_model): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            "  (classifier): Linear(in_features=512, out_features=7, bias=True)\n",
            ")\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 1.0182373964786529\n",
            "Training loss per 100 training steps: 0.791226202622056\n",
            "Training loss per 100 training steps: 0.662025935947895\n",
            "Training loss per 100 training steps: 0.5714407848287374\n",
            "Training loss per 100 training steps: 0.5176160708116367\n",
            "Training loss per 100 training steps: 0.4757629712865067\n",
            "Training loss per 100 training steps: 0.4447520678111219\n",
            "Training loss per 100 training steps: 0.42241849131823983\n",
            "Training loss per 100 training steps: 0.4000142237166357\n",
            "Training loss per 100 training steps: 0.3792930985692656\n",
            "Training loss per 100 training steps: 0.3609933934952344\n",
            "Training loss per 100 training steps: 0.34645371130657926\n",
            "Training loss per 100 training steps: 0.33244816086174417\n",
            "Training loss epoch: 0.3279245969939677\n",
            "Training accuracy epoch: 0.9168772888351338\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.229784220084548\n",
            "Validation loss per 100 evaluation steps: 0.21034279364626854\n",
            "Validation loss per 100 evaluation steps: 0.19168289171221356\n",
            "Validation loss per 100 evaluation steps: 0.19221905485726892\n",
            "Validation loss per 100 evaluation steps: 0.19141965131089092\n",
            "Validation Loss: 0.19141965131089092\n",
            "Validation Accuracy: 0.9424368283453002\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.71      0.61      0.66       309\n",
            "       B-ORG       0.64      0.53      0.58       736\n",
            "       B-PER       0.88      0.88      0.88      2323\n",
            "       I-LOC       0.67      0.46      0.55       431\n",
            "       I-ORG       0.51      0.44      0.47      1032\n",
            "       I-PER       0.85      0.87      0.86      4121\n",
            "           O       0.97      0.98      0.98     41492\n",
            "\n",
            "    accuracy                           0.94     50444\n",
            "   macro avg       0.75      0.68      0.71     50444\n",
            "weighted avg       0.94      0.94      0.94     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9418  0.9418  0.9418\n",
            "macro        0.7458  0.6818  0.7094\n",
            "weighted     0.9389  0.9418  0.9400\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.16748656426556408\n",
            "Test loss per 100 evaluation steps: 0.17060269022360444\n",
            "Test loss per 100 evaluation steps: 0.16792042751486103\n",
            "Test loss per 100 evaluation steps: 0.16723456625360997\n",
            "Test loss per 100 evaluation steps: 0.16624481124803425\n",
            "Test Loss: 0.16592227340213017\n",
            "Test Accuracy: 0.952010502811998\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.71      0.72      0.72       636\n",
            "       B-ORG       0.66      0.65      0.66      1090\n",
            "       B-PER       0.91      0.93      0.92      2650\n",
            "       I-LOC       0.74      0.58      0.65       835\n",
            "       I-ORG       0.57      0.59      0.58      1401\n",
            "       I-PER       0.92      0.91      0.91      4266\n",
            "           O       0.98      0.98      0.98     48594\n",
            "\n",
            "    accuracy                           0.95     59472\n",
            "   macro avg       0.79      0.77      0.77     59472\n",
            "weighted avg       0.95      0.95      0.95     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9519  0.9519  0.9519\n",
            "macro        0.7863  0.7655  0.7745\n",
            "weighted     0.9513  0.9519  0.9515\n",
            "Test steps: 501\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.13985493476036936\n",
            "Training loss per 100 training steps: 0.13140178257832302\n",
            "Training loss per 100 training steps: 0.12570566372169803\n",
            "Training loss per 100 training steps: 0.12578114377101884\n",
            "Training loss per 100 training steps: 0.12452071107178926\n",
            "Training loss per 100 training steps: 0.12306067509499068\n",
            "Training loss per 100 training steps: 0.12112031597577567\n",
            "Training loss per 100 training steps: 0.11890571786905639\n",
            "Training loss per 100 training steps: 0.119185049100779\n",
            "Training loss per 100 training steps: 0.11737947151949629\n",
            "Training loss per 100 training steps: 0.11608335576507009\n",
            "Training loss per 100 training steps: 0.1166443062822024\n",
            "Training loss per 100 training steps: 0.11636093816755769\n",
            "Training loss epoch: 0.11584774658453655\n",
            "Training accuracy epoch: 0.9646882179494481\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.19042193797184154\n",
            "Validation loss per 100 evaluation steps: 0.1848868634121027\n",
            "Validation loss per 100 evaluation steps: 0.2046140924996386\n",
            "Validation loss per 100 evaluation steps: 0.20021158565534278\n",
            "Validation loss per 100 evaluation steps: 0.19657136970665307\n",
            "Validation Loss: 0.19657136970665307\n",
            "Validation Accuracy: 0.9477760074991062\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.83      0.61      0.70       309\n",
            "       B-ORG       0.74      0.37      0.50       736\n",
            "       B-PER       0.87      0.84      0.86      2323\n",
            "       I-LOC       0.79      0.48      0.60       431\n",
            "       I-ORG       0.71      0.27      0.39      1032\n",
            "       I-PER       0.89      0.90      0.90      4121\n",
            "           O       0.96      0.99      0.98     41492\n",
            "\n",
            "    accuracy                           0.95     50444\n",
            "   macro avg       0.83      0.64      0.70     50444\n",
            "weighted avg       0.94      0.95      0.94     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9477  0.9477  0.9477\n",
            "macro        0.8278  0.6386  0.7026\n",
            "weighted     0.9421  0.9477  0.9414\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.17539223263971507\n",
            "Test loss per 100 evaluation steps: 0.17495073086116464\n",
            "Test loss per 100 evaluation steps: 0.16840537623502314\n",
            "Test loss per 100 evaluation steps: 0.179884570739232\n",
            "Test loss per 100 evaluation steps: 0.17394250162009847\n",
            "Test Loss: 0.1736219182557416\n",
            "Test Accuracy: 0.9553350411088009\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.81      0.70      0.75       636\n",
            "       B-ORG       0.75      0.61      0.67      1090\n",
            "       B-PER       0.89      0.93      0.91      2650\n",
            "       I-LOC       0.81      0.54      0.65       835\n",
            "       I-ORG       0.71      0.49      0.58      1401\n",
            "       I-PER       0.91      0.92      0.92      4266\n",
            "           O       0.98      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.96     59472\n",
            "   macro avg       0.84      0.74      0.78     59472\n",
            "weighted avg       0.95      0.96      0.95     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9561  0.9561  0.9561\n",
            "macro        0.8375  0.7407  0.7806\n",
            "weighted     0.9526  0.9561  0.9532\n",
            "Test steps: 501\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.07330375178949908\n",
            "Training loss per 100 training steps: 0.08616952335170935\n",
            "Training loss per 100 training steps: 0.08396866376860998\n",
            "Training loss per 100 training steps: 0.08192153143812902\n",
            "Training loss per 100 training steps: 0.08256704158196226\n",
            "Training loss per 100 training steps: 0.08215097555056369\n",
            "Training loss per 100 training steps: 0.08158080647400182\n",
            "Training loss per 100 training steps: 0.08134981172683182\n",
            "Training loss per 100 training steps: 0.08130287219977213\n",
            "Training loss per 100 training steps: 0.08154303549509495\n",
            "Training loss per 100 training steps: 0.08134720578779128\n",
            "Training loss per 100 training steps: 0.08278129568992881\n",
            "Training loss per 100 training steps: 0.08480343829831467\n",
            "Training loss epoch: 0.08549343738835724\n",
            "Training accuracy epoch: 0.9733297344635641\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.2696261490788311\n",
            "Validation loss per 100 evaluation steps: 0.24012766023515725\n",
            "Validation loss per 100 evaluation steps: 0.23495248365604007\n",
            "Validation loss per 100 evaluation steps: 0.24181294007401447\n",
            "Validation loss per 100 evaluation steps: 0.24458673205738887\n",
            "Validation Loss: 0.24458673205738887\n",
            "Validation Accuracy: 0.9264113837110041\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.86      0.55      0.67       309\n",
            "       B-ORG       0.73      0.35      0.47       736\n",
            "       B-PER       0.91      0.50      0.65      2323\n",
            "       I-LOC       0.79      0.45      0.57       431\n",
            "       I-ORG       0.67      0.35      0.46      1032\n",
            "       I-PER       0.91      0.82      0.86      4121\n",
            "           O       0.93      0.99      0.96     41492\n",
            "\n",
            "    accuracy                           0.93     50444\n",
            "   macro avg       0.83      0.57      0.66     50444\n",
            "weighted avg       0.92      0.93      0.92     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9264  0.9264  0.9264\n",
            "macro        0.8290  0.5745  0.6647\n",
            "weighted     0.9210  0.9264  0.9174\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.19045571712311357\n",
            "Test loss per 100 evaluation steps: 0.17854006629670038\n",
            "Test loss per 100 evaluation steps: 0.18434127850923687\n",
            "Test loss per 100 evaluation steps: 0.18663396422751247\n",
            "Test loss per 100 evaluation steps: 0.18636099866218864\n",
            "Test Loss: 0.1860011936836465\n",
            "Test Accuracy: 0.9429593558644314\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.88      0.65      0.75       636\n",
            "       B-ORG       0.74      0.60      0.66      1090\n",
            "       B-PER       0.95      0.68      0.79      2650\n",
            "       I-LOC       0.86      0.52      0.65       835\n",
            "       I-ORG       0.62      0.58      0.60      1401\n",
            "       I-PER       0.94      0.89      0.91      4266\n",
            "           O       0.96      0.99      0.97     48594\n",
            "\n",
            "    accuracy                           0.94     59472\n",
            "   macro avg       0.85      0.70      0.76     59472\n",
            "weighted avg       0.94      0.94      0.94     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9431  0.9431  0.9431\n",
            "macro        0.8480  0.7021  0.7625\n",
            "weighted     0.9409  0.9431  0.9398\n",
            "Test steps: 501\n",
            "Training epoch: 4\n",
            "Training loss per 100 training steps: 0.04821921837341506\n",
            "Training loss per 100 training steps: 0.054611995978921184\n",
            "Training loss per 100 training steps: 0.05513235012612616\n",
            "Training loss per 100 training steps: 0.05853819405674585\n",
            "Training loss per 100 training steps: 0.05708797492820304\n",
            "Training loss per 100 training steps: 0.058443266188939254\n",
            "Training loss per 100 training steps: 0.05766334711490864\n",
            "Training loss per 100 training steps: 0.05838338790417765\n",
            "Training loss per 100 training steps: 0.059140231214437844\n",
            "Training loss per 100 training steps: 0.059307772812317125\n",
            "Training loss per 100 training steps: 0.059273411100889045\n",
            "Training loss per 100 training steps: 0.05909838359477969\n",
            "Training loss per 100 training steps: 0.058994805443832704\n",
            "Training loss epoch: 0.0593924646729177\n",
            "Training accuracy epoch: 0.9817807085501153\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.19728646035771816\n",
            "Validation loss per 100 evaluation steps: 0.20526087801437826\n",
            "Validation loss per 100 evaluation steps: 0.2146602527378127\n",
            "Validation loss per 100 evaluation steps: 0.21545378406648524\n",
            "Validation loss per 100 evaluation steps: 0.20130433797603473\n",
            "Validation Loss: 0.20130433797603473\n",
            "Validation Accuracy: 0.9456213893062222\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.70      0.64      0.67       309\n",
            "       B-ORG       0.71      0.41      0.52       736\n",
            "       B-PER       0.89      0.79      0.84      2323\n",
            "       I-LOC       0.65      0.53      0.58       431\n",
            "       I-ORG       0.68      0.32      0.43      1032\n",
            "       I-PER       0.90      0.90      0.90      4121\n",
            "           O       0.96      0.99      0.98     41492\n",
            "\n",
            "    accuracy                           0.95     50444\n",
            "   macro avg       0.78      0.65      0.70     50444\n",
            "weighted avg       0.94      0.95      0.94     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9456  0.9456  0.9456\n",
            "macro        0.7838  0.6537  0.7022\n",
            "weighted     0.9399  0.9456  0.9405\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.1537433251645416\n",
            "Test loss per 100 evaluation steps: 0.15515866114059462\n",
            "Test loss per 100 evaluation steps: 0.15071683248349776\n",
            "Test loss per 100 evaluation steps: 0.16255936042987743\n",
            "Test loss per 100 evaluation steps: 0.16353390874806792\n",
            "Test Loss: 0.16368721647953946\n",
            "Test Accuracy: 0.9554446777109207\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.77      0.75      0.76       636\n",
            "       B-ORG       0.72      0.66      0.69      1090\n",
            "       B-PER       0.91      0.90      0.90      2650\n",
            "       I-LOC       0.75      0.68      0.71       835\n",
            "       I-ORG       0.69      0.54      0.61      1401\n",
            "       I-PER       0.91      0.92      0.92      4266\n",
            "           O       0.98      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.96     59472\n",
            "   macro avg       0.82      0.78      0.80     59472\n",
            "weighted avg       0.95      0.96      0.95     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9559  0.9559  0.9559\n",
            "macro        0.8191  0.7756  0.7956\n",
            "weighted     0.9536  0.9559  0.9544\n",
            "Test steps: 501\n",
            "Training epoch: 5\n",
            "Training loss per 100 training steps: 0.03731830760720186\n",
            "Training loss per 100 training steps: 0.035132888420630476\n",
            "Training loss per 100 training steps: 0.04052456184009012\n",
            "Training loss per 100 training steps: 0.04269137401577609\n",
            "Training loss per 100 training steps: 0.04147777823297656\n",
            "Training loss per 100 training steps: 0.04215696416317466\n",
            "Training loss per 100 training steps: 0.04308503371619736\n",
            "Training loss per 100 training steps: 0.0430578134192001\n",
            "Training loss per 100 training steps: 0.04212914300930505\n",
            "Training loss per 100 training steps: 0.04294226913937018\n",
            "Training loss per 100 training steps: 0.04323300621310905\n",
            "Training loss per 100 training steps: 0.04353280192738263\n",
            "Training loss per 100 training steps: 0.044890758256011534\n",
            "Training loss epoch: 0.044651191518075194\n",
            "Training accuracy epoch: 0.9862852795092574\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.2162652849778533\n",
            "Validation loss per 100 evaluation steps: 0.23801525598741136\n",
            "Validation loss per 100 evaluation steps: 0.24295661673493063\n",
            "Validation loss per 100 evaluation steps: 0.24373485016887572\n",
            "Validation loss per 100 evaluation steps: 0.25629390077016434\n",
            "Validation Loss: 0.25629390077016434\n",
            "Validation Accuracy: 0.9359583432691264\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.71      0.64      0.68       309\n",
            "       B-ORG       0.74      0.39      0.51       736\n",
            "       B-PER       0.90      0.64      0.75      2323\n",
            "       I-LOC       0.59      0.60      0.59       431\n",
            "       I-ORG       0.67      0.35      0.46      1032\n",
            "       I-PER       0.91      0.86      0.88      4121\n",
            "           O       0.95      0.99      0.97     41492\n",
            "\n",
            "    accuracy                           0.94     50444\n",
            "   macro avg       0.78      0.64      0.69     50444\n",
            "weighted avg       0.93      0.94      0.93     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9362  0.9362  0.9362\n",
            "macro        0.7808  0.6399  0.6922\n",
            "weighted     0.9311  0.9362  0.9305\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.21573162312153726\n",
            "Test loss per 100 evaluation steps: 0.20905076510040088\n",
            "Test loss per 100 evaluation steps: 0.21285535930733507\n",
            "Test loss per 100 evaluation steps: 0.20972039297324954\n",
            "Test loss per 100 evaluation steps: 0.20698044499277604\n",
            "Test Loss: 0.20692749231755864\n",
            "Test Accuracy: 0.9461971119503133\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.75      0.74      0.75       636\n",
            "       B-ORG       0.75      0.58      0.65      1090\n",
            "       B-PER       0.92      0.79      0.85      2650\n",
            "       I-LOC       0.63      0.71      0.67       835\n",
            "       I-ORG       0.65      0.51      0.57      1401\n",
            "       I-PER       0.91      0.90      0.91      4266\n",
            "           O       0.97      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.95     59472\n",
            "   macro avg       0.80      0.75      0.77     59472\n",
            "weighted avg       0.94      0.95      0.94     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9463  0.9463  0.9463\n",
            "macro        0.7968  0.7455  0.7675\n",
            "weighted     0.9439  0.9463  0.9444\n",
            "Test steps: 501\n",
            "Training epoch: 6\n",
            "Training loss per 100 training steps: 0.03157555869882344\n",
            "Training loss per 100 training steps: 0.03407056456337159\n",
            "Training loss per 100 training steps: 0.031483306660229575\n",
            "Training loss per 100 training steps: 0.031544108776834036\n",
            "Training loss per 100 training steps: 0.03234319138686988\n",
            "Training loss per 100 training steps: 0.03215126847967137\n",
            "Training loss per 100 training steps: 0.031750510609875035\n",
            "Training loss per 100 training steps: 0.032061167163756184\n",
            "Training loss per 100 training steps: 0.03251987058409011\n",
            "Training loss per 100 training steps: 0.03314220241481962\n",
            "Training loss per 100 training steps: 0.03296541252930183\n",
            "Training loss per 100 training steps: 0.03379004205889942\n",
            "Training loss per 100 training steps: 0.03362705566892356\n",
            "Training loss epoch: 0.03372693030950212\n",
            "Training accuracy epoch: 0.9899883286696654\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.3242405319557292\n",
            "Validation loss per 100 evaluation steps: 0.2864022211404517\n",
            "Validation loss per 100 evaluation steps: 0.27250229343228666\n",
            "Validation loss per 100 evaluation steps: 0.2758948013369809\n",
            "Validation loss per 100 evaluation steps: 0.27990091491390195\n",
            "Validation Loss: 0.27990091491390195\n",
            "Validation Accuracy: 0.9421858182429639\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.69      0.67      0.68       309\n",
            "       B-ORG       0.81      0.32      0.46       736\n",
            "       B-PER       0.86      0.81      0.83      2323\n",
            "       I-LOC       0.66      0.57      0.61       431\n",
            "       I-ORG       0.75      0.25      0.37      1032\n",
            "       I-PER       0.84      0.93      0.88      4121\n",
            "           O       0.96      0.99      0.98     41492\n",
            "\n",
            "    accuracy                           0.94     50444\n",
            "   macro avg       0.79      0.65      0.69     50444\n",
            "weighted avg       0.94      0.94      0.94     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9425  0.9425  0.9425\n",
            "macro        0.7944  0.6473  0.6866\n",
            "weighted     0.9387  0.9425  0.9361\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.2307704279309837\n",
            "Test loss per 100 evaluation steps: 0.21098148479301015\n",
            "Test loss per 100 evaluation steps: 0.20509744347034334\n",
            "Test loss per 100 evaluation steps: 0.21453578964836198\n",
            "Test loss per 100 evaluation steps: 0.21592427042871715\n",
            "Test Loss: 0.21549478117034987\n",
            "Test Accuracy: 0.954591335683318\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.70      0.76      0.73       636\n",
            "       B-ORG       0.79      0.55      0.65      1090\n",
            "       B-PER       0.89      0.92      0.90      2650\n",
            "       I-LOC       0.68      0.69      0.68       835\n",
            "       I-ORG       0.75      0.43      0.55      1401\n",
            "       I-PER       0.89      0.94      0.91      4266\n",
            "           O       0.98      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.95     59472\n",
            "   macro avg       0.81      0.75      0.77     59472\n",
            "weighted avg       0.95      0.95      0.95     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9540  0.9540  0.9540\n",
            "macro        0.8105  0.7544  0.7732\n",
            "weighted     0.9515  0.9540  0.9513\n",
            "Test steps: 501\n",
            "Training epoch: 7\n",
            "Training loss per 100 training steps: 0.03640015371202025\n",
            "Training loss per 100 training steps: 0.02853246327784291\n",
            "Training loss per 100 training steps: 0.02673884036594245\n",
            "Training loss per 100 training steps: 0.02666331513757541\n",
            "Training loss per 100 training steps: 0.02789093346615846\n",
            "Training loss per 100 training steps: 0.029655037660838084\n",
            "Training loss per 100 training steps: 0.028737852999422882\n",
            "Training loss per 100 training steps: 0.02743159345557615\n",
            "Training loss per 100 training steps: 0.027510877009602457\n",
            "Training loss per 100 training steps: 0.027146463451463206\n",
            "Training loss per 100 training steps: 0.0267662676797177\n",
            "Training loss per 100 training steps: 0.02694719302685068\n",
            "Training loss per 100 training steps: 0.026777036240970258\n",
            "Training loss epoch: 0.027090623429942614\n",
            "Training accuracy epoch: 0.991896971500545\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.2907122039480601\n",
            "Validation loss per 100 evaluation steps: 0.27374746471876277\n",
            "Validation loss per 100 evaluation steps: 0.2716682433352495\n",
            "Validation loss per 100 evaluation steps: 0.25274499493469194\n",
            "Validation loss per 100 evaluation steps: 0.2507987908434006\n",
            "Validation Loss: 0.2507987908434006\n",
            "Validation Accuracy: 0.9400131382151983\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.79      0.60      0.68       309\n",
            "       B-ORG       0.68      0.45      0.54       736\n",
            "       B-PER       0.89      0.65      0.75      2323\n",
            "       I-LOC       0.74      0.54      0.62       431\n",
            "       I-ORG       0.61      0.39      0.47      1032\n",
            "       I-PER       0.90      0.89      0.89      4121\n",
            "           O       0.96      0.99      0.97     41492\n",
            "\n",
            "    accuracy                           0.94     50444\n",
            "   macro avg       0.79      0.64      0.71     50444\n",
            "weighted avg       0.93      0.94      0.93     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9397  0.9397  0.9397\n",
            "macro        0.7923  0.6449  0.7054\n",
            "weighted     0.9341  0.9397  0.9348\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.2307048906083219\n",
            "Test loss per 100 evaluation steps: 0.2215942854352761\n",
            "Test loss per 100 evaluation steps: 0.21355685385953013\n",
            "Test loss per 100 evaluation steps: 0.21162764324850286\n",
            "Test loss per 100 evaluation steps: 0.21395470015634782\n",
            "Test Loss: 0.21445938544705725\n",
            "Test Accuracy: 0.9467593719298119\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.80      0.65      0.71       636\n",
            "       B-ORG       0.65      0.65      0.65      1090\n",
            "       B-PER       0.92      0.78      0.85      2650\n",
            "       I-LOC       0.74      0.57      0.64       835\n",
            "       I-ORG       0.61      0.60      0.61      1401\n",
            "       I-PER       0.93      0.90      0.92      4266\n",
            "           O       0.97      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.95     59472\n",
            "   macro avg       0.80      0.74      0.77     59472\n",
            "weighted avg       0.95      0.95      0.95     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9479  0.9479  0.9479\n",
            "macro        0.8034  0.7354  0.7658\n",
            "weighted     0.9463  0.9479  0.9465\n",
            "Test steps: 501\n",
            "====================================================================================================\n",
            "loss_name: dlite\n",
            "model configuration\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Ner_Model(\n",
            "  (model): DebertaModel(\n",
            "    (embeddings): DebertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "      (LayerNorm): DebertaLayerNorm()\n",
            "      (dropout): StableDropout()\n",
            "    )\n",
            "    (encoder): DebertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x DebertaLayer(\n",
            "          (attention): DebertaAttention(\n",
            "            (self): DisentangledSelfAttention(\n",
            "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
            "              (pos_dropout): StableDropout()\n",
            "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "            (output): DebertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): DebertaLayerNorm()\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "          )\n",
            "          (intermediate): DebertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): DebertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): DebertaLayerNorm()\n",
            "            (dropout): StableDropout()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (rel_embeddings): Embedding(1024, 768)\n",
            "    )\n",
            "  )\n",
            "  (linear_model): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            "  (classifier): Linear(in_features=512, out_features=7, bias=True)\n",
            ")\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.3812756617367268\n",
            "Training loss per 100 training steps: 0.2553033781191334\n",
            "Training loss per 100 training steps: 0.2096170913253445\n",
            "Training loss per 100 training steps: 0.18565529548126505\n",
            "Training loss per 100 training steps: 0.17323260219331132\n",
            "Training loss per 100 training steps: 0.1633052162184322\n",
            "Training loss per 100 training steps: 0.15640164163183987\n",
            "Training loss per 100 training steps: 0.15141467596421931\n",
            "Training loss per 100 training steps: 0.1471767336649115\n",
            "Training loss per 100 training steps: 0.1433397854818096\n",
            "Training loss per 100 training steps: 0.14139553453892487\n",
            "Training loss per 100 training steps: 0.13861732585790784\n",
            "Training loss per 100 training steps: 0.1360879591381415\n",
            "Training loss epoch: 0.1358455629588549\n",
            "Training accuracy epoch: 0.8818573982456536\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.1732502165964101\n",
            "Validation loss per 100 evaluation steps: 0.1783848094129013\n",
            "Validation loss per 100 evaluation steps: 0.185828418280247\n",
            "Validation loss per 100 evaluation steps: 0.18037178942982224\n",
            "Validation loss per 100 evaluation steps: 0.17904891913139176\n",
            "Validation Loss: 0.17904891913139176\n",
            "Validation Accuracy: 0.8209506396779197\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.00      0.00      0.00       309\n",
            "       B-ORG       0.00      0.00      0.00       736\n",
            "       B-PER       0.00      0.00      0.00      2323\n",
            "       I-LOC       0.00      0.00      0.00       431\n",
            "       I-ORG       0.00      0.00      0.00      1032\n",
            "       I-PER       0.00      0.00      0.00      4121\n",
            "           O       0.82      1.00      0.90     41492\n",
            "\n",
            "    accuracy                           0.82     50444\n",
            "   macro avg       0.12      0.14      0.13     50444\n",
            "weighted avg       0.68      0.82      0.74     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.8225  0.8225  0.8225\n",
            "macro        0.1175  0.1429  0.1289\n",
            "weighted     0.6766  0.8225  0.7424\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.19353516299277543\n",
            "Test loss per 100 evaluation steps: 0.18997127521783114\n",
            "Test loss per 100 evaluation steps: 0.19140023238956927\n",
            "Test loss per 100 evaluation steps: 0.18845721603759585\n",
            "Test loss per 100 evaluation steps: 0.1871257104830222\n",
            "Test Loss: 0.18675220668638284\n",
            "Test Accuracy: 0.8132474115351946\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.00      0.00      0.00       636\n",
            "       B-ORG       0.00      0.00      0.00      1090\n",
            "       B-PER       0.00      0.00      0.00      2650\n",
            "       I-LOC       0.00      0.00      0.00       835\n",
            "       I-ORG       0.00      0.00      0.00      1401\n",
            "       I-PER       0.00      0.00      0.00      4266\n",
            "           O       0.82      1.00      0.90     48594\n",
            "\n",
            "    accuracy                           0.82     59472\n",
            "   macro avg       0.12      0.14      0.13     59472\n",
            "weighted avg       0.67      0.82      0.73     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.8171  0.8171  0.8171\n",
            "macro        0.1167  0.1429  0.1285\n",
            "weighted     0.6676  0.8171  0.7348\n",
            "Test steps: 501\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.10488741320255172\n",
            "Training loss per 100 training steps: 0.10718229707428123\n",
            "Training loss per 100 training steps: 0.11155929291581099\n",
            "Training loss per 100 training steps: 0.11264448106087087\n",
            "Training loss per 100 training steps: 0.11375746626874161\n",
            "Training loss per 100 training steps: 0.11094065606203724\n",
            "Training loss per 100 training steps: 0.10766000352468674\n",
            "Training loss per 100 training steps: 0.10460790085673892\n",
            "Training loss per 100 training steps: 0.10121563227016421\n",
            "Training loss per 100 training steps: 0.09896583346794864\n",
            "Training loss per 100 training steps: 0.09602378706114967\n",
            "Training loss per 100 training steps: 0.09344725890936596\n",
            "Training loss per 100 training steps: 0.09062497683458419\n",
            "Training loss epoch: 0.08971693223247627\n",
            "Training accuracy epoch: 0.9057085772664206\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.07726638780400208\n",
            "Validation loss per 100 evaluation steps: 0.07826400583758186\n",
            "Validation loss per 100 evaluation steps: 0.07954724746645904\n",
            "Validation loss per 100 evaluation steps: 0.07928855118242155\n",
            "Validation loss per 100 evaluation steps: 0.07897211400834567\n",
            "Validation Loss: 0.07897211400834567\n",
            "Validation Accuracy: 0.9126265968524058\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.44      0.69      0.54       309\n",
            "       B-ORG       0.64      0.27      0.38       736\n",
            "       B-PER       0.89      0.53      0.67      2323\n",
            "       I-LOC       0.38      0.50      0.44       431\n",
            "       I-ORG       0.61      0.18      0.27      1032\n",
            "       I-PER       0.89      0.74      0.81      4121\n",
            "           O       0.93      0.99      0.96     41492\n",
            "\n",
            "    accuracy                           0.91     50444\n",
            "   macro avg       0.68      0.56      0.58     50444\n",
            "weighted avg       0.91      0.91      0.90     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9136  0.9136  0.9136\n",
            "macro        0.6850  0.5572  0.5805\n",
            "weighted     0.9083  0.9136  0.9039\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.06739231458166614\n",
            "Test loss per 100 evaluation steps: 0.06634819317492656\n",
            "Test loss per 100 evaluation steps: 0.06770518858684227\n",
            "Test loss per 100 evaluation steps: 0.06732389383309055\n",
            "Test loss per 100 evaluation steps: 0.06753917779168114\n",
            "Test Loss: 0.06740439193977373\n",
            "Test Accuracy: 0.9292543877289621\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.50      0.79      0.61       636\n",
            "       B-ORG       0.70      0.38      0.49      1090\n",
            "       B-PER       0.93      0.67      0.78      2650\n",
            "       I-LOC       0.50      0.64      0.56       835\n",
            "       I-ORG       0.69      0.30      0.42      1401\n",
            "       I-PER       0.94      0.82      0.88      4266\n",
            "           O       0.95      0.99      0.97     48594\n",
            "\n",
            "    accuracy                           0.93     59472\n",
            "   macro avg       0.75      0.66      0.67     59472\n",
            "weighted avg       0.93      0.93      0.93     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9305  0.9305  0.9305\n",
            "macro        0.7450  0.6556  0.6734\n",
            "weighted     0.9291  0.9305  0.9251\n",
            "Test steps: 501\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.04814462123031262\n",
            "Training loss per 100 training steps: 0.047082184264172613\n",
            "Training loss per 100 training steps: 0.045876678809696994\n",
            "Training loss per 100 training steps: 0.04414236057271864\n",
            "Training loss per 100 training steps: 0.04449043351585351\n",
            "Training loss per 100 training steps: 0.0432152397458079\n",
            "Training loss per 100 training steps: 0.04330860635825177\n",
            "Training loss per 100 training steps: 0.04291404695783285\n",
            "Training loss per 100 training steps: 0.04241710053194462\n",
            "Training loss per 100 training steps: 0.04223155908334138\n",
            "Training loss per 100 training steps: 0.04115403783032641\n",
            "Training loss per 100 training steps: 0.04030781465686156\n",
            "Training loss per 100 training steps: 0.03990562712168587\n",
            "Training loss epoch: 0.04024251265854446\n",
            "Training accuracy epoch: 0.9570619709313788\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.06433179043699055\n",
            "Validation loss per 100 evaluation steps: 0.060661011526972286\n",
            "Validation loss per 100 evaluation steps: 0.05742078509540685\n",
            "Validation loss per 100 evaluation steps: 0.057735127960695534\n",
            "Validation loss per 100 evaluation steps: 0.0598398323644069\n",
            "Validation Loss: 0.0598398323644069\n",
            "Validation Accuracy: 0.9325564504964058\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.64      0.68      0.66       309\n",
            "       B-ORG       0.61      0.42      0.50       736\n",
            "       B-PER       0.92      0.74      0.82      2323\n",
            "       I-LOC       0.39      0.62      0.48       431\n",
            "       I-ORG       0.48      0.43      0.45      1032\n",
            "       I-PER       0.91      0.81      0.86      4121\n",
            "           O       0.96      0.98      0.97     41492\n",
            "\n",
            "    accuracy                           0.93     50444\n",
            "   macro avg       0.70      0.67      0.68     50444\n",
            "weighted avg       0.93      0.93      0.93     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9324  0.9324  0.9324\n",
            "macro        0.7000  0.6679  0.6758\n",
            "weighted     0.9319  0.9324  0.9309\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.04876268710009754\n",
            "Test loss per 100 evaluation steps: 0.04812639299692819\n",
            "Test loss per 100 evaluation steps: 0.04773424042854458\n",
            "Test loss per 100 evaluation steps: 0.047372737929017604\n",
            "Test loss per 100 evaluation steps: 0.04793477425620949\n",
            "Test Loss: 0.04791189167924663\n",
            "Test Accuracy: 0.9468670238250041\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.72      0.72      0.72       636\n",
            "       B-ORG       0.67      0.62      0.64      1090\n",
            "       B-PER       0.94      0.86      0.89      2650\n",
            "       I-LOC       0.55      0.70      0.62       835\n",
            "       I-ORG       0.60      0.56      0.58      1401\n",
            "       I-PER       0.94      0.86      0.90      4266\n",
            "           O       0.97      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.95     59472\n",
            "   macro avg       0.77      0.76      0.76     59472\n",
            "weighted avg       0.95      0.95      0.95     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9468  0.9468  0.9468\n",
            "macro        0.7689  0.7577  0.7611\n",
            "weighted     0.9472  0.9468  0.9466\n",
            "Test steps: 501\n",
            "Training epoch: 4\n",
            "Training loss per 100 training steps: 0.03042378549765999\n",
            "Training loss per 100 training steps: 0.03162859676075641\n",
            "Training loss per 100 training steps: 0.030105009329293657\n",
            "Training loss per 100 training steps: 0.031560952050911056\n",
            "Training loss per 100 training steps: 0.030709437877651\n",
            "Training loss per 100 training steps: 0.0313911779029983\n",
            "Training loss per 100 training steps: 0.031099123840475452\n",
            "Training loss per 100 training steps: 0.0306218059660182\n",
            "Training loss per 100 training steps: 0.030252357287354245\n",
            "Training loss per 100 training steps: 0.029980240227721878\n",
            "Training loss per 100 training steps: 0.029998756527974447\n",
            "Training loss per 100 training steps: 0.030149722489791583\n",
            "Training loss per 100 training steps: 0.030454344569954557\n",
            "Training loss epoch: 0.0305080099638984\n",
            "Training accuracy epoch: 0.9669958550210404\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.06784850525067668\n",
            "Validation loss per 100 evaluation steps: 0.06242703237207024\n",
            "Validation loss per 100 evaluation steps: 0.060883221928714495\n",
            "Validation loss per 100 evaluation steps: 0.06197483630282363\n",
            "Validation loss per 100 evaluation steps: 0.06394686365940197\n",
            "Validation Loss: 0.06394686365940197\n",
            "Validation Accuracy: 0.9297416775306283\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.92      0.53      0.67       309\n",
            "       B-ORG       0.83      0.22      0.35       736\n",
            "       B-PER       0.88      0.57      0.69      2323\n",
            "       I-LOC       0.87      0.38      0.53       431\n",
            "       I-ORG       0.80      0.14      0.24      1032\n",
            "       I-PER       0.88      0.86      0.87      4121\n",
            "           O       0.94      1.00      0.97     41492\n",
            "\n",
            "    accuracy                           0.93     50444\n",
            "   macro avg       0.87      0.53      0.62     50444\n",
            "weighted avg       0.92      0.93      0.92     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9293  0.9293  0.9293\n",
            "macro        0.8730  0.5297  0.6182\n",
            "weighted     0.9244  0.9293  0.9162\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.05690342011977918\n",
            "Test loss per 100 evaluation steps: 0.05588202743179863\n",
            "Test loss per 100 evaluation steps: 0.05801022406473445\n",
            "Test loss per 100 evaluation steps: 0.054637185288333966\n",
            "Test loss per 100 evaluation steps: 0.05515724641709312\n",
            "Test Loss: 0.0550739703271529\n",
            "Test Accuracy: 0.9398334868305867\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.89      0.62      0.73       636\n",
            "       B-ORG       0.78      0.37      0.50      1090\n",
            "       B-PER       0.87      0.75      0.80      2650\n",
            "       I-LOC       0.87      0.50      0.63       835\n",
            "       I-ORG       0.79      0.28      0.41      1401\n",
            "       I-PER       0.86      0.92      0.89      4266\n",
            "           O       0.95      1.00      0.98     48594\n",
            "\n",
            "    accuracy                           0.94     59472\n",
            "   macro avg       0.86      0.63      0.71     59472\n",
            "weighted avg       0.94      0.94      0.93     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9403  0.9403  0.9403\n",
            "macro        0.8584  0.6338  0.7073\n",
            "weighted     0.9352  0.9403  0.9320\n",
            "Test steps: 501\n",
            "Training epoch: 5\n",
            "Training loss per 100 training steps: 0.024157660026467055\n",
            "Training loss per 100 training steps: 0.026356365513765922\n",
            "Training loss per 100 training steps: 0.027449887172066763\n",
            "Training loss per 100 training steps: 0.02651232778403937\n",
            "Training loss per 100 training steps: 0.0253029188785149\n",
            "Training loss per 100 training steps: 0.02591982731674458\n",
            "Training loss per 100 training steps: 0.02529016234246488\n",
            "Training loss per 100 training steps: 0.0254707087401232\n",
            "Training loss per 100 training steps: 0.02605377824954101\n",
            "Training loss per 100 training steps: 0.02568152379703952\n",
            "Training loss per 100 training steps: 0.025790532729918346\n",
            "Training loss per 100 training steps: 0.02581881037884083\n",
            "Training loss per 100 training steps: 0.026145273043222145\n",
            "Training loss epoch: 0.026015259426311402\n",
            "Training accuracy epoch: 0.9715563684438053\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.06766370288722101\n",
            "Validation loss per 100 evaluation steps: 0.06667571414429403\n",
            "Validation loss per 100 evaluation steps: 0.06295508331866585\n",
            "Validation loss per 100 evaluation steps: 0.06175612390254173\n",
            "Validation loss per 100 evaluation steps: 0.06123594783870794\n",
            "Validation Loss: 0.06123594783870794\n",
            "Validation Accuracy: 0.9318606721767254\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.83      0.63      0.71       309\n",
            "       B-ORG       0.75      0.40      0.52       736\n",
            "       B-PER       0.92      0.58      0.71      2323\n",
            "       I-LOC       0.69      0.52      0.60       431\n",
            "       I-ORG       0.71      0.31      0.43      1032\n",
            "       I-PER       0.93      0.82      0.87      4121\n",
            "           O       0.94      0.99      0.97     41492\n",
            "\n",
            "    accuracy                           0.93     50444\n",
            "   macro avg       0.82      0.61      0.69     50444\n",
            "weighted avg       0.93      0.93      0.92     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9325  0.9325  0.9325\n",
            "macro        0.8228  0.6074  0.6866\n",
            "weighted     0.9273  0.9325  0.9247\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.045143415553684466\n",
            "Test loss per 100 evaluation steps: 0.048954767871300646\n",
            "Test loss per 100 evaluation steps: 0.04897013464164653\n",
            "Test loss per 100 evaluation steps: 0.04771689394423902\n",
            "Test loss per 100 evaluation steps: 0.04732287107930824\n",
            "Test Loss: 0.04733722620308444\n",
            "Test Accuracy: 0.9467883476590447\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.83      0.68      0.75       636\n",
            "       B-ORG       0.75      0.62      0.68      1090\n",
            "       B-PER       0.91      0.75      0.82      2650\n",
            "       I-LOC       0.74      0.61      0.67       835\n",
            "       I-ORG       0.71      0.51      0.59      1401\n",
            "       I-PER       0.92      0.89      0.91      4266\n",
            "           O       0.96      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.95     59472\n",
            "   macro avg       0.83      0.72      0.77     59472\n",
            "weighted avg       0.94      0.95      0.94     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9474  0.9474  0.9474\n",
            "macro        0.8331  0.7222  0.7713\n",
            "weighted     0.9437  0.9474  0.9443\n",
            "Test steps: 501\n",
            "Training epoch: 6\n",
            "Training loss per 100 training steps: 0.02150344141548885\n",
            "Training loss per 100 training steps: 0.019819720436321405\n",
            "Training loss per 100 training steps: 0.02130440184814385\n",
            "Training loss per 100 training steps: 0.02052500271252484\n",
            "Training loss per 100 training steps: 0.02112398373731594\n",
            "Training loss per 100 training steps: 0.02200224687325317\n",
            "Training loss per 100 training steps: 0.021911719533488524\n",
            "Training loss per 100 training steps: 0.022248113202227842\n",
            "Training loss per 100 training steps: 0.022260814813757357\n",
            "Training loss per 100 training steps: 0.021952284449290347\n",
            "Training loss per 100 training steps: 0.021940251210248375\n",
            "Training loss per 100 training steps: 0.021814331128028545\n",
            "Training loss per 100 training steps: 0.021803279635620967\n",
            "Training loss epoch: 0.021677029845786375\n",
            "Training accuracy epoch: 0.976313895527829\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.05193443713971647\n",
            "Validation loss per 100 evaluation steps: 0.05528034005088557\n",
            "Validation loss per 100 evaluation steps: 0.05269706261254517\n",
            "Validation loss per 100 evaluation steps: 0.05051459647174852\n",
            "Validation loss per 100 evaluation steps: 0.049995127433122434\n",
            "Validation Loss: 0.049995127433122434\n",
            "Validation Accuracy: 0.9456073313926355\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.88      0.59      0.71       309\n",
            "       B-ORG       0.72      0.40      0.52       736\n",
            "       B-PER       0.89      0.77      0.83      2323\n",
            "       I-LOC       0.84      0.46      0.59       431\n",
            "       I-ORG       0.68      0.31      0.42      1032\n",
            "       I-PER       0.90      0.88      0.89      4121\n",
            "           O       0.96      0.99      0.98     41492\n",
            "\n",
            "    accuracy                           0.94     50444\n",
            "   macro avg       0.84      0.63      0.70     50444\n",
            "weighted avg       0.94      0.94      0.94     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9450  0.9450  0.9450\n",
            "macro        0.8382  0.6289  0.7045\n",
            "weighted     0.9392  0.9450  0.9388\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.043105735757490035\n",
            "Test loss per 100 evaluation steps: 0.04527194114019949\n",
            "Test loss per 100 evaluation steps: 0.045275961230605996\n",
            "Test loss per 100 evaluation steps: 0.04475930681703175\n",
            "Test loss per 100 evaluation steps: 0.04426583218181622\n",
            "Test Loss: 0.04417945227998531\n",
            "Test Accuracy: 0.9518092293185051\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.86      0.66      0.74       636\n",
            "       B-ORG       0.70      0.61      0.65      1090\n",
            "       B-PER       0.89      0.89      0.89      2650\n",
            "       I-LOC       0.84      0.51      0.64       835\n",
            "       I-ORG       0.64      0.50      0.56      1401\n",
            "       I-PER       0.90      0.92      0.91      4266\n",
            "           O       0.97      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.95     59472\n",
            "   macro avg       0.83      0.73      0.77     59472\n",
            "weighted avg       0.95      0.95      0.95     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9523  0.9523  0.9523\n",
            "macro        0.8274  0.7273  0.7683\n",
            "weighted     0.9491  0.9523  0.9497\n",
            "Test steps: 501\n",
            "Training epoch: 7\n",
            "Training loss per 100 training steps: 0.01456878240338483\n",
            "Training loss per 100 training steps: 0.017307794528799148\n",
            "Training loss per 100 training steps: 0.0179061419817981\n",
            "Training loss per 100 training steps: 0.019376308881816157\n",
            "Training loss per 100 training steps: 0.019550817871039725\n",
            "Training loss per 100 training steps: 0.019694584665487583\n",
            "Training loss per 100 training steps: 0.019955075930641733\n",
            "Training loss per 100 training steps: 0.019738646180643967\n",
            "Training loss per 100 training steps: 0.019014512409763585\n",
            "Training loss per 100 training steps: 0.01903305238805888\n",
            "Training loss per 100 training steps: 0.019130401270049332\n",
            "Training loss per 100 training steps: 0.01903584662044523\n",
            "Training loss per 100 training steps: 0.018826289238139417\n",
            "Training loss epoch: 0.018605421222191956\n",
            "Training accuracy epoch: 0.9794565629078293\n",
            "Training steps: 1335\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.0541000264617719\n",
            "Validation loss per 100 evaluation steps: 0.0550980737901989\n",
            "Validation loss per 100 evaluation steps: 0.052661253488321715\n",
            "Validation loss per 100 evaluation steps: 0.053167971977124466\n",
            "Validation loss per 100 evaluation steps: 0.05291726619617793\n",
            "Validation Loss: 0.05291726619617793\n",
            "Validation Accuracy: 0.9416388912899563\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.79      0.61      0.69       309\n",
            "       B-ORG       0.68      0.44      0.53       736\n",
            "       B-PER       0.90      0.73      0.81      2323\n",
            "       I-LOC       0.72      0.48      0.57       431\n",
            "       I-ORG       0.60      0.44      0.51      1032\n",
            "       I-PER       0.91      0.86      0.88      4121\n",
            "           O       0.96      0.99      0.97     41492\n",
            "\n",
            "    accuracy                           0.94     50444\n",
            "   macro avg       0.79      0.65      0.71     50444\n",
            "weighted avg       0.94      0.94      0.94     50444\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9416  0.9416  0.9416\n",
            "macro        0.7936  0.6497  0.7098\n",
            "weighted     0.9366  0.9416  0.9375\n",
            "Validation steps: 500\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.04723241717752899\n",
            "Test loss per 100 evaluation steps: 0.04382659082116788\n",
            "Test loss per 100 evaluation steps: 0.04331547628397857\n",
            "Test loss per 100 evaluation steps: 0.04294937031806285\n",
            "Test loss per 100 evaluation steps: 0.04267736154687554\n",
            "Test Loss: 0.042592399940530414\n",
            "Test Accuracy: 0.9533613996277998\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.82      0.69      0.75       636\n",
            "       B-ORG       0.70      0.66      0.68      1090\n",
            "       B-PER       0.93      0.87      0.90      2650\n",
            "       I-LOC       0.78      0.59      0.67       835\n",
            "       I-ORG       0.62      0.60      0.61      1401\n",
            "       I-PER       0.94      0.89      0.91      4266\n",
            "           O       0.97      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.95     59472\n",
            "   macro avg       0.82      0.76      0.79     59472\n",
            "weighted avg       0.95      0.95      0.95     59472\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9533  0.9533  0.9533\n",
            "macro        0.8239  0.7556  0.7867\n",
            "weighted     0.9516  0.9533  0.9520\n",
            "Test steps: 501\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "def train(config,loss_name):\n",
        "    print(\"=\" * 100)\n",
        "    print(f\"loss_name: {loss_name}\")\n",
        "    model = Ner_Model(config, len(label2id), loss_name).to(config.device)\n",
        "    optimizer = get_optimizer(model, config)\n",
        "\n",
        "    valid_each_label_p_r_f1_list = []\n",
        "    valid_p_r_f1_list = []\n",
        "    test_each_label_p_r_f1_list = []\n",
        "    test_p_r_f1_list = []\n",
        "\n",
        "    valid_loss_list = []\n",
        "    test_loss_list = []\n",
        "\n",
        "    model.train()\n",
        "    interval = 100\n",
        "    for epoch in range(config.epochs):\n",
        "        print(f\"Training epoch: {epoch + 1}\")\n",
        "        tr_preds,tr_labels = [], []\n",
        "        total_loss = 0.0\n",
        "        tr_accuracy = 0.0\n",
        "        # print(\"\\n\" + \"-\" * 30 + \"\\n\")\n",
        "        # print(f\"epoch: {epoch},  train dataloader size: {len(train_dataloader)}\")\n",
        "        # print(f\"epoch: {epoch},  valid dataloader size: {len(valid_dataloader)}\")\n",
        "        # print(f\"epoch: {epoch},  test dataloader size: {len(test_dataloader)}\")\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            attention_mask = batch[\"id\"].ne(0)\n",
        "            targets = batch['label_id']\n",
        "            loss, logit= model(batch[\"id\"], batch['seq_length'], attention_mask=attention_mask,\n",
        "                                             labels=targets)\n",
        "\n",
        "            # compute training accuracy\n",
        "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = logit.view(-1, len(label2id)) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            active_accuracy = flattened_targets.ne(-100) # active accuracy is also of shape (batch_size * seq_len,)\n",
        "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "            tr_accuracy += tmp_tr_accuracy\n",
        "            tr_preds.extend(predictions)\n",
        "            tr_labels.extend(targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            if (step + 1) % interval == 0:\n",
        "                print(f\"Training loss per 100 training steps: {total_loss / (step+1)}\")\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        print(f\"Training loss epoch: {total_loss / (step+1)}\")\n",
        "        print(f\"Training accuracy epoch: {tr_accuracy / (step+1)}\")\n",
        "        print(f\"Training steps: {step+1}\")\n",
        "        print(\"\\n\\n\")\n",
        "        model.eval()\n",
        "\n",
        "\n",
        "        valid_loss, valid_p_r_f1,  valid_each_label_p_r_f1 = evaluate(model,valid_dataloader, \"Validation\")\n",
        "        valid_loss_list.append(valid_loss)\n",
        "        valid_p_r_f1_list.append(valid_p_r_f1)\n",
        "        valid_each_label_p_r_f1_list.append(valid_each_label_p_r_f1)\n",
        "\n",
        "        print(\"\\n\\n\")\n",
        "        test_loss, test_p_r_f1,test_each_label_p_r_f1  = evaluate(model,test_dataloader, \"Test\")\n",
        "        test_loss_list.append(test_loss)\n",
        "        test_p_r_f1_list.append(test_p_r_f1)\n",
        "        test_each_label_p_r_f1_list.append(test_each_label_p_r_f1)\n",
        "\n",
        "\n",
        "        #print(f\"epoch: {epoch}, train_loss: {train_loss}, \\n{train_p_r_f1}\")\n",
        "        #print(f\"epoch: {epoch}, valid_loss: {valid_loss}, \\n{valid_p_r_f1}\")\n",
        "        #print(f\"epoch: {epoch}, test_loss: {test_loss},  \\n {test_p_r_f1}\")\n",
        "        model.train()\n",
        "    return   {\n",
        "              \"valid_loss_list\":valid_loss_list,\n",
        "              \"test_loss_list\":test_loss_list,\n",
        "\n",
        "              \"valid_p_r_f1_list\":valid_p_r_f1_list,\n",
        "              \"valid_each_label_p_r_f1_list\":valid_each_label_p_r_f1_list,\n",
        "\n",
        "              \"test_p_r_f1_list\":test_p_r_f1_list,\n",
        "              \"test_each_label_p_r_f1_list\": test_each_label_p_r_f1_list}\n",
        "\n",
        "\n",
        "result = {}\n",
        "for loss_name in ['l1', 'l2', 'ce', 'kl', 'dlite']:\n",
        "    r = train(Config, loss_name)\n",
        "    result[loss_name] = r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS1O-MxfGr33",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "source": [
        "## Result Comparison after cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciR0WOCJGr33",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(\"result.pkl\", \"wb\") as f:\n",
        "    pickle.dump(result, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYodcFxGGr33"
      },
      "source": [
        "#### Overall Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgKkKD11Gr33",
        "outputId": "8207ee2e-17c8-48a9-c4a3-b9538bf87d16",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "micro\n",
            "    loss  precision  recall      f1\n",
            "0     l1     0.8929  0.8929  0.8929\n",
            "1     l2     0.9520  0.9520  0.9520\n",
            "2     ce     0.9510  0.9510  0.9510\n",
            "3     kl     0.9479  0.9479  0.9479\n",
            "4  dlite     0.9533  0.9533  0.9533\n",
            "====================================================================================================\n",
            "macro\n",
            "    loss  precision  recall      f1\n",
            "0     l1     0.4301  0.4328  0.3895\n",
            "1     l2     0.8336  0.7285  0.7744\n",
            "2     ce     0.8140  0.7648  0.7835\n",
            "3     kl     0.8034  0.7354  0.7658\n",
            "4  dlite     0.8239  0.7556  0.7867\n",
            "====================================================================================================\n",
            "weighted\n",
            "    loss  precision  recall      f1\n",
            "0     l1     0.8943  0.8929  0.8864\n",
            "1     l2     0.9484  0.9520  0.9493\n",
            "2     ce     0.9524  0.9510  0.9509\n",
            "3     kl     0.9463  0.9479  0.9465\n",
            "4  dlite     0.9516  0.9533  0.9520\n"
          ]
        }
      ],
      "source": [
        "columns = ['loss', 'precision', 'recall', 'f1']\n",
        "for t in ['micro', 'macro', 'weighted']:\n",
        "    df = []\n",
        "    for loss_name in loss_list:\n",
        "        row = {'loss': loss_name}\n",
        "        row['precision'] = result[loss_name]['test_p_r_f1_list'][-1].loc[t, 'precision']\n",
        "        row['recall'] = result[loss_name]['test_p_r_f1_list'][-1].loc[t, 'recall']\n",
        "        row['f1'] = result[loss_name]['test_p_r_f1_list'][-1].loc[t, 'f1']\n",
        "        df.append(row)\n",
        "    print(\"=\"*100)\n",
        "    print(t)\n",
        "    print(pd.DataFrame(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKeZYzqvGr33"
      },
      "source": [
        "#### Each label Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejx5I88-Gr34",
        "outputId": "c1bc0c63-3ef0-4117-bacb-c47a738f769d",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test dataset\n",
            "--------------------------------------------------\n",
            "l1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.00      0.00      0.00       636\n",
            "       B-ORG       0.00      0.00      0.00      1090\n",
            "       B-PER       0.95      0.58      0.72      2650\n",
            "       I-LOC       0.14      0.79      0.24       835\n",
            "       I-ORG       0.00      0.00      0.00      1401\n",
            "       I-PER       0.96      0.67      0.79      4266\n",
            "           O       0.96      0.99      0.97     48594\n",
            "\n",
            "    accuracy                           0.89     59472\n",
            "   macro avg       0.43      0.43      0.39     59472\n",
            "weighted avg       0.89      0.89      0.89     59472\n",
            "\n",
            "--------------------------------------------------\n",
            "l2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.83      0.65      0.73       636\n",
            "       B-ORG       0.74      0.61      0.67      1090\n",
            "       B-PER       0.91      0.85      0.88      2650\n",
            "       I-LOC       0.79      0.55      0.65       835\n",
            "       I-ORG       0.68      0.53      0.60      1401\n",
            "       I-PER       0.92      0.91      0.91      4266\n",
            "           O       0.97      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.95     59472\n",
            "   macro avg       0.83      0.73      0.77     59472\n",
            "weighted avg       0.95      0.95      0.95     59472\n",
            "\n",
            "--------------------------------------------------\n",
            "ce\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.84      0.67      0.75       636\n",
            "       B-ORG       0.65      0.73      0.69      1090\n",
            "       B-PER       0.94      0.81      0.87      2650\n",
            "       I-LOC       0.79      0.57      0.66       835\n",
            "       I-ORG       0.57      0.69      0.62      1401\n",
            "       I-PER       0.93      0.90      0.92      4266\n",
            "           O       0.98      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.95     59472\n",
            "   macro avg       0.81      0.76      0.78     59472\n",
            "weighted avg       0.95      0.95      0.95     59472\n",
            "\n",
            "--------------------------------------------------\n",
            "kl\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.80      0.65      0.71       636\n",
            "       B-ORG       0.65      0.65      0.65      1090\n",
            "       B-PER       0.92      0.78      0.85      2650\n",
            "       I-LOC       0.74      0.57      0.64       835\n",
            "       I-ORG       0.61      0.60      0.61      1401\n",
            "       I-PER       0.93      0.90      0.92      4266\n",
            "           O       0.97      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.95     59472\n",
            "   macro avg       0.80      0.74      0.77     59472\n",
            "weighted avg       0.95      0.95      0.95     59472\n",
            "\n",
            "--------------------------------------------------\n",
            "dlite\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.82      0.69      0.75       636\n",
            "       B-ORG       0.70      0.66      0.68      1090\n",
            "       B-PER       0.93      0.87      0.90      2650\n",
            "       I-LOC       0.78      0.59      0.67       835\n",
            "       I-ORG       0.62      0.60      0.61      1401\n",
            "       I-PER       0.94      0.89      0.91      4266\n",
            "           O       0.97      0.99      0.98     48594\n",
            "\n",
            "    accuracy                           0.95     59472\n",
            "   macro avg       0.82      0.76      0.79     59472\n",
            "weighted avg       0.95      0.95      0.95     59472\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"test dataset\")\n",
        "for loss_name in loss_list:\n",
        "    print(\"-\"*50)\n",
        "    print(loss_name)\n",
        "    print(result[loss_name]['test_each_label_p_r_f1_list'][-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqfxwsPKGr38"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "053603035cfc49bda58394a1430fa2b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4172a659d1c4a22b6196023c523bd7a",
            "max": 474,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_48c239d10b444ff6a0b3df02f23987cb",
            "value": 474
          }
        },
        "05cb78e546ec4193b9dca2a05313e696": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b8ab7248a0545c09285b395e5e22ecb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13fdabcd3ec1414fbb91917499bf067a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30682e8e0cbb4d11ae69f30b81d52675",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b10d7e8a23cd4d8491c8eceea018e698",
            "value": 456318
          }
        },
        "1a7b93126ec644518f34658935d7e809": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2725dff4e9e5420f8797650cdb17ab56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27ac0084829a4b668009fc1774abb049": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "293233eb8ec24b80a72f1d4a57db2566": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b64e8d570634f34bf325232ff1508be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bd0f9d0f94a422f8e9bbca727eab857": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e1b8bac73c840ed9abb1cfc6032e341": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_293233eb8ec24b80a72f1d4a57db2566",
            "placeholder": "​",
            "style": "IPY_MODEL_c3e7a4da2dbd4bfb9db9c5d8b05910c6",
            "value": "merges.txt: 100%"
          }
        },
        "2e489ff248c34ce3a93ca757f09f9d7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9552ca48ea94ef591bfef6cc0e16d4b",
            "placeholder": "​",
            "style": "IPY_MODEL_f867a7e05f0f4dc99bef3ef27e839837",
            "value": " 52.0/52.0 [00:00&lt;00:00, 4.05kB/s]"
          }
        },
        "2eaa5c86ef7d44208b6ef72cf3a92686": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30682e8e0cbb4d11ae69f30b81d52675": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33b98c115c43494da099f16791ca439d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35035a00517d40a491e1b234ab759b48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3652cfa75ee349ac9900daa5a23e749a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97693e173fd8439982e698c40f06e7ac",
            "placeholder": "​",
            "style": "IPY_MODEL_5d6797166255429a9a9a5d6d8e344519",
            "value": " 456k/456k [00:00&lt;00:00, 31.0MB/s]"
          }
        },
        "3d0fe1de2a4640eeb06d62d0dbab2e7b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f5be85c0efe42f0bd83cd9c9bb531b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3fd2877370ff43a293f8c7d837e594a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4273ade980794722bb1606f017cf83bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a7b93126ec644518f34658935d7e809",
            "max": 52,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55fa61fb78574c6fb1adb9d9612c459a",
            "value": 52
          }
        },
        "45f8cb11996d4468901fe7d53a22c2ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56521d7163f84a188d0a2b6e62fd4c23",
            "placeholder": "​",
            "style": "IPY_MODEL_3f5be85c0efe42f0bd83cd9c9bb531b9",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "48c239d10b444ff6a0b3df02f23987cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48e7cdf29cf14346977b92411cf5684c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55af4e76445d4b089643ad643182b13c",
              "IPY_MODEL_6db4caa38b064dfdb3e2a0fba7758fde",
              "IPY_MODEL_fa01c3d6c221462fa0f1fa44c658d668"
            ],
            "layout": "IPY_MODEL_3fd2877370ff43a293f8c7d837e594a6"
          }
        },
        "50287fd7f7174e67904755e2d0a106b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55af4e76445d4b089643ad643182b13c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bd0f9d0f94a422f8e9bbca727eab857",
            "placeholder": "​",
            "style": "IPY_MODEL_2725dff4e9e5420f8797650cdb17ab56",
            "value": "vocab.json: 100%"
          }
        },
        "55fa61fb78574c6fb1adb9d9612c459a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "56521d7163f84a188d0a2b6e62fd4c23": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d6797166255429a9a9a5d6d8e344519": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6db4caa38b064dfdb3e2a0fba7758fde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d0fe1de2a4640eeb06d62d0dbab2e7b",
            "max": 898825,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da51306992fc46c081d060334dc025ce",
            "value": 898825
          }
        },
        "71b8d11b1c2e4c39bab31f14c57a4a04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eeea880059b343af93af6c814e8948be",
              "IPY_MODEL_f89a020d4307401ea8c19b7b77d05cc4",
              "IPY_MODEL_91bb307f4dac4698bf20439d15488ac3"
            ],
            "layout": "IPY_MODEL_7fb9d257657749218960091e8497f534"
          }
        },
        "7256149fd04948eca3dcf636f7d59a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_45f8cb11996d4468901fe7d53a22c2ae",
              "IPY_MODEL_4273ade980794722bb1606f017cf83bb",
              "IPY_MODEL_2e489ff248c34ce3a93ca757f09f9d7b"
            ],
            "layout": "IPY_MODEL_2b64e8d570634f34bf325232ff1508be"
          }
        },
        "7fb9d257657749218960091e8497f534": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d9d9027985142718e7cf8a06f58946d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91bb307f4dac4698bf20439d15488ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d9d9027985142718e7cf8a06f58946d",
            "placeholder": "​",
            "style": "IPY_MODEL_0b8ab7248a0545c09285b395e5e22ecb",
            "value": " 559M/559M [00:01&lt;00:00, 353MB/s]"
          }
        },
        "97693e173fd8439982e698c40f06e7ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c9077f56a7a4bec9d818d958d067c66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2e1b8bac73c840ed9abb1cfc6032e341",
              "IPY_MODEL_13fdabcd3ec1414fbb91917499bf067a",
              "IPY_MODEL_3652cfa75ee349ac9900daa5a23e749a"
            ],
            "layout": "IPY_MODEL_dc4f542c9c72483aaa3ddb19b1d15a69"
          }
        },
        "9db6ced7c2c246b7a179b40866da6b9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b10d7e8a23cd4d8491c8eceea018e698": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b583c35bff8a46a89b8d223afa4dfd92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc455f2a4c824381bea1f814cde5edc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05cb78e546ec4193b9dca2a05313e696",
            "placeholder": "​",
            "style": "IPY_MODEL_b583c35bff8a46a89b8d223afa4dfd92",
            "value": " 474/474 [00:00&lt;00:00, 37.1kB/s]"
          }
        },
        "c0fc05759e1448409fdf1257ca0477e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d39b0e5a8dad4098a1a957d3b4f9cf7d",
              "IPY_MODEL_053603035cfc49bda58394a1430fa2b7",
              "IPY_MODEL_bc455f2a4c824381bea1f814cde5edc0"
            ],
            "layout": "IPY_MODEL_2eaa5c86ef7d44208b6ef72cf3a92686"
          }
        },
        "c3e7a4da2dbd4bfb9db9c5d8b05910c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d39b0e5a8dad4098a1a957d3b4f9cf7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27ac0084829a4b668009fc1774abb049",
            "placeholder": "​",
            "style": "IPY_MODEL_ef633886efaf478e9c4002e949086579",
            "value": "config.json: 100%"
          }
        },
        "d4172a659d1c4a22b6196023c523bd7a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da51306992fc46c081d060334dc025ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dba6a3d3055a43b799dd15ffc51292f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc4f542c9c72483aaa3ddb19b1d15a69": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6a10ce2ceb24a86af9ecfcfe64c1c89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eeea880059b343af93af6c814e8948be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dba6a3d3055a43b799dd15ffc51292f4",
            "placeholder": "​",
            "style": "IPY_MODEL_e6a10ce2ceb24a86af9ecfcfe64c1c89",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "ef633886efaf478e9c4002e949086579": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f867a7e05f0f4dc99bef3ef27e839837": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f89a020d4307401ea8c19b7b77d05cc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50287fd7f7174e67904755e2d0a106b9",
            "max": 558614189,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9db6ced7c2c246b7a179b40866da6b9c",
            "value": 558614189
          }
        },
        "f9552ca48ea94ef591bfef6cc0e16d4b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa01c3d6c221462fa0f1fa44c658d668": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33b98c115c43494da099f16791ca439d",
            "placeholder": "​",
            "style": "IPY_MODEL_35035a00517d40a491e1b234ab759b48",
            "value": " 899k/899k [00:00&lt;00:00, 3.83MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}