{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEBdvK_nGr3w",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import classification_report\n",
        "import random ,json\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, Dataset\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hskApG2OGr3x"
      },
      "source": [
        "## Setting Basic Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlnriKCUGr3y",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    batch_size = 4\n",
        "    epochs = 1\n",
        "    lr = 1e-5\n",
        "    seed = 123\n",
        "\n",
        "\n",
        "    # Can only have one True\n",
        "    # only_use_standard_linear_layer=True, use linear layer\n",
        "    only_use_standard_linear_layer = True\n",
        "    # Setting the linear layer number\n",
        "    linear_layer_num = 0\n",
        "    linear_layer = [512, 512, 512]\n",
        "\n",
        "\n",
        "    # if only_use_standard_linear_layer is False, only_use_dropout is True, it means just use dropout.\n",
        "    only_use_dropout = False\n",
        "    dropout_prob = [0.05, 0, 0.05]\n",
        "\n",
        "\n",
        "    # if only_use_standard_linear_layer is False, only_use_residual = True, just use resdiual\n",
        "    only_use_residual = False\n",
        "    # if only_use_standard_linear_layer is False, only_use_residual_and_dropout = True, use residual and dropout together.\n",
        "    only_use_residual_and_dropout = False\n",
        "\n",
        "\n",
        "\n",
        "    assert sum([1 if only_use_standard_linear_layer else 0,\n",
        "                1 if only_use_dropout else 0,\n",
        "                1 if only_use_residual else 0,\n",
        "                1 if only_use_residual_and_dropout else 0]) == 1\n",
        "\n",
        "\n",
        "\n",
        "    # if lstm layer is 0, then not using the lstm layer. Setting the lstm layer based on layers number required\n",
        "    lstm_layer_num = 0\n",
        "    bi_lstm=True\n",
        "\n",
        "\n",
        "    # Internet resource; download from Internet\n",
        "    # model_name = \"microsoft/deberta-v3-base\"\n",
        "\n",
        "    model_name = \"microsoft/deberta-base\"\n",
        "    # model_name = \"deberta-base\"\n",
        "\n",
        "    hidden_size=768\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_data_name = \"wikineural\" # conll2003,  ner_datasetreference, broad_twitter_corpus, wnut, wikineural, multinerd\n",
        "\n",
        "    @classmethod\n",
        "    def describe(cls):\n",
        "        parm = {\"train_data_name\": cls.train_data_name,\n",
        "                \"encoder_name\": cls.model_name,\n",
        "                \"batch_size\": cls.batch_size,\n",
        "                \"epochs\": cls.epochs,\n",
        "                \"lr\": cls.lr,\n",
        "                \"seed\": cls.seed,\n",
        "                \"bi_lstm\": cls.bi_lstm,\n",
        "                \"lstm_layer_num\": cls.lstm_layer_num,\n",
        "                \"linear_layer\": cls.linear_layer,\n",
        "                \"linear_layer_num\":cls.linear_layer_num,\n",
        "                \"dropout_prob\": cls.dropout_prob,\n",
        "                \"only_use_standard_linear_layer\": cls.only_use_standard_linear_layer,\n",
        "                \"only_use_dropout\": cls.only_use_dropout,\n",
        "                \"only_use_residual\": cls.only_use_residual,\n",
        "                \"only_use_residual_and_dropout\": cls.only_use_residual_and_dropout}\n",
        "        return json.dumps(parm , ensure_ascii=False, indent=2)\n",
        "\n",
        "random.seed(Config.seed)\n",
        "np.random.seed(Config.seed)\n",
        "torch.manual_seed(Config.seed)\n",
        "torch.cuda.manual_seed_all(Config.seed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQGJgAySrwEr"
      },
      "source": [
        "## given configuration result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BObln4RIGr3y",
        "outputId": "add05b2c-9171-48cb-a579-bc4a27ff7529",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"train_data_name\": \"wikineural\",\n",
            "  \"encoder_name\": \"microsoft/deberta-base\",\n",
            "  \"batch_size\": 4,\n",
            "  \"epochs\": 1,\n",
            "  \"lr\": 1e-05,\n",
            "  \"seed\": 123,\n",
            "  \"bi_lstm\": true,\n",
            "  \"lstm_layer_num\": 0,\n",
            "  \"linear_layer\": [\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"linear_layer_num\": 0,\n",
            "  \"dropout_prob\": [\n",
            "    0.05,\n",
            "    0,\n",
            "    0.05\n",
            "  ],\n",
            "  \"only_use_standard_linear_layer\": true,\n",
            "  \"only_use_dropout\": false,\n",
            "  \"only_use_residual\": false,\n",
            "  \"only_use_residual_and_dropout\": false\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(Config.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9ig_SZXGr3z"
      },
      "source": [
        "## Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "7CY8ivunDRSs"
      },
      "outputs": [],
      "source": [
        "def read_broad_twitter_corpus(data_dir):\n",
        "    ret, sample  = [], []\n",
        "    for file in sorted(os.listdir(data_dir)):\n",
        "        if file.endswith(\".conll\"):\n",
        "            file = os.path.join(data_dir, file)\n",
        "            for idx,line in  enumerate(open(file, \"r\", encoding=\"utf-8\")):\n",
        "                line = line.strip()\n",
        "                if line == \"\":\n",
        "                    if len(sample) > 0 :\n",
        "                        ret.append({\"word\": [i[0] for i in sample], \"tag\": [i[1] for i in sample]})\n",
        "                    sample = []\n",
        "                else:\n",
        "                    tokens = line.split(\"\\t\")\n",
        "                    if len(tokens) != 2:\n",
        "                        continue\n",
        "                    else:\n",
        "                        sample.append(tokens)\n",
        "            if len(sample) > 0:\n",
        "                ret.append({\"word\": [i[0] for i in sample], \"tag\": [i[1] for i in sample]})\n",
        "\n",
        "    return pd.DataFrame(ret)\n",
        "\n",
        "def read_wnut(file_path):\n",
        "    data = []\n",
        "    sample = []\n",
        "    for idx, line in enumerate(open(file_path)):\n",
        "        if idx == 0:\n",
        "            continue\n",
        "        line = line.strip()\n",
        "        if line == \"\":\n",
        "            if len(sample) != 0:\n",
        "                data.append(sample)\n",
        "            sample = []\n",
        "        else:\n",
        "            line = line.split()\n",
        "            assert len(line) == 2\n",
        "            sample.append([line[0], line[-1]])\n",
        "    if len(sample) != 0:\n",
        "        data.append(sample)\n",
        "    data = [{\"word\": [i[0] for i in sample], \"tag\": [i[1] for i in sample]} for sample in data]\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "def read_multinerd(file_path):\n",
        "    labels = {\n",
        "    \"O\": 0,\n",
        "    \"B-PER\": 1,\n",
        "    \"I-PER\": 2,\n",
        "    \"B-ORG\": 3,\n",
        "    \"I-ORG\": 4,\n",
        "    \"B-LOC\": 5,\n",
        "    \"I-LOC\": 6,\n",
        "    \"B-ANIM\": 7,\n",
        "    \"I-ANIM\": 8,\n",
        "    \"B-BIO\": 9,\n",
        "    \"I-BIO\": 10,\n",
        "    \"B-CEL\": 11,\n",
        "    \"I-CEL\": 12,\n",
        "    \"B-DIS\": 13,\n",
        "    \"I-DIS\": 14,\n",
        "    \"B-EVE\": 15,\n",
        "    \"I-EVE\": 16,\n",
        "    \"B-FOOD\": 17,\n",
        "    \"I-FOOD\": 18,\n",
        "    \"B-INST\": 19,\n",
        "    \"I-INST\": 20,\n",
        "    \"B-MEDIA\": 21,\n",
        "    \"I-MEDIA\": 22,\n",
        "    \"B-MYTH\": 23,\n",
        "    \"I-MYTH\": 24,\n",
        "    \"B-PLANT\": 25,\n",
        "    \"I-PLANT\": 26,\n",
        "    \"B-TIME\": 27,\n",
        "    \"I-TIME\": 28,\n",
        "    \"B-VEHI\": 29,\n",
        "    \"I-VEHI\": 30,\n",
        "  }\n",
        "    id2label = dict([[v,k] for k,v in labels.items()])\n",
        "    data = []\n",
        "    for line in open(file_path, \"r\", encoding=\"utf-8\"):\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            line = json.loads(line)\n",
        "            tokens = line['tokens']\n",
        "            tags = [id2label[i] for i in line['ner_tags']]\n",
        "            assert line['lang'] == 'en'\n",
        "            data.append([[i,j] for i,j in zip(tokens, tags)])\n",
        "    data = [{\"word\": [i[0] for i in sample], \"tag\": [i[1] for i in sample]} for sample in data]\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "def read_wikineural(file_path):\n",
        "    labels = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
        "    id2label = dict([[v,k] for k,v in labels.items()])\n",
        "    data = []\n",
        "    for line in open(file_path, \"r\", encoding=\"utf-8\"):\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            line = json.loads(line)\n",
        "            tokens = line['tokens']\n",
        "            tags = [id2label[i] for i in line['ner_tags']]\n",
        "            assert line['lang'] == 'en'\n",
        "            data.append([[i,j] for i,j in zip(tokens, tags)])\n",
        "    data = [{\"word\": [i[0] for i in sample], \"tag\": [i[1] for i in sample]} for sample in data]\n",
        "    return pd.DataFrame(data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGZ_d-HrGr3z",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def read_conll2003(file_path):\n",
        "    data = []\n",
        "    sample = []\n",
        "    for idx, line in enumerate(open(file_path)):\n",
        "        if idx == 0:\n",
        "            continue\n",
        "        line = line.strip()\n",
        "        if line == \"\":\n",
        "            if len(sample) != 0:\n",
        "                data.append(sample)\n",
        "            sample = []\n",
        "        else:\n",
        "            line = line.split()\n",
        "            assert len(line) == 4\n",
        "            sample.append([line[0], line[-1]])\n",
        "    if len(sample) != 0:\n",
        "        data.append(sample)\n",
        "    data = [{\"word\": [i[0] for i in sample], \"tag\": [i[1] for i in sample]} for sample in data]\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Setting the chosen dataset, conll2003 or ner_datasetreference.\n",
        "if Config.train_data_name == \"conll2003\":\n",
        "    train_path = os.path.join(Config.train_data_name, 'train.txt')\n",
        "    dev_path = os.path.join(Config.train_data_name, 'valid.txt')\n",
        "    test_path = os.path.join(Config.train_data_name, 'test.txt')\n",
        "    train_df = read_conll2003(train_path)\n",
        "    valid_df = read_conll2003(dev_path)\n",
        "    test_df = read_conll2003(test_path)\n",
        "    print(train_df.shape, valid_df.shape, test_df.shape)\n",
        "elif Config.train_data_name == \"ner_datasetreference\":\n",
        "    df = pd.read_csv(\"ner_datasetreference.csv\", encoding='iso-8859-1')\n",
        "    data = []\n",
        "    word, tag = [], []\n",
        "    for i,j,k in zip(df['Sentence #'], df['Word'], df['Tag']):\n",
        "        if not pd.isnull(i):\n",
        "            assert i.startswith('Sentence')\n",
        "            if len(word) > 0:\n",
        "                data.append({\"word\":word, \"tag\":tag})\n",
        "            word, tag = [], []\n",
        "        if isinstance(j, str) and isinstance(k, str):\n",
        "            # remove 'art', 'eve', 'nat' label for better macro results\n",
        "            if any( t in k for t in ['art', 'eve', 'nat']):\n",
        "                continue\n",
        "            word.append(j)\n",
        "            tag.append(k)\n",
        "    if len(word) > 0:\n",
        "        data.append({\"word\":word, \"tag\":tag})\n",
        "        word, tag = [], []\n",
        "    print(data[0], data[-1])\n",
        "    df = pd.DataFrame(data)\n",
        "    train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
        "    valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "    print(df.shape, train_df.shape, valid_df.shape, test_df.shape)\n",
        "elif Config.train_data_name == 'broad_twitter_corpus':\n",
        "    train_df = read_broad_twitter_corpus(os.path.join(Config.train_data_name, 'train'))\n",
        "    valid_df = read_broad_twitter_corpus(os.path.join(Config.train_data_name, 'dev'))\n",
        "    test_df = read_broad_twitter_corpus(os.path.join(Config.train_data_name, 'test'))\n",
        "    print(train_df.shape, valid_df.shape, test_df.shape)\n",
        "elif Config.train_data_name == \"wikineural\":\n",
        "    train_path = os.path.join(Config.train_data_name, 'train_en.jsonl')\n",
        "    dev_path = os.path.join(Config.train_data_name, 'val_en.jsonl')\n",
        "    test_path = os.path.join(Config.train_data_name, 'test_en.jsonl')\n",
        "    train_df = read_wikineural(train_path)\n",
        "    valid_df = read_wikineural(dev_path)\n",
        "    test_df = read_wikineural(test_path)\n",
        "elif Config.train_data_name == \"multinerd\":\n",
        "    train_path = os.path.join(Config.train_data_name, 'train_en.jsonl')\n",
        "    dev_path = os.path.join(Config.train_data_name, 'val_val_en.jsonl')\n",
        "    test_path = os.path.join(Config.train_data_name, 'test_test_en.jsonl')\n",
        "    train_df = read_multinerd(train_path)\n",
        "    valid_df = read_multinerd(dev_path)\n",
        "    test_df = read_multinerd(test_path)\n",
        "elif Config.train_data_name == \"wnut\":\n",
        "    train_path = os.path.join(Config.train_data_name, 'train')\n",
        "    dev_path = os.path.join(Config.train_data_name, 'dev')\n",
        "    test_path = os.path.join(Config.train_data_name, 'test')\n",
        "    train_df = read_wnut(train_path)\n",
        "    valid_df = read_wnut(dev_path)\n",
        "    test_df = read_wnut(test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "BYJxdEf9Gr30",
        "outputId": "52ebf96c-79c5-48b0-fb23-96860a341dc8",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    word  \\\n",
              "0      [This, division, also, contains, the, Ventana,...   \n",
              "1      [\", So, here, is, the, balance, NBC, has, to, ...   \n",
              "2      [It, is, a, protest, song, that, \", creates, a...   \n",
              "3      [This, differs, from, approaches, such, as, IP...   \n",
              "4      [Since, then, ,, only, Terry, Bradshaw, in, 14...   \n",
              "...                                                  ...   \n",
              "92715  [The, couple, had, a, son, ,, David, ,, and, a...   \n",
              "92716  [The, Home, Secretary, ,, J., R., Clynes, ,, w...   \n",
              "92717  [At, the, time, of, her, birth, ,, she, was, f...   \n",
              "92718  [The, film, was, based, on, the, Broadway, pla...   \n",
              "92719  [The, couple, had, two, children, (, both, bor...   \n",
              "\n",
              "                                                     tag  \n",
              "0      [O, O, O, O, O, B-LOC, I-LOC, O, O, O, O, B-LO...  \n",
              "1      [O, O, O, O, O, O, B-ORG, O, O, O, O, B-MISC, ...  \n",
              "2      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "3      [O, O, O, O, O, O, O, O, B-MISC, O, O, O, O, O...  \n",
              "4      [O, O, O, O, B-PER, I-PER, O, O, O, O, B-PER, ...  \n",
              "...                                                  ...  \n",
              "92715  [O, O, O, O, O, O, B-PER, O, O, O, O, O, B-PER...  \n",
              "92716  [O, O, O, O, B-PER, I-PER, I-PER, O, O, O, O, ...  \n",
              "92717  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "92718       [O, O, O, O, O, O, B-MISC, O, O, O, O, O, O]  \n",
              "92719  [O, O, O, O, O, O, O, O, O, B-MISC, O, O, B-PE...  \n",
              "\n",
              "[92720 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0c8ca98d-8cbc-4601-a868-552035a3f750\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[This, division, also, contains, the, Ventana,...</td>\n",
              "      <td>[O, O, O, O, O, B-LOC, I-LOC, O, O, O, O, B-LO...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[\", So, here, is, the, balance, NBC, has, to, ...</td>\n",
              "      <td>[O, O, O, O, O, O, B-ORG, O, O, O, O, B-MISC, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[It, is, a, protest, song, that, \", creates, a...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[This, differs, from, approaches, such, as, IP...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, B-MISC, O, O, O, O, O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Since, then, ,, only, Terry, Bradshaw, in, 14...</td>\n",
              "      <td>[O, O, O, O, B-PER, I-PER, O, O, O, O, B-PER, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92715</th>\n",
              "      <td>[The, couple, had, a, son, ,, David, ,, and, a...</td>\n",
              "      <td>[O, O, O, O, O, O, B-PER, O, O, O, O, O, B-PER...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92716</th>\n",
              "      <td>[The, Home, Secretary, ,, J., R., Clynes, ,, w...</td>\n",
              "      <td>[O, O, O, O, B-PER, I-PER, I-PER, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92717</th>\n",
              "      <td>[At, the, time, of, her, birth, ,, she, was, f...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92718</th>\n",
              "      <td>[The, film, was, based, on, the, Broadway, pla...</td>\n",
              "      <td>[O, O, O, O, O, O, B-MISC, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92719</th>\n",
              "      <td>[The, couple, had, two, children, (, both, bor...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, B-MISC, O, O, B-PE...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>92720 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0c8ca98d-8cbc-4601-a868-552035a3f750')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0c8ca98d-8cbc-4601-a868-552035a3f750 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0c8ca98d-8cbc-4601-a868-552035a3f750');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e51e4b63-8e2f-4ba9-a6c5-468550d701a6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e51e4b63-8e2f-4ba9-a6c5-468550d701a6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e51e4b63-8e2f-4ba9-a6c5-468550d701a6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_6944e6e4-8959-43c4-a33b-d171b391ff4c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('train_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_6944e6e4-8959-43c4-a33b-d171b391ff4c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('train_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df",
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 92720,\n  \"fields\": [\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tag\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "qNmakWvLGr30",
        "outputId": "c5647593-f9c0-4b1f-9762-f74e9c5e65b9",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    word  \\\n",
              "0      [On, this, occasion, he, failed, to, gain, the...   \n",
              "1      [On, both, these, occasions, he, was, backed, ...   \n",
              "2      [He, also, appeared, as, himself, in, the, 199...   \n",
              "3      [The, Colorado, Rockies, were, created, as, an...   \n",
              "4      [He, kept, busy, recording, demo, tapes, at, h...   \n",
              "...                                                  ...   \n",
              "11592                                [com, ,, Amazon, .]   \n",
              "11593  [In, January, 2013, ,, the, European, Food, Sa...   \n",
              "11594  [All, of, the, games, had, art, true, to, the,...   \n",
              "11595  [There, was, also, a, game, made, for, the, Ga...   \n",
              "11596  [This, system, was, widely, copied, in, variou...   \n",
              "\n",
              "                                                     tag  \n",
              "0      [O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG...  \n",
              "1      [O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-OR...  \n",
              "2           [O, O, O, O, O, O, O, O, O, O, B-MISC, O, O]  \n",
              "3      [O, B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, B...  \n",
              "4      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "...                                                  ...  \n",
              "11592                                   [O, O, B-ORG, O]  \n",
              "11593  [O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, O,...  \n",
              "11594  [O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, O,...  \n",
              "11595  [O, O, O, O, O, O, O, O, B-MISC, I-MISC, O, O,...  \n",
              "11596                 [O, O, O, O, O, O, O, B-ORG, O, O]  \n",
              "\n",
              "[11597 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7d79e17a-4d93-44df-8cd0-d17b7a3cf16b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[On, this, occasion, he, failed, to, gain, the...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[On, both, these, occasions, he, was, backed, ...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-OR...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[He, also, appeared, as, himself, in, the, 199...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, B-MISC, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[The, Colorado, Rockies, were, created, as, an...</td>\n",
              "      <td>[O, B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[He, kept, busy, recording, demo, tapes, at, h...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11592</th>\n",
              "      <td>[com, ,, Amazon, .]</td>\n",
              "      <td>[O, O, B-ORG, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11593</th>\n",
              "      <td>[In, January, 2013, ,, the, European, Food, Sa...</td>\n",
              "      <td>[O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11594</th>\n",
              "      <td>[All, of, the, games, had, art, true, to, the,...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11595</th>\n",
              "      <td>[There, was, also, a, game, made, for, the, Ga...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, B-MISC, I-MISC, O, O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11596</th>\n",
              "      <td>[This, system, was, widely, copied, in, variou...</td>\n",
              "      <td>[O, O, O, O, O, O, O, B-ORG, O, O]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11597 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7d79e17a-4d93-44df-8cd0-d17b7a3cf16b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7d79e17a-4d93-44df-8cd0-d17b7a3cf16b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7d79e17a-4d93-44df-8cd0-d17b7a3cf16b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-253971b4-d8f1-4662-8473-d93698649d8d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-253971b4-d8f1-4662-8473-d93698649d8d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-253971b4-d8f1-4662-8473-d93698649d8d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_9831209f-0f67-4b1a-a7ca-e64d8146ef69\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('test_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9831209f-0f67-4b1a-a7ca-e64d8146ef69 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('test_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_df",
              "summary": "{\n  \"name\": \"test_df\",\n  \"rows\": 11597,\n  \"fields\": [\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tag\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "wdv02qIvGr30",
        "outputId": "b8235610-dd75-47ce-93d7-1f759ad5d850",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    word  \\\n",
              "0      [Her, visit, to, Tuvalu, was, cut, short, by, ...   \n",
              "1      [In, 1975, ,, the, Princess, was, listed, amon...   \n",
              "2      [In, 1974, ,, she, invited, him, as, a, guest,...   \n",
              "3      [She, co-starred, with, Richard, Widmark, and,...   \n",
              "4      [She, experienced, a, mild, stroke, on, 23, Fe...   \n",
              "...                                                  ...   \n",
              "11585  [However, ,, he, played, his, home, games, at,...   \n",
              "11586  [256, with, 11, triples, in, 277, at, bats, wh...   \n",
              "11587  [He, played, his, final, professional, games, ...   \n",
              "11588  [The, city, is, also, the, setting, for, the, ...   \n",
              "11589  [He, was, elected, as, Member, of, Parliament,...   \n",
              "\n",
              "                                                     tag  \n",
              "0      [O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O,...  \n",
              "1      [O, O, O, O, O, O, O, O, O, O, O, O, B-PER, I-...  \n",
              "2      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "3      [O, O, O, B-PER, I-PER, O, B-PER, I-PER, O, O,...  \n",
              "4      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-L...  \n",
              "...                                                  ...  \n",
              "11585  [O, O, O, O, O, O, O, O, B-LOC, I-LOC, O, O, O...  \n",
              "11586  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "11587    [O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, O]  \n",
              "11588  [O, O, O, O, O, O, O, O, B-ORG, I-ORG, O, O, B...  \n",
              "11589  [O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O]  \n",
              "\n",
              "[11590 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-87e5062e-eee5-48cf-a139-3eabb740dc1d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Her, visit, to, Tuvalu, was, cut, short, by, ...</td>\n",
              "      <td>[O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[In, 1975, ,, the, Princess, was, listed, amon...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B-PER, I-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[In, 1974, ,, she, invited, him, as, a, guest,...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[She, co-starred, with, Richard, Widmark, and,...</td>\n",
              "      <td>[O, O, O, B-PER, I-PER, O, B-PER, I-PER, O, O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[She, experienced, a, mild, stroke, on, 23, Fe...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-L...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11585</th>\n",
              "      <td>[However, ,, he, played, his, home, games, at,...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, B-LOC, I-LOC, O, O, O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11586</th>\n",
              "      <td>[256, with, 11, triples, in, 277, at, bats, wh...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11587</th>\n",
              "      <td>[He, played, his, final, professional, games, ...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11588</th>\n",
              "      <td>[The, city, is, also, the, setting, for, the, ...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, B-ORG, I-ORG, O, O, B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11589</th>\n",
              "      <td>[He, was, elected, as, Member, of, Parliament,...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11590 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-87e5062e-eee5-48cf-a139-3eabb740dc1d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-87e5062e-eee5-48cf-a139-3eabb740dc1d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-87e5062e-eee5-48cf-a139-3eabb740dc1d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e08b562c-ecbd-4914-9d0b-68e0a24bac59\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e08b562c-ecbd-4914-9d0b-68e0a24bac59')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e08b562c-ecbd-4914-9d0b-68e0a24bac59 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_120d5e21-0eeb-4a3e-bc02-413f93575738\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('valid_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_120d5e21-0eeb-4a3e-bc02-413f93575738 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('valid_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "valid_df",
              "summary": "{\n  \"name\": \"valid_df\",\n  \"rows\": 11590,\n  \"fields\": [\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tag\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "valid_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5vlJtg-Gr31",
        "outputId": "69e45c29-3a3d-4ab5-a295-02410210f9eb",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ner category ['LOC', 'MISC', 'ORG', 'PER'] .\n",
            "\n",
            "label list ['O', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC', 'B-ORG', 'I-ORG', 'B-PER', 'I-PER'] .\n",
            "\n",
            "label2id {'O': 0, 'B-LOC': 1, 'I-LOC': 2, 'B-MISC': 3, 'I-MISC': 4, 'B-ORG': 5, 'I-ORG': 6, 'B-PER': 7, 'I-PER': 8} .\n",
            "\n",
            "id2label {0: 'O', 1: 'B-LOC', 2: 'I-LOC', 3: 'B-MISC', 4: 'I-MISC', 5: 'B-ORG', 6: 'I-ORG', 7: 'B-PER', 8: 'I-PER'}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def collect_label(df_list):\n",
        "    ret = set()\n",
        "    for df in df_list:\n",
        "        for labels in df['tag']:\n",
        "            for l in labels:\n",
        "                if l == \"O\":\n",
        "                    continue\n",
        "                assert l.startswith(\"B-\") or l.startswith(\"I-\")\n",
        "                ret.add(l[2:])\n",
        "    return sorted(list(ret))\n",
        "\n",
        "ner_category = collect_label([train_df, valid_df, test_df])\n",
        "label_list = []\n",
        "for l in ner_category:\n",
        "    label_list.append(\"B-\" + l)\n",
        "    label_list.append(\"I-\" + l)\n",
        "label_list = ['O'] + label_list\n",
        "label2id = dict([(v, idx) for idx, v in enumerate(label_list)])\n",
        "id2label = dict([(idx, v) for idx, v in enumerate(label_list)])\n",
        "print(f\"ner category {ner_category} .\\n\\nlabel list {label_list} .\\n\\nlabel2id {label2id} .\\n\\nid2label {id2label}\\n\\n\")\n",
        "label_list = label_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLLN7xMQGr31"
      },
      "source": [
        "## Import Reberta Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sy1aItuvGr31",
        "outputId": "f5c2f684-9470-42b8-f7e5-33f7cd024334",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(Config.model_name, add_prefix_space=True)\n",
        "print(tokenizer.is_fast)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvXSJptjGr31"
      },
      "source": [
        "## tokenize and build Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G49tIcsZGr31",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def align(tag, word_ids):\n",
        "    aligned_tag = []\n",
        "    i = 0\n",
        "    while i < len(word_ids):\n",
        "        if word_ids[i] is None:\n",
        "            aligned_tag.append(None)\n",
        "            i += 1\n",
        "        elif tag[word_ids[i]] == \"O\":\n",
        "            aligned_tag.append(tag[word_ids[i]])\n",
        "            i += 1\n",
        "        elif tag[word_ids[i]].startswith(\"B-\"):\n",
        "            n = 0\n",
        "            while (i+n) < len(word_ids) and word_ids[i]  == word_ids[i+n]:\n",
        "                n += 1\n",
        "            aligned_tag.append(tag[word_ids[i]])\n",
        "            if n > 1:\n",
        "                aligned_tag.extend([\"I-\" + tag[word_ids[i]][2:] ] * (n-1))\n",
        "            i = i + n\n",
        "        else:\n",
        "            aligned_tag.append(tag[word_ids[i]])\n",
        "            i += 1\n",
        "    return aligned_tag\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LldB9JC9Gr31",
        "outputId": "22744aba-eb09-400a-f1fc-4f90276f0d78",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', '1996-08-22', '1996-08-22', 'I'] ['O', 'B-LOC', 'B-ORG', 'O']\n",
            "   tokens   tags  word-index\n",
            "0   [CLS]   None         NaN\n",
            "1      ĠI      O         0.0\n",
            "2   Ġ1996  B-LOC         1.0\n",
            "3       -  I-LOC         1.0\n",
            "4      08  I-LOC         1.0\n",
            "5       -  I-LOC         1.0\n",
            "6      22  I-LOC         1.0\n",
            "7   Ġ1996  B-ORG         2.0\n",
            "8       -  I-ORG         2.0\n",
            "9      08  I-ORG         2.0\n",
            "10      -  I-ORG         2.0\n",
            "11     22  I-ORG         2.0\n",
            "12     ĠI      O         3.0\n",
            "13  [SEP]   None         NaN\n"
          ]
        }
      ],
      "source": [
        "#words = train_df.iloc[2][\"word\"]\n",
        "#tag = train_df.iloc[2][\"label\"]\n",
        "words = ['I', '1996-08-22', '1996-08-22', 'I']\n",
        "tag = [\"O\", \"B-LOC\", \"B-ORG\", \"O\"]\n",
        "print(words, tag)\n",
        "s = tokenizer(words, truncation=True, is_split_into_words=True)\n",
        "word_ids = s.word_ids()\n",
        "# align tokens and words\n",
        "tokens = tokenizer.convert_ids_to_tokens(s['input_ids'])\n",
        "tags = align(tag, s.word_ids())\n",
        "print(pd.DataFrame(list(zip(tokens, tags, word_ids)), columns=['tokens', 'tags', 'word-index']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3i02DNiGr31",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def preprocess(x):\n",
        "    word = x['word']\n",
        "    r = tokenizer(word, truncation=True, is_split_into_words=True)\n",
        "    word_ids = r.word_ids()\n",
        "    tokens = tokenizer.convert_ids_to_tokens(r['input_ids'])\n",
        "    align_label = align(x['tag'], word_ids)\n",
        "    return tokens, align_label, r['input_ids'], [label2id[i] if i is not None else -100  for i in align_label], word_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmnu0LUoGr31",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_df[['token', 'label', 'id', 'label_id', 'word_ids']] = train_df.apply(lambda x: pd.Series(preprocess(x)), axis=1)\n",
        "valid_df[['token', 'label', 'id', 'label_id', 'word_ids']] = valid_df.apply(lambda x: pd.Series(preprocess(x)), axis=1)\n",
        "test_df[['token', 'label', 'id', 'label_id', 'word_ids']] = test_df.apply(lambda x: pd.Series(preprocess(x)), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        },
        "id": "LHAT0xb_Gr32",
        "outputId": "82edd8dc-699a-49ed-b39c-93dbc06f60a6",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    word  \\\n",
              "0      [On, this, occasion, he, failed, to, gain, the...   \n",
              "1      [On, both, these, occasions, he, was, backed, ...   \n",
              "2      [He, also, appeared, as, himself, in, the, 199...   \n",
              "3      [The, Colorado, Rockies, were, created, as, an...   \n",
              "4      [He, kept, busy, recording, demo, tapes, at, h...   \n",
              "...                                                  ...   \n",
              "11592                                [com, ,, Amazon, .]   \n",
              "11593  [In, January, 2013, ,, the, European, Food, Sa...   \n",
              "11594  [All, of, the, games, had, art, true, to, the,...   \n",
              "11595  [There, was, also, a, game, made, for, the, Ga...   \n",
              "11596  [This, system, was, widely, copied, in, variou...   \n",
              "\n",
              "                                                     tag  \\\n",
              "0      [O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG...   \n",
              "1      [O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-OR...   \n",
              "2           [O, O, O, O, O, O, O, O, O, O, B-MISC, O, O]   \n",
              "3      [O, B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, B...   \n",
              "4      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "...                                                  ...   \n",
              "11592                                   [O, O, B-ORG, O]   \n",
              "11593  [O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, O,...   \n",
              "11594  [O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, O,...   \n",
              "11595  [O, O, O, O, O, O, O, O, B-MISC, I-MISC, O, O,...   \n",
              "11596                 [O, O, O, O, O, O, O, B-ORG, O, O]   \n",
              "\n",
              "                                                   token  \\\n",
              "0      [[CLS], ĠOn, Ġthis, Ġoccasion, Ġhe, Ġfailed, Ġ...   \n",
              "1      [[CLS], ĠOn, Ġboth, Ġthese, Ġoccasions, Ġhe, Ġ...   \n",
              "2      [[CLS], ĠHe, Ġalso, Ġappeared, Ġas, Ġhimself, ...   \n",
              "3      [[CLS], ĠThe, ĠColorado, ĠRockies, Ġwere, Ġcre...   \n",
              "4      [[CLS], ĠHe, Ġkept, Ġbusy, Ġrecording, Ġdemo, ...   \n",
              "...                                                  ...   \n",
              "11592              [[CLS], Ġcom, Ġ,, ĠAmazon, Ġ., [SEP]]   \n",
              "11593  [[CLS], ĠIn, ĠJanuary, Ġ2013, Ġ,, Ġthe, ĠEurop...   \n",
              "11594  [[CLS], ĠAll, Ġof, Ġthe, Ġgames, Ġhad, Ġart, Ġ...   \n",
              "11595  [[CLS], ĠThere, Ġwas, Ġalso, Ġa, Ġgame, Ġmade,...   \n",
              "11596  [[CLS], ĠThis, Ġsystem, Ġwas, Ġwidely, Ġcopied...   \n",
              "\n",
              "                                                   label  \\\n",
              "0      [None, O, O, O, O, O, O, O, O, O, O, O, B-ORG,...   \n",
              "1      [None, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG...   \n",
              "2      [None, O, O, O, O, O, O, O, O, O, O, B-MISC, O...   \n",
              "3      [None, O, B-ORG, I-ORG, O, O, O, O, O, O, O, O...   \n",
              "4      [None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "...                                                  ...   \n",
              "11592                       [None, O, O, B-ORG, O, None]   \n",
              "11593  [None, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-O...   \n",
              "11594  [None, O, O, O, O, O, O, O, O, O, O, O, O, B-O...   \n",
              "11595  [None, O, O, O, O, O, O, O, O, B-MISC, I-MISC,...   \n",
              "11596     [None, O, O, O, O, O, O, O, B-ORG, O, O, None]   \n",
              "\n",
              "                                                      id  \\\n",
              "0      [1, 374, 42, 5852, 37, 1447, 7, 2364, 5, 323, ...   \n",
              "1      [1, 374, 258, 209, 7657, 37, 21, 4094, 30, 5, ...   \n",
              "2      [1, 91, 67, 1382, 25, 1003, 11, 5, 8008, 822, ...   \n",
              "3      [1, 20, 3004, 13478, 58, 1412, 25, 41, 2919, 3...   \n",
              "4      [1, 91, 1682, 3610, 5492, 19592, 23335, 23, 39...   \n",
              "...                                                  ...   \n",
              "11592                      [1, 3137, 2156, 1645, 479, 2]   \n",
              "11593  [1, 96, 644, 1014, 2156, 5, 796, 3652, 5264, 4...   \n",
              "11594  [1, 404, 9, 5, 426, 56, 1808, 1528, 7, 5, 651,...   \n",
              "11595  [1, 345, 21, 67, 10, 177, 156, 13, 5, 2436, 56...   \n",
              "11596  [1, 152, 467, 21, 3924, 15443, 11, 1337, 6169,...   \n",
              "\n",
              "                                                label_id  \\\n",
              "0      [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, ...   \n",
              "1      [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 6, 6, ...   \n",
              "2      [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, ...   \n",
              "3      [-100, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
              "4      [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "...                                                  ...   \n",
              "11592                           [-100, 0, 0, 5, 0, -100]   \n",
              "11593  [-100, 0, 0, 0, 0, 0, 5, 6, 6, 6, 0, 0, 0, 0, ...   \n",
              "11594  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, ...   \n",
              "11595  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, ...   \n",
              "11596         [-100, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, -100]   \n",
              "\n",
              "                                                word_ids  \n",
              "0      [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...  \n",
              "1      [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...  \n",
              "2      [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...  \n",
              "3      [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...  \n",
              "4      [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...  \n",
              "...                                                  ...  \n",
              "11592                           [None, 0, 1, 2, 3, None]  \n",
              "11593  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...  \n",
              "11594  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...  \n",
              "11595  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...  \n",
              "11596         [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, None]  \n",
              "\n",
              "[11597 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-165d2976-3502-42d6-a269-d1e06190c344\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "      <th>token</th>\n",
              "      <th>label</th>\n",
              "      <th>id</th>\n",
              "      <th>label_id</th>\n",
              "      <th>word_ids</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[On, this, occasion, he, failed, to, gain, the...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG...</td>\n",
              "      <td>[[CLS], ĠOn, Ġthis, Ġoccasion, Ġhe, Ġfailed, Ġ...</td>\n",
              "      <td>[None, O, O, O, O, O, O, O, O, O, O, O, B-ORG,...</td>\n",
              "      <td>[1, 374, 42, 5852, 37, 1447, 7, 2364, 5, 323, ...</td>\n",
              "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, ...</td>\n",
              "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[On, both, these, occasions, he, was, backed, ...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-OR...</td>\n",
              "      <td>[[CLS], ĠOn, Ġboth, Ġthese, Ġoccasions, Ġhe, Ġ...</td>\n",
              "      <td>[None, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG...</td>\n",
              "      <td>[1, 374, 258, 209, 7657, 37, 21, 4094, 30, 5, ...</td>\n",
              "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 6, 6, ...</td>\n",
              "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[He, also, appeared, as, himself, in, the, 199...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, B-MISC, O, O]</td>\n",
              "      <td>[[CLS], ĠHe, Ġalso, Ġappeared, Ġas, Ġhimself, ...</td>\n",
              "      <td>[None, O, O, O, O, O, O, O, O, O, O, B-MISC, O...</td>\n",
              "      <td>[1, 91, 67, 1382, 25, 1003, 11, 5, 8008, 822, ...</td>\n",
              "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, ...</td>\n",
              "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[The, Colorado, Rockies, were, created, as, an...</td>\n",
              "      <td>[O, B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, B...</td>\n",
              "      <td>[[CLS], ĠThe, ĠColorado, ĠRockies, Ġwere, Ġcre...</td>\n",
              "      <td>[None, O, B-ORG, I-ORG, O, O, O, O, O, O, O, O...</td>\n",
              "      <td>[1, 20, 3004, 13478, 58, 1412, 25, 41, 2919, 3...</td>\n",
              "      <td>[-100, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
              "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[He, kept, busy, recording, demo, tapes, at, h...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>[[CLS], ĠHe, Ġkept, Ġbusy, Ġrecording, Ġdemo, ...</td>\n",
              "      <td>[None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>[1, 91, 1682, 3610, 5492, 19592, 23335, 23, 39...</td>\n",
              "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11592</th>\n",
              "      <td>[com, ,, Amazon, .]</td>\n",
              "      <td>[O, O, B-ORG, O]</td>\n",
              "      <td>[[CLS], Ġcom, Ġ,, ĠAmazon, Ġ., [SEP]]</td>\n",
              "      <td>[None, O, O, B-ORG, O, None]</td>\n",
              "      <td>[1, 3137, 2156, 1645, 479, 2]</td>\n",
              "      <td>[-100, 0, 0, 5, 0, -100]</td>\n",
              "      <td>[None, 0, 1, 2, 3, None]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11593</th>\n",
              "      <td>[In, January, 2013, ,, the, European, Food, Sa...</td>\n",
              "      <td>[O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, O,...</td>\n",
              "      <td>[[CLS], ĠIn, ĠJanuary, Ġ2013, Ġ,, Ġthe, ĠEurop...</td>\n",
              "      <td>[None, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-O...</td>\n",
              "      <td>[1, 96, 644, 1014, 2156, 5, 796, 3652, 5264, 4...</td>\n",
              "      <td>[-100, 0, 0, 0, 0, 0, 5, 6, 6, 6, 0, 0, 0, 0, ...</td>\n",
              "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11594</th>\n",
              "      <td>[All, of, the, games, had, art, true, to, the,...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, O,...</td>\n",
              "      <td>[[CLS], ĠAll, Ġof, Ġthe, Ġgames, Ġhad, Ġart, Ġ...</td>\n",
              "      <td>[None, O, O, O, O, O, O, O, O, O, O, O, O, B-O...</td>\n",
              "      <td>[1, 404, 9, 5, 426, 56, 1808, 1528, 7, 5, 651,...</td>\n",
              "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, ...</td>\n",
              "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11595</th>\n",
              "      <td>[There, was, also, a, game, made, for, the, Ga...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, B-MISC, I-MISC, O, O,...</td>\n",
              "      <td>[[CLS], ĠThere, Ġwas, Ġalso, Ġa, Ġgame, Ġmade,...</td>\n",
              "      <td>[None, O, O, O, O, O, O, O, O, B-MISC, I-MISC,...</td>\n",
              "      <td>[1, 345, 21, 67, 10, 177, 156, 13, 5, 2436, 56...</td>\n",
              "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, ...</td>\n",
              "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11596</th>\n",
              "      <td>[This, system, was, widely, copied, in, variou...</td>\n",
              "      <td>[O, O, O, O, O, O, O, B-ORG, O, O]</td>\n",
              "      <td>[[CLS], ĠThis, Ġsystem, Ġwas, Ġwidely, Ġcopied...</td>\n",
              "      <td>[None, O, O, O, O, O, O, O, B-ORG, O, O, None]</td>\n",
              "      <td>[1, 152, 467, 21, 3924, 15443, 11, 1337, 6169,...</td>\n",
              "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, -100]</td>\n",
              "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, None]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11597 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-165d2976-3502-42d6-a269-d1e06190c344')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-165d2976-3502-42d6-a269-d1e06190c344 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-165d2976-3502-42d6-a269-d1e06190c344');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-917161c5-10e2-4d3f-aa6d-3cffce3bbcaa\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-917161c5-10e2-4d3f-aa6d-3cffce3bbcaa')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-917161c5-10e2-4d3f-aa6d-3cffce3bbcaa button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_1f2ee39b-7405-4450-aff2-cf9c340c31a1\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('test_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_1f2ee39b-7405-4450-aff2-cf9c340c31a1 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('test_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_df",
              "summary": "{\n  \"name\": \"test_df\",\n  \"rows\": 11597,\n  \"fields\": [\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tag\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"token\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_id\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word_ids\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWRSGLs4Gr32"
      },
      "source": [
        "## Building Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNHgo_2cGr32",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "class NerDataset(Dataset):\n",
        "    def __init__(self, df, device):\n",
        "        self.data = df.to_dict(orient='records')\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.data[item]\n",
        "\n",
        "    def collate_to_max_length(self, batch):\n",
        "        max_seq_length = max([len(s['id']) for s in batch])\n",
        "        batch = sorted(batch, key=lambda x: -len(x['id']))\n",
        "        seq_length = torch.tensor([len(x['id']) for x in batch])\n",
        "        input_ids = torch.tensor([x[\"id\"] + [0] * (max_seq_length - len(x['id'])) for x in batch]).to(self.device)\n",
        "        labels = torch.tensor([x[\"label_id\"] + [-100] * (max_seq_length - len(x['label_id'])) for x in batch]).to(self.device)\n",
        "        return {\"id\": input_ids, \"label_id\": labels, 'seq_length':seq_length, \"sample\":batch}\n",
        "\n",
        "\n",
        "dataset_train = NerDataset(train_df, Config.device)\n",
        "\n",
        "train_dataloader = DataLoader(dataset_train,\n",
        "                              sampler=RandomSampler(dataset_train),\n",
        "                              batch_size=Config.batch_size,\n",
        "                              drop_last=False,\n",
        "                              collate_fn=dataset_train.collate_to_max_length)\n",
        "\n",
        "\n",
        "\n",
        "dataset_valid = NerDataset(valid_df, Config.device)\n",
        "\n",
        "valid_dataloader = DataLoader(dataset_valid,\n",
        "                              sampler=RandomSampler(dataset_valid),\n",
        "                              batch_size=Config.batch_size,\n",
        "                              drop_last=False,\n",
        "                              collate_fn=dataset_valid.collate_to_max_length)\n",
        "\n",
        "\n",
        "dataset_test = NerDataset(test_df, Config.device)\n",
        "\n",
        "test_dataloader = DataLoader(dataset_test,\n",
        "                              sampler=RandomSampler(dataset_test),\n",
        "                              batch_size=Config.batch_size,\n",
        "                              drop_last=False,\n",
        "                              collate_fn=dataset_test.collate_to_max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5B33GkMGr32"
      },
      "source": [
        "## Building Custom loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp_D66RpGr32",
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "class L1_Loss:\n",
        "    def __init__(self):\n",
        "        self.l1_loss = nn.L1Loss()\n",
        "    def loss(self, target, logit, label_num):\n",
        "\n",
        "        target = target.view(-1)\n",
        "        logit = logit.view(-1, label_num)\n",
        "        mask = target.ne(-100).to(logit.device)\n",
        "        logit = torch.masked_select(logit, mask.unsqueeze(-1).expand_as(logit)).reshape(-1, label_num)\n",
        "        target = torch.masked_select(target, mask)\n",
        "\n",
        "        target = F.one_hot(target, num_classes=label_num)\n",
        "        return self.l1_loss(logit, target.float())\n",
        "\n",
        "\n",
        "class L2_Loss:\n",
        "    def __init__(self):\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "    def loss(self, target, logit,label_num):\n",
        "        target = target.view(-1)\n",
        "        logit = logit.view(-1, label_num)\n",
        "        mask = target.ne(-100).to(logit.device)\n",
        "        logit = torch.masked_select(logit, mask.unsqueeze(-1).expand_as(logit)).reshape(-1, label_num)\n",
        "        target = torch.masked_select(target, mask)\n",
        "\n",
        "        target = F.one_hot(target, num_classes=label_num)\n",
        "        loss = self.mse_loss(logit, target.float())\n",
        "        return loss\n",
        "\n",
        "class CE_Loss:\n",
        "    def __init__(self):\n",
        "        self.ce_loss = nn.CrossEntropyLoss(ignore_index=-100, reduce='mean')\n",
        "    def loss(self, target, logit, label_num):\n",
        "        return self.ce_loss(logit.reshape(-1, label_num), target.reshape(-1) )\n",
        "\n",
        "class KLDivergenceLoss:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def loss(self, target, logit, label_num):\n",
        "        target = target.view(-1)\n",
        "        logit = logit.view(-1, label_num)\n",
        "\n",
        "        mask = target.ne(-100).to(logit.device)\n",
        "        logit = torch.masked_select(logit, mask.unsqueeze(-1).expand_as(logit)).reshape(-1, label_num)\n",
        "        target = torch.masked_select(target, mask)\n",
        "\n",
        "        probs = F.softmax(logit, dim=-1)\n",
        "\n",
        "        # One-hot encode the targets to get true probabilities\n",
        "        true_probs = F.one_hot(target, num_classes=label_num).float()\n",
        "\n",
        "        mask_true_probs = true_probs > 0\n",
        "\n",
        "        # Calculate g function for non-zero elements using the mask\n",
        "        kl_values = torch.zeros_like(probs)\n",
        "        kl_values[mask_true_probs] = true_probs[mask_true_probs] * torch.log(true_probs[mask_true_probs]/probs[mask_true_probs])\n",
        "\n",
        "        # Sum over all classes and average over the batch size\n",
        "        loss = kl_values.sum(dim=-1).mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "# DLITE Loss function\n",
        "class DLITELoss:\n",
        "    def __init__(self):\n",
        "        super(DLITELoss, self).__init__()\n",
        "\n",
        "    def loss(self, targets, logits, label_num, epsilon=1e-10):\n",
        "        targets = targets.view(-1)\n",
        "        logits = logits.view(-1, label_num)\n",
        "\n",
        "        mask = targets.ne(-100).to(logits.device)\n",
        "        logits = torch.masked_select(logits, mask.unsqueeze(-1).expand_as(logits)).reshape(-1, label_num)\n",
        "        targets = torch.masked_select(targets, mask)\n",
        "\n",
        "        # Convert logits to probabilities using softmax\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # One-hot encode the targets to get true probabilities\n",
        "        true_probs = F.one_hot(targets, num_classes=probs.size(-1)).float()\n",
        "\n",
        "        # Define the g function\n",
        "        g_values = torch.abs(probs * (1 - torch.log(probs + epsilon)) - true_probs * (1 - torch.log(true_probs + epsilon)))\n",
        "\n",
        "        # Define the delta_h function\n",
        "        delta_h_values = torch.abs(probs**2 * (1 - 2 * torch.log(probs + epsilon)) - true_probs**2 * (1 - 2 * torch.log(true_probs + epsilon))) / (2 * (probs + true_probs))\n",
        "\n",
        "        # Compute DLITE loss for each class\n",
        "        dl_values = g_values - delta_h_values\n",
        "\n",
        "        # Sum over all classes and average over batch size\n",
        "        loss = dl_values.sum(dim=-1).mean()\n",
        "\n",
        "        return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0x9wjATtD7Y"
      },
      "source": [
        "## Adding Custom Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bf_rR5pGr32",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "class LSTMEncoder(nn.Module):\n",
        "    \"\"\"lstm encoder\n",
        "    \"\"\"\n",
        "    def __init__(self, config, hidden_size):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "        self.lstm = torch.nn.LSTM(hidden_size, hidden_size,\n",
        "                                  num_layers=config.lstm_layer_num, bidirectional=config.bi_lstm,\n",
        "                                  batch_first=True)\n",
        "\n",
        "    def forward(self, hidden_state, seq_length):\n",
        "        sequence_output = pack_padded_sequence(hidden_state, seq_length, batch_first=True)\n",
        "        sequence_output, (h_n, c_n) = self.lstm(sequence_output)\n",
        "        sequence_output, _ = pad_packed_sequence(sequence_output, batch_first=True)\n",
        "        return sequence_output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LinearResidualLayer(nn.Module):\n",
        "    def __init__(self, config, hidden_size, output_dim):\n",
        "        super(LinearResidualLayer, self).__init__()\n",
        "        self.config = config\n",
        "        self.linear_layer1 = nn.Linear(in_features=hidden_size, out_features=output_dim)\n",
        "        self.linear_layer2 = nn.Linear(in_features=output_dim, out_features=output_dim)\n",
        "        self.linear_layer3 = nn.Linear(in_features=output_dim, out_features=output_dim)\n",
        "        self.act_func = nn.ReLU()\n",
        "        if not self.config.only_use_residual:\n",
        "            self.dropout1 = nn.Dropout(config.dropout_prob[0])\n",
        "        self.ln_1 = nn.LayerNorm(output_dim)\n",
        "        if not self.config.only_use_residual:\n",
        "            self.dropout2 = nn.Dropout(config.dropout_prob[1])\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.act_func(self.linear_layer1(x))\n",
        "        # x = self.ln_1(x)\n",
        "\n",
        "        # y = self.dropout1(x) + self.dropout1(self.act_func(self.linear_layer2(x) )  )\n",
        "        # y = self.ln_2(y)\n",
        "        # z = self.dropout2(x) + self.dropout1(y) + self.dropout2(self.act_func(self.linear_layer3(y) )  )\n",
        "\n",
        "        x = self.act_func(self.linear_layer1(x))\n",
        "        x = self.ln_1(x)\n",
        "\n",
        "        if self.config.only_use_residual:\n",
        "            y = x + self.act_func(self.linear_layer2(x) )\n",
        "            z = x + y + self.act_func(self.linear_layer3(y) )\n",
        "        elif self.config.only_use_residual_and_dropout:\n",
        "            y = self.dropout1(x) + self.act_func(self.linear_layer2(x) )\n",
        "            z = self.dropout2(x) + self.dropout1(y) + self.act_func(self.linear_layer3(y) )\n",
        "        else:\n",
        "            assert ValueError(\"config error\")\n",
        "        return z\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Ner_Model(nn.Module):\n",
        "    def __init__(self,config, label_num, loss_name):\n",
        "        super(Ner_Model, self).__init__()\n",
        "        self.config = config\n",
        "        # deberat model\n",
        "        self.model = transformers.AutoModel.from_pretrained(config.model_name)\n",
        "\n",
        "        hidden_size = config.hidden_size\n",
        "\n",
        "        # using linear layer\n",
        "        linear_layer = []\n",
        "        if self.config.linear_layer_num  > 0:\n",
        "            # using linear layer\n",
        "            if self.config.only_use_standard_linear_layer:\n",
        "                for out_dim in config.linear_layer[0:config.linear_layer_num]:\n",
        "                    linear_layer.append( nn.Linear(in_features=hidden_size, out_features=out_dim) )\n",
        "                    linear_layer.append( nn.ReLU() )\n",
        "                    hidden_size = out_dim\n",
        "                self.linear_model = nn.Sequential(*linear_layer)\n",
        "            # just use dropout\n",
        "            elif self.config.only_use_dropout:\n",
        "                for i, out_dim in enumerate(config.linear_layer[0:config.linear_layer_num]):\n",
        "                    linear_layer.append( nn.Linear(in_features=hidden_size, out_features=out_dim) )\n",
        "                    linear_layer.append( nn.ReLU() )\n",
        "                    linear_layer.append( nn.Dropout(config.dropout_prob[i]) )\n",
        "                    hidden_size = out_dim\n",
        "                self.linear_model = nn.Sequential(*linear_layer)\n",
        "            else:\n",
        "                # use 3 linear layer for skip 2 dropout\n",
        "                assert config.linear_layer[0] == config.linear_layer[1] == config.linear_layer[2]\n",
        "                assert len(config.dropout_prob) == 2\n",
        "                self.linear_model = LinearResidualLayer(config, hidden_size,config.linear_layer[0])\n",
        "                hidden_size = config.linear_layer[0]\n",
        "\n",
        "        # whether to use lstm layer\n",
        "        if config.lstm_layer_num > 0:\n",
        "            self.lstm = LSTMEncoder(config,hidden_size)\n",
        "\n",
        "        # identify label number\n",
        "        self.label_num = label_num\n",
        "\n",
        "        # whether to use bi-lstm layer\n",
        "        if config.bi_lstm and config.lstm_layer_num > 0:\n",
        "            hidden_size = hidden_size * 2\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_size, label_num)\n",
        "\n",
        "        if loss_name == 'ce':\n",
        "            self.loss_func = CE_Loss()\n",
        "        elif loss_name == 'l1':\n",
        "            self.loss_func = L1_Loss()\n",
        "        elif loss_name == 'l2':\n",
        "            self.loss_func = L2_Loss()\n",
        "        elif loss_name == 'kl':\n",
        "            self.loss_func = KLDivergenceLoss()\n",
        "        elif loss_name == 'dlite':\n",
        "            self.loss_func = DLITELoss()\n",
        "        else:\n",
        "            assert 1==0\n",
        "\n",
        "        print(\"model configuration\")\n",
        "        print(\"%\" * 20)\n",
        "        print(self)\n",
        "        print(\"%\" * 20)\n",
        "\n",
        "    def forward(self, input_ids, seq_length, attention_mask, labels):\n",
        "        output = self.model(input_ids, attention_mask)\n",
        "        sequence_output = output[0]\n",
        "        if self.config.linear_layer_num > 0:\n",
        "            sequence_output = self.linear_model(sequence_output)\n",
        "\n",
        "        if self.config.lstm_layer_num > 0:\n",
        "            sequence_output = self.lstm(sequence_output, seq_length)\n",
        "\n",
        "        logit = self.classifier(sequence_output)\n",
        "        loss = self.loss_func.loss(labels, logit, len(label2id))\n",
        "        return loss, logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8F51tFmwGr32",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Building optimizer\n",
        "def get_optimizer(model, config):\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.1},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "\n",
        "    optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                      betas=(0.9, 0.98),\n",
        "                      lr=config.lr)\n",
        "    return optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uXq2R_yGr32"
      },
      "source": [
        "## Defining the training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDIJoYpWGr33",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate(model, data_loader, mode=\"Validation\"):\n",
        "    ground_truth, predict = [], []\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples = 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(data_loader):\n",
        "            attention_mask = batch[\"id\"].ne(0)\n",
        "            targets = batch['label_id']\n",
        "            loss, logit = model(batch[\"id\"], batch['seq_length'], attention_mask=attention_mask,\n",
        "                                             labels=targets)\n",
        "            eval_loss += loss.cpu().item()\n",
        "            if (step+1) % 100==0:\n",
        "                loss_step = eval_loss / (step+1)\n",
        "                print(f\"{mode} loss per 100 evaluation steps: {loss_step}\")\n",
        "\n",
        "            # compute training accuracy\n",
        "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = logit.view(-1, len(label2id)) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            active_accuracy = flattened_targets.ne(-100) # active accuracy is also of shape (batch_size * seq_len,)\n",
        "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "            eval_preds.extend(predictions.tolist())\n",
        "            eval_labels.extend(targets.tolist())\n",
        "\n",
        "    eval_loss = eval_loss / (step+1)\n",
        "    eval_accuracy = eval_accuracy / (step+1)\n",
        "    eval_labels,eval_preds = [id2label[i] for i in eval_labels], [id2label[i] for i in eval_preds]\n",
        "\n",
        "\n",
        "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(eval_labels, eval_preds, average='micro')\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(eval_labels, eval_preds, average='macro')\n",
        "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(eval_labels, eval_preds,average='weighted')\n",
        "\n",
        "    p_r_f1 = [[round(precision_micro,4), round(recall_micro,4), round(f1_micro,4)],\n",
        "              [round(precision_macro,4), round(recall_macro,4), round(f1_macro,4)],\n",
        "              [round(precision_weighted,4), round(recall_weighted,4), round(f1_weighted,4)]]\n",
        "\n",
        "    p_r_f1 = pd.DataFrame(p_r_f1, columns=['precision', 'recall', 'f1'], index=['micro', 'macro', 'weighted'])\n",
        "\n",
        "    print(f\"{mode} Loss: {eval_loss}\")\n",
        "    print(f\"{mode} Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    p_r_f1_each_label = classification_report(eval_labels, eval_preds)\n",
        "    print(f\"{mode} P-R-F1 for each label: \\n{p_r_f1_each_label}\")\n",
        "    print(f\"{mode} P-R-F1 tor all label: \\n{p_r_f1}\")\n",
        "    print(f\"{mode} steps: {(step+1)}\")\n",
        "    return eval_loss, p_r_f1, p_r_f1_each_label\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMapwwmhGr33"
      },
      "source": [
        "## Running under 5 custom loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzSeh-WWGr33",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "loss_list = ['l1', 'l2', 'ce', 'kl', 'dlite']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6U0QqPKGr33",
        "outputId": "b5dc629f-f3db-4cf4-a706-b2311255a86d",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "loss_name: l1\n",
            "model configuration\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Ner_Model(\n",
            "  (model): DebertaModel(\n",
            "    (embeddings): DebertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "      (LayerNorm): DebertaLayerNorm()\n",
            "      (dropout): StableDropout()\n",
            "    )\n",
            "    (encoder): DebertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x DebertaLayer(\n",
            "          (attention): DebertaAttention(\n",
            "            (self): DisentangledSelfAttention(\n",
            "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
            "              (pos_dropout): StableDropout()\n",
            "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "            (output): DebertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): DebertaLayerNorm()\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "          )\n",
            "          (intermediate): DebertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): DebertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): DebertaLayerNorm()\n",
            "            (dropout): StableDropout()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (rel_embeddings): Embedding(1024, 768)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
            ")\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.18773707665503025\n",
            "Training loss per 100 training steps: 0.12981800865381957\n",
            "Training loss per 100 training steps: 0.10805504174282153\n",
            "Training loss per 100 training steps: 0.09338944347575306\n",
            "Training loss per 100 training steps: 0.0831878880187869\n",
            "Training loss per 100 training steps: 0.07598662882111967\n",
            "Training loss per 100 training steps: 0.0702671558489757\n",
            "Training loss per 100 training steps: 0.06591336514800787\n",
            "Training loss per 100 training steps: 0.06231759606136216\n",
            "Training loss per 100 training steps: 0.0593881276845932\n",
            "Training loss per 100 training steps: 0.05691478783603419\n",
            "Training loss per 100 training steps: 0.05460164377000183\n",
            "Training loss per 100 training steps: 0.052562863137573\n",
            "Training loss per 100 training steps: 0.0508312174571412\n",
            "Training loss per 100 training steps: 0.049177048298840724\n",
            "Training loss per 100 training steps: 0.047732712485012596\n",
            "Training loss per 100 training steps: 0.04634622304535964\n",
            "Training loss per 100 training steps: 0.04505550732680907\n",
            "Training loss per 100 training steps: 0.04395263773879331\n",
            "Training loss per 100 training steps: 0.04286398419132456\n",
            "Training loss per 100 training steps: 0.041841293757426595\n",
            "Training loss per 100 training steps: 0.04085859622679312\n",
            "Training loss per 100 training steps: 0.04002294709217613\n",
            "Training loss per 100 training steps: 0.039189549695778016\n",
            "Training loss per 100 training steps: 0.03840174618586898\n",
            "Training loss per 100 training steps: 0.037640519056899045\n",
            "Training loss per 100 training steps: 0.03692489309719315\n",
            "Training loss per 100 training steps: 0.03623686788370833\n",
            "Training loss per 100 training steps: 0.03559591391894581\n",
            "Training loss per 100 training steps: 0.03500006890203804\n",
            "Training loss per 100 training steps: 0.034444148672984974\n",
            "Training loss per 100 training steps: 0.03390986413665814\n",
            "Training loss per 100 training steps: 0.033370432555675504\n",
            "Training loss per 100 training steps: 0.03283308227450642\n",
            "Training loss per 100 training steps: 0.03233503204624036\n",
            "Training loss per 100 training steps: 0.03189345568017517\n",
            "Training loss per 100 training steps: 0.031459782695070514\n",
            "Training loss per 100 training steps: 0.03104532425426633\n",
            "Training loss per 100 training steps: 0.030626468273261803\n",
            "Training loss per 100 training steps: 0.03021062462206464\n",
            "Training loss per 100 training steps: 0.029810252684372953\n",
            "Training loss per 100 training steps: 0.029403563670237505\n",
            "Training loss per 100 training steps: 0.029009968394651836\n",
            "Training loss per 100 training steps: 0.028673777800180357\n",
            "Training loss per 100 training steps: 0.028340494176579845\n",
            "Training loss per 100 training steps: 0.02799968568638534\n",
            "Training loss per 100 training steps: 0.02767609572047962\n",
            "Training loss per 100 training steps: 0.02737159641236455\n",
            "Training loss per 100 training steps: 0.02705656805569876\n",
            "Training loss per 100 training steps: 0.026788571677729488\n",
            "Training loss per 100 training steps: 0.02650227506571979\n",
            "Training loss per 100 training steps: 0.026222130697704134\n",
            "Training loss per 100 training steps: 0.02597307475591254\n",
            "Training loss per 100 training steps: 0.025718467891026563\n",
            "Training loss per 100 training steps: 0.025488423553126104\n",
            "Training loss per 100 training steps: 0.025231883163297816\n",
            "Training loss per 100 training steps: 0.025014722615709167\n",
            "Training loss per 100 training steps: 0.024780074061568954\n",
            "Training loss per 100 training steps: 0.0245580769711474\n",
            "Training loss per 100 training steps: 0.02433903714315966\n",
            "Training loss per 100 training steps: 0.024148105871940002\n",
            "Training loss per 100 training steps: 0.02393703440016496\n",
            "Training loss per 100 training steps: 0.023745427525617063\n",
            "Training loss per 100 training steps: 0.02355306666402612\n",
            "Training loss per 100 training steps: 0.02336064158530476\n",
            "Training loss per 100 training steps: 0.02317455981448857\n",
            "Training loss per 100 training steps: 0.023014066258598285\n",
            "Training loss per 100 training steps: 0.022837567023029002\n",
            "Training loss per 100 training steps: 0.022658524964531156\n",
            "Training loss per 100 training steps: 0.022516661785942103\n",
            "Training loss per 100 training steps: 0.022350016158192196\n",
            "Training loss per 100 training steps: 0.022191550284042023\n",
            "Training loss per 100 training steps: 0.022036465621004774\n",
            "Training loss per 100 training steps: 0.021888859629115038\n",
            "Training loss per 100 training steps: 0.021742105228515964\n",
            "Training loss per 100 training steps: 0.021596887862910273\n",
            "Training loss per 100 training steps: 0.02145752408916687\n",
            "Training loss per 100 training steps: 0.021318336153158153\n",
            "Training loss per 100 training steps: 0.021186266278736057\n",
            "Training loss per 100 training steps: 0.021050999310362386\n",
            "Training loss per 100 training steps: 0.02092983126712181\n",
            "Training loss per 100 training steps: 0.02079668128769277\n",
            "Training loss per 100 training steps: 0.0206675808305897\n",
            "Training loss per 100 training steps: 0.020556476103125273\n",
            "Training loss per 100 training steps: 0.02043302530146149\n",
            "Training loss per 100 training steps: 0.02030453197617452\n",
            "Training loss per 100 training steps: 0.020178598533678885\n",
            "Training loss per 100 training steps: 0.020068393717334734\n",
            "Training loss per 100 training steps: 0.019955953956565957\n",
            "Training loss per 100 training steps: 0.01984704747886604\n",
            "Training loss per 100 training steps: 0.019741456464398652\n",
            "Training loss per 100 training steps: 0.019644322871792373\n",
            "Training loss per 100 training steps: 0.01955787445320898\n",
            "Training loss per 100 training steps: 0.019463788867665532\n",
            "Training loss per 100 training steps: 0.01935588333878274\n",
            "Training loss per 100 training steps: 0.019241910765267677\n",
            "Training loss per 100 training steps: 0.019149939317121795\n",
            "Training loss per 100 training steps: 0.019048432119912945\n",
            "Training loss per 100 training steps: 0.018950511320550558\n",
            "Training loss per 100 training steps: 0.018859862144105136\n",
            "Training loss per 100 training steps: 0.018763281370162744\n",
            "Training loss per 100 training steps: 0.018675680213183273\n",
            "Training loss per 100 training steps: 0.018593788604677013\n",
            "Training loss per 100 training steps: 0.01850710972807764\n",
            "Training loss per 100 training steps: 0.018416381247186413\n",
            "Training loss per 100 training steps: 0.018335691605923028\n",
            "Training loss per 100 training steps: 0.018252881430263553\n",
            "Training loss per 100 training steps: 0.018164160582668114\n",
            "Training loss per 100 training steps: 0.018096925097222893\n",
            "Training loss per 100 training steps: 0.018009872456589207\n",
            "Training loss per 100 training steps: 0.017927581631066697\n",
            "Training loss per 100 training steps: 0.017852571676027894\n",
            "Training loss per 100 training steps: 0.017784666991421737\n",
            "Training loss per 100 training steps: 0.017712236722980283\n",
            "Training loss per 100 training steps: 0.0176292366607965\n",
            "Training loss per 100 training steps: 0.017552284644194074\n",
            "Training loss per 100 training steps: 0.017483877572051894\n",
            "Training loss per 100 training steps: 0.01741240973515294\n",
            "Training loss per 100 training steps: 0.017343457654830727\n",
            "Training loss per 100 training steps: 0.017280108314803026\n",
            "Training loss per 100 training steps: 0.017212789215759396\n",
            "Training loss per 100 training steps: 0.01715315805595429\n",
            "Training loss per 100 training steps: 0.017078563274259155\n",
            "Training loss per 100 training steps: 0.017012938894087358\n",
            "Training loss per 100 training steps: 0.016954371345769614\n",
            "Training loss per 100 training steps: 0.016893622331498635\n",
            "Training loss per 100 training steps: 0.016826100358464498\n",
            "Training loss per 100 training steps: 0.016766026104924094\n",
            "Training loss per 100 training steps: 0.016708863584718184\n",
            "Training loss per 100 training steps: 0.016653626545045812\n",
            "Training loss per 100 training steps: 0.01659007822733927\n",
            "Training loss per 100 training steps: 0.016522386233784457\n",
            "Training loss per 100 training steps: 0.0164680124664175\n",
            "Training loss per 100 training steps: 0.01640659496281991\n",
            "Training loss per 100 training steps: 0.016354243786419156\n",
            "Training loss per 100 training steps: 0.016304414968711652\n",
            "Training loss per 100 training steps: 0.01625267936931379\n",
            "Training loss per 100 training steps: 0.0162048449032886\n",
            "Training loss per 100 training steps: 0.016147602546152745\n",
            "Training loss per 100 training steps: 0.01609600828285329\n",
            "Training loss per 100 training steps: 0.016041785599383136\n",
            "Training loss per 100 training steps: 0.01598421907691862\n",
            "Training loss per 100 training steps: 0.015940787924951842\n",
            "Training loss per 100 training steps: 0.015889401807006733\n",
            "Training loss per 100 training steps: 0.015835562002973567\n",
            "Training loss per 100 training steps: 0.015790294449942264\n",
            "Training loss per 100 training steps: 0.0157370761641599\n",
            "Training loss per 100 training steps: 0.015688174595770023\n",
            "Training loss per 100 training steps: 0.01563453741588836\n",
            "Training loss per 100 training steps: 0.015586381843142833\n",
            "Training loss per 100 training steps: 0.015542144900594919\n",
            "Training loss per 100 training steps: 0.015492843326217936\n",
            "Training loss per 100 training steps: 0.015445278661349923\n",
            "Training loss per 100 training steps: 0.015403418712780581\n",
            "Training loss per 100 training steps: 0.01536394672665084\n",
            "Training loss per 100 training steps: 0.015322200137545736\n",
            "Training loss per 100 training steps: 0.015280604928206936\n",
            "Training loss per 100 training steps: 0.015237715711722834\n",
            "Training loss per 100 training steps: 0.015192878913638071\n",
            "Training loss per 100 training steps: 0.015151818153361092\n",
            "Training loss per 100 training steps: 0.015114851808551445\n",
            "Training loss per 100 training steps: 0.015066833557779498\n",
            "Training loss per 100 training steps: 0.015026380292545615\n",
            "Training loss per 100 training steps: 0.014982784786048663\n",
            "Training loss per 100 training steps: 0.014940833606870112\n",
            "Training loss per 100 training steps: 0.014901780848284757\n",
            "Training loss per 100 training steps: 0.014859954439825797\n",
            "Training loss per 100 training steps: 0.014825956211245198\n",
            "Training loss per 100 training steps: 0.014788041294210717\n",
            "Training loss per 100 training steps: 0.014750265423756312\n",
            "Training loss per 100 training steps: 0.01471490720419039\n",
            "Training loss per 100 training steps: 0.01468006230951666\n",
            "Training loss per 100 training steps: 0.01464226187784171\n",
            "Training loss per 100 training steps: 0.0146025197176616\n",
            "Training loss per 100 training steps: 0.014567782722706242\n",
            "Training loss per 100 training steps: 0.01453107474705162\n",
            "Training loss per 100 training steps: 0.014488856405069746\n",
            "Training loss per 100 training steps: 0.014453100843676837\n",
            "Training loss per 100 training steps: 0.01441765055034372\n",
            "Training loss per 100 training steps: 0.014387766645587463\n",
            "Training loss per 100 training steps: 0.014359682108666645\n",
            "Training loss per 100 training steps: 0.014325086243890717\n",
            "Training loss per 100 training steps: 0.014294477250818876\n",
            "Training loss per 100 training steps: 0.014264240591184479\n",
            "Training loss per 100 training steps: 0.014231680689127864\n",
            "Training loss per 100 training steps: 0.014200432514696713\n",
            "Training loss per 100 training steps: 0.014165401307238455\n",
            "Training loss per 100 training steps: 0.014134919712161447\n",
            "Training loss per 100 training steps: 0.014100126059930378\n",
            "Training loss per 100 training steps: 0.014075384819561517\n",
            "Training loss per 100 training steps: 0.014038797299662868\n",
            "Training loss per 100 training steps: 0.01401036793514019\n",
            "Training loss per 100 training steps: 0.013975883202525966\n",
            "Training loss per 100 training steps: 0.01394467181142074\n",
            "Training loss per 100 training steps: 0.01391111888538282\n",
            "Training loss per 100 training steps: 0.013879788353034219\n",
            "Training loss per 100 training steps: 0.013848269299904197\n",
            "Training loss per 100 training steps: 0.013818881129163949\n",
            "Training loss per 100 training steps: 0.013794676315404437\n",
            "Training loss per 100 training steps: 0.013765548116236459\n",
            "Training loss per 100 training steps: 0.01373371969969869\n",
            "Training loss per 100 training steps: 0.013704935917285003\n",
            "Training loss per 100 training steps: 0.013675629056524485\n",
            "Training loss per 100 training steps: 0.013644750015616106\n",
            "Training loss per 100 training steps: 0.01362063862838805\n",
            "Training loss per 100 training steps: 0.01359272052701201\n",
            "Training loss per 100 training steps: 0.013565137459365606\n",
            "Training loss per 100 training steps: 0.013536601297875258\n",
            "Training loss per 100 training steps: 0.013508085668071408\n",
            "Training loss per 100 training steps: 0.013476587988436222\n",
            "Training loss per 100 training steps: 0.01345190429127603\n",
            "Training loss per 100 training steps: 0.013420913040578663\n",
            "Training loss per 100 training steps: 0.013395692821102383\n",
            "Training loss per 100 training steps: 0.013373836830647872\n",
            "Training loss per 100 training steps: 0.013346597185780748\n",
            "Training loss per 100 training steps: 0.013318646595149336\n",
            "Training loss per 100 training steps: 0.013293460716433343\n",
            "Training loss per 100 training steps: 0.013271802706470917\n",
            "Training loss per 100 training steps: 0.013242196947034261\n",
            "Training loss per 100 training steps: 0.013213817344100045\n",
            "Training loss per 100 training steps: 0.013189167992530046\n",
            "Training loss per 100 training steps: 0.013162802364719142\n",
            "Training loss per 100 training steps: 0.013138181162129402\n",
            "Training loss per 100 training steps: 0.013118758989044832\n",
            "Training loss per 100 training steps: 0.013093489499928222\n",
            "Training loss per 100 training steps: 0.013067723737392921\n",
            "Training loss per 100 training steps: 0.01304518465429651\n",
            "Training loss per 100 training steps: 0.013019892418442555\n",
            "Training loss per 100 training steps: 0.01299408799120709\n",
            "Training loss per 100 training steps: 0.012975061966592203\n",
            "Training loss per 100 training steps: 0.012947025122897079\n",
            "Training loss epoch: 0.012933754813423456\n",
            "Training accuracy epoch: 0.9718315083998859\n",
            "Training steps: 23180\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.006200567673658952\n",
            "Validation loss per 100 evaluation steps: 0.006295849031885154\n",
            "Validation loss per 100 evaluation steps: 0.006260979413442934\n",
            "Validation loss per 100 evaluation steps: 0.0062636560524697415\n",
            "Validation loss per 100 evaluation steps: 0.006374693441903219\n",
            "Validation loss per 100 evaluation steps: 0.0063498588734849665\n",
            "Validation loss per 100 evaluation steps: 0.006342080862959847\n",
            "Validation loss per 100 evaluation steps: 0.006245368821109878\n",
            "Validation loss per 100 evaluation steps: 0.0061767169586124104\n",
            "Validation loss per 100 evaluation steps: 0.006237617378472351\n",
            "Validation loss per 100 evaluation steps: 0.006183782780056142\n",
            "Validation loss per 100 evaluation steps: 0.006121789529473365\n",
            "Validation loss per 100 evaluation steps: 0.00610045667209376\n",
            "Validation loss per 100 evaluation steps: 0.006130712061671407\n",
            "Validation loss per 100 evaluation steps: 0.006156486263731494\n",
            "Validation loss per 100 evaluation steps: 0.006112052872776985\n",
            "Validation loss per 100 evaluation steps: 0.0060984791074331635\n",
            "Validation loss per 100 evaluation steps: 0.006084321116149012\n",
            "Validation loss per 100 evaluation steps: 0.0060732497575104625\n",
            "Validation loss per 100 evaluation steps: 0.006082678756676614\n",
            "Validation loss per 100 evaluation steps: 0.006056589979811439\n",
            "Validation loss per 100 evaluation steps: 0.006045333035819402\n",
            "Validation loss per 100 evaluation steps: 0.006038097575428369\n",
            "Validation loss per 100 evaluation steps: 0.0060560538112864985\n",
            "Validation loss per 100 evaluation steps: 0.006024955293163657\n",
            "Validation loss per 100 evaluation steps: 0.0060335489119797085\n",
            "Validation loss per 100 evaluation steps: 0.006043062059519192\n",
            "Validation loss per 100 evaluation steps: 0.0060618782754422035\n",
            "Validation Loss: 0.006051533763988485\n",
            "Validation Accuracy: 0.982844445993946\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.92      0.94      0.93      5904\n",
            "      B-MISC       0.80      0.86      0.83      4173\n",
            "       B-ORG       0.89      0.91      0.90      3512\n",
            "       B-PER       0.95      0.95      0.95      5593\n",
            "       I-LOC       0.90      0.91      0.91      7891\n",
            "      I-MISC       0.90      0.84      0.87      8442\n",
            "       I-ORG       0.90      0.93      0.91      6001\n",
            "       I-PER       0.97      0.97      0.97     11654\n",
            "           O       1.00      1.00      1.00    253311\n",
            "\n",
            "    accuracy                           0.98    306481\n",
            "   macro avg       0.91      0.92      0.92    306481\n",
            "weighted avg       0.98      0.98      0.98    306481\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9826  0.9826  0.9826\n",
            "macro        0.9142  0.9226  0.9182\n",
            "weighted     0.9827  0.9826  0.9826\n",
            "Validation steps: 2898\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.00584087306400761\n",
            "Test loss per 100 evaluation steps: 0.005627617435529828\n",
            "Test loss per 100 evaluation steps: 0.005799965981083612\n",
            "Test loss per 100 evaluation steps: 0.00578280608082423\n",
            "Test loss per 100 evaluation steps: 0.005843524925177917\n",
            "Test loss per 100 evaluation steps: 0.005929064191295765\n",
            "Test loss per 100 evaluation steps: 0.006030724420311994\n",
            "Test loss per 100 evaluation steps: 0.006013927255553426\n",
            "Test loss per 100 evaluation steps: 0.006039937249928092\n",
            "Test loss per 100 evaluation steps: 0.006006063602049835\n",
            "Test loss per 100 evaluation steps: 0.006077081405092031\n",
            "Test loss per 100 evaluation steps: 0.006132863227200384\n",
            "Test loss per 100 evaluation steps: 0.006136072169487866\n",
            "Test loss per 100 evaluation steps: 0.006095579603653667\n",
            "Test loss per 100 evaluation steps: 0.006130099543215086\n",
            "Test loss per 100 evaluation steps: 0.006182419864635449\n",
            "Test loss per 100 evaluation steps: 0.006157721777939621\n",
            "Test loss per 100 evaluation steps: 0.006120519400186216\n",
            "Test loss per 100 evaluation steps: 0.006104115304095965\n",
            "Test loss per 100 evaluation steps: 0.006120295352768153\n",
            "Test loss per 100 evaluation steps: 0.006126621312550491\n",
            "Test loss per 100 evaluation steps: 0.0061236466534054755\n",
            "Test loss per 100 evaluation steps: 0.006125313196351509\n",
            "Test loss per 100 evaluation steps: 0.006132131491807134\n",
            "Test loss per 100 evaluation steps: 0.006165878902934492\n",
            "Test loss per 100 evaluation steps: 0.00615492883630885\n",
            "Test loss per 100 evaluation steps: 0.006155906530718009\n",
            "Test loss per 100 evaluation steps: 0.006146120850878236\n",
            "Test loss per 100 evaluation steps: 0.0061579636720426635\n",
            "Test Loss: 0.0061579636720426635\n",
            "Test Accuracy: 0.9823944919939084\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.90      0.94      0.92      5955\n",
            "      B-MISC       0.81      0.87      0.84      4505\n",
            "       B-ORG       0.90      0.92      0.91      3449\n",
            "       B-PER       0.95      0.95      0.95      5207\n",
            "       I-LOC       0.87      0.90      0.89      7240\n",
            "      I-MISC       0.91      0.83      0.87      9082\n",
            "       I-ORG       0.91      0.94      0.92      5934\n",
            "       I-PER       0.96      0.97      0.97     11879\n",
            "           O       1.00      1.00      1.00    253790\n",
            "\n",
            "    accuracy                           0.98    307041\n",
            "   macro avg       0.91      0.92      0.92    307041\n",
            "weighted avg       0.98      0.98      0.98    307041\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9819  0.9819  0.9819\n",
            "macro        0.9130  0.9234  0.9178\n",
            "weighted     0.9822  0.9819  0.9820\n",
            "Test steps: 2900\n",
            "====================================================================================================\n",
            "loss_name: l2\n",
            "model configuration\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Ner_Model(\n",
            "  (model): DebertaModel(\n",
            "    (embeddings): DebertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "      (LayerNorm): DebertaLayerNorm()\n",
            "      (dropout): StableDropout()\n",
            "    )\n",
            "    (encoder): DebertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x DebertaLayer(\n",
            "          (attention): DebertaAttention(\n",
            "            (self): DisentangledSelfAttention(\n",
            "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
            "              (pos_dropout): StableDropout()\n",
            "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "            (output): DebertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): DebertaLayerNorm()\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "          )\n",
            "          (intermediate): DebertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): DebertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): DebertaLayerNorm()\n",
            "            (dropout): StableDropout()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (rel_embeddings): Embedding(1024, 768)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
            ")\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.07258356114849449\n",
            "Training loss per 100 training steps: 0.04589457388967275\n",
            "Training loss per 100 training steps: 0.0353961227927357\n",
            "Training loss per 100 training steps: 0.02923213710775599\n",
            "Training loss per 100 training steps: 0.025105756349861622\n",
            "Training loss per 100 training steps: 0.022070324052668486\n",
            "Training loss per 100 training steps: 0.01981656167123999\n",
            "Training loss per 100 training steps: 0.01808296514223912\n",
            "Training loss per 100 training steps: 0.01669575983723108\n",
            "Training loss per 100 training steps: 0.015555258693406359\n",
            "Training loss per 100 training steps: 0.014569082202612084\n",
            "Training loss per 100 training steps: 0.013836073992376139\n",
            "Training loss per 100 training steps: 0.013226526847171884\n",
            "Training loss per 100 training steps: 0.01270921849824455\n",
            "Training loss per 100 training steps: 0.012224728556505094\n",
            "Training loss per 100 training steps: 0.01179403751360951\n",
            "Training loss per 100 training steps: 0.011356977574143778\n",
            "Training loss per 100 training steps: 0.01102725831785291\n",
            "Training loss per 100 training steps: 0.010668554885931754\n",
            "Training loss per 100 training steps: 0.010372934705083025\n",
            "Training loss per 100 training steps: 0.010111554774865951\n",
            "Training loss per 100 training steps: 0.009853783933424645\n",
            "Training loss per 100 training steps: 0.009593882158325236\n",
            "Training loss per 100 training steps: 0.009334846740081655\n",
            "Training loss per 100 training steps: 0.00914794988684589\n",
            "Training loss per 100 training steps: 0.008945833443990872\n",
            "Training loss per 100 training steps: 0.008757037534788079\n",
            "Training loss per 100 training steps: 0.008584378527905626\n",
            "Training loss per 100 training steps: 0.00841813326750055\n",
            "Training loss per 100 training steps: 0.008258918338658987\n",
            "Training loss per 100 training steps: 0.008131911105930716\n",
            "Training loss per 100 training steps: 0.008000635658936517\n",
            "Training loss per 100 training steps: 0.007890461265845012\n",
            "Training loss per 100 training steps: 0.0077567799971267745\n",
            "Training loss per 100 training steps: 0.007643661240656262\n",
            "Training loss per 100 training steps: 0.0075537661677582135\n",
            "Training loss per 100 training steps: 0.0074372654592121225\n",
            "Training loss per 100 training steps: 0.007356189157466073\n",
            "Training loss per 100 training steps: 0.0072621973973881\n",
            "Training loss per 100 training steps: 0.007179302186159475\n",
            "Training loss per 100 training steps: 0.007092487156602679\n",
            "Training loss per 100 training steps: 0.007027482477257893\n",
            "Training loss per 100 training steps: 0.006938170356192723\n",
            "Training loss per 100 training steps: 0.006871112798775853\n",
            "Training loss per 100 training steps: 0.006814449552802317\n",
            "Training loss per 100 training steps: 0.006748240329211314\n",
            "Training loss per 100 training steps: 0.006686345502948438\n",
            "Training loss per 100 training steps: 0.006609755099434551\n",
            "Training loss per 100 training steps: 0.006547131045093066\n",
            "Training loss per 100 training steps: 0.006504514451487921\n",
            "Training loss per 100 training steps: 0.0064634638403110916\n",
            "Training loss per 100 training steps: 0.006399846271714733\n",
            "Training loss per 100 training steps: 0.0063515196082002115\n",
            "Training loss per 100 training steps: 0.006293993093147744\n",
            "Training loss per 100 training steps: 0.006237480744854441\n",
            "Training loss per 100 training steps: 0.0061794881099012855\n",
            "Training loss per 100 training steps: 0.006141619231511549\n",
            "Training loss per 100 training steps: 0.006086142957679714\n",
            "Training loss per 100 training steps: 0.006036212247161499\n",
            "Training loss per 100 training steps: 0.0060017268691275\n",
            "Training loss per 100 training steps: 0.0059632238943905565\n",
            "Training loss per 100 training steps: 0.005921709006011482\n",
            "Training loss per 100 training steps: 0.005891523615317142\n",
            "Training loss per 100 training steps: 0.005848405952535813\n",
            "Training loss per 100 training steps: 0.005816647941363044\n",
            "Training loss per 100 training steps: 0.005780625841117316\n",
            "Training loss per 100 training steps: 0.00574229353077105\n",
            "Training loss per 100 training steps: 0.005706516370752497\n",
            "Training loss per 100 training steps: 0.005678006262207181\n",
            "Training loss per 100 training steps: 0.00564919605529784\n",
            "Training loss per 100 training steps: 0.005616963829057747\n",
            "Training loss per 100 training steps: 0.005593489147001188\n",
            "Training loss per 100 training steps: 0.005558847922098995\n",
            "Training loss per 100 training steps: 0.005541221310924959\n",
            "Training loss per 100 training steps: 0.005513147533459899\n",
            "Training loss per 100 training steps: 0.005488446193998435\n",
            "Training loss per 100 training steps: 0.005454070652878916\n",
            "Training loss per 100 training steps: 0.005431272300676284\n",
            "Training loss per 100 training steps: 0.0054076424784533965\n",
            "Training loss per 100 training steps: 0.005384552754067045\n",
            "Training loss per 100 training steps: 0.005357986198620364\n",
            "Training loss per 100 training steps: 0.005337783989415606\n",
            "Training loss per 100 training steps: 0.0053042284152992995\n",
            "Training loss per 100 training steps: 0.005278794281106717\n",
            "Training loss per 100 training steps: 0.005251335441571427\n",
            "Training loss per 100 training steps: 0.0052330468765200725\n",
            "Training loss per 100 training steps: 0.005209700602554012\n",
            "Training loss per 100 training steps: 0.005186696928341751\n",
            "Training loss per 100 training steps: 0.005163356875552759\n",
            "Training loss per 100 training steps: 0.005141493010176216\n",
            "Training loss per 100 training steps: 0.005127525111171993\n",
            "Training loss per 100 training steps: 0.005108312676809715\n",
            "Training loss per 100 training steps: 0.005088078741923661\n",
            "Training loss per 100 training steps: 0.005066600846900094\n",
            "Training loss per 100 training steps: 0.005047116994909869\n",
            "Training loss per 100 training steps: 0.005026322934971479\n",
            "Training loss per 100 training steps: 0.005004835696631416\n",
            "Training loss per 100 training steps: 0.004985277795025542\n",
            "Training loss per 100 training steps: 0.004971695972213785\n",
            "Training loss per 100 training steps: 0.004955182036044425\n",
            "Training loss per 100 training steps: 0.004938512545278974\n",
            "Training loss per 100 training steps: 0.0049140217220288015\n",
            "Training loss per 100 training steps: 0.00489292521370056\n",
            "Training loss per 100 training steps: 0.004877736308164738\n",
            "Training loss per 100 training steps: 0.004857772603039242\n",
            "Training loss per 100 training steps: 0.004840645799289769\n",
            "Training loss per 100 training steps: 0.00482829392366639\n",
            "Training loss per 100 training steps: 0.004813641604842916\n",
            "Training loss per 100 training steps: 0.004803554695586881\n",
            "Training loss per 100 training steps: 0.004787810246061947\n",
            "Training loss per 100 training steps: 0.004776344221794028\n",
            "Training loss per 100 training steps: 0.004764236626028183\n",
            "Training loss per 100 training steps: 0.004749316655389657\n",
            "Training loss per 100 training steps: 0.004739863970060374\n",
            "Training loss per 100 training steps: 0.004724514505949944\n",
            "Training loss per 100 training steps: 0.004709959038349991\n",
            "Training loss per 100 training steps: 0.004695480858562459\n",
            "Training loss per 100 training steps: 0.0046783809980245105\n",
            "Training loss per 100 training steps: 0.0046614945046081525\n",
            "Training loss per 100 training steps: 0.004647000474644908\n",
            "Training loss per 100 training steps: 0.0046324065619169725\n",
            "Training loss per 100 training steps: 0.004619314944727388\n",
            "Training loss per 100 training steps: 0.0046100222942606515\n",
            "Training loss per 100 training steps: 0.004600025559246851\n",
            "Training loss per 100 training steps: 0.004592705503967009\n",
            "Training loss per 100 training steps: 0.004583182923460776\n",
            "Training loss per 100 training steps: 0.004567871826904707\n",
            "Training loss per 100 training steps: 0.004555432450596868\n",
            "Training loss per 100 training steps: 0.0045454423655723105\n",
            "Training loss per 100 training steps: 0.004541763755319922\n",
            "Training loss per 100 training steps: 0.004531268247821211\n",
            "Training loss per 100 training steps: 0.004519481435653496\n",
            "Training loss per 100 training steps: 0.004509581922613328\n",
            "Training loss per 100 training steps: 0.004498826749788431\n",
            "Training loss per 100 training steps: 0.004483692304407466\n",
            "Training loss per 100 training steps: 0.004472862904884845\n",
            "Training loss per 100 training steps: 0.004459808235641218\n",
            "Training loss per 100 training steps: 0.004449661894018852\n",
            "Training loss per 100 training steps: 0.004442898383942075\n",
            "Training loss per 100 training steps: 0.004433610959787724\n",
            "Training loss per 100 training steps: 0.004422706893385872\n",
            "Training loss per 100 training steps: 0.004415518951953883\n",
            "Training loss per 100 training steps: 0.004405778050028342\n",
            "Training loss per 100 training steps: 0.004394726887801173\n",
            "Training loss per 100 training steps: 0.004384869948769659\n",
            "Training loss per 100 training steps: 0.004374470146900072\n",
            "Training loss per 100 training steps: 0.004369685582619704\n",
            "Training loss per 100 training steps: 0.004357389759762383\n",
            "Training loss per 100 training steps: 0.004348883564925195\n",
            "Training loss per 100 training steps: 0.004339097441957953\n",
            "Training loss per 100 training steps: 0.0043295450487845985\n",
            "Training loss per 100 training steps: 0.004321888009270831\n",
            "Training loss per 100 training steps: 0.004310825230306269\n",
            "Training loss per 100 training steps: 0.004300924672586609\n",
            "Training loss per 100 training steps: 0.004289478818543308\n",
            "Training loss per 100 training steps: 0.0042771324265352935\n",
            "Training loss per 100 training steps: 0.004267954190065072\n",
            "Training loss per 100 training steps: 0.004259879166693051\n",
            "Training loss per 100 training steps: 0.0042530243964172074\n",
            "Training loss per 100 training steps: 0.00424624995731483\n",
            "Training loss per 100 training steps: 0.004234114615528201\n",
            "Training loss per 100 training steps: 0.004225226201944432\n",
            "Training loss per 100 training steps: 0.004222842577291439\n",
            "Training loss per 100 training steps: 0.00421838888250488\n",
            "Training loss per 100 training steps: 0.004208566588922016\n",
            "Training loss per 100 training steps: 0.004196071483629895\n",
            "Training loss per 100 training steps: 0.0041877036899281\n",
            "Training loss per 100 training steps: 0.004180198366566401\n",
            "Training loss per 100 training steps: 0.004166945094810044\n",
            "Training loss per 100 training steps: 0.004163024687163724\n",
            "Training loss per 100 training steps: 0.00415198343920597\n",
            "Training loss per 100 training steps: 0.004144163479965734\n",
            "Training loss per 100 training steps: 0.004136424134327287\n",
            "Training loss per 100 training steps: 0.004129611447882697\n",
            "Training loss per 100 training steps: 0.004123249933522727\n",
            "Training loss per 100 training steps: 0.004116125208397947\n",
            "Training loss per 100 training steps: 0.004107845281742827\n",
            "Training loss per 100 training steps: 0.004101960890930263\n",
            "Training loss per 100 training steps: 0.004097920938149207\n",
            "Training loss per 100 training steps: 0.004089439494320282\n",
            "Training loss per 100 training steps: 0.004078803848533827\n",
            "Training loss per 100 training steps: 0.004071090523859791\n",
            "Training loss per 100 training steps: 0.00406568361709826\n",
            "Training loss per 100 training steps: 0.00405583863566671\n",
            "Training loss per 100 training steps: 0.0040520834890501005\n",
            "Training loss per 100 training steps: 0.0040445545216906825\n",
            "Training loss per 100 training steps: 0.00403666845633361\n",
            "Training loss per 100 training steps: 0.00403047371242405\n",
            "Training loss per 100 training steps: 0.004025106020293336\n",
            "Training loss per 100 training steps: 0.004023263097497923\n",
            "Training loss per 100 training steps: 0.004015278183666627\n",
            "Training loss per 100 training steps: 0.0040123742403163\n",
            "Training loss per 100 training steps: 0.004006700442431053\n",
            "Training loss per 100 training steps: 0.004000287829827339\n",
            "Training loss per 100 training steps: 0.0039944893387847775\n",
            "Training loss per 100 training steps: 0.003989431272063058\n",
            "Training loss per 100 training steps: 0.003986453000031873\n",
            "Training loss per 100 training steps: 0.003983611015951604\n",
            "Training loss per 100 training steps: 0.003978189774677034\n",
            "Training loss per 100 training steps: 0.003968820566533032\n",
            "Training loss per 100 training steps: 0.003963479387935669\n",
            "Training loss per 100 training steps: 0.003958551890285493\n",
            "Training loss per 100 training steps: 0.003952701074562122\n",
            "Training loss per 100 training steps: 0.003948142709969212\n",
            "Training loss per 100 training steps: 0.003943889095638197\n",
            "Training loss per 100 training steps: 0.003937242490671972\n",
            "Training loss per 100 training steps: 0.003932293239267892\n",
            "Training loss per 100 training steps: 0.0039281869636268035\n",
            "Training loss per 100 training steps: 0.003923125355264242\n",
            "Training loss per 100 training steps: 0.003917196548616284\n",
            "Training loss per 100 training steps: 0.003912600163891826\n",
            "Training loss per 100 training steps: 0.0039044348520903167\n",
            "Training loss per 100 training steps: 0.003900801446689514\n",
            "Training loss per 100 training steps: 0.003893777441174781\n",
            "Training loss per 100 training steps: 0.0038884619298799853\n",
            "Training loss per 100 training steps: 0.003882311769273117\n",
            "Training loss per 100 training steps: 0.0038767950202201326\n",
            "Training loss per 100 training steps: 0.003869697044492318\n",
            "Training loss per 100 training steps: 0.0038644124580512395\n",
            "Training loss per 100 training steps: 0.0038581992323501748\n",
            "Training loss per 100 training steps: 0.0038584665019758234\n",
            "Training loss per 100 training steps: 0.00385243057803195\n",
            "Training loss per 100 training steps: 0.0038471090094455347\n",
            "Training loss per 100 training steps: 0.00384214758663055\n",
            "Training loss per 100 training steps: 0.0038397629330908078\n",
            "Training loss per 100 training steps: 0.0038334770396517164\n",
            "Training loss per 100 training steps: 0.0038240372296115014\n",
            "Training loss per 100 training steps: 0.00382245053926709\n",
            "Training loss per 100 training steps: 0.0038156138621090823\n",
            "Training loss per 100 training steps: 0.003812489551369926\n",
            "Training loss per 100 training steps: 0.0038081720482324426\n",
            "Training loss epoch: 0.003805941634890712\n",
            "Training accuracy epoch: 0.9809572637838742\n",
            "Training steps: 23180\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.0023361138319887686\n",
            "Validation loss per 100 evaluation steps: 0.0025528700795257463\n",
            "Validation loss per 100 evaluation steps: 0.0025949507914386533\n",
            "Validation loss per 100 evaluation steps: 0.002815578943009314\n",
            "Validation loss per 100 evaluation steps: 0.0030197144249104893\n",
            "Validation loss per 100 evaluation steps: 0.0029695202246997118\n",
            "Validation loss per 100 evaluation steps: 0.0029723060949722172\n",
            "Validation loss per 100 evaluation steps: 0.00288997665833449\n",
            "Validation loss per 100 evaluation steps: 0.003002867026128418\n",
            "Validation loss per 100 evaluation steps: 0.0029713860801712146\n",
            "Validation loss per 100 evaluation steps: 0.0029662068545770704\n",
            "Validation loss per 100 evaluation steps: 0.0029033010103618534\n",
            "Validation loss per 100 evaluation steps: 0.0028956289960702774\n",
            "Validation loss per 100 evaluation steps: 0.0029069781765636957\n",
            "Validation loss per 100 evaluation steps: 0.00291649767186027\n",
            "Validation loss per 100 evaluation steps: 0.0029151154172632233\n",
            "Validation loss per 100 evaluation steps: 0.002924359652191496\n",
            "Validation loss per 100 evaluation steps: 0.0029103430353198847\n",
            "Validation loss per 100 evaluation steps: 0.002878272808356039\n",
            "Validation loss per 100 evaluation steps: 0.0028406766012485604\n",
            "Validation loss per 100 evaluation steps: 0.002837313111695472\n",
            "Validation loss per 100 evaluation steps: 0.0028538257733849936\n",
            "Validation loss per 100 evaluation steps: 0.0028600321805076776\n",
            "Validation loss per 100 evaluation steps: 0.0028363813920198786\n",
            "Validation loss per 100 evaluation steps: 0.0028301149572303986\n",
            "Validation loss per 100 evaluation steps: 0.0028315263349829294\n",
            "Validation loss per 100 evaluation steps: 0.0028224819813988653\n",
            "Validation loss per 100 evaluation steps: 0.0028237820800547653\n",
            "Validation Loss: 0.0028395069557700606\n",
            "Validation Accuracy: 0.9841825576089359\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.92      0.96      0.94      5904\n",
            "      B-MISC       0.83      0.85      0.84      4173\n",
            "       B-ORG       0.87      0.94      0.91      3512\n",
            "       B-PER       0.96      0.95      0.95      5593\n",
            "       I-LOC       0.90      0.94      0.92      7891\n",
            "      I-MISC       0.92      0.83      0.87      8442\n",
            "       I-ORG       0.88      0.96      0.92      6001\n",
            "       I-PER       0.98      0.97      0.97     11654\n",
            "           O       1.00      1.00      1.00    253311\n",
            "\n",
            "    accuracy                           0.98    306481\n",
            "   macro avg       0.92      0.93      0.92    306481\n",
            "weighted avg       0.98      0.98      0.98    306481\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9838  0.9838  0.9838\n",
            "macro        0.9167  0.9326  0.9240\n",
            "weighted     0.9842  0.9838  0.9839\n",
            "Validation steps: 2898\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.002215968400123529\n",
            "Test loss per 100 evaluation steps: 0.002419159317141748\n",
            "Test loss per 100 evaluation steps: 0.002592038765384738\n",
            "Test loss per 100 evaluation steps: 0.002652047980336647\n",
            "Test loss per 100 evaluation steps: 0.0028683014072303195\n",
            "Test loss per 100 evaluation steps: 0.0028396396185416963\n",
            "Test loss per 100 evaluation steps: 0.002819155542529188\n",
            "Test loss per 100 evaluation steps: 0.002755771311713033\n",
            "Test loss per 100 evaluation steps: 0.002696268553489871\n",
            "Test loss per 100 evaluation steps: 0.002712463256451883\n",
            "Test loss per 100 evaluation steps: 0.002704147529320008\n",
            "Test loss per 100 evaluation steps: 0.002706937004101443\n",
            "Test loss per 100 evaluation steps: 0.002724293714519053\n",
            "Test loss per 100 evaluation steps: 0.0027275199200812493\n",
            "Test loss per 100 evaluation steps: 0.0027646958295663355\n",
            "Test loss per 100 evaluation steps: 0.0027582184249968123\n",
            "Test loss per 100 evaluation steps: 0.0027821299734076843\n",
            "Test loss per 100 evaluation steps: 0.0027808689156922305\n",
            "Test loss per 100 evaluation steps: 0.002790306269345915\n",
            "Test loss per 100 evaluation steps: 0.002798027438002464\n",
            "Test loss per 100 evaluation steps: 0.002804324509793549\n",
            "Test loss per 100 evaluation steps: 0.0028358819951509674\n",
            "Test loss per 100 evaluation steps: 0.002841507805234236\n",
            "Test loss per 100 evaluation steps: 0.002824083526047616\n",
            "Test loss per 100 evaluation steps: 0.002819522327231243\n",
            "Test loss per 100 evaluation steps: 0.002812243638081082\n",
            "Test loss per 100 evaluation steps: 0.0028044377506443265\n",
            "Test loss per 100 evaluation steps: 0.0028163989759507654\n",
            "Test loss per 100 evaluation steps: 0.0028170727235907367\n",
            "Test Loss: 0.0028170727235907367\n",
            "Test Accuracy: 0.9843725211830168\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.91      0.95      0.93      5955\n",
            "      B-MISC       0.82      0.87      0.85      4505\n",
            "       B-ORG       0.89      0.95      0.92      3449\n",
            "       B-PER       0.96      0.96      0.96      5207\n",
            "       I-LOC       0.88      0.94      0.91      7240\n",
            "      I-MISC       0.92      0.83      0.87      9082\n",
            "       I-ORG       0.90      0.96      0.93      5934\n",
            "       I-PER       0.98      0.97      0.97     11879\n",
            "           O       1.00      1.00      1.00    253790\n",
            "\n",
            "    accuracy                           0.98    307041\n",
            "   macro avg       0.92      0.94      0.93    307041\n",
            "weighted avg       0.98      0.98      0.98    307041\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9838  0.9838  0.9838\n",
            "macro        0.9175  0.9357  0.9260\n",
            "weighted     0.9842  0.9838  0.9838\n",
            "Test steps: 2900\n",
            "====================================================================================================\n",
            "loss_name: ce\n",
            "model configuration\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Ner_Model(\n",
            "  (model): DebertaModel(\n",
            "    (embeddings): DebertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "      (LayerNorm): DebertaLayerNorm()\n",
            "      (dropout): StableDropout()\n",
            "    )\n",
            "    (encoder): DebertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x DebertaLayer(\n",
            "          (attention): DebertaAttention(\n",
            "            (self): DisentangledSelfAttention(\n",
            "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
            "              (pos_dropout): StableDropout()\n",
            "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "            (output): DebertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): DebertaLayerNorm()\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "          )\n",
            "          (intermediate): DebertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): DebertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): DebertaLayerNorm()\n",
            "            (dropout): StableDropout()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (rel_embeddings): Embedding(1024, 768)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
            ")\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.5760442492365837\n",
            "Training loss per 100 training steps: 0.37429249042645096\n",
            "Training loss per 100 training steps: 0.2932545390787224\n",
            "Training loss per 100 training steps: 0.2447682767175138\n",
            "Training loss per 100 training steps: 0.21762858563568444\n",
            "Training loss per 100 training steps: 0.19736093189567327\n",
            "Training loss per 100 training steps: 0.18312037374026008\n",
            "Training loss per 100 training steps: 0.17001419802661985\n",
            "Training loss per 100 training steps: 0.16000724265869293\n",
            "Training loss per 100 training steps: 0.15399322925880551\n",
            "Training loss per 100 training steps: 0.14546304418557238\n",
            "Training loss per 100 training steps: 0.1401523058778063\n",
            "Training loss per 100 training steps: 0.13460520163841116\n",
            "Training loss per 100 training steps: 0.12944686691393145\n",
            "Training loss per 100 training steps: 0.12604231521797676\n",
            "Training loss per 100 training steps: 0.12237978024204495\n",
            "Training loss per 100 training steps: 0.1189587466096889\n",
            "Training loss per 100 training steps: 0.11704541252007604\n",
            "Training loss per 100 training steps: 0.11486094576809065\n",
            "Training loss per 100 training steps: 0.11271314894931857\n",
            "Training loss per 100 training steps: 0.11042015162862039\n",
            "Training loss per 100 training steps: 0.1084308876452269\n",
            "Training loss per 100 training steps: 0.106682168135379\n",
            "Training loss per 100 training steps: 0.10516310585051541\n",
            "Training loss per 100 training steps: 0.10376671972079203\n",
            "Training loss per 100 training steps: 0.10228242180032585\n",
            "Training loss per 100 training steps: 0.1007165838950055\n",
            "Training loss per 100 training steps: 0.09941541557981899\n",
            "Training loss per 100 training steps: 0.09839074143361108\n",
            "Training loss per 100 training steps: 0.09726262406958266\n",
            "Training loss per 100 training steps: 0.0963064670527444\n",
            "Training loss per 100 training steps: 0.0954250506488097\n",
            "Training loss per 100 training steps: 0.09474592753209767\n",
            "Training loss per 100 training steps: 0.09421351302298717\n",
            "Training loss per 100 training steps: 0.09367339317324305\n",
            "Training loss per 100 training steps: 0.0926399154081365\n",
            "Training loss per 100 training steps: 0.09185735691110666\n",
            "Training loss per 100 training steps: 0.09105633239088715\n",
            "Training loss per 100 training steps: 0.09036307973439435\n",
            "Training loss per 100 training steps: 0.08958946322808334\n",
            "Training loss per 100 training steps: 0.08879356731804054\n",
            "Training loss per 100 training steps: 0.08805976379874794\n",
            "Training loss per 100 training steps: 0.08755500725502971\n",
            "Training loss per 100 training steps: 0.08685778903837094\n",
            "Training loss per 100 training steps: 0.08632012410362302\n",
            "Training loss per 100 training steps: 0.0858200220184264\n",
            "Training loss per 100 training steps: 0.08507770588066667\n",
            "Training loss per 100 training steps: 0.08432584336590176\n",
            "Training loss per 100 training steps: 0.0839255867600536\n",
            "Training loss per 100 training steps: 0.0835429348851554\n",
            "Training loss per 100 training steps: 0.08299781747316422\n",
            "Training loss per 100 training steps: 0.0828363814935997\n",
            "Training loss per 100 training steps: 0.08239356164335063\n",
            "Training loss per 100 training steps: 0.08207829662859525\n",
            "Training loss per 100 training steps: 0.08167259282212366\n",
            "Training loss per 100 training steps: 0.08128640585702669\n",
            "Training loss per 100 training steps: 0.08080708524037618\n",
            "Training loss per 100 training steps: 0.08025343955523567\n",
            "Training loss per 100 training steps: 0.07997337934646906\n",
            "Training loss per 100 training steps: 0.0794519196250864\n",
            "Training loss per 100 training steps: 0.07920247328471484\n",
            "Training loss per 100 training steps: 0.07903996678942349\n",
            "Training loss per 100 training steps: 0.07869654598568256\n",
            "Training loss per 100 training steps: 0.07857976411935852\n",
            "Training loss per 100 training steps: 0.07805149911874189\n",
            "Training loss per 100 training steps: 0.07749195066395315\n",
            "Training loss per 100 training steps: 0.07701721642628996\n",
            "Training loss per 100 training steps: 0.07678344925537257\n",
            "Training loss per 100 training steps: 0.07644828694670519\n",
            "Training loss per 100 training steps: 0.0760413523229363\n",
            "Training loss per 100 training steps: 0.07575539041739728\n",
            "Training loss per 100 training steps: 0.07534210938395214\n",
            "Training loss per 100 training steps: 0.07496090270062168\n",
            "Training loss per 100 training steps: 0.074618041296521\n",
            "Training loss per 100 training steps: 0.0743070125792486\n",
            "Training loss per 100 training steps: 0.07392249127838448\n",
            "Training loss per 100 training steps: 0.07372528454953314\n",
            "Training loss per 100 training steps: 0.0733723715124786\n",
            "Training loss per 100 training steps: 0.07309762621911087\n",
            "Training loss per 100 training steps: 0.07291106549085452\n",
            "Training loss per 100 training steps: 0.0726957904729801\n",
            "Training loss per 100 training steps: 0.0724596335978294\n",
            "Training loss per 100 training steps: 0.07216083921145308\n",
            "Training loss per 100 training steps: 0.07198452446562269\n",
            "Training loss per 100 training steps: 0.07173705126402338\n",
            "Training loss per 100 training steps: 0.07157126105315699\n",
            "Training loss per 100 training steps: 0.07131546393835357\n",
            "Training loss per 100 training steps: 0.07126507537181169\n",
            "Training loss per 100 training steps: 0.07105925297378\n",
            "Training loss per 100 training steps: 0.07081163651065758\n",
            "Training loss per 100 training steps: 0.07056100561544823\n",
            "Training loss per 100 training steps: 0.07054427375220099\n",
            "Training loss per 100 training steps: 0.07040988185217976\n",
            "Training loss per 100 training steps: 0.07020833664060697\n",
            "Training loss per 100 training steps: 0.07009958820010989\n",
            "Training loss per 100 training steps: 0.0699167081088035\n",
            "Training loss per 100 training steps: 0.06964384302468406\n",
            "Training loss per 100 training steps: 0.06951512187002737\n",
            "Training loss per 100 training steps: 0.06952484954624776\n",
            "Training loss per 100 training steps: 0.06953197839445202\n",
            "Training loss per 100 training steps: 0.06934102705245293\n",
            "Training loss per 100 training steps: 0.06933908217465597\n",
            "Training loss per 100 training steps: 0.06916644795813023\n",
            "Training loss per 100 training steps: 0.0691140513620149\n",
            "Training loss per 100 training steps: 0.06896704934433745\n",
            "Training loss per 100 training steps: 0.06872252205244808\n",
            "Training loss per 100 training steps: 0.06865923693496354\n",
            "Training loss per 100 training steps: 0.06853325148924476\n",
            "Training loss per 100 training steps: 0.06836266713416088\n",
            "Training loss per 100 training steps: 0.06814502875486769\n",
            "Training loss per 100 training steps: 0.06796229507398649\n",
            "Training loss per 100 training steps: 0.06777921495822999\n",
            "Training loss per 100 training steps: 0.06762373661214566\n",
            "Training loss per 100 training steps: 0.06743252568893679\n",
            "Training loss per 100 training steps: 0.06745640447025779\n",
            "Training loss per 100 training steps: 0.06734653859300782\n",
            "Training loss per 100 training steps: 0.06719582586363032\n",
            "Training loss per 100 training steps: 0.06705795451295903\n",
            "Training loss per 100 training steps: 0.06706916348940811\n",
            "Training loss per 100 training steps: 0.06693893157246567\n",
            "Training loss per 100 training steps: 0.06677189476680889\n",
            "Training loss per 100 training steps: 0.06668323392255186\n",
            "Training loss per 100 training steps: 0.06641570406860026\n",
            "Training loss per 100 training steps: 0.06623681387578297\n",
            "Training loss per 100 training steps: 0.06600583168844343\n",
            "Training loss per 100 training steps: 0.06591070968304444\n",
            "Training loss per 100 training steps: 0.06575685572221635\n",
            "Training loss per 100 training steps: 0.06569591415469063\n",
            "Training loss per 100 training steps: 0.06569158997915639\n",
            "Training loss per 100 training steps: 0.06554570101985659\n",
            "Training loss per 100 training steps: 0.06535433492338341\n",
            "Training loss per 100 training steps: 0.06528575658225291\n",
            "Training loss per 100 training steps: 0.06514805138907531\n",
            "Training loss per 100 training steps: 0.0649455726465917\n",
            "Training loss per 100 training steps: 0.06487634120411045\n",
            "Training loss per 100 training steps: 0.06470611058165399\n",
            "Training loss per 100 training steps: 0.06453229613711248\n",
            "Training loss per 100 training steps: 0.0642923588110326\n",
            "Training loss per 100 training steps: 0.06420601363496402\n",
            "Training loss per 100 training steps: 0.06405654912280963\n",
            "Training loss per 100 training steps: 0.0639822235634228\n",
            "Training loss per 100 training steps: 0.06385860931102132\n",
            "Training loss per 100 training steps: 0.0638178898815981\n",
            "Training loss per 100 training steps: 0.06367982234103996\n",
            "Training loss per 100 training steps: 0.06360405338247976\n",
            "Training loss per 100 training steps: 0.06349487867188294\n",
            "Training loss per 100 training steps: 0.06335717363079807\n",
            "Training loss per 100 training steps: 0.06319120818467569\n",
            "Training loss per 100 training steps: 0.06312427834030761\n",
            "Training loss per 100 training steps: 0.06313671552853291\n",
            "Training loss per 100 training steps: 0.06301668809971325\n",
            "Training loss per 100 training steps: 0.06296083673199973\n",
            "Training loss per 100 training steps: 0.06287925235455072\n",
            "Training loss per 100 training steps: 0.06275436236983933\n",
            "Training loss per 100 training steps: 0.06265262177012929\n",
            "Training loss per 100 training steps: 0.06251947150999816\n",
            "Training loss per 100 training steps: 0.06244361584501157\n",
            "Training loss per 100 training steps: 0.062359128598423304\n",
            "Training loss per 100 training steps: 0.062287077090573725\n",
            "Training loss per 100 training steps: 0.06221764611585059\n",
            "Training loss per 100 training steps: 0.06209489150485819\n",
            "Training loss per 100 training steps: 0.06197312465084524\n",
            "Training loss per 100 training steps: 0.06185537875839402\n",
            "Training loss per 100 training steps: 0.06182247463651903\n",
            "Training loss per 100 training steps: 0.06173496054988143\n",
            "Training loss per 100 training steps: 0.06164288439981132\n",
            "Training loss per 100 training steps: 0.06151929172401771\n",
            "Training loss per 100 training steps: 0.06137634302792278\n",
            "Training loss per 100 training steps: 0.06131831330810184\n",
            "Training loss per 100 training steps: 0.06123091334628578\n",
            "Training loss per 100 training steps: 0.06112467672657977\n",
            "Training loss per 100 training steps: 0.06099928348010973\n",
            "Training loss per 100 training steps: 0.06089844877323582\n",
            "Training loss per 100 training steps: 0.060845124378623765\n",
            "Training loss per 100 training steps: 0.06068954306047443\n",
            "Training loss per 100 training steps: 0.06059258704891611\n",
            "Training loss per 100 training steps: 0.06055914662334421\n",
            "Training loss per 100 training steps: 0.060431355384663744\n",
            "Training loss per 100 training steps: 0.060564871514229075\n",
            "Training loss per 100 training steps: 0.06045432725853526\n",
            "Training loss per 100 training steps: 0.06035338708381741\n",
            "Training loss per 100 training steps: 0.06022454867126826\n",
            "Training loss per 100 training steps: 0.06015604833904826\n",
            "Training loss per 100 training steps: 0.0600654803051997\n",
            "Training loss per 100 training steps: 0.05995870548883072\n",
            "Training loss per 100 training steps: 0.05988122598381143\n",
            "Training loss per 100 training steps: 0.059819946587936275\n",
            "Training loss per 100 training steps: 0.05970876476709546\n",
            "Training loss per 100 training steps: 0.05960032213701255\n",
            "Training loss per 100 training steps: 0.05957442443364217\n",
            "Training loss per 100 training steps: 0.05948544848030016\n",
            "Training loss per 100 training steps: 0.0594848928217372\n",
            "Training loss per 100 training steps: 0.059400524627645786\n",
            "Training loss per 100 training steps: 0.059311054136425394\n",
            "Training loss per 100 training steps: 0.05922834685481771\n",
            "Training loss per 100 training steps: 0.059183659785082214\n",
            "Training loss per 100 training steps: 0.059085056510634115\n",
            "Training loss per 100 training steps: 0.05906772770457434\n",
            "Training loss per 100 training steps: 0.059038299816398354\n",
            "Training loss per 100 training steps: 0.05896383716186465\n",
            "Training loss per 100 training steps: 0.058921927915847966\n",
            "Training loss per 100 training steps: 0.0588458947510619\n",
            "Training loss per 100 training steps: 0.05877331523745498\n",
            "Training loss per 100 training steps: 0.05871581926558172\n",
            "Training loss per 100 training steps: 0.05864496096028029\n",
            "Training loss per 100 training steps: 0.05860293343542138\n",
            "Training loss per 100 training steps: 0.05853426795896983\n",
            "Training loss per 100 training steps: 0.058446638153968354\n",
            "Training loss per 100 training steps: 0.05840572430551554\n",
            "Training loss per 100 training steps: 0.05839392516598898\n",
            "Training loss per 100 training steps: 0.05834139990819076\n",
            "Training loss per 100 training steps: 0.05825606927558871\n",
            "Training loss per 100 training steps: 0.05823714611365104\n",
            "Training loss per 100 training steps: 0.05816267516095029\n",
            "Training loss per 100 training steps: 0.058106153796375515\n",
            "Training loss per 100 training steps: 0.05803040401623245\n",
            "Training loss per 100 training steps: 0.05801590529306012\n",
            "Training loss per 100 training steps: 0.05798813965096279\n",
            "Training loss per 100 training steps: 0.057931899012346966\n",
            "Training loss per 100 training steps: 0.05788490817859565\n",
            "Training loss per 100 training steps: 0.057838605930567145\n",
            "Training loss per 100 training steps: 0.05778040981362063\n",
            "Training loss per 100 training steps: 0.057702234448580414\n",
            "Training loss per 100 training steps: 0.05759489094723839\n",
            "Training loss per 100 training steps: 0.05753604299592617\n",
            "Training loss per 100 training steps: 0.05746477478803228\n",
            "Training loss per 100 training steps: 0.05738390103990129\n",
            "Training loss per 100 training steps: 0.057313766748151175\n",
            "Training loss per 100 training steps: 0.05727402827584268\n",
            "Training loss per 100 training steps: 0.0572320529010157\n",
            "Training loss per 100 training steps: 0.057181248538929606\n",
            "Training loss epoch: 0.057155191355436466\n",
            "Training accuracy epoch: 0.9817456815100025\n",
            "Training steps: 23180\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.050127586996386524\n",
            "Validation loss per 100 evaluation steps: 0.04877004037360166\n",
            "Validation loss per 100 evaluation steps: 0.04466281570006686\n",
            "Validation loss per 100 evaluation steps: 0.04599099011693397\n",
            "Validation loss per 100 evaluation steps: 0.04634079296789423\n",
            "Validation loss per 100 evaluation steps: 0.045910441906053166\n",
            "Validation loss per 100 evaluation steps: 0.04685836210010166\n",
            "Validation loss per 100 evaluation steps: 0.04537062619817334\n",
            "Validation loss per 100 evaluation steps: 0.04519044532506086\n",
            "Validation loss per 100 evaluation steps: 0.04378662370954407\n",
            "Validation loss per 100 evaluation steps: 0.04364629930128682\n",
            "Validation loss per 100 evaluation steps: 0.04415417053035829\n",
            "Validation loss per 100 evaluation steps: 0.044229711268943414\n",
            "Validation loss per 100 evaluation steps: 0.0440106801800513\n",
            "Validation loss per 100 evaluation steps: 0.043158787408387675\n",
            "Validation loss per 100 evaluation steps: 0.042958738190318396\n",
            "Validation loss per 100 evaluation steps: 0.04309726519257631\n",
            "Validation loss per 100 evaluation steps: 0.04436830991713375\n",
            "Validation loss per 100 evaluation steps: 0.04438602742536061\n",
            "Validation loss per 100 evaluation steps: 0.04488165723320708\n",
            "Validation loss per 100 evaluation steps: 0.04512041980003622\n",
            "Validation loss per 100 evaluation steps: 0.04559244015838141\n",
            "Validation loss per 100 evaluation steps: 0.045292192023347444\n",
            "Validation loss per 100 evaluation steps: 0.04535526445586584\n",
            "Validation loss per 100 evaluation steps: 0.045003268189448865\n",
            "Validation loss per 100 evaluation steps: 0.04465043868915158\n",
            "Validation loss per 100 evaluation steps: 0.044479616206762905\n",
            "Validation loss per 100 evaluation steps: 0.044514867619457485\n",
            "Validation Loss: 0.0445636774414312\n",
            "Validation Accuracy: 0.9859783264052193\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.94      0.95      0.94      5904\n",
            "      B-MISC       0.85      0.87      0.86      4173\n",
            "       B-ORG       0.92      0.92      0.92      3512\n",
            "       B-PER       0.94      0.97      0.96      5593\n",
            "       I-LOC       0.95      0.91      0.93      7891\n",
            "      I-MISC       0.91      0.86      0.89      8442\n",
            "       I-ORG       0.93      0.93      0.93      6001\n",
            "       I-PER       0.96      0.98      0.97     11654\n",
            "           O       1.00      1.00      1.00    253311\n",
            "\n",
            "    accuracy                           0.99    306481\n",
            "   macro avg       0.93      0.93      0.93    306481\n",
            "weighted avg       0.99      0.99      0.99    306481\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9858  0.9858  0.9858\n",
            "macro        0.9335  0.9331  0.9331\n",
            "weighted     0.9858  0.9858  0.9858\n",
            "Validation steps: 2898\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.04645780569408089\n",
            "Test loss per 100 evaluation steps: 0.053829740506535016\n",
            "Test loss per 100 evaluation steps: 0.04952994306488108\n",
            "Test loss per 100 evaluation steps: 0.04709169428892892\n",
            "Test loss per 100 evaluation steps: 0.044656127471906074\n",
            "Test loss per 100 evaluation steps: 0.04431765204467107\n",
            "Test loss per 100 evaluation steps: 0.04367225299710429\n",
            "Test loss per 100 evaluation steps: 0.04318346313054008\n",
            "Test loss per 100 evaluation steps: 0.04270974178046243\n",
            "Test loss per 100 evaluation steps: 0.042083608461030965\n",
            "Test loss per 100 evaluation steps: 0.04169776667822589\n",
            "Test loss per 100 evaluation steps: 0.04122387365207942\n",
            "Test loss per 100 evaluation steps: 0.04090858339180089\n",
            "Test loss per 100 evaluation steps: 0.041270746748177774\n",
            "Test loss per 100 evaluation steps: 0.04094886862741987\n",
            "Test loss per 100 evaluation steps: 0.04140837561053786\n",
            "Test loss per 100 evaluation steps: 0.04168840825360188\n",
            "Test loss per 100 evaluation steps: 0.042125050667946196\n",
            "Test loss per 100 evaluation steps: 0.04164426536180384\n",
            "Test loss per 100 evaluation steps: 0.04145505444393166\n",
            "Test loss per 100 evaluation steps: 0.041501528121947816\n",
            "Test loss per 100 evaluation steps: 0.0414160940293517\n",
            "Test loss per 100 evaluation steps: 0.041095011811606956\n",
            "Test loss per 100 evaluation steps: 0.041483054073498046\n",
            "Test loss per 100 evaluation steps: 0.04144619104859739\n",
            "Test loss per 100 evaluation steps: 0.04109743662700287\n",
            "Test loss per 100 evaluation steps: 0.04095528566094328\n",
            "Test loss per 100 evaluation steps: 0.040754708705729045\n",
            "Test loss per 100 evaluation steps: 0.04109869225917746\n",
            "Test Loss: 0.04109869225917746\n",
            "Test Accuracy: 0.9864058180582829\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.94      0.94      0.94      5955\n",
            "      B-MISC       0.85      0.89      0.87      4505\n",
            "       B-ORG       0.94      0.94      0.94      3449\n",
            "       B-PER       0.95      0.97      0.96      5207\n",
            "       I-LOC       0.94      0.90      0.92      7240\n",
            "      I-MISC       0.92      0.86      0.89      9082\n",
            "       I-ORG       0.95      0.95      0.95      5934\n",
            "       I-PER       0.96      0.98      0.97     11879\n",
            "           O       1.00      1.00      1.00    253790\n",
            "\n",
            "    accuracy                           0.99    307041\n",
            "   macro avg       0.94      0.94      0.94    307041\n",
            "weighted avg       0.99      0.99      0.99    307041\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9860  0.9860  0.9860\n",
            "macro        0.9363  0.9368  0.9363\n",
            "weighted     0.9860  0.9860  0.9859\n",
            "Test steps: 2900\n",
            "====================================================================================================\n",
            "loss_name: kl\n",
            "model configuration\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Ner_Model(\n",
            "  (model): DebertaModel(\n",
            "    (embeddings): DebertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "      (LayerNorm): DebertaLayerNorm()\n",
            "      (dropout): StableDropout()\n",
            "    )\n",
            "    (encoder): DebertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x DebertaLayer(\n",
            "          (attention): DebertaAttention(\n",
            "            (self): DisentangledSelfAttention(\n",
            "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
            "              (pos_dropout): StableDropout()\n",
            "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "            (output): DebertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): DebertaLayerNorm()\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "          )\n",
            "          (intermediate): DebertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): DebertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): DebertaLayerNorm()\n",
            "            (dropout): StableDropout()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (rel_embeddings): Embedding(1024, 768)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
            ")\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.6187008218467236\n",
            "Training loss per 100 training steps: 0.40743478637188674\n",
            "Training loss per 100 training steps: 0.32223974690462154\n",
            "Training loss per 100 training steps: 0.2682594688609242\n",
            "Training loss per 100 training steps: 0.233802408403717\n",
            "Training loss per 100 training steps: 0.20999538903590292\n",
            "Training loss per 100 training steps: 0.1907312153495981\n",
            "Training loss per 100 training steps: 0.17619482048088686\n",
            "Training loss per 100 training steps: 0.16858695437200366\n",
            "Training loss per 100 training steps: 0.16042974638170562\n",
            "Training loss per 100 training steps: 0.153443534207234\n",
            "Training loss per 100 training steps: 0.14709632589889224\n",
            "Training loss per 100 training steps: 0.14157352401812273\n",
            "Training loss per 100 training steps: 0.13739020395782842\n",
            "Training loss per 100 training steps: 0.1333107401578066\n",
            "Training loss per 100 training steps: 0.12966426854836755\n",
            "Training loss per 100 training steps: 0.12602076553890262\n",
            "Training loss per 100 training steps: 0.12278471148236551\n",
            "Training loss per 100 training steps: 0.12045936712713325\n",
            "Training loss per 100 training steps: 0.11805860962203588\n",
            "Training loss per 100 training steps: 0.11539553982613143\n",
            "Training loss per 100 training steps: 0.11309362196128032\n",
            "Training loss per 100 training steps: 0.11077328472461491\n",
            "Training loss per 100 training steps: 0.10868657867528479\n",
            "Training loss per 100 training steps: 0.10647659794981591\n",
            "Training loss per 100 training steps: 0.10444595431127639\n",
            "Training loss per 100 training steps: 0.10283091689860965\n",
            "Training loss per 100 training steps: 0.10142893765292163\n",
            "Training loss per 100 training steps: 0.09997576030555326\n",
            "Training loss per 100 training steps: 0.09880870210501598\n",
            "Training loss per 100 training steps: 0.09799610631311342\n",
            "Training loss per 100 training steps: 0.09721780509080417\n",
            "Training loss per 100 training steps: 0.09666052908456715\n",
            "Training loss per 100 training steps: 0.09569537129633211\n",
            "Training loss per 100 training steps: 0.09484149385350091\n",
            "Training loss per 100 training steps: 0.09371311845791448\n",
            "Training loss per 100 training steps: 0.09283796828274767\n",
            "Training loss per 100 training steps: 0.09218660593719075\n",
            "Training loss per 100 training steps: 0.0912793012523355\n",
            "Training loss per 100 training steps: 0.09027030063646088\n",
            "Training loss per 100 training steps: 0.08973292986290222\n",
            "Training loss per 100 training steps: 0.08891107081619945\n",
            "Training loss per 100 training steps: 0.08823343145656995\n",
            "Training loss per 100 training steps: 0.08757612953737738\n",
            "Training loss per 100 training steps: 0.08686043720755778\n",
            "Training loss per 100 training steps: 0.08622734536281433\n",
            "Training loss per 100 training steps: 0.08570584502894718\n",
            "Training loss per 100 training steps: 0.08503487119760393\n",
            "Training loss per 100 training steps: 0.08460264978508646\n",
            "Training loss per 100 training steps: 0.08436292919105617\n",
            "Training loss per 100 training steps: 0.0838736704128735\n",
            "Training loss per 100 training steps: 0.08367844934551753\n",
            "Training loss per 100 training steps: 0.08302279422280405\n",
            "Training loss per 100 training steps: 0.08281379937898169\n",
            "Training loss per 100 training steps: 0.08246663910368542\n",
            "Training loss per 100 training steps: 0.08213443311180786\n",
            "Training loss per 100 training steps: 0.08156011720333992\n",
            "Training loss per 100 training steps: 0.08105592412945357\n",
            "Training loss per 100 training steps: 0.08066178172604789\n",
            "Training loss per 100 training steps: 0.08017841397819575\n",
            "Training loss per 100 training steps: 0.07970067069366704\n",
            "Training loss per 100 training steps: 0.07918341723887998\n",
            "Training loss per 100 training steps: 0.07872460793010384\n",
            "Training loss per 100 training steps: 0.07833217103267089\n",
            "Training loss per 100 training steps: 0.07793251076323661\n",
            "Training loss per 100 training steps: 0.07767302872711408\n",
            "Training loss per 100 training steps: 0.0773957124287254\n",
            "Training loss per 100 training steps: 0.07709235398920154\n",
            "Training loss per 100 training steps: 0.07677927415138465\n",
            "Training loss per 100 training steps: 0.07693717269673028\n",
            "Training loss per 100 training steps: 0.07667918715899823\n",
            "Training loss per 100 training steps: 0.07632062537716088\n",
            "Training loss per 100 training steps: 0.07609797768201919\n",
            "Training loss per 100 training steps: 0.0758716988533372\n",
            "Training loss per 100 training steps: 0.07562388474175628\n",
            "Training loss per 100 training steps: 0.07545519513089906\n",
            "Training loss per 100 training steps: 0.07512826935194249\n",
            "Training loss per 100 training steps: 0.07476295732962825\n",
            "Training loss per 100 training steps: 0.07455416488048217\n",
            "Training loss per 100 training steps: 0.07436289714706436\n",
            "Training loss per 100 training steps: 0.0742124709453438\n",
            "Training loss per 100 training steps: 0.07403944837301046\n",
            "Training loss per 100 training steps: 0.07374472776753617\n",
            "Training loss per 100 training steps: 0.07344369410894745\n",
            "Training loss per 100 training steps: 0.07308544672841741\n",
            "Training loss per 100 training steps: 0.07272911572423804\n",
            "Training loss per 100 training steps: 0.07245814961609738\n",
            "Training loss per 100 training steps: 0.07206843722530969\n",
            "Training loss per 100 training steps: 0.07181555166728062\n",
            "Training loss per 100 training steps: 0.07149985257672638\n",
            "Training loss per 100 training steps: 0.0711671440437779\n",
            "Training loss per 100 training steps: 0.07101835673542076\n",
            "Training loss per 100 training steps: 0.07088035659698684\n",
            "Training loss per 100 training steps: 0.0705668856215631\n",
            "Training loss per 100 training steps: 0.07055040026216547\n",
            "Training loss per 100 training steps: 0.0702179129758224\n",
            "Training loss per 100 training steps: 0.070016615956166\n",
            "Training loss per 100 training steps: 0.06979780818343731\n",
            "Training loss per 100 training steps: 0.06963257271083569\n",
            "Training loss per 100 training steps: 0.06931109197734914\n",
            "Training loss per 100 training steps: 0.06916509487267669\n",
            "Training loss per 100 training steps: 0.0689834431875052\n",
            "Training loss per 100 training steps: 0.06870433658382344\n",
            "Training loss per 100 training steps: 0.06851333645627444\n",
            "Training loss per 100 training steps: 0.06840090464374295\n",
            "Training loss per 100 training steps: 0.06815522683682503\n",
            "Training loss per 100 training steps: 0.0679210387132099\n",
            "Training loss per 100 training steps: 0.06777208425494556\n",
            "Training loss per 100 training steps: 0.06769397977764938\n",
            "Training loss per 100 training steps: 0.06756911794485958\n",
            "Training loss per 100 training steps: 0.0674007227145705\n",
            "Training loss per 100 training steps: 0.06719274239649167\n",
            "Training loss per 100 training steps: 0.06709489396761173\n",
            "Training loss per 100 training steps: 0.0668842495404431\n",
            "Training loss per 100 training steps: 0.06668837579858297\n",
            "Training loss per 100 training steps: 0.0665384825040483\n",
            "Training loss per 100 training steps: 0.0663927214308423\n",
            "Training loss per 100 training steps: 0.0661050858795623\n",
            "Training loss per 100 training steps: 0.06606076937373787\n",
            "Training loss per 100 training steps: 0.0659147012926478\n",
            "Training loss per 100 training steps: 0.06578246608459966\n",
            "Training loss per 100 training steps: 0.06561247844259983\n",
            "Training loss per 100 training steps: 0.06552964124209115\n",
            "Training loss per 100 training steps: 0.06545946266782383\n",
            "Training loss per 100 training steps: 0.06533594290222507\n",
            "Training loss per 100 training steps: 0.06521122590950985\n",
            "Training loss per 100 training steps: 0.06510445261815027\n",
            "Training loss per 100 training steps: 0.06491955735747978\n",
            "Training loss per 100 training steps: 0.06483595389230475\n",
            "Training loss per 100 training steps: 0.06466276487131611\n",
            "Training loss per 100 training steps: 0.06450722556881373\n",
            "Training loss per 100 training steps: 0.06440333633967825\n",
            "Training loss per 100 training steps: 0.06430426743373238\n",
            "Training loss per 100 training steps: 0.06420352289268752\n",
            "Training loss per 100 training steps: 0.06421994846576558\n",
            "Training loss per 100 training steps: 0.06410540421165506\n",
            "Training loss per 100 training steps: 0.06388374540650832\n",
            "Training loss per 100 training steps: 0.06372351128237969\n",
            "Training loss per 100 training steps: 0.063663312294109\n",
            "Training loss per 100 training steps: 0.06363739093484037\n",
            "Training loss per 100 training steps: 0.06347569041497488\n",
            "Training loss per 100 training steps: 0.06328178930650634\n",
            "Training loss per 100 training steps: 0.06314495976886389\n",
            "Training loss per 100 training steps: 0.06300981721611708\n",
            "Training loss per 100 training steps: 0.06284837882845072\n",
            "Training loss per 100 training steps: 0.06275803018263752\n",
            "Training loss per 100 training steps: 0.06260990594152886\n",
            "Training loss per 100 training steps: 0.06251668934486047\n",
            "Training loss per 100 training steps: 0.06242367736381403\n",
            "Training loss per 100 training steps: 0.062324225678103785\n",
            "Training loss per 100 training steps: 0.06218253003254387\n",
            "Training loss per 100 training steps: 0.06204135329337269\n",
            "Training loss per 100 training steps: 0.06197532469095047\n",
            "Training loss per 100 training steps: 0.06192234185180236\n",
            "Training loss per 100 training steps: 0.06170697877327104\n",
            "Training loss per 100 training steps: 0.06161354066264418\n",
            "Training loss per 100 training steps: 0.06157346611046977\n",
            "Training loss per 100 training steps: 0.061515291713752314\n",
            "Training loss per 100 training steps: 0.06144756166386779\n",
            "Training loss per 100 training steps: 0.06130602414283294\n",
            "Training loss per 100 training steps: 0.06112343797673235\n",
            "Training loss per 100 training steps: 0.06099910088688319\n",
            "Training loss per 100 training steps: 0.060916544002578295\n",
            "Training loss per 100 training steps: 0.06084622862172138\n",
            "Training loss per 100 training steps: 0.06071837131071493\n",
            "Training loss per 100 training steps: 0.06066459469833914\n",
            "Training loss per 100 training steps: 0.06056430626214007\n",
            "Training loss per 100 training steps: 0.060544425519324464\n",
            "Training loss per 100 training steps: 0.06042424044134226\n",
            "Training loss per 100 training steps: 0.06031446832900025\n",
            "Training loss per 100 training steps: 0.0601664339036821\n",
            "Training loss per 100 training steps: 0.06014915575269443\n",
            "Training loss per 100 training steps: 0.06004479625499693\n",
            "Training loss per 100 training steps: 0.0599725783293486\n",
            "Training loss per 100 training steps: 0.05996032643464527\n",
            "Training loss per 100 training steps: 0.059956999191485505\n",
            "Training loss per 100 training steps: 0.05993526868940802\n",
            "Training loss per 100 training steps: 0.05995449220735169\n",
            "Training loss per 100 training steps: 0.0598688213949387\n",
            "Training loss per 100 training steps: 0.05978216105611904\n",
            "Training loss per 100 training steps: 0.05968035284627727\n",
            "Training loss per 100 training steps: 0.05966800255364173\n",
            "Training loss per 100 training steps: 0.05958027335200929\n",
            "Training loss per 100 training steps: 0.05948219413784777\n",
            "Training loss per 100 training steps: 0.059412451870050405\n",
            "Training loss per 100 training steps: 0.059384746525436016\n",
            "Training loss per 100 training steps: 0.05929457419176049\n",
            "Training loss per 100 training steps: 0.059235245763304696\n",
            "Training loss per 100 training steps: 0.059138141345051326\n",
            "Training loss per 100 training steps: 0.059010002847976896\n",
            "Training loss per 100 training steps: 0.058926664001531764\n",
            "Training loss per 100 training steps: 0.05886432497796401\n",
            "Training loss per 100 training steps: 0.05879856605808838\n",
            "Training loss per 100 training steps: 0.058677218002273185\n",
            "Training loss per 100 training steps: 0.05865308069671543\n",
            "Training loss per 100 training steps: 0.058596227104795354\n",
            "Training loss per 100 training steps: 0.058503097061725684\n",
            "Training loss per 100 training steps: 0.05844200511830239\n",
            "Training loss per 100 training steps: 0.058401175285032136\n",
            "Training loss per 100 training steps: 0.058416361869029786\n",
            "Training loss per 100 training steps: 0.05834206508384164\n",
            "Training loss per 100 training steps: 0.05824086402891667\n",
            "Training loss per 100 training steps: 0.05812623934163128\n",
            "Training loss per 100 training steps: 0.058055332493917304\n",
            "Training loss per 100 training steps: 0.05799287920602045\n",
            "Training loss per 100 training steps: 0.05794287126208995\n",
            "Training loss per 100 training steps: 0.05789746908165609\n",
            "Training loss per 100 training steps: 0.05783742200224599\n",
            "Training loss per 100 training steps: 0.057801513620850696\n",
            "Training loss per 100 training steps: 0.057689485821037945\n",
            "Training loss per 100 training steps: 0.057611143502430365\n",
            "Training loss per 100 training steps: 0.057519601897001164\n",
            "Training loss per 100 training steps: 0.05746847033671673\n",
            "Training loss per 100 training steps: 0.05737844515270018\n",
            "Training loss per 100 training steps: 0.05729850299668439\n",
            "Training loss per 100 training steps: 0.05723899658330252\n",
            "Training loss per 100 training steps: 0.05718917328118634\n",
            "Training loss per 100 training steps: 0.05714517715179181\n",
            "Training loss per 100 training steps: 0.0570823952616017\n",
            "Training loss per 100 training steps: 0.05702920802970303\n",
            "Training loss per 100 training steps: 0.05698041853475913\n",
            "Training loss per 100 training steps: 0.056921221362864746\n",
            "Training loss per 100 training steps: 0.05683488790248233\n",
            "Training loss per 100 training steps: 0.056745385770673236\n",
            "Training loss per 100 training steps: 0.056708270871509904\n",
            "Training loss per 100 training steps: 0.056658929677859654\n",
            "Training loss per 100 training steps: 0.05660529853740968\n",
            "Training loss per 100 training steps: 0.05653584881384424\n",
            "Training loss per 100 training steps: 0.056502064639480286\n",
            "Training loss per 100 training steps: 0.05645753149574233\n",
            "Training loss per 100 training steps: 0.05646180486293067\n",
            "Training loss epoch: 0.056414111620429704\n",
            "Training accuracy epoch: 0.9821925491931781\n",
            "Training steps: 23180\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.04232362921378808\n",
            "Validation loss per 100 evaluation steps: 0.036564307139706216\n",
            "Validation loss per 100 evaluation steps: 0.03684703696504585\n",
            "Validation loss per 100 evaluation steps: 0.03827078838981834\n",
            "Validation loss per 100 evaluation steps: 0.04071070966913248\n",
            "Validation loss per 100 evaluation steps: 0.041201244533852634\n",
            "Validation loss per 100 evaluation steps: 0.041560983970807035\n",
            "Validation loss per 100 evaluation steps: 0.04099869057781689\n",
            "Validation loss per 100 evaluation steps: 0.04210234184919197\n",
            "Validation loss per 100 evaluation steps: 0.041481063186045504\n",
            "Validation loss per 100 evaluation steps: 0.04251815404830268\n",
            "Validation loss per 100 evaluation steps: 0.04334144731989606\n",
            "Validation loss per 100 evaluation steps: 0.043571439804889874\n",
            "Validation loss per 100 evaluation steps: 0.04336612541586094\n",
            "Validation loss per 100 evaluation steps: 0.043215013374108824\n",
            "Validation loss per 100 evaluation steps: 0.04314215098595014\n",
            "Validation loss per 100 evaluation steps: 0.043140012482716185\n",
            "Validation loss per 100 evaluation steps: 0.042854603978699335\n",
            "Validation loss per 100 evaluation steps: 0.042976674404417284\n",
            "Validation loss per 100 evaluation steps: 0.04274427970900433\n",
            "Validation loss per 100 evaluation steps: 0.04318990201699697\n",
            "Validation loss per 100 evaluation steps: 0.04312721762585914\n",
            "Validation loss per 100 evaluation steps: 0.04321913320786932\n",
            "Validation loss per 100 evaluation steps: 0.042959641220337894\n",
            "Validation loss per 100 evaluation steps: 0.04320294867800549\n",
            "Validation loss per 100 evaluation steps: 0.04338229444159459\n",
            "Validation loss per 100 evaluation steps: 0.04363310971882095\n",
            "Validation loss per 100 evaluation steps: 0.04371499864448457\n",
            "Validation Loss: 0.04359083665768789\n",
            "Validation Accuracy: 0.9858776414744376\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.94      0.95      0.94      5904\n",
            "      B-MISC       0.80      0.91      0.85      4173\n",
            "       B-ORG       0.93      0.92      0.92      3512\n",
            "       B-PER       0.96      0.95      0.95      5593\n",
            "       I-LOC       0.95      0.91      0.93      7891\n",
            "      I-MISC       0.88      0.89      0.89      8442\n",
            "       I-ORG       0.95      0.92      0.93      6001\n",
            "       I-PER       0.98      0.97      0.98     11654\n",
            "           O       1.00      1.00      1.00    253311\n",
            "\n",
            "    accuracy                           0.99    306481\n",
            "   macro avg       0.93      0.93      0.93    306481\n",
            "weighted avg       0.99      0.99      0.99    306481\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9858  0.9858  0.9858\n",
            "macro        0.9328  0.9350  0.9334\n",
            "weighted     0.9861  0.9858  0.9859\n",
            "Validation steps: 2898\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.05696285114594502\n",
            "Test loss per 100 evaluation steps: 0.048417574084742225\n",
            "Test loss per 100 evaluation steps: 0.045544948784421044\n",
            "Test loss per 100 evaluation steps: 0.04228979558734863\n",
            "Test loss per 100 evaluation steps: 0.04207475810917095\n",
            "Test loss per 100 evaluation steps: 0.042506460628210334\n",
            "Test loss per 100 evaluation steps: 0.04318804416332569\n",
            "Test loss per 100 evaluation steps: 0.04467708968010811\n",
            "Test loss per 100 evaluation steps: 0.043721968297120634\n",
            "Test loss per 100 evaluation steps: 0.04359088003276702\n",
            "Test loss per 100 evaluation steps: 0.04358063889388921\n",
            "Test loss per 100 evaluation steps: 0.043188329872955365\n",
            "Test loss per 100 evaluation steps: 0.04290247416471315\n",
            "Test loss per 100 evaluation steps: 0.04243948867686933\n",
            "Test loss per 100 evaluation steps: 0.042688938418752514\n",
            "Test loss per 100 evaluation steps: 0.04364332355667102\n",
            "Test loss per 100 evaluation steps: 0.04336068589844645\n",
            "Test loss per 100 evaluation steps: 0.04313478102198537\n",
            "Test loss per 100 evaluation steps: 0.043893860023795\n",
            "Test loss per 100 evaluation steps: 0.043628583610319766\n",
            "Test loss per 100 evaluation steps: 0.042920062388854735\n",
            "Test loss per 100 evaluation steps: 0.04286613907115349\n",
            "Test loss per 100 evaluation steps: 0.04294066437155656\n",
            "Test loss per 100 evaluation steps: 0.043074498708435084\n",
            "Test loss per 100 evaluation steps: 0.04318118901961716\n",
            "Test loss per 100 evaluation steps: 0.04311255134952192\n",
            "Test loss per 100 evaluation steps: 0.043038747271477606\n",
            "Test loss per 100 evaluation steps: 0.042914777100943734\n",
            "Test loss per 100 evaluation steps: 0.04275887072189201\n",
            "Test Loss: 0.04275887072189201\n",
            "Test Accuracy: 0.9856159332269327\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.93      0.94      0.94      5955\n",
            "      B-MISC       0.81      0.92      0.86      4505\n",
            "       B-ORG       0.95      0.92      0.94      3449\n",
            "       B-PER       0.97      0.96      0.96      5207\n",
            "       I-LOC       0.93      0.89      0.91      7240\n",
            "      I-MISC       0.87      0.89      0.88      9082\n",
            "       I-ORG       0.96      0.92      0.94      5934\n",
            "       I-PER       0.98      0.98      0.98     11879\n",
            "           O       1.00      1.00      1.00    253790\n",
            "\n",
            "    accuracy                           0.99    307041\n",
            "   macro avg       0.93      0.93      0.93    307041\n",
            "weighted avg       0.99      0.99      0.99    307041\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9852  0.9852  0.9852\n",
            "macro        0.9321  0.9344  0.9327\n",
            "weighted     0.9855  0.9852  0.9853\n",
            "Test steps: 2900\n",
            "====================================================================================================\n",
            "loss_name: dlite\n",
            "model configuration\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Ner_Model(\n",
            "  (model): DebertaModel(\n",
            "    (embeddings): DebertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "      (LayerNorm): DebertaLayerNorm()\n",
            "      (dropout): StableDropout()\n",
            "    )\n",
            "    (encoder): DebertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x DebertaLayer(\n",
            "          (attention): DebertaAttention(\n",
            "            (self): DisentangledSelfAttention(\n",
            "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
            "              (pos_dropout): StableDropout()\n",
            "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "            (output): DebertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): DebertaLayerNorm()\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "          )\n",
            "          (intermediate): DebertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): DebertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): DebertaLayerNorm()\n",
            "            (dropout): StableDropout()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (rel_embeddings): Embedding(1024, 768)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
            ")\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.2129407974332571\n",
            "Training loss per 100 training steps: 0.15081721754744648\n",
            "Training loss per 100 training steps: 0.11594921975862235\n",
            "Training loss per 100 training steps: 0.09646021662571001\n",
            "Training loss per 100 training steps: 0.08354685017419979\n",
            "Training loss per 100 training steps: 0.07498509342782199\n",
            "Training loss per 100 training steps: 0.06735142254869321\n",
            "Training loss per 100 training steps: 0.061896983771694065\n",
            "Training loss per 100 training steps: 0.057512123695067\n",
            "Training loss per 100 training steps: 0.0540754867814976\n",
            "Training loss per 100 training steps: 0.051105115172527305\n",
            "Training loss per 100 training steps: 0.04914795190137132\n",
            "Training loss per 100 training steps: 0.04726475552195692\n",
            "Training loss per 100 training steps: 0.045266655675726984\n",
            "Training loss per 100 training steps: 0.043929772132200014\n",
            "Training loss per 100 training steps: 0.04248031088640118\n",
            "Training loss per 100 training steps: 0.04107541198527221\n",
            "Training loss per 100 training steps: 0.04028416464778679\n",
            "Training loss per 100 training steps: 0.03923612090450517\n",
            "Training loss per 100 training steps: 0.03853407795468047\n",
            "Training loss per 100 training steps: 0.037731892250617204\n",
            "Training loss per 100 training steps: 0.03686305391721444\n",
            "Training loss per 100 training steps: 0.035970044395648784\n",
            "Training loss per 100 training steps: 0.03520049021976016\n",
            "Training loss per 100 training steps: 0.034462109404998775\n",
            "Training loss per 100 training steps: 0.03380123123719246\n",
            "Training loss per 100 training steps: 0.03331242740656981\n",
            "Training loss per 100 training steps: 0.032987681554417836\n",
            "Training loss per 100 training steps: 0.0324705180192014\n",
            "Training loss per 100 training steps: 0.032142111793696436\n",
            "Training loss per 100 training steps: 0.03172675562899742\n",
            "Training loss per 100 training steps: 0.031273546452744085\n",
            "Training loss per 100 training steps: 0.030969284598022843\n",
            "Training loss per 100 training steps: 0.030650607775633568\n",
            "Training loss per 100 training steps: 0.03023586882636716\n",
            "Training loss per 100 training steps: 0.029978597466516578\n",
            "Training loss per 100 training steps: 0.029465413170482617\n",
            "Training loss per 100 training steps: 0.02917881157863552\n",
            "Training loss per 100 training steps: 0.028832079802445943\n",
            "Training loss per 100 training steps: 0.028589191292800024\n",
            "Training loss per 100 training steps: 0.02833595790620077\n",
            "Training loss per 100 training steps: 0.028188483573725793\n",
            "Training loss per 100 training steps: 0.02791080417997956\n",
            "Training loss per 100 training steps: 0.02769189379207629\n",
            "Training loss per 100 training steps: 0.02745659962214874\n",
            "Training loss per 100 training steps: 0.027275926027318154\n",
            "Training loss per 100 training steps: 0.027025503364027904\n",
            "Training loss per 100 training steps: 0.026856921347137283\n",
            "Training loss per 100 training steps: 0.026691427285805935\n",
            "Training loss per 100 training steps: 0.026616715824612766\n",
            "Training loss per 100 training steps: 0.026406283571864773\n",
            "Training loss per 100 training steps: 0.026208163608164865\n",
            "Training loss per 100 training steps: 0.026015196245123592\n",
            "Training loss per 100 training steps: 0.02579746713353921\n",
            "Training loss per 100 training steps: 0.025629166928479788\n",
            "Training loss per 100 training steps: 0.025454688987055565\n",
            "Training loss per 100 training steps: 0.025251554482121016\n",
            "Training loss per 100 training steps: 0.0251002648321261\n",
            "Training loss per 100 training steps: 0.02503968308097714\n",
            "Training loss per 100 training steps: 0.024863530257497587\n",
            "Training loss per 100 training steps: 0.02473472228102138\n",
            "Training loss per 100 training steps: 0.024594556043194906\n",
            "Training loss per 100 training steps: 0.0245197573119803\n",
            "Training loss per 100 training steps: 0.024417893423182024\n",
            "Training loss per 100 training steps: 0.02432290102865227\n",
            "Training loss per 100 training steps: 0.02422220520208694\n",
            "Training loss per 100 training steps: 0.02408848647543606\n",
            "Training loss per 100 training steps: 0.023966699668371536\n",
            "Training loss per 100 training steps: 0.023875782782625733\n",
            "Training loss per 100 training steps: 0.023732549973064773\n",
            "Training loss per 100 training steps: 0.02362866471431811\n",
            "Training loss per 100 training steps: 0.0235634398851337\n",
            "Training loss per 100 training steps: 0.02349137410067495\n",
            "Training loss per 100 training steps: 0.02336892822158338\n",
            "Training loss per 100 training steps: 0.023263683042156844\n",
            "Training loss per 100 training steps: 0.023177751893991076\n",
            "Training loss per 100 training steps: 0.023067229578213323\n",
            "Training loss per 100 training steps: 0.022923316265267273\n",
            "Training loss per 100 training steps: 0.022805320024866012\n",
            "Training loss per 100 training steps: 0.02272035889021993\n",
            "Training loss per 100 training steps: 0.02266225442005275\n",
            "Training loss per 100 training steps: 0.022589013491345924\n",
            "Training loss per 100 training steps: 0.02250198916566671\n",
            "Training loss per 100 training steps: 0.022395784380775066\n",
            "Training loss per 100 training steps: 0.022312350817732935\n",
            "Training loss per 100 training steps: 0.022276141507779654\n",
            "Training loss per 100 training steps: 0.02218578465525292\n",
            "Training loss per 100 training steps: 0.022126337734252235\n",
            "Training loss per 100 training steps: 0.022054726629265827\n",
            "Training loss per 100 training steps: 0.02195905846366309\n",
            "Training loss per 100 training steps: 0.02182252804542079\n",
            "Training loss per 100 training steps: 0.02176981170828689\n",
            "Training loss per 100 training steps: 0.021742350779094845\n",
            "Training loss per 100 training steps: 0.02163806637992685\n",
            "Training loss per 100 training steps: 0.021559730617335987\n",
            "Training loss per 100 training steps: 0.021529759950887902\n",
            "Training loss per 100 training steps: 0.021490738409038573\n",
            "Training loss per 100 training steps: 0.02144011754180536\n",
            "Training loss per 100 training steps: 0.021373886679170156\n",
            "Training loss per 100 training steps: 0.02130613355988735\n",
            "Training loss per 100 training steps: 0.02121289900388601\n",
            "Training loss per 100 training steps: 0.021172790645501822\n",
            "Training loss per 100 training steps: 0.021098275023757718\n",
            "Training loss per 100 training steps: 0.0210514069966473\n",
            "Training loss per 100 training steps: 0.020981531355674418\n",
            "Training loss per 100 training steps: 0.020922727694470303\n",
            "Training loss per 100 training steps: 0.02084671864034288\n",
            "Training loss per 100 training steps: 0.02078694437165243\n",
            "Training loss per 100 training steps: 0.020755544905254317\n",
            "Training loss per 100 training steps: 0.02074859359871392\n",
            "Training loss per 100 training steps: 0.020709203752560062\n",
            "Training loss per 100 training steps: 0.020627403413835572\n",
            "Training loss per 100 training steps: 0.020553154528843473\n",
            "Training loss per 100 training steps: 0.020521414738399184\n",
            "Training loss per 100 training steps: 0.020508555651418094\n",
            "Training loss per 100 training steps: 0.02047192686698791\n",
            "Training loss per 100 training steps: 0.020458196596824938\n",
            "Training loss per 100 training steps: 0.020394550496876343\n",
            "Training loss per 100 training steps: 0.02033230164246882\n",
            "Training loss per 100 training steps: 0.020313609729354237\n",
            "Training loss per 100 training steps: 0.020291591586852776\n",
            "Training loss per 100 training steps: 0.020227802879689394\n",
            "Training loss per 100 training steps: 0.020161216747881523\n",
            "Training loss per 100 training steps: 0.02010445589969931\n",
            "Training loss per 100 training steps: 0.0200891541846552\n",
            "Training loss per 100 training steps: 0.020062138232313172\n",
            "Training loss per 100 training steps: 0.02002135169709661\n",
            "Training loss per 100 training steps: 0.019991189808483866\n",
            "Training loss per 100 training steps: 0.01996379789583408\n",
            "Training loss per 100 training steps: 0.019928751243290067\n",
            "Training loss per 100 training steps: 0.019858505481221724\n",
            "Training loss per 100 training steps: 0.019839093247527032\n",
            "Training loss per 100 training steps: 0.019807012752968745\n",
            "Training loss per 100 training steps: 0.019804409773704373\n",
            "Training loss per 100 training steps: 0.019765770965629533\n",
            "Training loss per 100 training steps: 0.019714617691142078\n",
            "Training loss per 100 training steps: 0.019680207449110428\n",
            "Training loss per 100 training steps: 0.019645247861103565\n",
            "Training loss per 100 training steps: 0.019618123265697524\n",
            "Training loss per 100 training steps: 0.01957633719816571\n",
            "Training loss per 100 training steps: 0.01953300056008884\n",
            "Training loss per 100 training steps: 0.01949760518355724\n",
            "Training loss per 100 training steps: 0.01944948704094704\n",
            "Training loss per 100 training steps: 0.019414697071351492\n",
            "Training loss per 100 training steps: 0.019396977347459203\n",
            "Training loss per 100 training steps: 0.01938776362917305\n",
            "Training loss per 100 training steps: 0.01935356545677718\n",
            "Training loss per 100 training steps: 0.019323501753613474\n",
            "Training loss per 100 training steps: 0.01929693057578782\n",
            "Training loss per 100 training steps: 0.019282422686858595\n",
            "Training loss per 100 training steps: 0.019255846592665743\n",
            "Training loss per 100 training steps: 0.019218812173881234\n",
            "Training loss per 100 training steps: 0.019193756677592953\n",
            "Training loss per 100 training steps: 0.01916907976525107\n",
            "Training loss per 100 training steps: 0.019128398687071856\n",
            "Training loss per 100 training steps: 0.01911144253436932\n",
            "Training loss per 100 training steps: 0.019066792463493146\n",
            "Training loss per 100 training steps: 0.01906622915087296\n",
            "Training loss per 100 training steps: 0.019071413244275373\n",
            "Training loss per 100 training steps: 0.01904619894421202\n",
            "Training loss per 100 training steps: 0.01900750876546986\n",
            "Training loss per 100 training steps: 0.019001570854345742\n",
            "Training loss per 100 training steps: 0.018985635947739023\n",
            "Training loss per 100 training steps: 0.018959097009416653\n",
            "Training loss per 100 training steps: 0.018939507542536017\n",
            "Training loss per 100 training steps: 0.0189199783553226\n",
            "Training loss per 100 training steps: 0.018861074457743047\n",
            "Training loss per 100 training steps: 0.018845390863504088\n",
            "Training loss per 100 training steps: 0.018822792157151925\n",
            "Training loss per 100 training steps: 0.018794889823115255\n",
            "Training loss per 100 training steps: 0.018771247589224063\n",
            "Training loss per 100 training steps: 0.018742903818804942\n",
            "Training loss per 100 training steps: 0.01872548718552785\n",
            "Training loss per 100 training steps: 0.018724426463900035\n",
            "Training loss per 100 training steps: 0.018711781479104815\n",
            "Training loss per 100 training steps: 0.018696632737149575\n",
            "Training loss per 100 training steps: 0.01867346526724145\n",
            "Training loss per 100 training steps: 0.018637561495302716\n",
            "Training loss per 100 training steps: 0.018630705160646646\n",
            "Training loss per 100 training steps: 0.018638148989327266\n",
            "Training loss per 100 training steps: 0.01863024809088554\n",
            "Training loss per 100 training steps: 0.018616867527324218\n",
            "Training loss per 100 training steps: 0.01860767546590635\n",
            "Training loss per 100 training steps: 0.018591412130966725\n",
            "Training loss per 100 training steps: 0.018569722049925935\n",
            "Training loss per 100 training steps: 0.01854320696650311\n",
            "Training loss per 100 training steps: 0.018527348013086403\n",
            "Training loss per 100 training steps: 0.018503314960575386\n",
            "Training loss per 100 training steps: 0.018503289894640805\n",
            "Training loss per 100 training steps: 0.018497243816569513\n",
            "Training loss per 100 training steps: 0.01845998797548604\n",
            "Training loss per 100 training steps: 0.01844496648031793\n",
            "Training loss per 100 training steps: 0.018407330790914077\n",
            "Training loss per 100 training steps: 0.01837469636394942\n",
            "Training loss per 100 training steps: 0.01837582781648113\n",
            "Training loss per 100 training steps: 0.018391199880600707\n",
            "Training loss per 100 training steps: 0.01837480082793337\n",
            "Training loss per 100 training steps: 0.018359183071652158\n",
            "Training loss per 100 training steps: 0.018351438668003896\n",
            "Training loss per 100 training steps: 0.01834286462738778\n",
            "Training loss per 100 training steps: 0.01833002319115943\n",
            "Training loss per 100 training steps: 0.018306922012895736\n",
            "Training loss per 100 training steps: 0.018299839443547475\n",
            "Training loss per 100 training steps: 0.018274876635609203\n",
            "Training loss per 100 training steps: 0.01824524482597842\n",
            "Training loss per 100 training steps: 0.018229484023432208\n",
            "Training loss per 100 training steps: 0.01819909200105615\n",
            "Training loss per 100 training steps: 0.018187840308004386\n",
            "Training loss per 100 training steps: 0.018183028864230993\n",
            "Training loss per 100 training steps: 0.01816324380733602\n",
            "Training loss per 100 training steps: 0.018153775529382975\n",
            "Training loss per 100 training steps: 0.01813459225985624\n",
            "Training loss per 100 training steps: 0.01812055230720457\n",
            "Training loss per 100 training steps: 0.0181032272042985\n",
            "Training loss per 100 training steps: 0.018073546212267862\n",
            "Training loss per 100 training steps: 0.018035575300988326\n",
            "Training loss per 100 training steps: 0.018013337477715403\n",
            "Training loss per 100 training steps: 0.01799920318385417\n",
            "Training loss per 100 training steps: 0.017987961916533096\n",
            "Training loss per 100 training steps: 0.01798250374161522\n",
            "Training loss per 100 training steps: 0.017962345443089955\n",
            "Training loss per 100 training steps: 0.017949194857192687\n",
            "Training loss per 100 training steps: 0.017928662611939095\n",
            "Training loss per 100 training steps: 0.01790146170160065\n",
            "Training loss per 100 training steps: 0.01790125005623397\n",
            "Training loss per 100 training steps: 0.01788923001345923\n",
            "Training loss per 100 training steps: 0.017872286947362755\n",
            "Training loss per 100 training steps: 0.01785375675770466\n",
            "Training loss per 100 training steps: 0.01783619063927004\n",
            "Training loss per 100 training steps: 0.017821944377867983\n",
            "Training loss per 100 training steps: 0.017808509341687123\n",
            "Training loss epoch: 0.01778998112880108\n",
            "Training accuracy epoch: 0.9801109061295052\n",
            "Training steps: 23180\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.01372706213809579\n",
            "Validation loss per 100 evaluation steps: 0.013341846411392452\n",
            "Validation loss per 100 evaluation steps: 0.01339411550791586\n",
            "Validation loss per 100 evaluation steps: 0.014217533011790238\n",
            "Validation loss per 100 evaluation steps: 0.014662015724836237\n",
            "Validation loss per 100 evaluation steps: 0.014782659803719678\n",
            "Validation loss per 100 evaluation steps: 0.014763261429996939\n",
            "Validation loss per 100 evaluation steps: 0.014514371816046605\n",
            "Validation loss per 100 evaluation steps: 0.01440855573985876\n",
            "Validation loss per 100 evaluation steps: 0.014435178475568137\n",
            "Validation loss per 100 evaluation steps: 0.014149443211102732\n",
            "Validation loss per 100 evaluation steps: 0.0140156709522239\n",
            "Validation loss per 100 evaluation steps: 0.013909629136777627\n",
            "Validation loss per 100 evaluation steps: 0.013852456343418973\n",
            "Validation loss per 100 evaluation steps: 0.01387474485636873\n",
            "Validation loss per 100 evaluation steps: 0.013737060403766179\n",
            "Validation loss per 100 evaluation steps: 0.013768792328978634\n",
            "Validation loss per 100 evaluation steps: 0.013703547977644347\n",
            "Validation loss per 100 evaluation steps: 0.0135671105645041\n",
            "Validation loss per 100 evaluation steps: 0.013569947368842235\n",
            "Validation loss per 100 evaluation steps: 0.013554693966208063\n",
            "Validation loss per 100 evaluation steps: 0.01348649808639493\n",
            "Validation loss per 100 evaluation steps: 0.013598487830806492\n",
            "Validation loss per 100 evaluation steps: 0.013574934158776556\n",
            "Validation loss per 100 evaluation steps: 0.013560710954466686\n",
            "Validation loss per 100 evaluation steps: 0.01368189478731359\n",
            "Validation loss per 100 evaluation steps: 0.013766033936315461\n",
            "Validation loss per 100 evaluation steps: 0.013777508607521034\n",
            "Validation Loss: 0.013761904264297182\n",
            "Validation Accuracy: 0.9845576321236014\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.94      0.94      0.94      5904\n",
            "      B-MISC       0.80      0.90      0.84      4173\n",
            "       B-ORG       0.93      0.89      0.91      3512\n",
            "       B-PER       0.94      0.96      0.95      5593\n",
            "       I-LOC       0.94      0.91      0.92      7891\n",
            "      I-MISC       0.90      0.87      0.88      8442\n",
            "       I-ORG       0.93      0.91      0.92      6001\n",
            "       I-PER       0.95      0.98      0.97     11654\n",
            "           O       1.00      1.00      1.00    253311\n",
            "\n",
            "    accuracy                           0.98    306481\n",
            "   macro avg       0.92      0.93      0.93    306481\n",
            "weighted avg       0.98      0.98      0.98    306481\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9842  0.9842  0.9842\n",
            "macro        0.9249  0.9290  0.9265\n",
            "weighted     0.9845  0.9842  0.9843\n",
            "Validation steps: 2898\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.015161363021129547\n",
            "Test loss per 100 evaluation steps: 0.014754763472103605\n",
            "Test loss per 100 evaluation steps: 0.013695115539633965\n",
            "Test loss per 100 evaluation steps: 0.013901969902958626\n",
            "Test loss per 100 evaluation steps: 0.013918031209766013\n",
            "Test loss per 100 evaluation steps: 0.01364649678297269\n",
            "Test loss per 100 evaluation steps: 0.013931351096724874\n",
            "Test loss per 100 evaluation steps: 0.01327478768202667\n",
            "Test loss per 100 evaluation steps: 0.012919068619333857\n",
            "Test loss per 100 evaluation steps: 0.012979977994940555\n",
            "Test loss per 100 evaluation steps: 0.013021226229919152\n",
            "Test loss per 100 evaluation steps: 0.013294038847628637\n",
            "Test loss per 100 evaluation steps: 0.013030320835961307\n",
            "Test loss per 100 evaluation steps: 0.012830175020102712\n",
            "Test loss per 100 evaluation steps: 0.012783893536104946\n",
            "Test loss per 100 evaluation steps: 0.012729683885906553\n",
            "Test loss per 100 evaluation steps: 0.012937225263295419\n",
            "Test loss per 100 evaluation steps: 0.012894554427091282\n",
            "Test loss per 100 evaluation steps: 0.013086570678789676\n",
            "Test loss per 100 evaluation steps: 0.013181476101009025\n",
            "Test loss per 100 evaluation steps: 0.01316728154916908\n",
            "Test loss per 100 evaluation steps: 0.013289260752151841\n",
            "Test loss per 100 evaluation steps: 0.013437839552797334\n",
            "Test loss per 100 evaluation steps: 0.013368314812008559\n",
            "Test loss per 100 evaluation steps: 0.013344994477225555\n",
            "Test loss per 100 evaluation steps: 0.013306593507884023\n",
            "Test loss per 100 evaluation steps: 0.013254701156769846\n",
            "Test loss per 100 evaluation steps: 0.013356987798776214\n",
            "Test loss per 100 evaluation steps: 0.013393919213874518\n",
            "Test Loss: 0.013393919213874518\n",
            "Test Accuracy: 0.9847103028121631\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.94      0.94      0.94      5955\n",
            "      B-MISC       0.80      0.91      0.85      4505\n",
            "       B-ORG       0.94      0.91      0.93      3449\n",
            "       B-PER       0.95      0.96      0.96      5207\n",
            "       I-LOC       0.93      0.90      0.91      7240\n",
            "      I-MISC       0.89      0.86      0.87      9082\n",
            "       I-ORG       0.94      0.94      0.94      5934\n",
            "       I-PER       0.96      0.98      0.97     11879\n",
            "           O       1.00      1.00      1.00    253790\n",
            "\n",
            "    accuracy                           0.98    307041\n",
            "   macro avg       0.93      0.93      0.93    307041\n",
            "weighted avg       0.98      0.98      0.98    307041\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9844  0.9844  0.9844\n",
            "macro        0.9263  0.9324  0.9289\n",
            "weighted     0.9846  0.9844  0.9844\n",
            "Test steps: 2900\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "def train(config,loss_name):\n",
        "    print(\"=\" * 100)\n",
        "    print(f\"loss_name: {loss_name}\")\n",
        "    model = Ner_Model(config, len(label2id), loss_name).to(config.device)\n",
        "    optimizer = get_optimizer(model, config)\n",
        "\n",
        "    valid_each_label_p_r_f1_list = []\n",
        "    valid_p_r_f1_list = []\n",
        "    test_each_label_p_r_f1_list = []\n",
        "    test_p_r_f1_list = []\n",
        "\n",
        "    valid_loss_list = []\n",
        "    test_loss_list = []\n",
        "\n",
        "    model.train()\n",
        "    interval = 100\n",
        "    for epoch in range(config.epochs):\n",
        "        print(f\"Training epoch: {epoch + 1}\")\n",
        "        tr_preds,tr_labels = [], []\n",
        "        total_loss = 0.0\n",
        "        tr_accuracy = 0.0\n",
        "        # print(\"\\n\" + \"-\" * 30 + \"\\n\")\n",
        "        # print(f\"epoch: {epoch},  train dataloader size: {len(train_dataloader)}\")\n",
        "        # print(f\"epoch: {epoch},  valid dataloader size: {len(valid_dataloader)}\")\n",
        "        # print(f\"epoch: {epoch},  test dataloader size: {len(test_dataloader)}\")\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            attention_mask = batch[\"id\"].ne(0)\n",
        "            targets = batch['label_id']\n",
        "            loss, logit= model(batch[\"id\"], batch['seq_length'], attention_mask=attention_mask,\n",
        "                                             labels=targets)\n",
        "\n",
        "            # compute training accuracy\n",
        "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = logit.view(-1, len(label2id)) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            active_accuracy = flattened_targets.ne(-100) # active accuracy is also of shape (batch_size * seq_len,)\n",
        "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "            tr_accuracy += tmp_tr_accuracy\n",
        "            tr_preds.extend(predictions)\n",
        "            tr_labels.extend(targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            if (step + 1) % interval == 0:\n",
        "                print(f\"Training loss per 100 training steps: {total_loss / (step+1)}\")\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        print(f\"Training loss epoch: {total_loss / (step+1)}\")\n",
        "        print(f\"Training accuracy epoch: {tr_accuracy / (step+1)}\")\n",
        "        print(f\"Training steps: {step+1}\")\n",
        "        print(\"\\n\\n\")\n",
        "        model.eval()\n",
        "\n",
        "\n",
        "        valid_loss, valid_p_r_f1,  valid_each_label_p_r_f1 = evaluate(model,valid_dataloader, \"Validation\")\n",
        "        valid_loss_list.append(valid_loss)\n",
        "        valid_p_r_f1_list.append(valid_p_r_f1)\n",
        "        valid_each_label_p_r_f1_list.append(valid_each_label_p_r_f1)\n",
        "\n",
        "        print(\"\\n\\n\")\n",
        "        test_loss, test_p_r_f1,test_each_label_p_r_f1  = evaluate(model,test_dataloader, \"Test\")\n",
        "        test_loss_list.append(test_loss)\n",
        "        test_p_r_f1_list.append(test_p_r_f1)\n",
        "        test_each_label_p_r_f1_list.append(test_each_label_p_r_f1)\n",
        "\n",
        "\n",
        "        #print(f\"epoch: {epoch}, train_loss: {train_loss}, \\n{train_p_r_f1}\")\n",
        "        #print(f\"epoch: {epoch}, valid_loss: {valid_loss}, \\n{valid_p_r_f1}\")\n",
        "        #print(f\"epoch: {epoch}, test_loss: {test_loss},  \\n {test_p_r_f1}\")\n",
        "        model.train()\n",
        "    return   {\n",
        "              \"valid_loss_list\":valid_loss_list,\n",
        "              \"test_loss_list\":test_loss_list,\n",
        "\n",
        "              \"valid_p_r_f1_list\":valid_p_r_f1_list,\n",
        "              \"valid_each_label_p_r_f1_list\":valid_each_label_p_r_f1_list,\n",
        "\n",
        "              \"test_p_r_f1_list\":test_p_r_f1_list,\n",
        "              \"test_each_label_p_r_f1_list\": test_each_label_p_r_f1_list}\n",
        "\n",
        "\n",
        "result = {}\n",
        "for loss_name in ['l1', 'l2', 'ce', 'kl', 'dlite']:\n",
        "    r = train(Config, loss_name)\n",
        "    result[loss_name] = r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS1O-MxfGr33",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "source": [
        "## Result Comparison after cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciR0WOCJGr33",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(\"result.pkl\", \"wb\") as f:\n",
        "    pickle.dump(result, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYodcFxGGr33"
      },
      "source": [
        "#### Overall Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgKkKD11Gr33",
        "outputId": "95ecc7e0-ff53-4af0-eb88-831a5aab84c9",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "micro\n",
            "    loss  precision  recall      f1\n",
            "0     l1     0.9819  0.9819  0.9819\n",
            "1     l2     0.9838  0.9838  0.9838\n",
            "2     ce     0.9860  0.9860  0.9860\n",
            "3     kl     0.9852  0.9852  0.9852\n",
            "4  dlite     0.9844  0.9844  0.9844\n",
            "====================================================================================================\n",
            "macro\n",
            "    loss  precision  recall      f1\n",
            "0     l1     0.9130  0.9234  0.9178\n",
            "1     l2     0.9175  0.9357  0.9260\n",
            "2     ce     0.9363  0.9368  0.9363\n",
            "3     kl     0.9321  0.9344  0.9327\n",
            "4  dlite     0.9263  0.9324  0.9289\n",
            "====================================================================================================\n",
            "weighted\n",
            "    loss  precision  recall      f1\n",
            "0     l1     0.9822  0.9819  0.9820\n",
            "1     l2     0.9842  0.9838  0.9838\n",
            "2     ce     0.9860  0.9860  0.9859\n",
            "3     kl     0.9855  0.9852  0.9853\n",
            "4  dlite     0.9846  0.9844  0.9844\n"
          ]
        }
      ],
      "source": [
        "columns = ['loss', 'precision', 'recall', 'f1']\n",
        "for t in ['micro', 'macro', 'weighted']:\n",
        "    df = []\n",
        "    for loss_name in loss_list:\n",
        "        row = {'loss': loss_name}\n",
        "        row['precision'] = result[loss_name]['test_p_r_f1_list'][-1].loc[t, 'precision']\n",
        "        row['recall'] = result[loss_name]['test_p_r_f1_list'][-1].loc[t, 'recall']\n",
        "        row['f1'] = result[loss_name]['test_p_r_f1_list'][-1].loc[t, 'f1']\n",
        "        df.append(row)\n",
        "    print(\"=\"*100)\n",
        "    print(t)\n",
        "    print(pd.DataFrame(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKeZYzqvGr33"
      },
      "source": [
        "#### Each label Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejx5I88-Gr34",
        "outputId": "df992e8e-7260-4826-b8d5-9baf9e12797c",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test dataset\n",
            "--------------------------------------------------\n",
            "l1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.90      0.94      0.92      5955\n",
            "      B-MISC       0.81      0.87      0.84      4505\n",
            "       B-ORG       0.90      0.92      0.91      3449\n",
            "       B-PER       0.95      0.95      0.95      5207\n",
            "       I-LOC       0.87      0.90      0.89      7240\n",
            "      I-MISC       0.91      0.83      0.87      9082\n",
            "       I-ORG       0.91      0.94      0.92      5934\n",
            "       I-PER       0.96      0.97      0.97     11879\n",
            "           O       1.00      1.00      1.00    253790\n",
            "\n",
            "    accuracy                           0.98    307041\n",
            "   macro avg       0.91      0.92      0.92    307041\n",
            "weighted avg       0.98      0.98      0.98    307041\n",
            "\n",
            "--------------------------------------------------\n",
            "l2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.91      0.95      0.93      5955\n",
            "      B-MISC       0.82      0.87      0.85      4505\n",
            "       B-ORG       0.89      0.95      0.92      3449\n",
            "       B-PER       0.96      0.96      0.96      5207\n",
            "       I-LOC       0.88      0.94      0.91      7240\n",
            "      I-MISC       0.92      0.83      0.87      9082\n",
            "       I-ORG       0.90      0.96      0.93      5934\n",
            "       I-PER       0.98      0.97      0.97     11879\n",
            "           O       1.00      1.00      1.00    253790\n",
            "\n",
            "    accuracy                           0.98    307041\n",
            "   macro avg       0.92      0.94      0.93    307041\n",
            "weighted avg       0.98      0.98      0.98    307041\n",
            "\n",
            "--------------------------------------------------\n",
            "ce\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.94      0.94      0.94      5955\n",
            "      B-MISC       0.85      0.89      0.87      4505\n",
            "       B-ORG       0.94      0.94      0.94      3449\n",
            "       B-PER       0.95      0.97      0.96      5207\n",
            "       I-LOC       0.94      0.90      0.92      7240\n",
            "      I-MISC       0.92      0.86      0.89      9082\n",
            "       I-ORG       0.95      0.95      0.95      5934\n",
            "       I-PER       0.96      0.98      0.97     11879\n",
            "           O       1.00      1.00      1.00    253790\n",
            "\n",
            "    accuracy                           0.99    307041\n",
            "   macro avg       0.94      0.94      0.94    307041\n",
            "weighted avg       0.99      0.99      0.99    307041\n",
            "\n",
            "--------------------------------------------------\n",
            "kl\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.93      0.94      0.94      5955\n",
            "      B-MISC       0.81      0.92      0.86      4505\n",
            "       B-ORG       0.95      0.92      0.94      3449\n",
            "       B-PER       0.97      0.96      0.96      5207\n",
            "       I-LOC       0.93      0.89      0.91      7240\n",
            "      I-MISC       0.87      0.89      0.88      9082\n",
            "       I-ORG       0.96      0.92      0.94      5934\n",
            "       I-PER       0.98      0.98      0.98     11879\n",
            "           O       1.00      1.00      1.00    253790\n",
            "\n",
            "    accuracy                           0.99    307041\n",
            "   macro avg       0.93      0.93      0.93    307041\n",
            "weighted avg       0.99      0.99      0.99    307041\n",
            "\n",
            "--------------------------------------------------\n",
            "dlite\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.94      0.94      0.94      5955\n",
            "      B-MISC       0.80      0.91      0.85      4505\n",
            "       B-ORG       0.94      0.91      0.93      3449\n",
            "       B-PER       0.95      0.96      0.96      5207\n",
            "       I-LOC       0.93      0.90      0.91      7240\n",
            "      I-MISC       0.89      0.86      0.87      9082\n",
            "       I-ORG       0.94      0.94      0.94      5934\n",
            "       I-PER       0.96      0.98      0.97     11879\n",
            "           O       1.00      1.00      1.00    253790\n",
            "\n",
            "    accuracy                           0.98    307041\n",
            "   macro avg       0.93      0.93      0.93    307041\n",
            "weighted avg       0.98      0.98      0.98    307041\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"test dataset\")\n",
        "for loss_name in loss_list:\n",
        "    print(\"-\"*50)\n",
        "    print(loss_name)\n",
        "    print(result[loss_name]['test_each_label_p_r_f1_list'][-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqfxwsPKGr38"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OsiRbqeDRTI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}