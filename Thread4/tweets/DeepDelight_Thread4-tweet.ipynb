{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73408189-29c0-461a-8e72-4a0bff2295f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "import random ,json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed64ec7-6986-45f7-b3eb-a686c51a47e8",
   "metadata": {},
   "source": [
    "# Setting Basic Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7652ee7c-923c-4960-ac32-382f6d8f4360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    batch_size = 4\n",
    "    epochs = 1\n",
    "    lr = 1e-5\n",
    "    seed = 123\n",
    "    lstm_layer_num = 0                             # adding custom layer; make this from 10 to 0\n",
    "    bi_lstm=False\n",
    "\n",
    "    # Internet resource; download from Internet\n",
    "    # model_name = \"microsoft/deberta-v3-base\"\n",
    "    model_name = \"microsoft/deberta-base\"\n",
    "\n",
    "    hidden_size=768\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_data_name = \"conll2003\"                  # Setting the databset\n",
    "\n",
    "    @classmethod\n",
    "    def describe(cls):\n",
    "        parm = {\"train_data_name\": cls.train_data_name,\n",
    "                \"encoder_name\": cls.model_name,\n",
    "                \"batch_size\": cls.batch_size,\n",
    "                \"epochs\": cls.epochs,\n",
    "                \"lr\": cls.lr,\n",
    "                \"seed\": cls.seed,\n",
    "                \"bi_lstm\": cls.bi_lstm,\n",
    "                \"lstm_layer_num\": cls.lstm_layer_num}\n",
    "        return json.dumps(parm , ensure_ascii=False, indent=2)\n",
    "\n",
    "random.seed(Config.seed)\n",
    "np.random.seed(Config.seed)\n",
    "torch.manual_seed(Config.seed)\n",
    "torch.cuda.manual_seed_all(Config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb33048d-758f-4cad-9c05-bae5a7c62125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given configuration result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e753da72-6767-44e3-be03-eb7b222d5965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"train_data_name\": \"conll2003\",\n",
      "  \"encoder_name\": \"microsoft/deberta-base\",\n",
      "  \"batch_size\": 4,\n",
      "  \"epochs\": 1,\n",
      "  \"lr\": 1e-05,\n",
      "  \"seed\": 123,\n",
      "  \"bi_lstm\": false,\n",
      "  \"lstm_layer_num\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(Config.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de48a36a-349e-4fe9-b714-efa3ec3b9a16",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29be764a-2be7-4dfb-8df2-e49493969cfc",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f020c6e-b74a-4495-81e9-3baf2335e627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3967, 2) (1301, 2) (1303, 2)\n"
     ]
    }
   ],
   "source": [
    "def read_conll2003(file_path):\n",
    "    data = []\n",
    "    sample = []\n",
    "    for idx, line in enumerate(open(file_path)):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        line = line.strip()\n",
    "        if line == \"\":\n",
    "            if len(sample) != 0:\n",
    "                data.append(sample)\n",
    "            sample = []\n",
    "        else:\n",
    "            line = line.split()\n",
    "            assert len(line) == 2\n",
    "            sample.append([line[0], line[-1]])\n",
    "    if len(sample) != 0:\n",
    "        data.append(sample)\n",
    "    data = [{\"word\": [i[0] for i in sample], \"tag\": [i[1] for i in sample]} for sample in data]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "if Config.train_data_name == \"conll2003\":\n",
    "    train_path = os.path.join(Config.train_data_name, 'tweets.train.txt')\n",
    "    dev_path = os.path.join(Config.train_data_name, 'tweets.valid.txt')\n",
    "    test_path = os.path.join(Config.train_data_name, 'tweets.test.txt')\n",
    "    train_df = read_conll2003(train_path)\n",
    "    valid_df = read_conll2003(dev_path)\n",
    "    test_df = read_conll2003(test_path)\n",
    "    print(train_df.shape, valid_df.shape, test_df.shape)\n",
    "elif Config.train_data_name == \"ner_datasetreference\":\n",
    "    df = pd.read_csv(\"ner_datasetreference.csv\", encoding='iso-8859-1')\n",
    "    data = []\n",
    "    word, tag = [], []\n",
    "    for i,j,k in zip(df['Sentence #'], df['Word'], df['Tag']):\n",
    "        if not pd.isnull(i):\n",
    "            assert i.startswith('Sentence')\n",
    "            if len(word) > 0:\n",
    "                data.append({\"word\":word, \"tag\":tag})\n",
    "            word, tag = [], []\n",
    "        if isinstance(j, str) and isinstance(k, str):\n",
    "            word.append(j)\n",
    "            tag.append(k)\n",
    "    if len(word) > 0:\n",
    "        data.append({\"word\":word, \"tag\":tag})\n",
    "        word, tag = [], []\n",
    "    print(data[0], data[-1])\n",
    "    df = pd.DataFrame(data)\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
    "    valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "    print(df.shape, train_df.shape, valid_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7e09b85-0612-4b83-9c1a-617e2f068447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Just, received, word, that, UHVictoria, and, ...</td>\n",
       "      <td>[O, O, O, O, B-POINT, O, B-POINT, I-POINT, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-DOCSTART-, Dudes, from, Austin, drove, down,...</td>\n",
       "      <td>[O, O, O, B-AREA, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-DOCSTART-, RT, @fbcoem, :, SHELTER, AT, SACR...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-DOCSTART-, RT, @fbcoem, :, SHELTER, AT, SACR...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-DOCSTART-, RT, @Stafford_PD, :, SACRED, HEAR...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3962</th>\n",
       "      <td>[-DOCSTART-, RT, @KPRC2, :, WATCH, LIVE, :, ht...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3963</th>\n",
       "      <td>[-DOCSTART-, @ukuleledan, 59, feet, is, the, f...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3964</th>\n",
       "      <td>[-DOCSTART-, This, is, some, interesting, pers...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3965</th>\n",
       "      <td>[-DOCSTART-, Officials, are, urging, everyone,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3966</th>\n",
       "      <td>[-DOCSTART-, The, pool, of, Addicks, Reservoir...</td>\n",
       "      <td>[O, O, O, O, B-AREA, I-AREA, O, O, B-POINT, I-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3967 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   word  \\\n",
       "0     [Just, received, word, that, UHVictoria, and, ...   \n",
       "1     [-DOCSTART-, Dudes, from, Austin, drove, down,...   \n",
       "2     [-DOCSTART-, RT, @fbcoem, :, SHELTER, AT, SACR...   \n",
       "3     [-DOCSTART-, RT, @fbcoem, :, SHELTER, AT, SACR...   \n",
       "4     [-DOCSTART-, RT, @Stafford_PD, :, SACRED, HEAR...   \n",
       "...                                                 ...   \n",
       "3962  [-DOCSTART-, RT, @KPRC2, :, WATCH, LIVE, :, ht...   \n",
       "3963  [-DOCSTART-, @ukuleledan, 59, feet, is, the, f...   \n",
       "3964  [-DOCSTART-, This, is, some, interesting, pers...   \n",
       "3965  [-DOCSTART-, Officials, are, urging, everyone,...   \n",
       "3966  [-DOCSTART-, The, pool, of, Addicks, Reservoir...   \n",
       "\n",
       "                                                    tag  \n",
       "0     [O, O, O, O, B-POINT, O, B-POINT, I-POINT, O, ...  \n",
       "1       [O, O, O, B-AREA, O, O, O, O, O, O, O, O, O, O]  \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "4     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "...                                                 ...  \n",
       "3962                     [O, O, O, O, O, O, O, O, O, O]  \n",
       "3963  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3964         [O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "3965  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3966  [O, O, O, O, B-AREA, I-AREA, O, O, B-POINT, I-...  \n",
       "\n",
       "[3967 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9072a5ed-cac1-4a3c-95c9-1b981148d088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[BREAKING, :, One, firefighter, injured, after...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, B-POIN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-DOCSTART-, @Robert1288, Main, St, near, Med,...</td>\n",
       "      <td>[O, O, B-POINT, I-POINT, I-POINT, I-POINT, I-P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-DOCSTART-, Important, Message, from, Spring,...</td>\n",
       "      <td>[O, O, O, O, B-AREA, I-AREA, O, O, O, O, B-ARE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-DOCSTART-, RT, @kiii3news, :, Harvey, will, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-DOCSTART-, RT, @KHOU, :, JUST, IN, :, Fort, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-AREA, I-AREA, I-AREA, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>[-DOCSTART-, Shutting, down, my, Twitter, for,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>[-DOCSTART-, Si, necesita, rescate, en, el, co...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>[-DOCSTART-, @XBrittanyDukeX, @taavi_rautavirt...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>[-DOCSTART-, RT, @KPRC2, :, WATCH, LIVE, :, ht...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>[-DOCSTART-, Another, sad, story, from, this, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1303 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   word  \\\n",
       "0     [BREAKING, :, One, firefighter, injured, after...   \n",
       "1     [-DOCSTART-, @Robert1288, Main, St, near, Med,...   \n",
       "2     [-DOCSTART-, Important, Message, from, Spring,...   \n",
       "3     [-DOCSTART-, RT, @kiii3news, :, Harvey, will, ...   \n",
       "4     [-DOCSTART-, RT, @KHOU, :, JUST, IN, :, Fort, ...   \n",
       "...                                                 ...   \n",
       "1298  [-DOCSTART-, Shutting, down, my, Twitter, for,...   \n",
       "1299  [-DOCSTART-, Si, necesita, rescate, en, el, co...   \n",
       "1300  [-DOCSTART-, @XBrittanyDukeX, @taavi_rautavirt...   \n",
       "1301  [-DOCSTART-, RT, @KPRC2, :, WATCH, LIVE, :, ht...   \n",
       "1302  [-DOCSTART-, Another, sad, story, from, this, ...   \n",
       "\n",
       "                                                    tag  \n",
       "0     [O, O, O, O, O, O, O, O, O, O, O, O, O, B-POIN...  \n",
       "1     [O, O, B-POINT, I-POINT, I-POINT, I-POINT, I-P...  \n",
       "2     [O, O, O, O, B-AREA, I-AREA, O, O, O, O, B-ARE...  \n",
       "3     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "4     [O, O, O, O, O, O, O, B-AREA, I-AREA, I-AREA, ...  \n",
       "...                                                 ...  \n",
       "1298  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1299  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1300  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1301                     [O, O, O, O, O, O, O, O, O, O]  \n",
       "1302  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "\n",
       "[1303 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e326acb3-3e2e-4f31-8d54-f83d0421f9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Food, Town, at, the, corner, of, Richey, St, ...</td>\n",
       "      <td>[O, O, O, O, B-POINT, I-POINT, I-POINT, I-POIN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-DOCSTART-, Conroe, calls, for, evacuations, ...</td>\n",
       "      <td>[O, B-AREA, O, O, O, O, O, O, O, O, O, B-AREA,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-DOCSTART-, I-10, and, Westside, ., Houston, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-DOCSTART-, Neighborhoods, along, the, San, J...</td>\n",
       "      <td>[O, O, O, O, B-RIVER, I-RIVER, I-RIVER, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-DOCSTART-, Water, Rescue, -, E065, -, Kings,...</td>\n",
       "      <td>[O, O, O, O, O, O, B-POINT, I-POINT, I-POINT, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>[-DOCSTART-, RT, @radioguycliff, :, Can, confi...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297</th>\n",
       "      <td>[-DOCSTART-, Water, Rescue, -, E034, -, Hirsch...</td>\n",
       "      <td>[O, O, O, O, O, O, B-POINT, I-POINT, I-POINT, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>[-DOCSTART-, @XBrittanyDukeX, @taavi_rautavirt...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>[-DOCSTART-, .@KPRCJonathan, just, reported, l...</td>\n",
       "      <td>[O, O, O, O, O, O, B-POINT, I-POINT, I-POINT, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>[-DOCSTART-, @XBrittanyDukeX, @taavi_rautavirt...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1301 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   word  \\\n",
       "0     [Food, Town, at, the, corner, of, Richey, St, ...   \n",
       "1     [-DOCSTART-, Conroe, calls, for, evacuations, ...   \n",
       "2     [-DOCSTART-, I-10, and, Westside, ., Houston, ...   \n",
       "3     [-DOCSTART-, Neighborhoods, along, the, San, J...   \n",
       "4     [-DOCSTART-, Water, Rescue, -, E065, -, Kings,...   \n",
       "...                                                 ...   \n",
       "1296  [-DOCSTART-, RT, @radioguycliff, :, Can, confi...   \n",
       "1297  [-DOCSTART-, Water, Rescue, -, E034, -, Hirsch...   \n",
       "1298  [-DOCSTART-, @XBrittanyDukeX, @taavi_rautavirt...   \n",
       "1299  [-DOCSTART-, .@KPRCJonathan, just, reported, l...   \n",
       "1300  [-DOCSTART-, @XBrittanyDukeX, @taavi_rautavirt...   \n",
       "\n",
       "                                                    tag  \n",
       "0     [O, O, O, O, B-POINT, I-POINT, I-POINT, I-POIN...  \n",
       "1     [O, B-AREA, O, O, O, O, O, O, O, O, O, B-AREA,...  \n",
       "2                           [O, O, O, O, O, O, O, O, O]  \n",
       "3     [O, O, O, O, B-RIVER, I-RIVER, I-RIVER, O, O, ...  \n",
       "4     [O, O, O, O, O, O, B-POINT, I-POINT, I-POINT, ...  \n",
       "...                                                 ...  \n",
       "1296  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1297  [O, O, O, O, O, O, B-POINT, I-POINT, I-POINT, ...  \n",
       "1298  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1299  [O, O, O, O, O, O, B-POINT, I-POINT, I-POINT, ...  \n",
       "1300  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "\n",
       "[1301 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f880974-d26c-43e8-993b-754a08047854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner category ['AREA', 'POINT', 'RIVER', 'ROAD'] .\n",
      "\n",
      "label list ['O', 'B-AREA', 'I-AREA', 'B-POINT', 'I-POINT', 'B-RIVER', 'I-RIVER', 'B-ROAD', 'I-ROAD'] .\n",
      "\n",
      "label2id {'O': 0, 'B-AREA': 1, 'I-AREA': 2, 'B-POINT': 3, 'I-POINT': 4, 'B-RIVER': 5, 'I-RIVER': 6, 'B-ROAD': 7, 'I-ROAD': 8} .\n",
      "\n",
      "id2label {0: 'O', 1: 'B-AREA', 2: 'I-AREA', 3: 'B-POINT', 4: 'I-POINT', 5: 'B-RIVER', 6: 'I-RIVER', 7: 'B-ROAD', 8: 'I-ROAD'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def collect_label(df_list):\n",
    "    ret = set()\n",
    "    for df in df_list:\n",
    "        for labels in df['tag']:\n",
    "            for l in labels:\n",
    "                if l == \"O\":\n",
    "                    continue\n",
    "                assert l.startswith(\"B-\") or l.startswith(\"I-\")\n",
    "                ret.add(l[2:])\n",
    "    return sorted(list(ret))\n",
    "\n",
    "ner_category = collect_label([train_df, valid_df, test_df])\n",
    "label_list = []\n",
    "for l in ner_category:\n",
    "    label_list.append(\"B-\" + l)\n",
    "    label_list.append(\"I-\" + l)\n",
    "label_list = ['O'] + label_list\n",
    "label2id = dict([(v, idx) for idx, v in enumerate(label_list)])\n",
    "id2label = dict([(idx, v) for idx, v in enumerate(label_list)])\n",
    "print(f\"ner category {ner_category} .\\n\\nlabel list {label_list} .\\n\\nlabel2id {label2id} .\\n\\nid2label {id2label}\\n\\n\")\n",
    "label_list = label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e89a549-15fc-4812-ba0f-10cf4d0f2e27",
   "metadata": {},
   "source": [
    "# Import Reberta Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "494b1c7b-4a1e-481f-b31e-e83cd236f504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(Config.model_name, add_prefix_space=True)\n",
    "print(tokenizer.is_fast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bfb0a4-99ba-4377-ab42-7f0841d4ed95",
   "metadata": {},
   "source": [
    "## tokenize and build Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14a6a6a3-af0c-4a9e-8d50-9463987b0d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align(tag, word_ids):\n",
    "    aligned_tag = []\n",
    "    i = 0\n",
    "    while i < len(word_ids):\n",
    "        if word_ids[i] is None:\n",
    "            aligned_tag.append(None)\n",
    "            i += 1\n",
    "        elif tag[word_ids[i]] == \"O\":\n",
    "            aligned_tag.append(tag[word_ids[i]])\n",
    "            i += 1\n",
    "        elif tag[word_ids[i]].startswith(\"B-\"):\n",
    "            n = 0\n",
    "            while (i+n) < len(word_ids) and word_ids[i]  == word_ids[i+n]:\n",
    "                n += 1\n",
    "            aligned_tag.append(tag[word_ids[i]])\n",
    "            if n > 1:\n",
    "                aligned_tag.extend([\"I-\" + tag[word_ids[i]][2:] ] * (n-1))\n",
    "            i = i + n\n",
    "        else:\n",
    "            aligned_tag.append(tag[word_ids[i]])\n",
    "            i += 1\n",
    "    return aligned_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1449b2bc-8d4d-4eb1-bc9c-33295c4dd61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', '1996-08-22', '1996-08-22', 'I'] ['O', 'B-LOC', 'B-ORG', 'O']\n",
      "   tokens   tags  word-index\n",
      "0   [CLS]   None         NaN\n",
      "1      Ä I      O         0.0\n",
      "2   Ä 1996  B-LOC         1.0\n",
      "3       -  I-LOC         1.0\n",
      "4      08  I-LOC         1.0\n",
      "5       -  I-LOC         1.0\n",
      "6      22  I-LOC         1.0\n",
      "7   Ä 1996  B-ORG         2.0\n",
      "8       -  I-ORG         2.0\n",
      "9      08  I-ORG         2.0\n",
      "10      -  I-ORG         2.0\n",
      "11     22  I-ORG         2.0\n",
      "12     Ä I      O         3.0\n",
      "13  [SEP]   None         NaN\n"
     ]
    }
   ],
   "source": [
    "#words = train_df.iloc[2][\"word\"]\n",
    "#tag = train_df.iloc[2][\"label\"]\n",
    "words = ['I', '1996-08-22', '1996-08-22', 'I']\n",
    "tag = [\"O\", \"B-LOC\", \"B-ORG\", \"O\"]\n",
    "print(words, tag)\n",
    "s = tokenizer(words, truncation=True, is_split_into_words=True)\n",
    "word_ids = s.word_ids()\n",
    "# align tokens and words\n",
    "tokens = tokenizer.convert_ids_to_tokens(s['input_ids'])\n",
    "tags = align(tag, s.word_ids())\n",
    "print(pd.DataFrame(list(zip(tokens, tags, word_ids)), columns=['tokens', 'tags', 'word-index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "365fff44-d75a-4ada-b182-e99334a5e4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    word = x['word']\n",
    "    r = tokenizer(word, truncation=True, is_split_into_words=True)\n",
    "    word_ids = r.word_ids()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(r['input_ids'])\n",
    "    align_label = align(x['tag'], word_ids)\n",
    "    return tokens, align_label, r['input_ids'], [label2id[i] if i is not None else -100  for i in align_label], word_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbdd123a-9cc4-4256-9490-3d55f36be153",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['token', 'label', 'id', 'label_id', 'word_ids']] = train_df.apply(lambda x: pd.Series(preprocess(x)), axis=1)\n",
    "valid_df[['token', 'label', 'id', 'label_id', 'word_ids']] = valid_df.apply(lambda x: pd.Series(preprocess(x)), axis=1)\n",
    "test_df[['token', 'label', 'id', 'label_id', 'word_ids']] = test_df.apply(lambda x: pd.Series(preprocess(x)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01b860a7-f189-484b-bafd-09441ea30c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>label_id</th>\n",
       "      <th>word_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[BREAKING, :, One, firefighter, injured, after...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, B-POIN...</td>\n",
       "      <td>[[CLS], Ä BRE, AKING, Ä :, Ä One, Ä firefighter, Ä ...</td>\n",
       "      <td>[None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[1, 13530, 16371, 4832, 509, 15788, 1710, 71, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-DOCSTART-, @Robert1288, Main, St, near, Med,...</td>\n",
       "      <td>[O, O, B-POINT, I-POINT, I-POINT, I-POINT, I-P...</td>\n",
       "      <td>[[CLS], Ä -, DOC, ST, ART, -, Ä @, Robert, 12, 8...</td>\n",
       "      <td>[None, O, O, O, O, O, O, O, O, O, B-POINT, I-P...</td>\n",
       "      <td>[1, 111, 46570, 4014, 11328, 12, 787, 25244, 1...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, ...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 3, 4, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-DOCSTART-, Important, Message, from, Spring,...</td>\n",
       "      <td>[O, O, O, O, B-AREA, I-AREA, O, O, O, O, B-ARE...</td>\n",
       "      <td>[[CLS], Ä -, DOC, ST, ART, -, Ä Important, Ä Mess...</td>\n",
       "      <td>[None, O, O, O, O, O, O, O, O, B-AREA, I-AREA,...</td>\n",
       "      <td>[1, 111, 46570, 4014, 11328, 12, 28997, 32236,...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 5, 6, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-DOCSTART-, RT, @kiii3news, :, Harvey, will, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[[CLS], Ä -, DOC, ST, ART, -, Ä RT, Ä @, ki, ii, ...</td>\n",
       "      <td>[None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[1, 111, 46570, 4014, 11328, 12, 10541, 787, 3...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 3, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-DOCSTART-, RT, @KHOU, :, JUST, IN, :, Fort, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-AREA, I-AREA, I-AREA, ...</td>\n",
       "      <td>[[CLS], Ä -, DOC, ST, ART, -, Ä RT, Ä @, K, HOU, ...</td>\n",
       "      <td>[None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[1, 111, 46570, 4014, 11328, 12, 10541, 787, 5...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 0, 1, 2, 2, 2, 3, 4, 5, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>[-DOCSTART-, Shutting, down, my, Twitter, for,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[[CLS], Ä -, DOC, ST, ART, -, Ä Shut, ting, Ä dow...</td>\n",
       "      <td>[None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[1, 111, 46570, 4014, 11328, 12, 36707, 2577, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 0, 1, 1, 2, 3, 4, 5, 6, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>[-DOCSTART-, Si, necesita, rescate, en, el, co...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[[CLS], Ä -, DOC, ST, ART, -, Ä Si, Ä ne, ces, it...</td>\n",
       "      <td>[None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[1, 111, 46570, 4014, 11328, 12, 11065, 3087, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 0, 1, 2, 2, 2, 3, 3, 4, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>[-DOCSTART-, @XBrittanyDukeX, @taavi_rautavirt...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[[CLS], Ä -, DOC, ST, ART, -, Ä @, X, Br, itt, a...</td>\n",
       "      <td>[None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[1, 111, 46570, 4014, 11328, 12, 787, 1000, 16...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>[-DOCSTART-, RT, @KPRC2, :, WATCH, LIVE, :, ht...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[[CLS], Ä -, DOC, ST, ART, -, Ä RT, Ä @, K, PR, C...</td>\n",
       "      <td>[None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[1, 111, 46570, 4014, 11328, 12, 10541, 787, 5...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 3, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>[-DOCSTART-, Another, sad, story, from, this, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[[CLS], Ä -, DOC, ST, ART, -, Ä Another, Ä sad, Ä ...</td>\n",
       "      <td>[None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[1, 111, 46570, 4014, 11328, 12, 2044, 5074, 5...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1303 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   word  \\\n",
       "0     [BREAKING, :, One, firefighter, injured, after...   \n",
       "1     [-DOCSTART-, @Robert1288, Main, St, near, Med,...   \n",
       "2     [-DOCSTART-, Important, Message, from, Spring,...   \n",
       "3     [-DOCSTART-, RT, @kiii3news, :, Harvey, will, ...   \n",
       "4     [-DOCSTART-, RT, @KHOU, :, JUST, IN, :, Fort, ...   \n",
       "...                                                 ...   \n",
       "1298  [-DOCSTART-, Shutting, down, my, Twitter, for,...   \n",
       "1299  [-DOCSTART-, Si, necesita, rescate, en, el, co...   \n",
       "1300  [-DOCSTART-, @XBrittanyDukeX, @taavi_rautavirt...   \n",
       "1301  [-DOCSTART-, RT, @KPRC2, :, WATCH, LIVE, :, ht...   \n",
       "1302  [-DOCSTART-, Another, sad, story, from, this, ...   \n",
       "\n",
       "                                                    tag  \\\n",
       "0     [O, O, O, O, O, O, O, O, O, O, O, O, O, B-POIN...   \n",
       "1     [O, O, B-POINT, I-POINT, I-POINT, I-POINT, I-P...   \n",
       "2     [O, O, O, O, B-AREA, I-AREA, O, O, O, O, B-ARE...   \n",
       "3     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "4     [O, O, O, O, O, O, O, B-AREA, I-AREA, I-AREA, ...   \n",
       "...                                                 ...   \n",
       "1298  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1299  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1300  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1301                     [O, O, O, O, O, O, O, O, O, O]   \n",
       "1302  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                                  token  \\\n",
       "0     [[CLS], Ä BRE, AKING, Ä :, Ä One, Ä firefighter, Ä ...   \n",
       "1     [[CLS], Ä -, DOC, ST, ART, -, Ä @, Robert, 12, 8...   \n",
       "2     [[CLS], Ä -, DOC, ST, ART, -, Ä Important, Ä Mess...   \n",
       "3     [[CLS], Ä -, DOC, ST, ART, -, Ä RT, Ä @, ki, ii, ...   \n",
       "4     [[CLS], Ä -, DOC, ST, ART, -, Ä RT, Ä @, K, HOU, ...   \n",
       "...                                                 ...   \n",
       "1298  [[CLS], Ä -, DOC, ST, ART, -, Ä Shut, ting, Ä dow...   \n",
       "1299  [[CLS], Ä -, DOC, ST, ART, -, Ä Si, Ä ne, ces, it...   \n",
       "1300  [[CLS], Ä -, DOC, ST, ART, -, Ä @, X, Br, itt, a...   \n",
       "1301  [[CLS], Ä -, DOC, ST, ART, -, Ä RT, Ä @, K, PR, C...   \n",
       "1302  [[CLS], Ä -, DOC, ST, ART, -, Ä Another, Ä sad, Ä ...   \n",
       "\n",
       "                                                  label  \\\n",
       "0     [None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1     [None, O, O, O, O, O, O, O, O, O, B-POINT, I-P...   \n",
       "2     [None, O, O, O, O, O, O, O, O, B-AREA, I-AREA,...   \n",
       "3     [None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "4     [None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "...                                                 ...   \n",
       "1298  [None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1299  [None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1300  [None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1301  [None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1302  [None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                                     id  \\\n",
       "0     [1, 13530, 16371, 4832, 509, 15788, 1710, 71, ...   \n",
       "1     [1, 111, 46570, 4014, 11328, 12, 787, 25244, 1...   \n",
       "2     [1, 111, 46570, 4014, 11328, 12, 28997, 32236,...   \n",
       "3     [1, 111, 46570, 4014, 11328, 12, 10541, 787, 3...   \n",
       "4     [1, 111, 46570, 4014, 11328, 12, 10541, 787, 5...   \n",
       "...                                                 ...   \n",
       "1298  [1, 111, 46570, 4014, 11328, 12, 36707, 2577, ...   \n",
       "1299  [1, 111, 46570, 4014, 11328, 12, 11065, 3087, ...   \n",
       "1300  [1, 111, 46570, 4014, 11328, 12, 787, 1000, 16...   \n",
       "1301  [1, 111, 46570, 4014, 11328, 12, 10541, 787, 5...   \n",
       "1302  [1, 111, 46570, 4014, 11328, 12, 2044, 5074, 5...   \n",
       "\n",
       "                                               label_id  \\\n",
       "0     [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, ...   \n",
       "2     [-100, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, ...   \n",
       "3     [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "1298  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1299  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1300  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1301  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1302  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                               word_ids  \n",
       "0     [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...  \n",
       "1     [None, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 3, 4, 5, ...  \n",
       "2     [None, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 5, 6, 7, ...  \n",
       "3     [None, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 3, 4, ...  \n",
       "4     [None, 0, 0, 0, 0, 0, 1, 2, 2, 2, 3, 4, 5, 6, ...  \n",
       "...                                                 ...  \n",
       "1298  [None, 0, 0, 0, 0, 0, 1, 1, 2, 3, 4, 5, 6, 7, ...  \n",
       "1299  [None, 0, 0, 0, 0, 0, 1, 2, 2, 2, 3, 3, 4, 5, ...  \n",
       "1300  [None, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1301  [None, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 3, 4, ...  \n",
       "1302  [None, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, ...  \n",
       "\n",
       "[1303 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94367bf2-3306-4a12-9e93-dd070437a2bc",
   "metadata": {},
   "source": [
    "# Building Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb9d6329-a6d3-4f6d-8be8-0f5a21f95023",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerDataset(Dataset):\n",
    "    def __init__(self, df, device):\n",
    "        self.data = df.to_dict(orient='records')\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item]\n",
    "\n",
    "    def collate_to_max_length(self, batch):\n",
    "        max_seq_length = max([len(s['id']) for s in batch])\n",
    "        batch = sorted(batch, key=lambda x: -len(x['id']))\n",
    "        seq_length = torch.tensor([len(x['id']) for x in batch])\n",
    "        input_ids = torch.tensor([x[\"id\"] + [0] * (max_seq_length - len(x['id'])) for x in batch]).to(self.device)\n",
    "        labels = torch.tensor([x[\"label_id\"] + [-100] * (max_seq_length - len(x['label_id'])) for x in batch]).to(self.device)\n",
    "        return {\"id\": input_ids, \"label_id\": labels, 'seq_length':seq_length, \"sample\":batch}\n",
    "\n",
    "\n",
    "dataset_train = NerDataset(train_df, Config.device)\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train,\n",
    "                              sampler=RandomSampler(dataset_train),\n",
    "                              batch_size=Config.batch_size,\n",
    "                              drop_last=False,\n",
    "                              collate_fn=dataset_train.collate_to_max_length)\n",
    "\n",
    "\n",
    "\n",
    "dataset_valid = NerDataset(valid_df, Config.device)\n",
    "\n",
    "valid_dataloader = DataLoader(dataset_valid,\n",
    "                              sampler=RandomSampler(dataset_valid),\n",
    "                              batch_size=Config.batch_size,\n",
    "                              drop_last=False,\n",
    "                              collate_fn=dataset_valid.collate_to_max_length)\n",
    "\n",
    "\n",
    "dataset_test = NerDataset(test_df, Config.device)\n",
    "\n",
    "test_dataloader = DataLoader(dataset_test,\n",
    "                              sampler=RandomSampler(dataset_test),\n",
    "                              batch_size=Config.batch_size,\n",
    "                              drop_last=False,\n",
    "                              collate_fn=dataset_test.collate_to_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283cca93-6d53-4434-84a4-94054cd08726",
   "metadata": {},
   "source": [
    "# Building Custom loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "374bc422-d85f-4046-9661-d68f253ac3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1_Loss:\n",
    "    def __init__(self):\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "    def loss(self, target, logit, label_num):\n",
    "\n",
    "        target = target.view(-1)\n",
    "        logit = logit.view(-1, label_num)\n",
    "\n",
    "        mask = target.ne(-100).to(logit.device)\n",
    "        logit = torch.masked_select(logit, mask.unsqueeze(-1).expand_as(logit)).reshape(-1, label_num)\n",
    "        target = torch.masked_select(target, mask)\n",
    "\n",
    "        target = F.one_hot(target, num_classes=label_num)\n",
    "        return self.l1_loss(logit, target.float())\n",
    "\n",
    "\n",
    "class L2_Loss:\n",
    "    def __init__(self):\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    def loss(self, target, logit,label_num):\n",
    "        target = target.view(-1)\n",
    "        logit = logit.view(-1, label_num)\n",
    "\n",
    "        mask = target.ne(-100).to(logit.device)\n",
    "        logit = torch.masked_select(logit, mask.unsqueeze(-1).expand_as(logit)).reshape(-1, label_num)\n",
    "        target = torch.masked_select(target, mask)\n",
    "\n",
    "        target = F.one_hot(target, num_classes=label_num)\n",
    "        loss = self.mse_loss(logit, target.float())\n",
    "        return loss\n",
    "\n",
    "class CE_Loss:\n",
    "    def __init__(self):\n",
    "        self.ce_loss = nn.CrossEntropyLoss(ignore_index=-100, reduce='mean')\n",
    "    def loss(self, target, logit, label_num):\n",
    "        return self.ce_loss(logit.reshape(-1, label_num), target.reshape(-1) )\n",
    "\n",
    "class KLDivergenceLoss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def loss(self, target, logit, label_num):\n",
    "        target = target.view(-1)\n",
    "        logit = logit.view(-1, label_num)\n",
    "\n",
    "        mask = target.ne(-100).to(logit.device)\n",
    "        logit = torch.masked_select(logit, mask.unsqueeze(-1).expand_as(logit)).reshape(-1, label_num)\n",
    "        target = torch.masked_select(target, mask)\n",
    "\n",
    "        probs = F.softmax(logit, dim=-1)\n",
    "\n",
    "        # One-hot encode the targets to get true probabilities\n",
    "        true_probs = F.one_hot(target, num_classes=label_num).float()\n",
    "\n",
    "        mask_true_probs = true_probs > 0\n",
    "\n",
    "        # Calculate g function for non-zero elements using the mask\n",
    "        kl_values = torch.zeros_like(probs)\n",
    "        kl_values[mask_true_probs] = true_probs[mask_true_probs] * torch.log(true_probs[mask_true_probs]/probs[mask_true_probs])\n",
    "\n",
    "        # Sum over all classes and average over the batch size\n",
    "        loss = kl_values.sum(dim=-1).mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "# DLITE Loss function\n",
    "class DLITELoss:\n",
    "    def __init__(self):\n",
    "        super(DLITELoss, self).__init__()\n",
    "\n",
    "    def loss(self, targets, logits, label_num, epsilon=1e-10):\n",
    "        targets = targets.view(-1)\n",
    "        logits = logits.view(-1, label_num)\n",
    "\n",
    "        mask = targets.ne(-100).to(logits.device)\n",
    "        logits = torch.masked_select(logits, mask.unsqueeze(-1).expand_as(logits)).reshape(-1, label_num)\n",
    "        targets = torch.masked_select(targets, mask)\n",
    "\n",
    "        # Convert logits to probabilities using softmax\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # One-hot encode the targets to get true probabilities\n",
    "        true_probs = F.one_hot(targets, num_classes=probs.size(-1)).float()\n",
    "\n",
    "        # Define the g function\n",
    "        g_values = torch.abs(probs * (1 - torch.log(probs + epsilon)) - true_probs * (1 - torch.log(true_probs + epsilon)))\n",
    "\n",
    "        # Define the delta_h function\n",
    "        delta_h_values = torch.abs(probs**2 * (1 - 2 * torch.log(probs + epsilon)) - true_probs**2 * (1 - 2 * torch.log(true_probs + epsilon))) / (2 * (probs + true_probs))\n",
    "\n",
    "        # Compute DLITE loss for each class\n",
    "        dl_values = g_values - delta_h_values\n",
    "\n",
    "        # Sum over all classes and average over batch size\n",
    "        loss = dl_values.sum(dim=-1).mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Adding more Custom Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bd9a40-398d-4aeb-a4e9-f7af5bdeae13",
   "metadata": {},
   "source": [
    "# Adding Custom Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d001079-7ed8-45dd-a683-c1a758a5dd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    \"\"\"lstm encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(config.hidden_size, config.hidden_size,\n",
    "                                  num_layers=config.lstm_layer_num, bidirectional=config.bi_lstm,\n",
    "                                  batch_first=True)\n",
    "\n",
    "    def forward(self, hidden_state, seq_length):\n",
    "        sequence_output = pack_padded_sequence(hidden_state, seq_length, batch_first=True)\n",
    "        sequence_output, (h_n, c_n) = self.lstm(sequence_output)\n",
    "        sequence_output, _ = pad_packed_sequence(sequence_output, batch_first=True)\n",
    "        return sequence_output\n",
    "\n",
    "\n",
    "\n",
    "class Ner_Model(nn.Module):\n",
    "    def __init__(self,config, label_num, loss_name):\n",
    "        super(Ner_Model, self).__init__()\n",
    "        self.config = config\n",
    "        # deberat model\n",
    "        self.model = transformers.AutoModel.from_pretrained(config.model_name)\n",
    "        # using custom layer\n",
    "        if config.lstm_layer_num > 0:\n",
    "            self.lstm = LSTMEncoder(config)\n",
    "\n",
    "\n",
    "        self.label_num = label_num\n",
    "\n",
    "        if config.bi_lstm and config.lstm_layer_num > 0:\n",
    "            self.classifier = nn.Linear(config.hidden_size * 2 , label_num)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(config.hidden_size  , label_num)\n",
    "\n",
    "        if loss_name == 'ce':\n",
    "            self.loss_func = CE_Loss()\n",
    "        elif loss_name == 'l1':\n",
    "            self.loss_func = L1_Loss()\n",
    "        elif loss_name == 'l2':\n",
    "            self.loss_func = L2_Loss()\n",
    "        elif loss_name == 'kl':\n",
    "            self.loss_func = KLDivergenceLoss()\n",
    "        elif loss_name == 'dlite':\n",
    "            self.loss_func = DLITELoss()\n",
    "        else:\n",
    "            assert 1==0\n",
    "\n",
    "    def forward(self, input_ids, seq_length, attention_mask, labels):\n",
    "        output = self.model(input_ids, attention_mask)\n",
    "        sequence_output = output[0]\n",
    "        if self.config.lstm_layer_num > 0:\n",
    "            sequence_output = self.lstm(sequence_output, seq_length)\n",
    "        logit = self.classifier(sequence_output)\n",
    "        loss = self.loss_func.loss(labels, logit, len(label2id))\n",
    "        return loss, logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9409d161-0eb2-4517-a08b-3ac191bdc3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Optimizer\n",
    "def get_optimizer(model, config):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.1},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                      betas=(0.9, 0.98),\n",
    "                      lr=config.lr)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d185c63f-5aaa-45b3-ba55-32bd51551cb3",
   "metadata": {},
   "source": [
    "# Define the training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6b186ad-6069-4fbd-bef6-ca49540d2291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, mode=\"Validation\"):\n",
    "    ground_truth, predict = [], []\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples = 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(data_loader):\n",
    "            attention_mask = batch[\"id\"].ne(0)\n",
    "            targets = batch['label_id']\n",
    "            loss, logit = model(batch[\"id\"], batch['seq_length'], attention_mask=attention_mask,\n",
    "                                             labels=targets)\n",
    "            eval_loss += loss.cpu().item()\n",
    "            if (step+1) % 100==0:\n",
    "                loss_step = eval_loss / (step+1)\n",
    "                print(f\"{mode} loss per 100 evaluation steps: {loss_step}\")\n",
    "\n",
    "            # compute training accuracy\n",
    "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = logit.view(-1, len(label2id)) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            active_accuracy = flattened_targets.ne(-100) # active accuracy is also of shape (batch_size * seq_len,)\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "            eval_preds.extend(predictions.tolist())\n",
    "            eval_labels.extend(targets.tolist())\n",
    "\n",
    "    eval_loss = eval_loss / (step+1)\n",
    "    eval_accuracy = eval_accuracy / (step+1)\n",
    "\n",
    "    eval_labels,eval_preds = [id2label[i] for i in eval_labels], [id2label[i] for i in eval_preds]\n",
    "\n",
    "\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(eval_labels, eval_preds, average='micro')\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(eval_labels, eval_preds, average='macro')\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(eval_labels, eval_preds,average='weighted')\n",
    "\n",
    "    p_r_f1 = [[round(precision_micro,4), round(recall_micro,4), round(f1_micro,4)],\n",
    "              [round(precision_macro,4), round(recall_macro,4), round(f1_macro,4)],\n",
    "              [round(precision_weighted,4), round(recall_weighted,4), round(f1_weighted,4)]]\n",
    "\n",
    "    p_r_f1 = pd.DataFrame(p_r_f1, columns=['precision', 'recall', 'f1'], index=['micro', 'macro', 'weighted'])\n",
    "\n",
    "    print(f\"{mode} Loss: {eval_loss}\")\n",
    "    print(f\"{mode} Accuracy: {eval_accuracy}\")\n",
    "\n",
    "    p_r_f1_each_label = classification_report(eval_labels, eval_preds)\n",
    "    print(f\"{mode} P-R-F1 for each label: \\n{p_r_f1_each_label}\")\n",
    "    print(f\"{mode} P-R-F1 tor all label: \\n{p_r_f1}\")\n",
    "    print(f\"{mode} steps: {(step+1)}\")\n",
    "    return eval_loss, p_r_f1, p_r_f1_each_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b3d6e63-f953-41c8-8660-da2dfc277f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "loss_list = ['l1', 'l2', 'ce', 'kl', 'dlite']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66840c9-38f6-4e1e-a597-e860d438c701",
   "metadata": {},
   "source": [
    "# Running under 5 custom loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60c4674f-26e6-4aff-b16c-8191b51f76ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "loss_name: l1\n",
      "Training epoch: 1\n",
      "Training loss per 100 training steps: 0.14019331697374582\n",
      "Training loss per 100 training steps: 0.09047539689578116\n",
      "Training loss per 100 training steps: 0.07123233652363221\n",
      "Training loss per 100 training steps: 0.061272875699214635\n",
      "Training loss per 100 training steps: 0.05376829232275486\n",
      "Training loss per 100 training steps: 0.048991816507962846\n",
      "Training loss per 100 training steps: 0.04550060167243438\n",
      "Training loss per 100 training steps: 0.042346934363013136\n",
      "Training loss per 100 training steps: 0.03998903840159376\n",
      "Training loss epoch: 0.03816235883841141\n",
      "Training accuracy epoch: 0.9532297103797043\n",
      "Training steps: 992\n",
      "\n",
      "\n",
      "\n",
      "Validation loss per 100 evaluation steps: 0.01085593380499631\n",
      "Validation loss per 100 evaluation steps: 0.010502557061845436\n",
      "Validation loss per 100 evaluation steps: 0.010914422939531505\n",
      "Validation Loss: 0.010800059584820746\n",
      "Validation Accuracy: 0.9612090411365917\n",
      "Validation P-R-F1 for each label: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-AREA       0.00      0.00      0.00       236\n",
      "     B-POINT       0.00      0.00      0.00       206\n",
      "     B-RIVER       0.00      0.00      0.00        30\n",
      "      B-ROAD       0.00      0.00      0.00        51\n",
      "      I-AREA       0.00      0.00      0.00       387\n",
      "     I-POINT       0.00      0.00      0.00       978\n",
      "     I-RIVER       0.00      0.00      0.00        75\n",
      "      I-ROAD       0.00      0.00      0.00       104\n",
      "           O       0.96      1.00      0.98     52584\n",
      "\n",
      "    accuracy                           0.96     54651\n",
      "   macro avg       0.11      0.11      0.11     54651\n",
      "weighted avg       0.93      0.96      0.94     54651\n",
      "\n",
      "Validation P-R-F1 tor all label: \n",
      "          precision  recall      f1\n",
      "micro        0.9622  0.9622  0.9622\n",
      "macro        0.1069  0.1111  0.1090\n",
      "weighted     0.9258  0.9622  0.9436\n",
      "Validation steps: 326\n",
      "\n",
      "\n",
      "\n",
      "Test loss per 100 evaluation steps: 0.010561072821728886\n",
      "Test loss per 100 evaluation steps: 0.011458265947876498\n",
      "Test loss per 100 evaluation steps: 0.01125718273843328\n",
      "Test Loss: 0.011167795098089597\n",
      "Test Accuracy: 0.9595131605600233\n",
      "Test P-R-F1 for each label: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-AREA       0.00      0.00      0.00       212\n",
      "     B-POINT       0.00      0.00      0.00       202\n",
      "     B-RIVER       0.00      0.00      0.00        29\n",
      "      B-ROAD       0.00      0.00      0.00        57\n",
      "      I-AREA       0.00      0.00      0.00       349\n",
      "     I-POINT       0.00      0.00      0.00      1184\n",
      "     I-RIVER       0.00      0.00      0.00        56\n",
      "      I-ROAD       0.00      0.00      0.00        92\n",
      "           O       0.96      1.00      0.98     52742\n",
      "\n",
      "    accuracy                           0.96     54923\n",
      "   macro avg       0.11      0.11      0.11     54923\n",
      "weighted avg       0.92      0.96      0.94     54923\n",
      "\n",
      "Test P-R-F1 tor all label: \n",
      "          precision  recall      f1\n",
      "micro        0.9603  0.9603  0.9603\n",
      "macro        0.1067  0.1111  0.1089\n",
      "weighted     0.9222  0.9603  0.9408\n",
      "Test steps: 326\n",
      "====================================================================================================\n",
      "loss_name: l2\n",
      "Training epoch: 1\n",
      "Training loss per 100 training steps: 0.053237045542337\n",
      "Training loss per 100 training steps: 0.03153900913894177\n",
      "Training loss per 100 training steps: 0.023806464253769567\n",
      "Training loss per 100 training steps: 0.019421378493425435\n",
      "Training loss per 100 training steps: 0.016568814201047645\n",
      "Training loss per 100 training steps: 0.014669047473095513\n",
      "Training loss per 100 training steps: 0.013249899963515678\n",
      "Training loss per 100 training steps: 0.01211710052790295\n",
      "Training loss per 100 training steps: 0.011201530844878613\n",
      "Training loss epoch: 0.010479792979147591\n",
      "Training accuracy epoch: 0.9676578147478367\n",
      "Training steps: 992\n",
      "\n",
      "\n",
      "\n",
      "Validation loss per 100 evaluation steps: 0.0034872827923027217\n",
      "Validation loss per 100 evaluation steps: 0.002998006012294354\n",
      "Validation loss per 100 evaluation steps: 0.0029894587450568604\n",
      "Validation Loss: 0.0030221297300806016\n",
      "Validation Accuracy: 0.983787342355763\n",
      "Validation P-R-F1 for each label: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-AREA       0.71      0.76      0.74       236\n",
      "     B-POINT       0.67      0.36      0.47       206\n",
      "     B-RIVER       0.79      0.50      0.61        30\n",
      "      B-ROAD       0.00      0.00      0.00        51\n",
      "      I-AREA       0.72      0.65      0.69       387\n",
      "     I-POINT       0.71      0.84      0.77       978\n",
      "     I-RIVER       0.81      0.69      0.75        75\n",
      "      I-ROAD       0.50      0.02      0.04       104\n",
      "           O       0.99      1.00      0.99     52584\n",
      "\n",
      "    accuracy                           0.98     54651\n",
      "   macro avg       0.66      0.54      0.56     54651\n",
      "weighted avg       0.98      0.98      0.98     54651\n",
      "\n",
      "Validation P-R-F1 tor all label: \n",
      "          precision  recall      f1\n",
      "micro        0.9837  0.9837  0.9837\n",
      "macro        0.6572  0.5357  0.5614\n",
      "weighted     0.9820  0.9837  0.9821\n",
      "Validation steps: 326\n",
      "\n",
      "\n",
      "\n",
      "Test loss per 100 evaluation steps: 0.0031402551489372855\n",
      "Test loss per 100 evaluation steps: 0.0029695160062874495\n",
      "Test loss per 100 evaluation steps: 0.0029960267673110746\n",
      "Test Loss: 0.0028877262417611807\n",
      "Test Accuracy: 0.9842476782302063\n",
      "Test P-R-F1 for each label: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-AREA       0.66      0.70      0.68       212\n",
      "     B-POINT       0.80      0.48      0.60       202\n",
      "     B-RIVER       0.75      0.41      0.53        29\n",
      "      B-ROAD       0.00      0.00      0.00        57\n",
      "      I-AREA       0.65      0.58      0.62       349\n",
      "     I-POINT       0.77      0.87      0.81      1184\n",
      "     I-RIVER       0.72      0.64      0.68        56\n",
      "      I-ROAD       0.33      0.05      0.09        92\n",
      "           O       0.99      1.00      1.00     52742\n",
      "\n",
      "    accuracy                           0.98     54923\n",
      "   macro avg       0.63      0.53      0.56     54923\n",
      "weighted avg       0.98      0.98      0.98     54923\n",
      "\n",
      "Test P-R-F1 tor all label: \n",
      "          precision  recall      f1\n",
      "micro        0.9844  0.9844  0.9844\n",
      "macro        0.6308  0.5268  0.5569\n",
      "weighted     0.9826  0.9844  0.9831\n",
      "Test steps: 326\n",
      "====================================================================================================\n",
      "loss_name: ce\n",
      "Training epoch: 1\n",
      "Training loss per 100 training steps: 0.23976940133143215\n",
      "Training loss per 100 training steps: 0.1723582451592665\n",
      "Training loss per 100 training steps: 0.13890517255931628\n",
      "Training loss per 100 training steps: 0.12263132183754351\n",
      "Training loss per 100 training steps: 0.11395025556220208\n",
      "Training loss per 100 training steps: 0.10678402853140141\n",
      "Training loss per 100 training steps: 0.10203232308533708\n",
      "Training loss per 100 training steps: 0.09756777419956052\n",
      "Training loss per 100 training steps: 0.09217464768887212\n",
      "Training loss epoch: 0.0888474994336291\n",
      "Training accuracy epoch: 0.9756423745842919\n",
      "Training steps: 992\n",
      "\n",
      "\n",
      "\n",
      "Validation loss per 100 evaluation steps: 0.05144525706127752\n",
      "Validation loss per 100 evaluation steps: 0.055360878924984716\n",
      "Validation loss per 100 evaluation steps: 0.05263681006821571\n",
      "Validation Loss: 0.05156269902479478\n",
      "Validation Accuracy: 0.9841050430472997\n",
      "Validation P-R-F1 for each label: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-AREA       0.70      0.81      0.75       236\n",
      "     B-POINT       0.77      0.43      0.55       206\n",
      "     B-RIVER       0.68      0.87      0.76        30\n",
      "      B-ROAD       0.47      0.57      0.51        51\n",
      "      I-AREA       0.71      0.73      0.72       387\n",
      "     I-POINT       0.84      0.68      0.75       978\n",
      "     I-RIVER       0.68      0.81      0.74        75\n",
      "      I-ROAD       0.54      0.42      0.48       104\n",
      "           O       0.99      1.00      0.99     52584\n",
      "\n",
      "    accuracy                           0.98     54651\n",
      "   macro avg       0.71      0.70      0.70     54651\n",
      "weighted avg       0.98      0.98      0.98     54651\n",
      "\n",
      "Validation P-R-F1 tor all label: \n",
      "          precision  recall      f1\n",
      "micro        0.9843  0.9843  0.9843\n",
      "macro        0.7106  0.7018  0.6961\n",
      "weighted     0.9837  0.9843  0.9836\n",
      "Validation steps: 326\n",
      "\n",
      "\n",
      "\n",
      "Test loss per 100 evaluation steps: 0.05569077074324014\n",
      "Test loss per 100 evaluation steps: 0.046005918681039475\n",
      "Test loss per 100 evaluation steps: 0.05014867907923569\n",
      "Test Loss: 0.04988652860754358\n",
      "Test Accuracy: 0.9845603438355837\n",
      "Test P-R-F1 for each label: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-AREA       0.61      0.74      0.67       212\n",
      "     B-POINT       0.80      0.59      0.68       202\n",
      "     B-RIVER       0.72      0.90      0.80        29\n",
      "      B-ROAD       0.62      0.56      0.59        57\n",
      "      I-AREA       0.62      0.63      0.63       349\n",
      "     I-POINT       0.89      0.75      0.81      1184\n",
      "     I-RIVER       0.73      0.95      0.82        56\n",
      "      I-ROAD       0.70      0.43      0.54        92\n",
      "           O       0.99      1.00      0.99     52742\n",
      "\n",
      "    accuracy                           0.98     54923\n",
      "   macro avg       0.74      0.73      0.73     54923\n",
      "weighted avg       0.98      0.98      0.98     54923\n",
      "\n",
      "Test P-R-F1 tor all label: \n",
      "          precision  recall      f1\n",
      "micro        0.9847  0.9847  0.9847\n",
      "macro        0.7422  0.7273  0.7253\n",
      "weighted     0.9844  0.9847  0.9842\n",
      "Test steps: 326\n",
      "====================================================================================================\n",
      "loss_name: kl\n",
      "Training epoch: 1\n",
      "Training loss per 100 training steps: 0.23954986793687566\n",
      "Training loss per 100 training steps: 0.1644793340464821\n",
      "Training loss per 100 training steps: 0.13886033841990866\n",
      "Training loss per 100 training steps: 0.12109369052574039\n",
      "Training loss per 100 training steps: 0.10820701743080281\n",
      "Training loss per 100 training steps: 0.10309924711027027\n",
      "Training loss per 100 training steps: 0.09619793667857136\n",
      "Training loss per 100 training steps: 0.09192562653537607\n",
      "Training loss per 100 training steps: 0.08677843853436773\n",
      "Training loss epoch: 0.08466953067535278\n",
      "Training accuracy epoch: 0.9760886679055325\n",
      "Training steps: 992\n",
      "\n",
      "\n",
      "\n",
      "Validation loss per 100 evaluation steps: 0.05176605026746984\n",
      "Validation loss per 100 evaluation steps: 0.04429186865585507\n",
      "Validation loss per 100 evaluation steps: 0.04809887681287364\n",
      "Validation Loss: 0.047547193946385756\n",
      "Validation Accuracy: 0.9843020903399349\n",
      "Validation P-R-F1 for each label: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-AREA       0.76      0.84      0.80       236\n",
      "     B-POINT       0.76      0.58      0.66       206\n",
      "     B-RIVER       0.73      0.90      0.81        30\n",
      "      B-ROAD       0.69      0.22      0.33        51\n",
      "      I-AREA       0.74      0.74      0.74       387\n",
      "     I-POINT       0.82      0.69      0.75       978\n",
      "     I-RIVER       0.78      0.81      0.80        75\n",
      "      I-ROAD       1.00      0.01      0.02       104\n",
      "           O       0.99      1.00      0.99     52584\n",
      "\n",
      "    accuracy                           0.98     54651\n",
      "   macro avg       0.81      0.64      0.65     54651\n",
      "weighted avg       0.98      0.98      0.98     54651\n",
      "\n",
      "Validation P-R-F1 tor all label: \n",
      "          precision  recall      f1\n",
      "micro        0.9846  0.9846  0.9846\n",
      "macro        0.8079  0.6429  0.6545\n",
      "weighted     0.9839  0.9846  0.9830\n",
      "Validation steps: 326\n",
      "\n",
      "\n",
      "\n",
      "Test loss per 100 evaluation steps: 0.05155947881372413\n",
      "Test loss per 100 evaluation steps: 0.044826900829502844\n",
      "Test loss per 100 evaluation steps: 0.04566555376591471\n",
      "Test Loss: 0.04738778533503996\n",
      "Test Accuracy: 0.9842276074086779\n",
      "Test P-R-F1 for each label: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-AREA       0.73      0.76      0.74       212\n",
      "     B-POINT       0.77      0.71      0.74       202\n",
      "     B-RIVER       0.77      0.83      0.80        29\n",
      "      B-ROAD       0.72      0.23      0.35        57\n",
      "      I-AREA       0.69      0.62      0.65       349\n",
      "     I-POINT       0.86      0.73      0.79      1184\n",
      "     I-RIVER       0.79      0.80      0.80        56\n",
      "      I-ROAD       1.00      0.07      0.12        92\n",
      "           O       0.99      1.00      0.99     52742\n",
      "\n",
      "    accuracy                           0.98     54923\n",
      "   macro avg       0.81      0.64      0.66     54923\n",
      "weighted avg       0.98      0.98      0.98     54923\n",
      "\n",
      "Test P-R-F1 tor all label: \n",
      "          precision  recall      f1\n",
      "micro        0.9845  0.9845  0.9845\n",
      "macro        0.8137  0.6381  0.6650\n",
      "weighted     0.9837  0.9845  0.9832\n",
      "Test steps: 326\n",
      "====================================================================================================\n",
      "loss_name: dlite\n",
      "Training epoch: 1\n",
      "Training loss per 100 training steps: 0.08525059362174943\n",
      "Training loss per 100 training steps: 0.062071965465074756\n",
      "Training loss per 100 training steps: 0.05581489807964923\n",
      "Training loss per 100 training steps: 0.0516398719749759\n",
      "Training loss per 100 training steps: 0.04876840931896004\n",
      "Training loss per 100 training steps: 0.04691962280712763\n",
      "Training loss per 100 training steps: 0.04588748723028272\n",
      "Training loss per 100 training steps: 0.04514836239596391\n",
      "Training loss per 100 training steps: 0.04384212604545547\n",
      "Training loss epoch: 0.043538932915253604\n",
      "Training accuracy epoch: 0.9570358662567892\n",
      "Training steps: 992\n",
      "\n",
      "\n",
      "\n",
      "Validation loss per 100 evaluation steps: 0.037884772262034405\n",
      "Validation loss per 100 evaluation steps: 0.038218292562487474\n",
      "Validation loss per 100 evaluation steps: 0.03888346353305299\n",
      "Validation Loss: 0.0380276171459294\n",
      "Validation Accuracy: 0.9619726294464871\n",
      "Validation P-R-F1 for each label: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-AREA       0.00      0.00      0.00       236\n",
      "     B-POINT       0.00      0.00      0.00       206\n",
      "     B-RIVER       0.00      0.00      0.00        30\n",
      "      B-ROAD       0.00      0.00      0.00        51\n",
      "      I-AREA       0.00      0.00      0.00       387\n",
      "     I-POINT       0.00      0.00      0.00       978\n",
      "     I-RIVER       0.00      0.00      0.00        75\n",
      "      I-ROAD       0.00      0.00      0.00       104\n",
      "           O       0.96      1.00      0.98     52584\n",
      "\n",
      "    accuracy                           0.96     54651\n",
      "   macro avg       0.11      0.11      0.11     54651\n",
      "weighted avg       0.93      0.96      0.94     54651\n",
      "\n",
      "Validation P-R-F1 tor all label: \n",
      "          precision  recall      f1\n",
      "micro        0.9622  0.9622  0.9622\n",
      "macro        0.1069  0.1111  0.1090\n",
      "weighted     0.9258  0.9622  0.9436\n",
      "Validation steps: 326\n",
      "\n",
      "\n",
      "\n",
      "Test loss per 100 evaluation steps: 0.03704709775727764\n",
      "Test loss per 100 evaluation steps: 0.038525694042710085\n",
      "Test loss per 100 evaluation steps: 0.04088884767346229\n",
      "Test Loss: 0.04098584304140408\n",
      "Test Accuracy: 0.9590143820036563\n",
      "Test P-R-F1 for each label: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-AREA       0.00      0.00      0.00       212\n",
      "     B-POINT       0.00      0.00      0.00       202\n",
      "     B-RIVER       0.00      0.00      0.00        29\n",
      "      B-ROAD       0.00      0.00      0.00        57\n",
      "      I-AREA       0.00      0.00      0.00       349\n",
      "     I-POINT       0.00      0.00      0.00      1184\n",
      "     I-RIVER       0.00      0.00      0.00        56\n",
      "      I-ROAD       0.00      0.00      0.00        92\n",
      "           O       0.96      1.00      0.98     52742\n",
      "\n",
      "    accuracy                           0.96     54923\n",
      "   macro avg       0.11      0.11      0.11     54923\n",
      "weighted avg       0.92      0.96      0.94     54923\n",
      "\n",
      "Test P-R-F1 tor all label: \n",
      "          precision  recall      f1\n",
      "micro        0.9603  0.9603  0.9603\n",
      "macro        0.1067  0.1111  0.1089\n",
      "weighted     0.9222  0.9603  0.9408\n",
      "Test steps: 326\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def train(config,loss_name):\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"loss_name: {loss_name}\")\n",
    "    model = Ner_Model(config, len(label2id), loss_name).to(config.device)\n",
    "    optimizer = get_optimizer(model, config)\n",
    "\n",
    "    valid_each_label_p_r_f1_list = []\n",
    "    valid_p_r_f1_list = []\n",
    "    test_each_label_p_r_f1_list = []\n",
    "    test_p_r_f1_list = []\n",
    "\n",
    "    valid_loss_list = []\n",
    "    test_loss_list = []\n",
    "\n",
    "    model.train()\n",
    "    interval = 100\n",
    "    for epoch in range(config.epochs):\n",
    "        print(f\"Training epoch: {epoch + 1}\")\n",
    "        tr_preds,tr_labels = [], []\n",
    "        total_loss = 0.0\n",
    "        tr_accuracy = 0.0\n",
    "        # print(\"\\n\" + \"-\" * 30 + \"\\n\")\n",
    "        # print(f\"epoch: {epoch},  train dataloader size: {len(train_dataloader)}\")\n",
    "        # print(f\"epoch: {epoch},  valid dataloader size: {len(valid_dataloader)}\")\n",
    "        # print(f\"epoch: {epoch},  test dataloader size: {len(test_dataloader)}\")\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            attention_mask = batch[\"id\"].ne(0)\n",
    "            targets = batch['label_id']\n",
    "            loss, logit= model(batch[\"id\"], batch['seq_length'], attention_mask=attention_mask,\n",
    "                                             labels=targets)\n",
    "\n",
    "            # compute training accuracy\n",
    "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = logit.view(-1, len(label2id)) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            active_accuracy = flattened_targets.ne(-100) # active accuracy is also of shape (batch_size * seq_len,)\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "            tr_accuracy += tmp_tr_accuracy\n",
    "            tr_preds.extend(predictions)\n",
    "            tr_labels.extend(targets)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            if (step + 1) % interval == 0:\n",
    "                print(f\"Training loss per 100 training steps: {total_loss / (step+1)}\")\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Training loss epoch: {total_loss / (step+1)}\")\n",
    "        print(f\"Training accuracy epoch: {tr_accuracy / (step+1)}\")\n",
    "        print(f\"Training steps: {step+1}\")\n",
    "        print(\"\\n\\n\")\n",
    "        model.eval()\n",
    "\n",
    "\n",
    "        valid_loss, valid_p_r_f1,  valid_each_label_p_r_f1 = evaluate(model,valid_dataloader, \"Validation\")\n",
    "        valid_loss_list.append(valid_loss)\n",
    "        valid_p_r_f1_list.append(valid_p_r_f1)\n",
    "        valid_each_label_p_r_f1_list.append(valid_each_label_p_r_f1)\n",
    "\n",
    "        print(\"\\n\\n\")\n",
    "        test_loss, test_p_r_f1,test_each_label_p_r_f1  = evaluate(model,test_dataloader, \"Test\")\n",
    "        test_loss_list.append(test_loss)\n",
    "        test_p_r_f1_list.append(test_p_r_f1)\n",
    "        test_each_label_p_r_f1_list.append(test_each_label_p_r_f1)\n",
    "\n",
    "\n",
    "        #print(f\"epoch: {epoch}, train_loss: {train_loss}, \\n{train_p_r_f1}\")\n",
    "        #print(f\"epoch: {epoch}, valid_loss: {valid_loss}, \\n{valid_p_r_f1}\")\n",
    "        #print(f\"epoch: {epoch}, test_loss: {test_loss},  \\n {test_p_r_f1}\")\n",
    "        model.train()\n",
    "    return   {\n",
    "              \"valid_loss_list\":valid_loss_list,\n",
    "              \"test_loss_list\":test_loss_list,\n",
    "              \"valid_p_r_f1_list\":valid_p_r_f1_list,\n",
    "              \"valid_each_label_p_r_f1_list\":valid_each_label_p_r_f1_list,\n",
    "\n",
    "              \"test_p_r_f1_list\":test_p_r_f1_list,\n",
    "              \"test_each_label_p_r_f1_list\": test_each_label_p_r_f1_list}\n",
    "\n",
    "\n",
    "result = {}\n",
    "for loss_name in ['l1', 'l2', 'ce', 'kl', 'dlite']:\n",
    "    r = train(Config, loss_name)\n",
    "    result[loss_name] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d260e8d-e27f-4945-9259-217cf74609f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"result.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ceae00-18e9-40f8-b333-e491e31634e3",
   "metadata": {},
   "source": [
    "# Result Comparison after cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1febb46b-e370-4eb4-8507-e27a861559a9",
   "metadata": {},
   "source": [
    "### Overall Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1435632e-8f8b-4abd-b786-e81d029b0bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "micro\n",
      "    loss  precision  recall      f1\n",
      "0     l1     0.7659  0.7659  0.7659\n",
      "1     l2     0.7659  0.7659  0.7659\n",
      "2     ce     0.8442  0.8442  0.8442\n",
      "3     kl     0.7693  0.7693  0.7693\n",
      "4  dlite     0.7831  0.7831  0.7831\n",
      "====================================================================================================\n",
      "macro\n",
      "    loss  precision  recall      f1\n",
      "0     l1     0.0851  0.1111  0.0964\n",
      "1     l2     0.1962  0.1111  0.0964\n",
      "2     ce     0.2874  0.3402  0.2967\n",
      "3     kl     0.1271  0.1368  0.1297\n",
      "4  dlite     0.1251  0.1724  0.1434\n",
      "====================================================================================================\n",
      "weighted\n",
      "    loss  precision  recall      f1\n",
      "0     l1     0.5866  0.7659  0.6644\n",
      "1     l2     0.6486  0.7659  0.6644\n",
      "2     ce     0.7934  0.8442  0.8131\n",
      "3     kl     0.6255  0.7693  0.6884\n",
      "4  dlite     0.6485  0.7831  0.7086\n"
     ]
    }
   ],
   "source": [
    "columns = ['loss', 'precision', 'recall', 'f1']\n",
    "for t in ['micro', 'macro', 'weighted']:\n",
    "    df = []\n",
    "    for loss_name in loss_list:\n",
    "        row = {'loss': loss_name}\n",
    "        row['precision'] = result[loss_name]['test_p_r_f1_list'][-1].loc[t, 'precision']\n",
    "        row['recall'] = result[loss_name]['test_p_r_f1_list'][-1].loc[t, 'recall']\n",
    "        row['f1'] = result[loss_name]['test_p_r_f1_list'][-1].loc[t, 'f1']\n",
    "        df.append(row)\n",
    "    print(\"=\"*100)\n",
    "    print(t)\n",
    "    print(pd.DataFrame(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4667a44d-ad27-421f-ad47-428f624be2fa",
   "metadata": {},
   "source": [
    "### Each Label Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "386c239d-5023-40d8-956a-bb2f68666d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset\n",
      "--------------------------------------------------\n",
      "l1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00      1668\n",
      "      B-MISC       0.00      0.00      0.00       702\n",
      "       B-ORG       0.00      0.00      0.00      1661\n",
      "       B-PER       0.00      0.00      0.00      1617\n",
      "       I-LOC       0.00      0.00      0.00      1394\n",
      "      I-MISC       0.00      0.00      0.00       736\n",
      "       I-ORG       0.00      0.00      0.00      2804\n",
      "       I-PER       0.00      0.00      0.00      3810\n",
      "           O       0.77      1.00      0.87     47094\n",
      "\n",
      "    accuracy                           0.77     61486\n",
      "   macro avg       0.09      0.11      0.10     61486\n",
      "weighted avg       0.59      0.77      0.66     61486\n",
      "\n",
      "--------------------------------------------------\n",
      "l2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00      1668\n",
      "      B-MISC       0.00      0.00      0.00       702\n",
      "       B-ORG       0.00      0.00      0.00      1661\n",
      "       B-PER       0.00      0.00      0.00      1617\n",
      "       I-LOC       0.00      0.00      0.00      1394\n",
      "      I-MISC       0.00      0.00      0.00       736\n",
      "       I-ORG       0.00      0.00      0.00      2804\n",
      "       I-PER       1.00      0.00      0.00      3810\n",
      "           O       0.77      1.00      0.87     47094\n",
      "\n",
      "    accuracy                           0.77     61486\n",
      "   macro avg       0.20      0.11      0.10     61486\n",
      "weighted avg       0.65      0.77      0.66     61486\n",
      "\n",
      "--------------------------------------------------\n",
      "ce\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.31      0.07      0.11      1668\n",
      "      B-MISC       0.00      0.00      0.00       702\n",
      "       B-ORG       0.00      0.00      0.00      1661\n",
      "       B-PER       0.38      0.65      0.48      1617\n",
      "       I-LOC       0.00      0.00      0.00      1394\n",
      "      I-MISC       0.00      0.00      0.00       736\n",
      "       I-ORG       0.43      0.52      0.47      2804\n",
      "       I-PER       0.52      0.84      0.64      3810\n",
      "           O       0.94      0.98      0.96     47094\n",
      "\n",
      "    accuracy                           0.84     61486\n",
      "   macro avg       0.29      0.34      0.30     61486\n",
      "weighted avg       0.79      0.84      0.81     61486\n",
      "\n",
      "--------------------------------------------------\n",
      "kl\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00      1668\n",
      "      B-MISC       0.00      0.00      0.00       702\n",
      "       B-ORG       0.00      0.00      0.00      1661\n",
      "       B-PER       0.00      0.00      0.00      1617\n",
      "       I-LOC       0.00      0.00      0.00      1394\n",
      "      I-MISC       0.00      0.00      0.00       736\n",
      "       I-ORG       0.00      0.00      0.00      2804\n",
      "       I-PER       0.36      0.25      0.29      3810\n",
      "           O       0.79      0.98      0.88     47094\n",
      "\n",
      "    accuracy                           0.77     61486\n",
      "   macro avg       0.13      0.14      0.13     61486\n",
      "weighted avg       0.63      0.77      0.69     61486\n",
      "\n",
      "--------------------------------------------------\n",
      "dlite\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00      1668\n",
      "      B-MISC       0.00      0.00      0.00       702\n",
      "       B-ORG       0.00      0.00      0.00      1661\n",
      "       B-PER       0.00      0.00      0.00      1617\n",
      "       I-LOC       0.00      0.00      0.00      1394\n",
      "      I-MISC       0.00      0.00      0.00       736\n",
      "       I-ORG       0.30      0.56      0.39      2804\n",
      "       I-PER       0.00      0.00      0.00      3810\n",
      "           O       0.83      0.99      0.90     47094\n",
      "\n",
      "    accuracy                           0.78     61486\n",
      "   macro avg       0.13      0.17      0.14     61486\n",
      "weighted avg       0.65      0.78      0.71     61486\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"test dataset\")\n",
    "for loss_name in loss_list:\n",
    "    print(\"-\"*50)\n",
    "    print(loss_name)\n",
    "    print(result[loss_name]['test_each_label_p_r_f1_list'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e3108f-d63b-4db1-9abd-9f16ff8883a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
