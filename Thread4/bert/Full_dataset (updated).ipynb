{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82fdcbca",
   "metadata": {
    "id": "82fdcbca"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertConfig, BertForTokenClassification\n",
    "# Using the dlite function\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f8d35a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f8d35a4",
    "outputId": "f80bbcfd-f45c-4679-d1ad-a79da4e63769"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
      "Requirement already satisfied: seqeval[gpu] in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval[gpu]) (1.2.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers seqeval[gpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "oJUgaJuLMMMB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oJUgaJuLMMMB",
    "outputId": "42222412-e40f-4c26-eeba-6ae21267f835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c8951f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c8951f4",
    "outputId": "c3253e13-67b6-4a7b-a9cf-45aebbec013d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c27912f",
   "metadata": {
    "id": "2c27912f"
   },
   "outputs": [],
   "source": [
    "# Ner dataset website ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eeab7a44",
   "metadata": {
    "id": "eeab7a44"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/namanj27/ner-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b64e9",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER) Dataset\n",
    "\n",
    "This is a very clean dataset and is for anyone who wants to try his/her hand on the NER ( Named Entity recognition ) task of NLP. The dataset with 1M x 4 dimensions contains columns = ['# Sentence', 'Word', 'POS', 'Tag'] and is grouped by #Sentence.\n",
    "\n",
    "\n",
    "\n",
    "* **`Geographic location`**: `'B-geo'`  and `'I-geo'`\n",
    "\n",
    "* **`geopolitical `**: `'B-gpe'`  and `'I-gpe'`\n",
    "\n",
    "* **`Organization`**: `'B-ORG'` and `'I-ORG'`\n",
    "\n",
    "* **`Person`**: `'B-PER'` and  `'I-PER'`\n",
    "\n",
    "* **`Time`**: `'B-tim'` and `'I-tim'`\n",
    "\n",
    "* **`Other(non-named entity)`**: `'O'`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fddbe20",
   "metadata": {},
   "source": [
    "## Micro Table\n",
    "\n",
    "| Loss | Train Iteration | Precision | Recall | F1-score | Training time |\n",
    "| --- | --- | --- | --- | --- | --- | \n",
    "| CE | 9515 | 0.79 | 0.81 | 0.80 | 19 min 11s |\n",
    "| L1 | 9515 | 0.77 | 0.59 | 0.67 | 19 min 19s |\n",
    "| D-Lite | 9515 | 0.79 | 0.79 | 0.79 | 19 min 19s |\n",
    "| KL | 9515 | 0.80 | 0.81 | 0.80 | 19 min 26 s |\n",
    "\n",
    "\n",
    "## Macro Table\n",
    "\n",
    "| Loss | Train Iteration | Precision | Recall | F1-score | Training time |\n",
    "| --- | --- | --- | --- | --- | --- | \n",
    "| CE | 9515 | 0.80 | 0.81 | 0.80 | 19 min 11s |\n",
    "| L1 | 9515 | 0.63 | 0.60 | 0.61 | 19 min 19s |\n",
    "| D-Lite | 9515 | 0.79 | 0.79 | 0.79 | 19 min 19s |\n",
    "| KL | 9515 | 0.81 | 0.81 | 0.81 | 19 min 26 s |\n",
    "\n",
    "## Weighted Table\n",
    "\n",
    "| Loss | Train Iteration | Precision | Recall | F1-score | Training time |\n",
    "| --- | --- | --- | --- | --- | --- | \n",
    "| CE | 9515 | 0.80 | 0.81 | 0.80 | 19 min 11s |\n",
    "| L1 | 9515 | 0.61 | 0.59 | 0.60 | 19 min 19s |\n",
    "| D-Lite | 9515 | 0.78 | 0.79 | 0.79 | 19 min 19s |\n",
    "| KL | 9515 | 0.80 | 0.81 | 0.80 | 19 min 26 s |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46be561",
   "metadata": {},
   "source": [
    "# New Coding methods\n",
    "\n",
    "## Micro Table\n",
    "\n",
    "| Loss | Train Iteration | Precision | Recall | F1-score | Training time |\n",
    "| --- | --- | --- | --- | --- | --- | \n",
    "| CE | 9515 | 0.96 | 0.96 | 0.96 | 19 min 11s |\n",
    "| L1 | 9515 | 0.92 | 0.92 | 0.92 | 19 min 19s |\n",
    "| D-Lite | 9515 | 0.96 | 0.96 | 0.96 | 19 min 19s |\n",
    "| D-Lite Prefer | 9515 | 0.82 | 0.82 | 0.82 | 19 min 27s |\n",
    "| KL | 9515 | 0.96 | 0.96 | 0.96 | 19 min 26 s |\n",
    "| D-Lite Cubic Root | 9515 | 0.82 | 0.82 | 0.82 | 19 min 29s |\n",
    "\n",
    "## Macro Table\n",
    "\n",
    "| Loss | Train Iteration | Precision | Recall | F1-score | Training time |\n",
    "| --- | --- | --- | --- | --- | --- | \n",
    "| CE | 9515 | 0.84 | 0.77 | 0.80 | 19 min 11s |\n",
    "| L1 | 9515 | 0.93 | 0.47 | 0.47 | 19 min 19s |\n",
    "| D-Lite | 9515 | 0.85 | 0.70 | 0.72 | 19 min 19s |\n",
    "| D-Lite Prefer | 9515 | 0.98 | 0.09 | 0.09 | 19 min 27s |\n",
    "| KL | 9515 | 0.85 | 0.78| 0.79 | 19 min 26 s |\n",
    "| D-Lite Cubic Root | 9515 | 0.98 | 0.09 | 0.08 | 19 min 29s |\n",
    "\n",
    "## Weighted Table\n",
    "\n",
    "| Loss | Train Iteration | Precision | Recall | F1-score | Training time |\n",
    "| --- | --- | --- | --- | --- | --- | \n",
    "| CE | 9515 | 0.95 | 0.96 | 0.96 | 19 min 11s |\n",
    "| L1 | 9515 | 0.93 | 0.92 | 0.90 | 19 min 19s |\n",
    "| D-Lite | 9515 | 0.95 | 0.96 | 0.95 | 19 min 19s |\n",
    "| D-Lite Prefer | 9515 | 0.86 | 0.82 | 0.74 | 19 min 27s |\n",
    "| KL | 9515 | 0.96 | 0.96 | 0.96 | 19 min 26 s |\n",
    "| D-Lite Cubic Root | 9515 | 0.85| 0.82 | 0.75 | 19 min 29s |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8328f08",
   "metadata": {
    "id": "e8328f08"
   },
   "source": [
    "# Data preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36181a52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "36181a52",
    "outputId": "5c2944ba-7741-4210-f891-090678591b59"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-0fbda91b-bcd0-4b72-9d16-0a2917a9309b\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0fbda91b-bcd0-4b72-9d16-0a2917a9309b')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-0fbda91b-bcd0-4b72-9d16-0a2917a9309b button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-0fbda91b-bcd0-4b72-9d16-0a2917a9309b');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-18a8853d-b467-4b7d-8539-2bc1aa99573d\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-18a8853d-b467-4b7d-8539-2bc1aa99573d')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-18a8853d-b467-4b7d-8539-2bc1aa99573d button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"ner_datasetreference.csv\", encoding='unicode_escape')\n",
    "data.head() # preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "374ae95e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "374ae95e",
    "outputId": "a6b873ba-5ef8-494f-88fb-eb58692cd6e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence #      47959\n",
       "Word          1048575\n",
       "POS           1048575\n",
       "Tag           1048575\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()\n",
    "# count of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d45f5e61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d45f5e61",
    "outputId": "d8bcffe7-1777-4f40-fdb0-66e5b63ea0b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tags: 17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "O        887908\n",
       "B-geo     37644\n",
       "B-tim     20333\n",
       "B-org     20143\n",
       "I-per     17251\n",
       "B-per     16990\n",
       "I-org     16784\n",
       "B-gpe     15870\n",
       "I-geo      7414\n",
       "I-tim      6528\n",
       "B-art       402\n",
       "B-eve       308\n",
       "I-art       297\n",
       "I-eve       253\n",
       "B-nat       201\n",
       "I-gpe       198\n",
       "I-nat        51\n",
       "Name: Tag, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of tags: {}\".format(len(data.Tag.unique())))\n",
    "frequencies = data.Tag.value_counts()\n",
    "frequencies\n",
    "# Look at differnt NER tags and their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc1c532e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc1c532e",
    "outputId": "97105de9-bd2d-4b5e-f544-9cbb67bc11ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('geo', 45058), ('org', 36927), ('per', 34241), ('tim', 26861), ('gpe', 16068), ('art', 699), ('eve', 561), ('nat', 252)]\n"
     ]
    }
   ],
   "source": [
    "tags = {}\n",
    "for tag, count in zip(frequencies.index, frequencies):\n",
    "    if tag != \"O\":\n",
    "        if tag[2:5] not in tags.keys():\n",
    "            tags[tag[2:5]] = count\n",
    "        else:\n",
    "            tags[tag[2:5]] += count\n",
    "    continue\n",
    "\n",
    "print(sorted(tags.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f98c4bf1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "f98c4bf1",
    "outputId": "a5292a3a-4099-4b0e-f7e4-5c9679d6501e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-d911b0ea-8d21-4b82-8f67-7f4afa7b3d89\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d911b0ea-8d21-4b82-8f67-7f4afa7b3d89')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-d911b0ea-8d21-4b82-8f67-7f4afa7b3d89 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-d911b0ea-8d21-4b82-8f67-7f4afa7b3d89');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-dd2e9798-fd59-40a7-9a0c-f4c34d1a3fa4\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dd2e9798-fd59-40a7-9a0c-f4c34d1a3fa4')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-dd2e9798-fd59-40a7-9a0c-f4c34d1a3fa4 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_to_remove = [\"B-art\", \"I-art\", \"B-eve\", \"I-eve\", \"B-nat\", \"I-nat\"]\n",
    "data = data[~data.Tag.isin(entities_to_remove)]\n",
    "data.head()\n",
    "\n",
    "# removed \"art\", \"eve\", \"nat\" named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e00e8202",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "e00e8202",
    "outputId": "199bb85d-e8aa-42cd-824c-86c1be58f764"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-5b886c21-d465-40d1-94e7-ee5d4186a974\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5b886c21-d465-40d1-94e7-ee5d4186a974')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-5b886c21-d465-40d1-94e7-ee5d4186a974 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-5b886c21-d465-40d1-94e7-ee5d4186a974');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-ccd2af16-2b14-48cd-96ea-35fa83f88f62\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ccd2af16-2b14-48cd-96ea-35fa83f88f62')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-ccd2af16-2b14-48cd-96ea-35fa83f88f62 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1  Sentence: 1             of   IN   O\n",
       "2  Sentence: 1  demonstrators  NNS   O\n",
       "3  Sentence: 1           have  VBP   O\n",
       "4  Sentence: 1        marched  VBN   O"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas has a very handy \"forward fill\" function to fill missing values based on the last upper non-nan value\n",
    "data = data.fillna(method='ffill')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50f30fd7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "50f30fd7",
    "outputId": "06e8d8c0-4072-4a01-8c94-a51eeb66339e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-837a012f-6463-44b2-bd96-21961e10b317\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-837a012f-6463-44b2-bd96-21961e10b317')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-837a012f-6463-44b2-bd96-21961e10b317 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-837a012f-6463-44b2-bd96-21961e10b317');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-9d84e886-337e-41b0-8714-2bb6f78763ff\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9d84e886-337e-41b0-8714-2bb6f78763ff')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-9d84e886-337e-41b0-8714-2bb6f78763ff button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag  \\\n",
       "0  Sentence: 1      Thousands  NNS   O   \n",
       "1  Sentence: 1             of   IN   O   \n",
       "2  Sentence: 1  demonstrators  NNS   O   \n",
       "3  Sentence: 1           have  VBP   O   \n",
       "4  Sentence: 1        marched  VBN   O   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  Thousands of demonstrators have marched throug...   \n",
       "1  Thousands of demonstrators have marched throug...   \n",
       "2  Thousands of demonstrators have marched throug...   \n",
       "3  Thousands of demonstrators have marched throug...   \n",
       "4  Thousands of demonstrators have marched throug...   \n",
       "\n",
       "                                         word_labels  \n",
       "0  O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...  \n",
       "1  O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...  \n",
       "2  O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...  \n",
       "3  O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...  \n",
       "4  O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's create a new column called \"sentence\" which groups the words by sentence\n",
    "data['sentence'] = data[['Sentence #','Word','Tag']].groupby(['Sentence #'])['Word'].transform(lambda x: ' '.join(x))\n",
    "# let's also create a new column called \"word_labels\" which groups the tags by sentence\n",
    "data['word_labels'] = data[['Sentence #','Word','Tag']].groupby(['Sentence #'])['Tag'].transform(lambda x: ','.join(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18fa680b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18fa680b",
    "outputId": "c282e8bf-42fb-4f46-ba3e-425533e71add"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-geo': 1,\n",
       " 'B-gpe': 2,\n",
       " 'B-per': 3,\n",
       " 'I-geo': 4,\n",
       " 'B-org': 5,\n",
       " 'I-org': 6,\n",
       " 'B-tim': 7,\n",
       " 'I-per': 8,\n",
       " 'I-gpe': 9,\n",
       " 'I-tim': 10}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {k: v for v, k in enumerate(data.Tag.unique())}\n",
    "id2label = {v: k for v, k in enumerate(data.Tag.unique())}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2a382d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "d2a382d4",
    "outputId": "26821e9b-17d9-41c2-edd8-ddc54e7662e8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-9150d8ef-5fb7-445a-8bd0-33a6df339bf2\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Families of soldiers killed in the conflict jo...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-per,O,O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They marched from the Houses of Parliament to ...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,B-geo,I-geo,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Police put the number of marchers at 10,000 wh...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The protest comes on the eve of the annual con...</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,B-geo,O,O,B-org,I-org,O,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9150d8ef-5fb7-445a-8bd0-33a6df339bf2')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-9150d8ef-5fb7-445a-8bd0-33a6df339bf2 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-9150d8ef-5fb7-445a-8bd0-33a6df339bf2');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-c0deb98d-7db4-4545-9daa-e302e909415a\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c0deb98d-7db4-4545-9daa-e302e909415a')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-c0deb98d-7db4-4545-9daa-e302e909415a button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  Thousands of demonstrators have marched throug...   \n",
       "1  Families of soldiers killed in the conflict jo...   \n",
       "2  They marched from the Houses of Parliament to ...   \n",
       "3  Police put the number of marchers at 10,000 wh...   \n",
       "4  The protest comes on the eve of the annual con...   \n",
       "\n",
       "                                         word_labels  \n",
       "0  O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...  \n",
       "1  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-per,O,O,...  \n",
       "2                O,O,O,O,O,O,O,O,O,O,O,B-geo,I-geo,O  \n",
       "3                      O,O,O,O,O,O,O,O,O,O,O,O,O,O,O  \n",
       "4  O,O,O,O,O,O,O,O,O,O,O,B-geo,O,O,B-org,I-org,O,...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44849c60",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44849c60",
    "outputId": "1c96b834-ff7c-44f8-fa47-14bc7c1f7afe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47571"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c11a364",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "1c11a364",
    "outputId": "7b88c01c-4797-4b11-c979-ff55eb2a4f42"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Bedfordshire police said Tuesday that Omar Khayam was arrested in Bedford for breaching the conditions of his parole .'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[41].sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4a449a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "d4a449a6",
    "outputId": "d4407b91-d04f-4d81-b1c6-fc0d93f879cb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'B-gpe,O,O,B-tim,O,B-per,I-per,O,O,O,B-geo,O,O,O,O,O,O,O,O'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[41].word_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1ea60b",
   "metadata": {
    "id": "0b1ea60b"
   },
   "source": [
    "# Data preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4905bf7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388,
     "referenced_widgets": [
      "f8235fe44b404c95a24dadc7e3e6f703",
      "c1e690cbf82942d997c62e48249ed74c",
      "c90782f047654ba9acdddfb97cd2b5d1",
      "75c43918b8974816aa5613c12935b789",
      "40555c8120ed42599a152208926a34d5",
      "d4594f8894db4360ac0298a466b5dc23",
      "5df80a5fca0247f7aeae095bcb557783",
      "91e8394b20094101a4125c21a50939da",
      "68f0a32e8add4924a23e6244420a4097",
      "e4780b187f3e4ad09cebd74744943ffa",
      "e93905cc931c4c7cb6012450f40a2a20",
      "138f78a807c44077acc47b16fc42dd67",
      "2232ac4603f840689b772854343f55eb",
      "80fd709f663446abbe9a9045349ae5f2",
      "e3398dc1bdc149a1ae5a09aa13eb625a",
      "13fae4cb29134d62966f3cf140caa4a8",
      "8589f00ef3c848bb9630ef30418d1ea5",
      "2988641d13d246ccb678c118b1e7aac2",
      "9edcf5221340497ba4e0d0c486207f3b",
      "7da4f585b97d4121a1c231d368d02c2c",
      "6bbc04eda753406cb7804b2373e64e47",
      "e516609cfff940f78c7eee96ab2b1464",
      "0118e9713f624e96a48d579636a4b988",
      "a50cf9045a9d4713b1b882b8fe5983aa",
      "2656e3b279814c3ba96c2d887ed0a23c",
      "9e1e8507a3604db8b41e721f42a92a22",
      "d26fd5ed228047ed8d1a3e1f372cb023",
      "e23c972ae1234ef693acf50f5635fe58",
      "f478c0d581d148b4bc28cf562f861ddc",
      "7cbec94204f248f596e5e85d4cb5a68b",
      "8c81a1bf403740c8b42665fcccfc2887",
      "5d2ca44bcf1549a6881634d6af96c405",
      "90e33b49bec747d5a4b3ee38d72e4b4a",
      "462792fae6074789bdc7e675f820a4c1",
      "0e89d5a0770f41559be9b818ed986db9",
      "e8fdfdf53c26418485c5f05c6029a111",
      "388035b119b64f7c8d525316299cdfd5",
      "d235ea9a08fd4056a73784f34be49966",
      "cb80aac8df054449a9d9e813d7ac46ef",
      "72bdce3d60ea49acabf03d6fcf45458a",
      "4ee4ac924bce4a8a8c10de3b03187056",
      "7eb5064ad48d44c9babdc53c9efe0e97",
      "d04b634bf96c4a53b4860cd7721dedd2",
      "5cec4229c71b48eab696c84790f199d4",
      "7d807fede1d142828a8e8a1326736d75",
      "b890ba1ede044bafb3e6be5dcbd7ac8e",
      "73038743885d457f83ced11dcd108488",
      "5e35936100c4479c9f3094778aa32aaf",
      "d5578e892d134774a360965cb533818c",
      "696102f8d24f4bffb765a618f45642b2",
      "be66705cc9fc41238670ecf5dcfab3b0",
      "56f660b673744d85a2cc9897465cd65e",
      "f9e395910dbe41b583b4c1456838913e",
      "deeb7c7b77ad4501bd2d0a58f0602111",
      "797c2bf7975f4166bd82c48b67121fbd"
     ]
    },
    "id": "4905bf7d",
    "outputId": "4ff46e06-34e7-4271-c1dc-11fecb6dcd14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8235fe44b404c95a24dadc7e3e6f703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138f78a807c44077acc47b16fc42dd67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0118e9713f624e96a48d579636a4b988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462792fae6074789bdc7e675f820a4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (47571, 2)\n",
      "TRAIN Dataset: (38057, 2)\n",
      "TEST Dataset: (9514, 2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d807fede1d142828a8e8a1326736d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 5e-05\n",
    "MAX_GRAD_NORM = 10\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# tokenization\n",
    "\n",
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Word piece tokenization makes it difficult to match word labels\n",
    "    back up with individual word pieces. This function tokenizes each\n",
    "    word one at a time so that it is easier to preserve the correct\n",
    "    label for each subword. It is, of course, a bit slower in processing\n",
    "    time, but it will help our model achieve higher accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels\n",
    "\n",
    "# Bert imput\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # step 1: tokenize (and adapt corresponding labels)\n",
    "        sentence = self.data.sentence[index]\n",
    "        word_labels = self.data.word_labels[index]\n",
    "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
    "\n",
    "        # step 2: add special tokens (and corresponding labels)\n",
    "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n",
    "        labels.insert(0, \"O\") # add outside label for [CLS] token\n",
    "        labels.insert(-1, \"O\") # add outside label for [SEP] token\n",
    "\n",
    "        # step 3: truncating/padding\n",
    "        maxlen = self.max_len\n",
    "\n",
    "        if (len(tokenized_sentence) > maxlen):\n",
    "          # truncate\n",
    "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
    "          labels = labels[:maxlen]\n",
    "        else:\n",
    "          # pad\n",
    "          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n",
    "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
    "\n",
    "        # step 4: obtain the attention mask\n",
    "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
    "\n",
    "        # step 5: convert tokens to input ids\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "\n",
    "        label_ids = [label2id[label] for label in labels]\n",
    "        # the following line is deprecated\n",
    "        #label_ids = [label if label != 0 else -100 for label in label_ids]\n",
    "\n",
    "        return {\n",
    "              'ids': torch.tensor(ids, dtype=torch.long),\n",
    "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
    "              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# define training size 80/20 split\n",
    "\n",
    "train_size = 0.8\n",
    "train_dataset = data.sample(frac=train_size,random_state=200)\n",
    "test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(data.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)\n",
    "\n",
    "# pyTorch dataloaders\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased',\n",
    "                                                   num_labels=len(id2label),\n",
    "                                                   id2label=id2label,\n",
    "                                                   label2id=label2id)\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "ids = training_set[0][\"ids\"].unsqueeze(0)\n",
    "mask = training_set[0][\"mask\"].unsqueeze(0)\n",
    "targets = training_set[0][\"targets\"].unsqueeze(0)\n",
    "ids = ids.to(device)\n",
    "mask = mask.to(device)\n",
    "targets = targets.to(device)\n",
    "outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "initial_loss = outputs[0]\n",
    "initial_loss\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7c632e",
   "metadata": {
    "id": "7d7c632e"
   },
   "source": [
    "# Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9efa25b",
   "metadata": {
    "id": "f9efa25b"
   },
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the bert model\n",
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "\n",
    "    for idx, batch in enumerate(training_loader):\n",
    "\n",
    "        ids = batch['ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['mask'].to(device, dtype = torch.long)\n",
    "        targets = batch['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "        loss, tr_logits = outputs.loss, outputs.logits\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets.size(0)\n",
    "\n",
    "        if idx % 100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
    "\n",
    "        # compute training accuracy\n",
    "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "        tr_preds.extend(predictions)\n",
    "        tr_labels.extend(targets)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n",
    "    print(f\"Training steps: {nb_tr_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d46c2d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d46c2d4",
    "outputId": "15d6d009-799f-42b6-95de-e7f0101732f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Training loss per 100 training steps: 2.3791043758392334\n",
      "Training loss per 100 training steps: 0.2129646395102586\n",
      "Training loss per 100 training steps: 0.14433084238339122\n",
      "Training loss per 100 training steps: 0.11512936255739896\n",
      "Training loss per 100 training steps: 0.09988697412530624\n",
      "Training loss per 100 training steps: 0.08829879445003387\n",
      "Training loss per 100 training steps: 0.08111471737566064\n",
      "Training loss per 100 training steps: 0.07585982469371527\n",
      "Training loss per 100 training steps: 0.0717737768689803\n",
      "Training loss per 100 training steps: 0.0681112039671202\n",
      "Training loss per 100 training steps: 0.06544883730465437\n",
      "Training loss per 100 training steps: 0.06292880434962878\n",
      "Training loss per 100 training steps: 0.0609583691698689\n",
      "Training loss per 100 training steps: 0.05908632298709923\n",
      "Training loss per 100 training steps: 0.057205480710868534\n",
      "Training loss per 100 training steps: 0.0556674356291607\n",
      "Training loss per 100 training steps: 0.054505845240328483\n",
      "Training loss per 100 training steps: 0.05347795679441271\n",
      "Training loss per 100 training steps: 0.05242549616266298\n",
      "Training loss per 100 training steps: 0.051313569825342874\n",
      "Training loss per 100 training steps: 0.050596323222775866\n",
      "Training loss per 100 training steps: 0.04976854078037638\n",
      "Training loss per 100 training steps: 0.04900058214765062\n",
      "Training loss per 100 training steps: 0.04823224333177203\n",
      "Training loss per 100 training steps: 0.04749377207923011\n",
      "Training loss per 100 training steps: 0.046871841244690654\n",
      "Training loss per 100 training steps: 0.04618273017924558\n",
      "Training loss per 100 training steps: 0.04558583272083611\n",
      "Training loss per 100 training steps: 0.04510022982757268\n",
      "Training loss per 100 training steps: 0.04466694952550367\n",
      "Training loss per 100 training steps: 0.04423440158919674\n",
      "Training loss per 100 training steps: 0.04382812835795735\n",
      "Training loss per 100 training steps: 0.04351565178057087\n",
      "Training loss per 100 training steps: 0.04316806931550864\n",
      "Training loss per 100 training steps: 0.04276585796774448\n",
      "Training loss per 100 training steps: 0.042433825637198525\n",
      "Training loss per 100 training steps: 0.04217729609482366\n",
      "Training loss per 100 training steps: 0.04190420751894197\n",
      "Training loss per 100 training steps: 0.04171125404644686\n",
      "Training loss per 100 training steps: 0.04165940785848023\n",
      "Training loss per 100 training steps: 0.041358101187148805\n",
      "Training loss per 100 training steps: 0.041077950411128224\n",
      "Training loss per 100 training steps: 0.040813826699628956\n",
      "Training loss per 100 training steps: 0.04057216294740023\n",
      "Training loss per 100 training steps: 0.04047997886439392\n",
      "Training loss per 100 training steps: 0.040259513345324745\n",
      "Training loss per 100 training steps: 0.04001876105699459\n",
      "Training loss per 100 training steps: 0.03982579233112594\n",
      "Training loss per 100 training steps: 0.03964097247679119\n",
      "Training loss per 100 training steps: 0.039340905212597865\n",
      "Training loss per 100 training steps: 0.03909352791345743\n",
      "Training loss per 100 training steps: 0.0390241650589653\n",
      "Training loss per 100 training steps: 0.03880006325999378\n",
      "Training loss per 100 training steps: 0.03854944203562338\n",
      "Training loss per 100 training steps: 0.03839711972923811\n",
      "Training loss per 100 training steps: 0.038302619374584596\n",
      "Training loss per 100 training steps: 0.03824477905372869\n",
      "Training loss per 100 training steps: 0.038052087268864526\n",
      "Training loss per 100 training steps: 0.03800661371846158\n",
      "Training loss per 100 training steps: 0.0379226063289157\n",
      "Training loss per 100 training steps: 0.03781657694591396\n",
      "Training loss per 100 training steps: 0.03771855690095129\n",
      "Training loss per 100 training steps: 0.03762887680451224\n",
      "Training loss per 100 training steps: 0.03753860825222447\n",
      "Training loss per 100 training steps: 0.03736245320210714\n",
      "Training loss per 100 training steps: 0.03721924035517392\n",
      "Training loss per 100 training steps: 0.03707430893650209\n",
      "Training loss per 100 training steps: 0.03697202602090133\n",
      "Training loss per 100 training steps: 0.036885317681227804\n",
      "Training loss per 100 training steps: 0.03673557733068592\n",
      "Training loss per 100 training steps: 0.03664531072968972\n",
      "Training loss per 100 training steps: 0.036537892961665625\n",
      "Training loss per 100 training steps: 0.03649044872212866\n",
      "Training loss per 100 training steps: 0.036350375468641026\n",
      "Training loss per 100 training steps: 0.03626615569194999\n",
      "Training loss per 100 training steps: 0.03611706449207824\n",
      "Training loss per 100 training steps: 0.0360850268070476\n",
      "Training loss per 100 training steps: 0.035967176267643286\n",
      "Training loss per 100 training steps: 0.03587906941289647\n",
      "Training loss per 100 training steps: 0.035757880459951104\n",
      "Training loss per 100 training steps: 0.03567879673196696\n",
      "Training loss per 100 training steps: 0.03559275019804646\n",
      "Training loss per 100 training steps: 0.035482030082387914\n",
      "Training loss per 100 training steps: 0.03533091706866777\n",
      "Training loss per 100 training steps: 0.03518661430721199\n",
      "Training loss per 100 training steps: 0.035062141231408994\n",
      "Training loss per 100 training steps: 0.034972281563301684\n",
      "Training loss per 100 training steps: 0.034888599459510235\n",
      "Training loss per 100 training steps: 0.034759958462342685\n",
      "Training loss per 100 training steps: 0.03465845741005108\n",
      "Training loss per 100 training steps: 0.03457501323123968\n",
      "Training loss per 100 training steps: 0.03452147629608014\n",
      "Training loss per 100 training steps: 0.034426671804838736\n",
      "Training loss per 100 training steps: 0.0343413609811117\n",
      "Training loss per 100 training steps: 0.03429471086040043\n",
      "Training loss per 100 training steps: 0.0342546617253614\n",
      "Training loss epoch: 0.034241136768724215\n",
      "Training accuracy epoch: 0.9520615753470021\n",
      "Training steps: 9515\n",
      "CPU times: user 18min 52s, sys: 5.25 s, total: 18min 57s\n",
      "Wall time: 19min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9e097d3",
   "metadata": {
    "id": "b9e097d3"
   },
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "\n",
    "            ids = batch['ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['mask'].to(device, dtype = torch.long)\n",
    "            targets = batch['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "            loss, eval_logits = outputs.loss, outputs.logits\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += targets.size(0)\n",
    "\n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "\n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "            eval_labels.extend(targets)\n",
    "            eval_preds.extend(predictions)\n",
    "\n",
    "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    #print(eval_labels)\n",
    "    #print(eval_preds)\n",
    "\n",
    "    labels = [id2label[id.item()] for id in eval_labels]\n",
    "    predictions = [id2label[id.item()] for id in eval_preds]\n",
    "\n",
    "    #print(labels)\n",
    "    #print(predictions)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "    print(f\"Validation steps: {nb_eval_steps}\")\n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a520c16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a520c16",
    "outputId": "0799f92b-0e77-4ddb-db35-3ed1a03d70f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.03979242220520973\n",
      "Validation loss per 100 evaluation steps: 0.03212601878577417\n",
      "Validation loss per 100 evaluation steps: 0.029379939633027468\n",
      "Validation loss per 100 evaluation steps: 0.028973091289126358\n",
      "Validation loss per 100 evaluation steps: 0.029023670108433732\n",
      "Validation loss per 100 evaluation steps: 0.02897917770747017\n",
      "Validation loss per 100 evaluation steps: 0.027953680077425422\n",
      "Validation loss per 100 evaluation steps: 0.027798135196521065\n",
      "Validation loss per 100 evaluation steps: 0.027602699063624576\n",
      "Validation loss per 100 evaluation steps: 0.027647424314782093\n",
      "Validation loss per 100 evaluation steps: 0.027412535782568956\n",
      "Validation loss per 100 evaluation steps: 0.027425056389659284\n",
      "Validation loss per 100 evaluation steps: 0.027297009527109846\n",
      "Validation loss per 100 evaluation steps: 0.027296470642174137\n",
      "Validation loss per 100 evaluation steps: 0.027095979226039936\n",
      "Validation loss per 100 evaluation steps: 0.02704196127905974\n",
      "Validation loss per 100 evaluation steps: 0.026959575102675163\n",
      "Validation loss per 100 evaluation steps: 0.02715155494168326\n",
      "Validation loss per 100 evaluation steps: 0.027126873873889682\n",
      "Validation loss per 100 evaluation steps: 0.02694258073431388\n",
      "Validation loss per 100 evaluation steps: 0.02698655309013875\n",
      "Validation loss per 100 evaluation steps: 0.027073269839557892\n",
      "Validation loss per 100 evaluation steps: 0.02708853964979408\n",
      "Validation loss per 100 evaluation steps: 0.02691723243903207\n",
      "Validation loss per 100 evaluation steps: 0.026801741853182563\n",
      "Validation loss per 100 evaluation steps: 0.026821194425022565\n",
      "Validation loss per 100 evaluation steps: 0.026871139869351232\n",
      "Validation loss per 100 evaluation steps: 0.02654300004946572\n",
      "Validation loss per 100 evaluation steps: 0.026448417534938946\n",
      "Validation loss per 100 evaluation steps: 0.026540787380071436\n",
      "Validation loss per 100 evaluation steps: 0.02670313893605201\n",
      "Validation loss per 100 evaluation steps: 0.026789477291336742\n",
      "Validation loss per 100 evaluation steps: 0.026711604241475\n",
      "Validation loss per 100 evaluation steps: 0.02649985977723149\n",
      "Validation loss per 100 evaluation steps: 0.026293907581223876\n",
      "Validation loss per 100 evaluation steps: 0.02623395876095303\n",
      "Validation loss per 100 evaluation steps: 0.026271173032183518\n",
      "Validation loss per 100 evaluation steps: 0.026137673488998592\n",
      "Validation loss per 100 evaluation steps: 0.0261836282324315\n",
      "Validation loss per 100 evaluation steps: 0.026188623075088752\n",
      "Validation loss per 100 evaluation steps: 0.02601358374272121\n",
      "Validation loss per 100 evaluation steps: 0.025958589841617998\n",
      "Validation loss per 100 evaluation steps: 0.025996750222325136\n",
      "Validation loss per 100 evaluation steps: 0.025927777135217742\n",
      "Validation loss per 100 evaluation steps: 0.025971972501907137\n",
      "Validation loss per 100 evaluation steps: 0.025885283457824263\n",
      "Validation loss per 100 evaluation steps: 0.025920254741903386\n",
      "Validation loss per 100 evaluation steps: 0.025898897040236967\n",
      "Validation Loss: 0.02592030772141638\n",
      "Validation Accuracy: 0.9623902689215783\n",
      "Validation steps: 4757\n",
      "CPU times: user 1min 39s, sys: 339 ms, total: 1min 39s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "labels, predictions = valid(model, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92b96e45",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92b96e45",
    "outputId": "7c82a701-229e-4db9-d876-7ab9363b6d6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         geo       0.85      0.84      0.85     11232\n",
      "         gpe       0.93      0.93      0.93      3293\n",
      "         org       0.66      0.67      0.66      6531\n",
      "         per       0.70      0.81      0.75      5196\n",
      "         tim       0.87      0.82      0.84      4360\n",
      "\n",
      "   micro avg       0.79      0.81      0.80     30612\n",
      "   macro avg       0.80      0.81      0.81     30612\n",
      "weighted avg       0.80      0.81      0.80     30612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "print(classification_report([labels], [predictions]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffdcb88c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffdcb88c",
    "outputId": "39a3bc54-7f5c-4770-a09d-03d20c5e08e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9604703438378376, 0.9604703438378376, 0.9604703438378376, None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(labels, predictions, average = \"micro\", zero_division = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e94b9fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2e94b9fc",
    "outputId": "9089d421-8887-46c0-f750-5e1953043ffc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8484082549164551, 0.7703515830070448, 0.7964413014184355, None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(labels, predictions, average = \"macro\", zero_division = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11b33051",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11b33051",
    "outputId": "d9d5fd2d-4eef-49cb-f1ae-b3377844c500"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9599868706792414, 0.9604703438378376, 0.9599303898729689, None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(labels, predictions, average = \"weighted\", zero_division = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9910bf9f",
   "metadata": {
    "id": "9910bf9f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "955c0d17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "955c0d17",
    "outputId": "a5d9135c-532b-4563-91d7-9efa0d09fbec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india has a capital called mumbai . on wednesday , the president will give a presentation\n",
      "['B-geo', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"India has a capital called Mumbai. On Wednesday, the president will give a presentation\"\n",
    "\n",
    "inputs = tokenizer(sentence, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "\n",
    "# move to gpu\n",
    "ids = inputs[\"input_ids\"].to(device)\n",
    "mask = inputs[\"attention_mask\"].to(device)\n",
    "# forward pass\n",
    "outputs = model(ids, mask)\n",
    "logits = outputs[0]\n",
    "\n",
    "active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "token_predictions = [id2label[i] for i in flattened_predictions.cpu().numpy()]\n",
    "wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
    "\n",
    "word_level_predictions = []\n",
    "for pair in wp_preds:\n",
    "  if (pair[0].startswith(\" ##\")) or (pair[0] in ['[CLS]', '[SEP]', '[PAD]']):\n",
    "    # skip prediction\n",
    "    continue\n",
    "  else:\n",
    "    word_level_predictions.append(pair[1])\n",
    "\n",
    "# we join tokens, if they are not special ones\n",
    "str_rep = \" \".join([t[0] for t in wp_preds if t[0] not in ['[CLS]', '[SEP]', '[PAD]']]).replace(\" ##\", \"\")\n",
    "print(str_rep)\n",
    "print(word_level_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e09cf3f",
   "metadata": {
    "id": "6e09cf3f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a28eb627",
   "metadata": {
    "id": "a28eb627"
   },
   "outputs": [],
   "source": [
    "# L1 loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d582914",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d582914",
    "outputId": "37b4174d-dc23-4104-877a-7975db1514b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (47571, 2)\n",
      "TRAIN Dataset: (38057, 2)\n",
      "TEST Dataset: (9514, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 5e-05\n",
    "MAX_GRAD_NORM = 10\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# tokenization\n",
    "\n",
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Word piece tokenization makes it difficult to match word labels\n",
    "    back up with individual word pieces. This function tokenizes each\n",
    "    word one at a time so that it is easier to preserve the correct\n",
    "    label for each subword. It is, of course, a bit slower in processing\n",
    "    time, but it will help our model achieve higher accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels\n",
    "\n",
    "# Bert imput\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # step 1: tokenize (and adapt corresponding labels)\n",
    "        sentence = self.data.sentence[index]\n",
    "        word_labels = self.data.word_labels[index]\n",
    "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
    "\n",
    "        # step 2: add special tokens (and corresponding labels)\n",
    "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n",
    "        labels.insert(0, \"O\") # add outside label for [CLS] token\n",
    "        labels.insert(-1, \"O\") # add outside label for [SEP] token\n",
    "\n",
    "        # step 3: truncating/padding\n",
    "        maxlen = self.max_len\n",
    "\n",
    "        if (len(tokenized_sentence) > maxlen):\n",
    "          # truncate\n",
    "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
    "          labels = labels[:maxlen]\n",
    "        else:\n",
    "          # pad\n",
    "          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n",
    "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
    "\n",
    "        # step 4: obtain the attention mask\n",
    "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
    "\n",
    "        # step 5: convert tokens to input ids\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "\n",
    "        label_ids = [label2id[label] for label in labels]\n",
    "        # the following line is deprecated\n",
    "        #label_ids = [label if label != 0 else -100 for label in label_ids]\n",
    "\n",
    "        return {\n",
    "              'ids': torch.tensor(ids, dtype=torch.long),\n",
    "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
    "              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# define training size 80/20 split\n",
    "\n",
    "train_size = 0.8\n",
    "train_dataset = data.sample(frac=train_size,random_state=200)\n",
    "test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(data.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)\n",
    "\n",
    "# pyTorch dataloaders\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased',\n",
    "                                                   num_labels=len(id2label),\n",
    "                                                   id2label=id2label,\n",
    "                                                   label2id=label2id)\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "ids = training_set[0][\"ids\"].unsqueeze(0)\n",
    "mask = training_set[0][\"mask\"].unsqueeze(0)\n",
    "targets = training_set[0][\"targets\"].unsqueeze(0)\n",
    "ids = ids.to(device)\n",
    "mask = mask.to(device)\n",
    "targets = targets.to(device)\n",
    "outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "initial_loss = outputs[0]\n",
    "initial_loss\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66fefe2b",
   "metadata": {
    "id": "66fefe2b"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(targets, num_classes):\n",
    "    \"\"\"\n",
    "    One-hot encode a tensor of class indices.\n",
    "\n",
    "    Arguments:\n",
    "    - targets (torch.Tensor): A tensor of class indices, shape [N].\n",
    "    - num_classes (int): The number of classes.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: The one-hot encoded tensor, shape [N, num_classes].\n",
    "    \"\"\"\n",
    "    return F.one_hot(targets, num_classes=num_classes).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4db7e09f",
   "metadata": {
    "id": "4db7e09f"
   },
   "outputs": [],
   "source": [
    "l1_loss_fn = nn.L1Loss()\n",
    "\n",
    "# Defining the training function on the 80% of the dataset for tuning the bert model\n",
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "\n",
    "    for idx, batch in enumerate(training_loader):\n",
    "\n",
    "        ids = batch['ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['mask'].to(device, dtype = torch.long)\n",
    "        targets = batch['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "        loss, tr_logits = outputs.loss, outputs.logits\n",
    "\n",
    "        # implement l1 loss function\n",
    "        flatten_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "        flatten_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "\n",
    "        targets_one_hot = one_hot_encode(flatten_targets, num_classes= flatten_logits.size(-1))\n",
    "        loss = l1_loss_fn(flatten_logits, targets_one_hot)\n",
    "\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets.size(0)\n",
    "\n",
    "        if idx % 100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training l1 loss per 100 training steps: {loss_step}\")\n",
    "\n",
    "        # compute training accuracy\n",
    "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "        tr_preds.extend(predictions)\n",
    "        tr_labels.extend(targets)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n",
    "    print(f\"Training steps: {nb_tr_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9792cf7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9792cf7",
    "outputId": "a22509ed-1cb2-43f2-e5dc-e3502aef060d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Training l1 loss per 100 training steps: 0.2394075095653534\n",
      "Training l1 loss per 100 training steps: 0.04623041538023712\n",
      "Training l1 loss per 100 training steps: 0.03725019235404866\n",
      "Training l1 loss per 100 training steps: 0.033315364085756666\n",
      "Training l1 loss per 100 training steps: 0.030939670261042076\n",
      "Training l1 loss per 100 training steps: 0.02933197402341399\n",
      "Training l1 loss per 100 training steps: 0.027763065651133532\n",
      "Training l1 loss per 100 training steps: 0.02640066592663144\n",
      "Training l1 loss per 100 training steps: 0.0252586757896768\n",
      "Training l1 loss per 100 training steps: 0.02435843565694566\n",
      "Training l1 loss per 100 training steps: 0.023551771031147414\n",
      "Training l1 loss per 100 training steps: 0.02285645228660886\n",
      "Training l1 loss per 100 training steps: 0.022241896275837256\n",
      "Training l1 loss per 100 training steps: 0.021645176812124474\n",
      "Training l1 loss per 100 training steps: 0.02111138085130706\n",
      "Training l1 loss per 100 training steps: 0.020664297911071085\n",
      "Training l1 loss per 100 training steps: 0.02024903062319175\n",
      "Training l1 loss per 100 training steps: 0.01981153265616338\n",
      "Training l1 loss per 100 training steps: 0.019452017237603467\n",
      "Training l1 loss per 100 training steps: 0.019094541742868607\n",
      "Training l1 loss per 100 training steps: 0.018752066304498556\n",
      "Training l1 loss per 100 training steps: 0.018416343632729096\n",
      "Training l1 loss per 100 training steps: 0.018108219727078306\n",
      "Training l1 loss per 100 training steps: 0.017827240257211895\n",
      "Training l1 loss per 100 training steps: 0.01754516168384627\n",
      "Training l1 loss per 100 training steps: 0.01726939660965836\n",
      "Training l1 loss per 100 training steps: 0.017011973804585502\n",
      "Training l1 loss per 100 training steps: 0.016761913038762642\n",
      "Training l1 loss per 100 training steps: 0.01651909948675436\n",
      "Training l1 loss per 100 training steps: 0.016274954868119078\n",
      "Training l1 loss per 100 training steps: 0.016045283744504545\n",
      "Training l1 loss per 100 training steps: 0.01581869120654037\n",
      "Training l1 loss per 100 training steps: 0.01559759029331281\n",
      "Training l1 loss per 100 training steps: 0.015383393256576611\n",
      "Training l1 loss per 100 training steps: 0.015172196658132655\n",
      "Training l1 loss per 100 training steps: 0.014974533606712196\n",
      "Training l1 loss per 100 training steps: 0.014781768951653996\n",
      "Training l1 loss per 100 training steps: 0.01459740352032012\n",
      "Training l1 loss per 100 training steps: 0.014422598093506992\n",
      "Training l1 loss per 100 training steps: 0.014240315653597643\n",
      "Training l1 loss per 100 training steps: 0.014072984545003978\n",
      "Training l1 loss per 100 training steps: 0.013908077732534291\n",
      "Training l1 loss per 100 training steps: 0.013748536259290633\n",
      "Training l1 loss per 100 training steps: 0.013592117674340414\n",
      "Training l1 loss per 100 training steps: 0.013452015593275513\n",
      "Training l1 loss per 100 training steps: 0.013308897712866953\n",
      "Training l1 loss per 100 training steps: 0.013162699341612053\n",
      "Training l1 loss per 100 training steps: 0.013027684263238385\n",
      "Training l1 loss per 100 training steps: 0.012895774322728164\n",
      "Training l1 loss per 100 training steps: 0.012767288365534916\n",
      "Training l1 loss per 100 training steps: 0.012646053573930473\n",
      "Training l1 loss per 100 training steps: 0.012534845069377966\n",
      "Training l1 loss per 100 training steps: 0.012420861668085403\n",
      "Training l1 loss per 100 training steps: 0.012312321217494869\n",
      "Training l1 loss per 100 training steps: 0.01220214378569462\n",
      "Training l1 loss per 100 training steps: 0.012093923144745767\n",
      "Training l1 loss per 100 training steps: 0.011983324640712018\n",
      "Training l1 loss per 100 training steps: 0.01188224270364632\n",
      "Training l1 loss per 100 training steps: 0.011779259774203616\n",
      "Training l1 loss per 100 training steps: 0.011675041722616068\n",
      "Training l1 loss per 100 training steps: 0.011582387752690996\n",
      "Training l1 loss per 100 training steps: 0.011494987081202659\n",
      "Training l1 loss per 100 training steps: 0.01140491860789388\n",
      "Training l1 loss per 100 training steps: 0.011318083106185558\n",
      "Training l1 loss per 100 training steps: 0.011230339739853457\n",
      "Training l1 loss per 100 training steps: 0.011143805037102614\n",
      "Training l1 loss per 100 training steps: 0.01105924031162913\n",
      "Training l1 loss per 100 training steps: 0.010979407219037774\n",
      "Training l1 loss per 100 training steps: 0.01089717682430195\n",
      "Training l1 loss per 100 training steps: 0.01081805785481195\n",
      "Training l1 loss per 100 training steps: 0.01074392792216448\n",
      "Training l1 loss per 100 training steps: 0.010668516242209077\n",
      "Training l1 loss per 100 training steps: 0.01059514022979473\n",
      "Training l1 loss per 100 training steps: 0.010526239589558792\n",
      "Training l1 loss per 100 training steps: 0.010457665330013483\n",
      "Training l1 loss per 100 training steps: 0.010389569773854123\n",
      "Training l1 loss per 100 training steps: 0.010321193144619592\n",
      "Training l1 loss per 100 training steps: 0.010252056581508419\n",
      "Training l1 loss per 100 training steps: 0.010187519044008733\n",
      "Training l1 loss per 100 training steps: 0.010122135173059776\n",
      "Training l1 loss per 100 training steps: 0.010058381766670543\n",
      "Training l1 loss per 100 training steps: 0.00999669110366941\n",
      "Training l1 loss per 100 training steps: 0.009938487856549041\n",
      "Training l1 loss per 100 training steps: 0.00987621816281876\n",
      "Training l1 loss per 100 training steps: 0.009817824455594704\n",
      "Training l1 loss per 100 training steps: 0.009758838711139053\n",
      "Training l1 loss per 100 training steps: 0.009700791482744248\n",
      "Training l1 loss per 100 training steps: 0.009647534438170756\n",
      "Training l1 loss per 100 training steps: 0.00958946404271014\n",
      "Training l1 loss per 100 training steps: 0.009531753295700867\n",
      "Training l1 loss per 100 training steps: 0.009478712450997873\n",
      "Training l1 loss per 100 training steps: 0.009425094124916628\n",
      "Training l1 loss per 100 training steps: 0.00936997713207696\n",
      "Training l1 loss per 100 training steps: 0.009319537959746263\n",
      "Training l1 loss per 100 training steps: 0.009268930988372306\n",
      "Training l1 loss per 100 training steps: 0.009219441023920291\n",
      "Training loss epoch: 0.009212713919154696\n",
      "Training accuracy epoch: 0.9082592749071567\n",
      "Training steps: 9515\n",
      "CPU times: user 19min 7s, sys: 3.82 s, total: 19min 11s\n",
      "Wall time: 19min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e363015",
   "metadata": {
    "id": "0e363015"
   },
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "\n",
    "            ids = batch['ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['mask'].to(device, dtype = torch.long)\n",
    "            targets = batch['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "            loss, eval_logits = outputs.loss, outputs.logits\n",
    "\n",
    "\n",
    "            # implement l1 loss\n",
    "            flatten_targets = targets.view(-1)\n",
    "            flatten_logits = eval_logits.view(-1, model.num_labels)\n",
    "            targets_one_hot = one_hot_encode(flatten_targets, num_classes= flatten_logits.size(-1))\n",
    "            loss = l1_loss_fn(flatten_logits, targets_one_hot)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += targets.size(0)\n",
    "\n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation l1 loss per 100 evaluation steps: {loss_step}\")\n",
    "\n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "            eval_labels.extend(targets)\n",
    "            eval_preds.extend(predictions)\n",
    "\n",
    "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    #print(eval_labels)\n",
    "    #print(eval_preds)\n",
    "\n",
    "    labels = [id2label[id.item()] for id in eval_labels]\n",
    "    predictions = [id2label[id.item()] for id in eval_preds]\n",
    "\n",
    "    #print(labels)\n",
    "    #print(predictions)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "    print(f\"Validation steps: {nb_eval_steps}\")\n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f817764b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f817764b",
    "outputId": "4c9b2510-dc53-4cb3-c95c-40088294646d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation l1 loss per 100 evaluation steps: 0.0027898570988327265\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031827386494006705\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031562807711434378\n",
      "Validation l1 loss per 100 evaluation steps: 0.003085373146645106\n",
      "Validation l1 loss per 100 evaluation steps: 0.003105709006565105\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031740496555731786\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031640029612296384\n",
      "Validation l1 loss per 100 evaluation steps: 0.003182253178980297\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031241659585446975\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031577659742808495\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031438690183296257\n",
      "Validation l1 loss per 100 evaluation steps: 0.003164293875513458\n",
      "Validation l1 loss per 100 evaluation steps: 0.003124356911090016\n",
      "Validation l1 loss per 100 evaluation steps: 0.00315158203539319\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031596570385242417\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031564830490440716\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031387834428116762\n",
      "Validation l1 loss per 100 evaluation steps: 0.003141319031607051\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031439568038134193\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031567265676787177\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031648717697247628\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031754101651176804\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031625364016860417\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031597685713506346\n",
      "Validation l1 loss per 100 evaluation steps: 0.003148535739452807\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031584001364844028\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031596699767874577\n",
      "Validation l1 loss per 100 evaluation steps: 0.003154904042285795\n",
      "Validation l1 loss per 100 evaluation steps: 0.00315291401017035\n",
      "Validation l1 loss per 100 evaluation steps: 0.003155904384942289\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031433922635647074\n",
      "Validation l1 loss per 100 evaluation steps: 0.003148005873398685\n",
      "Validation l1 loss per 100 evaluation steps: 0.003174150101467356\n",
      "Validation l1 loss per 100 evaluation steps: 0.003173821198482318\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031922368015742087\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031765886585825884\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031748556970050716\n",
      "Validation l1 loss per 100 evaluation steps: 0.003168076552844852\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031620223246726985\n",
      "Validation l1 loss per 100 evaluation steps: 0.003157728673264497\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031602458371042925\n",
      "Validation l1 loss per 100 evaluation steps: 0.003165337169811223\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031637535532053015\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031617752496578395\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031565002975060645\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031595401236036544\n",
      "Validation l1 loss per 100 evaluation steps: 0.0031602925342674453\n",
      "Validation l1 loss per 100 evaluation steps: 0.00316729977090701\n",
      "Validation Loss: 0.0031671869052990655\n",
      "Validation Accuracy: 0.9257968342555514\n",
      "Validation steps: 4757\n",
      "CPU times: user 1min 39s, sys: 326 ms, total: 1min 39s\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "labels, predictions = valid(model, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "889fd862",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "889fd862",
    "outputId": "fb4f5165-73a1-4426-821a-3119c63e67ae",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         geo       0.75      0.76      0.76     11232\n",
      "         gpe       0.95      0.88      0.91      3293\n",
      "         org       0.00      0.00      0.00      6531\n",
      "         per       0.72      0.76      0.74      5196\n",
      "         tim       0.76      0.60      0.67      4360\n",
      "\n",
      "   micro avg       0.77      0.59      0.67     30612\n",
      "   macro avg       0.63      0.60      0.61     30612\n",
      "weighted avg       0.61      0.59      0.60     30612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "print(classification_report([labels], [predictions]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3abf94dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3abf94dd",
    "outputId": "481a63e6-cc3b-47f2-cbad-0235c09ca7bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.922403574359442, 0.922403574359442, 0.922403574359442, None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(labels, predictions, average = \"micro\", zero_division = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "36819f3d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36819f3d",
    "outputId": "ddeabde7-3c11-4ee0-e55f-70f66909c328"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9330834438118937, 0.46807499480688525, 0.471961161165302, None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(labels, predictions, average = \"macro\", zero_division = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "02efda06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02efda06",
    "outputId": "7ffca160-239e-429c-c9a5-e889cfac63c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9264408891077844, 0.922403574359442, 0.8957137182367844, None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(labels, predictions, average = \"weighted\", zero_division = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "79a50260",
   "metadata": {
    "id": "79a50260"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd307163",
   "metadata": {
    "id": "bd307163"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb278f56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb278f56",
    "outputId": "4e759651-faa0-4498-9df4-2d81061e4e6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india has a capital called mumbai . on wednesday , the president will give a presentation\n",
      "['B-geo', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"India has a capital called Mumbai. On Wednesday, the president will give a presentation\"\n",
    "\n",
    "inputs = tokenizer(sentence, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "\n",
    "# move to gpu\n",
    "ids = inputs[\"input_ids\"].to(device)\n",
    "mask = inputs[\"attention_mask\"].to(device)\n",
    "# forward pass\n",
    "outputs = model(ids, mask)\n",
    "logits = outputs[0]\n",
    "\n",
    "active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "token_predictions = [id2label[i] for i in flattened_predictions.cpu().numpy()]\n",
    "wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
    "\n",
    "word_level_predictions = []\n",
    "for pair in wp_preds:\n",
    "  if (pair[0].startswith(\" ##\")) or (pair[0] in ['[CLS]', '[SEP]', '[PAD]']):\n",
    "    # skip prediction\n",
    "    continue\n",
    "  else:\n",
    "    word_level_predictions.append(pair[1])\n",
    "\n",
    "# we join tokens, if they are not special ones\n",
    "str_rep = \" \".join([t[0] for t in wp_preds if t[0] not in ['[CLS]', '[SEP]', '[PAD]']]).replace(\" ##\", \"\")\n",
    "print(str_rep)\n",
    "print(word_level_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b5659cba",
   "metadata": {
    "id": "b5659cba"
   },
   "outputs": [],
   "source": [
    "#Dlite loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "be6377da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "be6377da",
    "outputId": "647351ab-71bd-42e4-f1b9-1f20d29a07cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (47571, 2)\n",
      "TRAIN Dataset: (38057, 2)\n",
      "TEST Dataset: (9514, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 5e-05\n",
    "MAX_GRAD_NORM = 10\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# tokenization\n",
    "\n",
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Word piece tokenization makes it difficult to match word labels\n",
    "    back up with individual word pieces. This function tokenizes each\n",
    "    word one at a time so that it is easier to preserve the correct\n",
    "    label for each subword. It is, of course, a bit slower in processing\n",
    "    time, but it will help our model achieve higher accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels\n",
    "\n",
    "# Bert imput\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # step 1: tokenize (and adapt corresponding labels)\n",
    "        sentence = self.data.sentence[index]\n",
    "        word_labels = self.data.word_labels[index]\n",
    "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
    "\n",
    "        # step 2: add special tokens (and corresponding labels)\n",
    "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n",
    "        labels.insert(0, \"O\") # add outside label for [CLS] token\n",
    "        labels.insert(-1, \"O\") # add outside label for [SEP] token\n",
    "\n",
    "        # step 3: truncating/padding\n",
    "        maxlen = self.max_len\n",
    "\n",
    "        if (len(tokenized_sentence) > maxlen):\n",
    "          # truncate\n",
    "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
    "          labels = labels[:maxlen]\n",
    "        else:\n",
    "          # pad\n",
    "          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n",
    "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
    "\n",
    "        # step 4: obtain the attention mask\n",
    "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
    "\n",
    "        # step 5: convert tokens to input ids\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "\n",
    "        label_ids = [label2id[label] for label in labels]\n",
    "        # the following line is deprecated\n",
    "        #label_ids = [label if label != 0 else -100 for label in label_ids]\n",
    "\n",
    "        return {\n",
    "              'ids': torch.tensor(ids, dtype=torch.long),\n",
    "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
    "              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# define training size 80/20 split\n",
    "\n",
    "train_size = 0.8\n",
    "train_dataset = data.sample(frac=train_size,random_state=200)\n",
    "test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(data.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)\n",
    "\n",
    "# pyTorch dataloaders\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased',\n",
    "                                                   num_labels=len(id2label),\n",
    "                                                   id2label=id2label,\n",
    "                                                   label2id=label2id)\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "ids = training_set[0][\"ids\"].unsqueeze(0)\n",
    "mask = training_set[0][\"mask\"].unsqueeze(0)\n",
    "targets = training_set[0][\"targets\"].unsqueeze(0)\n",
    "ids = ids.to(device)\n",
    "mask = mask.to(device)\n",
    "targets = targets.to(device)\n",
    "outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "initial_loss = outputs[0]\n",
    "initial_loss\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9a113a92",
   "metadata": {
    "id": "9a113a92"
   },
   "outputs": [],
   "source": [
    "# DLITE Loss function\n",
    "class DLITELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DLITELoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits, targets, epsilon=1e-10):\n",
    "        # Convert logits to probabilities using softmax\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # One-hot encode the targets to get true probabilities\n",
    "        true_probs = F.one_hot(targets, num_classes=probs.size(-1)).float()\n",
    "\n",
    "        # Define the g function\n",
    "        g_values = torch.abs(probs * (1 - torch.log(probs + epsilon)) - true_probs * (1 - torch.log(true_probs + epsilon)))\n",
    "\n",
    "        # Define the delta_h function\n",
    "        delta_h_values = torch.abs(probs**2 * (1 - 2 * torch.log(probs + epsilon)) - true_probs**2 * (1 - 2 * torch.log(true_probs + epsilon))) / (2 * (probs + true_probs))\n",
    "\n",
    "        # Compute DLITE loss for each class\n",
    "        dl_values = g_values - delta_h_values\n",
    "\n",
    "        # Sum over all classes and average over batch size\n",
    "        loss = dl_values.sum(dim=-1).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56a99bb4",
   "metadata": {
    "id": "56a99bb4"
   },
   "outputs": [],
   "source": [
    "dlite_loss_fn = DLITELoss()\n",
    "\n",
    "# Defining the training function on the 80% of the dataset for tuning the bert model\n",
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    for idx, batch in enumerate(training_loader):\n",
    "\n",
    "        ids = batch['ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['mask'].to(device, dtype = torch.long)\n",
    "        targets = batch['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "        loss, tr_logits = outputs.loss, outputs.logits\n",
    "\n",
    "        # compute training accuracy\n",
    "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "        tr_preds.extend(predictions)\n",
    "        tr_labels.extend(targets)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "\n",
    "        # implement dlite loss function\n",
    "\n",
    "        loss = dlite_loss_fn(active_logits, flattened_targets)\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets.size(0)\n",
    "\n",
    "        if idx % 100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training dlite loss per 100 training steps: {loss_step}\")\n",
    "\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n",
    "    print(f\"Training steps: {nb_tr_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cb554fde",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cb554fde",
    "outputId": "c3a16eca-417a-4daf-e4b3-f53f0e1e11bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Training dlite loss per 100 training steps: 0.6701711416244507\n",
      "Training dlite loss per 100 training steps: 0.05769165834909914\n",
      "Training dlite loss per 100 training steps: 0.047614418416149655\n",
      "Training dlite loss per 100 training steps: 0.04399121047024293\n",
      "Training dlite loss per 100 training steps: 0.04163791797046336\n",
      "Training dlite loss per 100 training steps: 0.040304058108085156\n",
      "Training dlite loss per 100 training steps: 0.039417414897566616\n",
      "Training dlite loss per 100 training steps: 0.038917180273182674\n",
      "Training dlite loss per 100 training steps: 0.03822329767082804\n",
      "Training dlite loss per 100 training steps: 0.03777877508072037\n",
      "Training dlite loss per 100 training steps: 0.0373606435017724\n",
      "Training dlite loss per 100 training steps: 0.03712185569519163\n",
      "Training dlite loss per 100 training steps: 0.03674490858625165\n",
      "Training dlite loss per 100 training steps: 0.03634340749247924\n",
      "Training dlite loss per 100 training steps: 0.03601110813638437\n",
      "Training dlite loss per 100 training steps: 0.035629255405452996\n",
      "Training dlite loss per 100 training steps: 0.03545573038298961\n",
      "Training dlite loss per 100 training steps: 0.03501544169789585\n",
      "Training dlite loss per 100 training steps: 0.03469265453892151\n",
      "Training dlite loss per 100 training steps: 0.03424587509485424\n",
      "Training dlite loss per 100 training steps: 0.03378188819492128\n",
      "Training dlite loss per 100 training steps: 0.03314144662866466\n",
      "Training dlite loss per 100 training steps: 0.03259021873464069\n",
      "Training dlite loss per 100 training steps: 0.03190986176911284\n",
      "Training dlite loss per 100 training steps: 0.031246835445881534\n",
      "Training dlite loss per 100 training steps: 0.030590141249576402\n",
      "Training dlite loss per 100 training steps: 0.029925297077782673\n",
      "Training dlite loss per 100 training steps: 0.029263096560278642\n",
      "Training dlite loss per 100 training steps: 0.028670809258974913\n",
      "Training dlite loss per 100 training steps: 0.028074223561995338\n",
      "Training dlite loss per 100 training steps: 0.027568180161856137\n",
      "Training dlite loss per 100 training steps: 0.027044935966701683\n",
      "Training dlite loss per 100 training steps: 0.026575355214534777\n",
      "Training dlite loss per 100 training steps: 0.026111067271746825\n",
      "Training dlite loss per 100 training steps: 0.025681875232827226\n",
      "Training dlite loss per 100 training steps: 0.02529817974003118\n",
      "Training dlite loss per 100 training steps: 0.024870571862240173\n",
      "Training dlite loss per 100 training steps: 0.024526313194461184\n",
      "Training dlite loss per 100 training steps: 0.0241948981677083\n",
      "Training dlite loss per 100 training steps: 0.023896169778463042\n",
      "Training dlite loss per 100 training steps: 0.023584406362617782\n",
      "Training dlite loss per 100 training steps: 0.023253324158745387\n",
      "Training dlite loss per 100 training steps: 0.022953640194693814\n",
      "Training dlite loss per 100 training steps: 0.02267181456962725\n",
      "Training dlite loss per 100 training steps: 0.022382185413071323\n",
      "Training dlite loss per 100 training steps: 0.022088985076853276\n",
      "Training dlite loss per 100 training steps: 0.02183884077525214\n",
      "Training dlite loss per 100 training steps: 0.021567135109640718\n",
      "Training dlite loss per 100 training steps: 0.02132430710921997\n",
      "Training dlite loss per 100 training steps: 0.02107169268911175\n",
      "Training dlite loss per 100 training steps: 0.020862512728065826\n",
      "Training dlite loss per 100 training steps: 0.020674258236372933\n",
      "Training dlite loss per 100 training steps: 0.020464512890258132\n",
      "Training dlite loss per 100 training steps: 0.02025745820190334\n",
      "Training dlite loss per 100 training steps: 0.020055417290789537\n",
      "Training dlite loss per 100 training steps: 0.019870429109320353\n",
      "Training dlite loss per 100 training steps: 0.019696053826545885\n",
      "Training dlite loss per 100 training steps: 0.019536409238215605\n",
      "Training dlite loss per 100 training steps: 0.01934701182006896\n",
      "Training dlite loss per 100 training steps: 0.01918977028233546\n",
      "Training dlite loss per 100 training steps: 0.01899253901789656\n",
      "Training dlite loss per 100 training steps: 0.0188401834877153\n",
      "Training dlite loss per 100 training steps: 0.018680023277434024\n",
      "Training dlite loss per 100 training steps: 0.01854130688883563\n",
      "Training dlite loss per 100 training steps: 0.018417234967864623\n",
      "Training dlite loss per 100 training steps: 0.018264553186400207\n",
      "Training dlite loss per 100 training steps: 0.01810540933577016\n",
      "Training dlite loss per 100 training steps: 0.017964310274005076\n",
      "Training dlite loss per 100 training steps: 0.01785150078901662\n",
      "Training dlite loss per 100 training steps: 0.017729672530796162\n",
      "Training dlite loss per 100 training steps: 0.017618816044255213\n",
      "Training dlite loss per 100 training steps: 0.017507338492561222\n",
      "Training dlite loss per 100 training steps: 0.0174052222749642\n",
      "Training dlite loss per 100 training steps: 0.017281131952544312\n",
      "Training dlite loss per 100 training steps: 0.017172994657811542\n",
      "Training dlite loss per 100 training steps: 0.01708656256657891\n",
      "Training dlite loss per 100 training steps: 0.016998260662750685\n",
      "Training dlite loss per 100 training steps: 0.01688734965270073\n",
      "Training dlite loss per 100 training steps: 0.016788064924784653\n",
      "Training dlite loss per 100 training steps: 0.016685388546002714\n",
      "Training dlite loss per 100 training steps: 0.01657589188420656\n",
      "Training dlite loss per 100 training steps: 0.016485457743221212\n",
      "Training dlite loss per 100 training steps: 0.016390384703034958\n",
      "Training dlite loss per 100 training steps: 0.016317576986776012\n",
      "Training dlite loss per 100 training steps: 0.01623511058991931\n",
      "Training dlite loss per 100 training steps: 0.016138678619333737\n",
      "Training dlite loss per 100 training steps: 0.01605549316315667\n",
      "Training dlite loss per 100 training steps: 0.01597279313182037\n",
      "Training dlite loss per 100 training steps: 0.015908862574113936\n",
      "Training dlite loss per 100 training steps: 0.0158394564219074\n",
      "Training dlite loss per 100 training steps: 0.015772285226613877\n",
      "Training dlite loss per 100 training steps: 0.01570093307055281\n",
      "Training dlite loss per 100 training steps: 0.01562805020268695\n",
      "Training dlite loss per 100 training steps: 0.015556638898112428\n",
      "Training dlite loss per 100 training steps: 0.015479818617501107\n",
      "Training dlite loss per 100 training steps: 0.015413796832744726\n",
      "Training loss epoch: 0.01540280169418864\n",
      "Training accuracy epoch: 0.9233585362026956\n",
      "Training steps: 9515\n",
      "CPU times: user 19min 7s, sys: 4.07 s, total: 19min 11s\n",
      "Wall time: 19min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fb6661bf",
   "metadata": {
    "id": "fb6661bf"
   },
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "\n",
    "            ids = batch['ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['mask'].to(device, dtype = torch.long)\n",
    "            targets = batch['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "            loss, eval_logits = outputs.loss, outputs.logits\n",
    "\n",
    "\n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "            eval_labels.extend(targets)\n",
    "            eval_preds.extend(predictions)\n",
    "\n",
    "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "            # implement dlite loss\n",
    "            loss = dlite_loss_fn(active_logits, flattened_targets)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += targets.size(0)\n",
    "\n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation dlite loss per 100 evaluation steps: {loss_step}\")\n",
    "\n",
    "    #print(eval_labels)\n",
    "    #print(eval_preds)\n",
    "\n",
    "    labels = [id2label[id.item()] for id in eval_labels]\n",
    "    predictions = [id2label[id.item()] for id in eval_preds]\n",
    "\n",
    "    #print(labels)\n",
    "    #print(predictions)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "    print(f\"Validation steps: {nb_eval_steps}\")\n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "af598b56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "af598b56",
    "outputId": "3988e118-7b22-4c38-c40f-64fbb6edeb80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dlite loss per 100 evaluation steps: 1.2743247680191416e-05\n",
      "Validation dlite loss per 100 evaluation steps: 0.008395245265270917\n",
      "Validation dlite loss per 100 evaluation steps: 0.00930756381088504\n",
      "Validation dlite loss per 100 evaluation steps: 0.009289750550728091\n",
      "Validation dlite loss per 100 evaluation steps: 0.009136076587287562\n",
      "Validation dlite loss per 100 evaluation steps: 0.00898776701236518\n",
      "Validation dlite loss per 100 evaluation steps: 0.008582566701280566\n",
      "Validation dlite loss per 100 evaluation steps: 0.008644724162973709\n",
      "Validation dlite loss per 100 evaluation steps: 0.008348244577333139\n",
      "Validation dlite loss per 100 evaluation steps: 0.008285038917551345\n",
      "Validation dlite loss per 100 evaluation steps: 0.008192941036195993\n",
      "Validation dlite loss per 100 evaluation steps: 0.00841401809703783\n",
      "Validation dlite loss per 100 evaluation steps: 0.008307703983750131\n",
      "Validation dlite loss per 100 evaluation steps: 0.008399318640158613\n",
      "Validation dlite loss per 100 evaluation steps: 0.008411554731690075\n",
      "Validation dlite loss per 100 evaluation steps: 0.008299857050525706\n",
      "Validation dlite loss per 100 evaluation steps: 0.008389197878703116\n",
      "Validation dlite loss per 100 evaluation steps: 0.008409962176949936\n",
      "Validation dlite loss per 100 evaluation steps: 0.008385174384554831\n",
      "Validation dlite loss per 100 evaluation steps: 0.008411410081374296\n",
      "Validation dlite loss per 100 evaluation steps: 0.008467357987325395\n",
      "Validation dlite loss per 100 evaluation steps: 0.0084613139180345\n",
      "Validation dlite loss per 100 evaluation steps: 0.008400582208701237\n",
      "Validation dlite loss per 100 evaluation steps: 0.008390658949515427\n",
      "Validation dlite loss per 100 evaluation steps: 0.008405340377753558\n",
      "Validation dlite loss per 100 evaluation steps: 0.0082984997207201\n",
      "Validation dlite loss per 100 evaluation steps: 0.008298218965271375\n",
      "Validation dlite loss per 100 evaluation steps: 0.00823684981514652\n",
      "Validation dlite loss per 100 evaluation steps: 0.00828745166327097\n",
      "Validation dlite loss per 100 evaluation steps: 0.00829102087200833\n",
      "Validation dlite loss per 100 evaluation steps: 0.008381308243794508\n",
      "Validation dlite loss per 100 evaluation steps: 0.00835951368480833\n",
      "Validation dlite loss per 100 evaluation steps: 0.008365692450804955\n",
      "Validation dlite loss per 100 evaluation steps: 0.008353087939649005\n",
      "Validation dlite loss per 100 evaluation steps: 0.008364476281492308\n",
      "Validation dlite loss per 100 evaluation steps: 0.008358808949882586\n",
      "Validation dlite loss per 100 evaluation steps: 0.008343241830684068\n",
      "Validation dlite loss per 100 evaluation steps: 0.008330248648056567\n",
      "Validation dlite loss per 100 evaluation steps: 0.00830015757983663\n",
      "Validation dlite loss per 100 evaluation steps: 0.008268899837999355\n",
      "Validation dlite loss per 100 evaluation steps: 0.008272571043540159\n",
      "Validation dlite loss per 100 evaluation steps: 0.0083029812402086\n",
      "Validation dlite loss per 100 evaluation steps: 0.008361526547322026\n",
      "Validation dlite loss per 100 evaluation steps: 0.008365640618557395\n",
      "Validation dlite loss per 100 evaluation steps: 0.008364682187139302\n",
      "Validation dlite loss per 100 evaluation steps: 0.00836241316440744\n",
      "Validation dlite loss per 100 evaluation steps: 0.008367121359210373\n",
      "Validation dlite loss per 100 evaluation steps: 0.008365125151173686\n",
      "Validation Loss: 0.008341707666366275\n",
      "Validation Accuracy: 0.9579371076295481\n",
      "Validation steps: 4757\n",
      "CPU times: user 1min 42s, sys: 328 ms, total: 1min 42s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "labels, predictions = valid(model, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0c838bbe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0c838bbe",
    "outputId": "b07886af-fb66-4515-ee40-61837bc66762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         geo       0.82      0.87      0.84     11232\n",
      "         gpe       0.86      0.90      0.88      3293\n",
      "         org       0.71      0.59      0.64      6531\n",
      "         per       0.73      0.80      0.76      5196\n",
      "         tim       0.81      0.78      0.79      4360\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     30612\n",
      "   macro avg       0.79      0.79      0.79     30612\n",
      "weighted avg       0.78      0.79      0.79     30612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "print(classification_report([labels], [predictions]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8c5f8ac0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c5f8ac0",
    "outputId": "52cff9c3-f391-4ad2-cc95-72f8d172e295"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.956022216848173, 0.956022216848173, 0.956022216848173, None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(labels, predictions, average = \"micro\", zero_division = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4803e9d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4803e9d7",
    "outputId": "9427dfcb-5f22-40c3-c332-1d575864f33f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8469900224475623, 0.7034375900553037, 0.723326981483235, None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(labels, predictions, average = \"macro\", zero_division = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f4dc2cb9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4dc2cb9",
    "outputId": "8d1b6d13-66e4-4807-980e-9c9188ebb7f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9544472470413031, 0.956022216848173, 0.9542314119362216, None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(labels, predictions, average = \"weighted\", zero_division = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6007f4f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6007f4f7",
    "outputId": "702ddf95-a505-49c4-a4ba-71207001508a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india has a capital called mumbai . on wednesday , the president will give a presentation\n",
      "['B-geo', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"India has a capital called Mumbai. On Wednesday, the president will give a presentation\"\n",
    "\n",
    "inputs = tokenizer(sentence, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "\n",
    "# move to gpu\n",
    "ids = inputs[\"input_ids\"].to(device)\n",
    "mask = inputs[\"attention_mask\"].to(device)\n",
    "# forward pass\n",
    "outputs = model(ids, mask)\n",
    "logits = outputs[0]\n",
    "\n",
    "active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "token_predictions = [id2label[i] for i in flattened_predictions.cpu().numpy()]\n",
    "wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
    "\n",
    "word_level_predictions = []\n",
    "for pair in wp_preds:\n",
    "  if (pair[0].startswith(\" ##\")) or (pair[0] in ['[CLS]', '[SEP]', '[PAD]']):\n",
    "    # skip prediction\n",
    "    continue\n",
    "  else:\n",
    "    word_level_predictions.append(pair[1])\n",
    "\n",
    "# we join tokens, if they are not special ones\n",
    "str_rep = \" \".join([t[0] for t in wp_preds if t[0] not in ['[CLS]', '[SEP]', '[PAD]']]).replace(\" ##\", \"\")\n",
    "print(str_rep)\n",
    "print(word_level_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3c0812b0",
   "metadata": {
    "id": "3c0812b0"
   },
   "outputs": [],
   "source": [
    "# using preferred function, dlite loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "68a1f32a",
   "metadata": {
    "id": "68a1f32a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e10b7ad3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e10b7ad3",
    "outputId": "51bf5491-f3ea-44ce-e1cc-fb253562fb98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (47571, 2)\n",
      "TRAIN Dataset: (38057, 2)\n",
      "TEST Dataset: (9514, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 5e-05\n",
    "MAX_GRAD_NORM = 10\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# tokenization\n",
    "\n",
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Word piece tokenization makes it difficult to match word labels\n",
    "    back up with individual word pieces. This function tokenizes each\n",
    "    word one at a time so that it is easier to preserve the correct\n",
    "    label for each subword. It is, of course, a bit slower in processing\n",
    "    time, but it will help our model achieve higher accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels\n",
    "\n",
    "# Bert imput\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # step 1: tokenize (and adapt corresponding labels)\n",
    "        sentence = self.data.sentence[index]\n",
    "        word_labels = self.data.word_labels[index]\n",
    "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
    "\n",
    "        # step 2: add special tokens (and corresponding labels)\n",
    "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n",
    "        labels.insert(0, \"O\") # add outside label for [CLS] token\n",
    "        labels.insert(-1, \"O\") # add outside label for [SEP] token\n",
    "\n",
    "        # step 3: truncating/padding\n",
    "        maxlen = self.max_len\n",
    "\n",
    "        if (len(tokenized_sentence) > maxlen):\n",
    "          # truncate\n",
    "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
    "          labels = labels[:maxlen]\n",
    "        else:\n",
    "          # pad\n",
    "          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n",
    "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
    "\n",
    "        # step 4: obtain the attention mask\n",
    "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
    "\n",
    "        # step 5: convert tokens to input ids\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "\n",
    "        label_ids = [label2id[label] for label in labels]\n",
    "        # the following line is deprecated\n",
    "        #label_ids = [label if label != 0 else -100 for label in label_ids]\n",
    "\n",
    "        return {\n",
    "              'ids': torch.tensor(ids, dtype=torch.long),\n",
    "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
    "              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# define training size 80/20 split\n",
    "\n",
    "train_size = 0.8\n",
    "train_dataset = data.sample(frac=train_size,random_state=200)\n",
    "test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(data.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)\n",
    "\n",
    "# pyTorch dataloaders\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased',\n",
    "                                                   num_labels=len(id2label),\n",
    "                                                   id2label=id2label,\n",
    "                                                   label2id=label2id)\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "ids = training_set[0][\"ids\"].unsqueeze(0)\n",
    "mask = training_set[0][\"mask\"].unsqueeze(0)\n",
    "targets = training_set[0][\"targets\"].unsqueeze(0)\n",
    "ids = ids.to(device)\n",
    "mask = mask.to(device)\n",
    "targets = targets.to(device)\n",
    "outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "initial_loss = outputs[0]\n",
    "initial_loss\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7df515e3",
   "metadata": {
    "id": "7df515e3"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DLITELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DLITELoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Convert logits to probabilities using softmax\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # One-hot encode the targets to get true probabilities\n",
    "        true_probs = F.one_hot(targets, num_classes=probs.size(-1)).float()\n",
    "\n",
    "        # Masks for non-zero elements of probs and true_probs\n",
    "        mask_probs = probs > 0\n",
    "        mask_true_probs = true_probs > 0\n",
    "\n",
    "        # Calculate g function for non-zero elements using the mask\n",
    "        g_probs = torch.zeros_like(probs)\n",
    "        g_true_probs = torch.zeros_like(true_probs)\n",
    "        g_probs[mask_probs] = probs[mask_probs] * (1 - torch.log(probs[mask_probs]))\n",
    "        g_true_probs[mask_true_probs] = true_probs[mask_true_probs] * (1 - torch.log(true_probs[mask_true_probs]))\n",
    "        g_values = torch.abs(g_probs - g_true_probs)\n",
    "\n",
    "        # Calculate delta_h function for non-zero elements using the mask\n",
    "        delta_h_probs = torch.zeros_like(probs)\n",
    "        delta_h_true_probs = torch.zeros_like(true_probs)\n",
    "        delta_h_probs[mask_probs] = probs[mask_probs]**2 * (1 - 2 * torch.log(probs[mask_probs]))\n",
    "        delta_h_true_probs[mask_true_probs] = true_probs[mask_true_probs]**2 * (1 - 2 * torch.log(true_probs[mask_true_probs]))\n",
    "        delta_h_values = torch.abs(delta_h_probs - delta_h_true_probs) / (2 * (probs + true_probs))\n",
    "\n",
    "        # Compute DLITE loss for each class\n",
    "        dl_values = g_values - delta_h_values\n",
    "\n",
    "        # Sum over all classes and average over the batch size\n",
    "        loss = dl_values.sum(dim=-1).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6b3dfbbd",
   "metadata": {
    "id": "6b3dfbbd"
   },
   "outputs": [],
   "source": [
    "dlite_loss_fn = DLITELoss()\n",
    "\n",
    "# Defining the training function on the 80% of the dataset for tuning the bert model\n",
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    for idx, batch in enumerate(training_loader):\n",
    "\n",
    "        ids = batch['ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['mask'].to(device, dtype = torch.long)\n",
    "        targets = batch['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "        loss, tr_logits = outputs.loss, outputs.logits\n",
    "\n",
    "        # implement dlite loss function\n",
    "        flatten_targets = targets.view(-1)\n",
    "        flatten_logits = tr_logits.view(-1, model.num_labels)\n",
    "\n",
    "        loss = dlite_loss_fn(flatten_logits, flatten_targets)\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets.size(0)\n",
    "\n",
    "        if idx % 100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training dlite loss per 100 training steps: {loss_step}\")\n",
    "\n",
    "        # compute training accuracy\n",
    "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "        tr_preds.extend(predictions)\n",
    "        tr_labels.extend(targets)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n",
    "    print(f\"Training steps: {nb_tr_steps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aa08c800",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa08c800",
    "outputId": "13cb01f1-b765-42fd-84f0-5b5e2bdce654"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Training dlite loss per 100 training steps: 0.6854904890060425\n",
      "Training dlite loss per 100 training steps: 0.05783206148598675\n",
      "Training dlite loss per 100 training steps: 0.047690981780109346\n",
      "Training dlite loss per 100 training steps: 0.04389238159599222\n",
      "Training dlite loss per 100 training steps: 0.042020012994419945\n",
      "Training dlite loss per 100 training steps: 0.0412303753212994\n",
      "Training dlite loss per 100 training steps: 0.040369918493603586\n",
      "Training dlite loss per 100 training steps: 0.039499940703717325\n",
      "Training dlite loss per 100 training steps: 0.03925809412979557\n",
      "Training dlite loss per 100 training steps: 0.0392555067918835\n",
      "Training dlite loss per 100 training steps: 0.03904800563255189\n",
      "Training dlite loss per 100 training steps: 0.038930943350161305\n",
      "Training dlite loss per 100 training steps: 0.03868664463041358\n",
      "Training dlite loss per 100 training steps: 0.038568167386900096\n",
      "Training dlite loss per 100 training steps: 0.03844406175710325\n",
      "Training dlite loss per 100 training steps: 0.038267330793778025\n",
      "Training dlite loss per 100 training steps: 0.03822229589018005\n",
      "Training dlite loss per 100 training steps: 0.038143377046728925\n",
      "Training dlite loss per 100 training steps: 0.0381240727866472\n",
      "Training dlite loss per 100 training steps: 0.03812724972845825\n",
      "Training dlite loss per 100 training steps: 0.038083175173502286\n",
      "Training dlite loss per 100 training steps: 0.0379000742031375\n",
      "Training dlite loss per 100 training steps: 0.037975786933785526\n",
      "Training dlite loss per 100 training steps: 0.0379141580397592\n",
      "Training dlite loss per 100 training steps: 0.03793082420078846\n",
      "Training dlite loss per 100 training steps: 0.0379351842673649\n",
      "Training dlite loss per 100 training steps: 0.037876851899851655\n",
      "Training dlite loss per 100 training steps: 0.037826423364474636\n",
      "Training dlite loss per 100 training steps: 0.03780327481890663\n",
      "Training dlite loss per 100 training steps: 0.03775544196812547\n",
      "Training dlite loss per 100 training steps: 0.03770817208430318\n",
      "Training dlite loss per 100 training steps: 0.037695422397457407\n",
      "Training dlite loss per 100 training steps: 0.037562645448576085\n",
      "Training dlite loss per 100 training steps: 0.03749292158274287\n",
      "Training dlite loss per 100 training steps: 0.03754271031457604\n",
      "Training dlite loss per 100 training steps: 0.03755784549760526\n",
      "Training dlite loss per 100 training steps: 0.03747721405757553\n",
      "Training dlite loss per 100 training steps: 0.037429426268517946\n",
      "Training dlite loss per 100 training steps: 0.03737283994261937\n",
      "Training dlite loss per 100 training steps: 0.03734367887657597\n",
      "Training dlite loss per 100 training steps: 0.037364782948911705\n",
      "Training dlite loss per 100 training steps: 0.037341512805723245\n",
      "Training dlite loss per 100 training steps: 0.037350028475039126\n",
      "Training dlite loss per 100 training steps: 0.037297293082574075\n",
      "Training dlite loss per 100 training steps: 0.03728999604545964\n",
      "Training dlite loss per 100 training steps: 0.03732294013792864\n",
      "Training dlite loss per 100 training steps: 0.03729459399545305\n",
      "Training dlite loss per 100 training steps: 0.03731356631368152\n",
      "Training dlite loss per 100 training steps: 0.037290657058771985\n",
      "Training dlite loss per 100 training steps: 0.03722922678453114\n",
      "Training dlite loss per 100 training steps: 0.03721086673610644\n",
      "Training dlite loss per 100 training steps: 0.03724721076334684\n",
      "Training dlite loss per 100 training steps: 0.0373230866905124\n",
      "Training dlite loss per 100 training steps: 0.03732977840747269\n",
      "Training dlite loss per 100 training steps: 0.03734562232685543\n",
      "Training dlite loss per 100 training steps: 0.03736017820396062\n",
      "Training dlite loss per 100 training steps: 0.03734945439631655\n",
      "Training dlite loss per 100 training steps: 0.03727846652216616\n",
      "Training dlite loss per 100 training steps: 0.03723349235926392\n",
      "Training dlite loss per 100 training steps: 0.03722479385724083\n",
      "Training dlite loss per 100 training steps: 0.037207922109597974\n",
      "Training dlite loss per 100 training steps: 0.037198644778342595\n",
      "Training dlite loss per 100 training steps: 0.03719344518481447\n",
      "Training dlite loss per 100 training steps: 0.03720297816982239\n",
      "Training dlite loss per 100 training steps: 0.03724425049054202\n",
      "Training dlite loss per 100 training steps: 0.037191718863303434\n",
      "Training dlite loss per 100 training steps: 0.03720528022181008\n",
      "Training dlite loss per 100 training steps: 0.03716859530606678\n",
      "Training dlite loss per 100 training steps: 0.03714763469886176\n",
      "Training dlite loss per 100 training steps: 0.03713379031781664\n",
      "Training dlite loss per 100 training steps: 0.037108344769419675\n",
      "Training dlite loss per 100 training steps: 0.03708168992887866\n",
      "Training dlite loss per 100 training steps: 0.03707991419443605\n",
      "Training dlite loss per 100 training steps: 0.03709450495751443\n",
      "Training dlite loss per 100 training steps: 0.0370952420854466\n",
      "Training dlite loss per 100 training steps: 0.03707564938921487\n",
      "Training dlite loss per 100 training steps: 0.03705168962715134\n",
      "Training dlite loss per 100 training steps: 0.0370428080047479\n",
      "Training dlite loss per 100 training steps: 0.03705117876961782\n",
      "Training dlite loss per 100 training steps: 0.03703659495823043\n",
      "Training dlite loss per 100 training steps: 0.03708511153245363\n",
      "Training dlite loss per 100 training steps: 0.03709168478442126\n",
      "Training dlite loss per 100 training steps: 0.037107147398378726\n",
      "Training dlite loss per 100 training steps: 0.03710623767190826\n",
      "Training dlite loss per 100 training steps: 0.03710372192874292\n",
      "Training dlite loss per 100 training steps: 0.03711344201815415\n",
      "Training dlite loss per 100 training steps: 0.03711430678816972\n",
      "Training dlite loss per 100 training steps: 0.03710572372630912\n",
      "Training dlite loss per 100 training steps: 0.03711930564294135\n",
      "Training dlite loss per 100 training steps: 0.03713543478505419\n",
      "Training dlite loss per 100 training steps: 0.037121260831576165\n",
      "Training dlite loss per 100 training steps: 0.03713701373012134\n",
      "Training dlite loss per 100 training steps: 0.03714923996606581\n",
      "Training dlite loss per 100 training steps: 0.03716771289631248\n",
      "Training dlite loss per 100 training steps: 0.03716086186882219\n",
      "Training dlite loss per 100 training steps: 0.03716669473216309\n",
      "Training loss epoch: 0.037169689690246706\n",
      "Training accuracy epoch: 0.8246417288118657\n",
      "Training steps: 9515\n",
      "CPU times: user 19min 15s, sys: 3.93 s, total: 19min 19s\n",
      "Wall time: 19min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "11a7c9af",
   "metadata": {
    "id": "11a7c9af"
   },
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "\n",
    "            ids = batch['ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['mask'].to(device, dtype = torch.long)\n",
    "            targets = batch['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "            loss, eval_logits = outputs.loss, outputs.logits\n",
    "\n",
    "\n",
    "            # implement dlite loss\n",
    "            flatten_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "            flatten_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            loss = dlite_loss_fn(flatten_logits, flatten_targets)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += targets.size(0)\n",
    "\n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation dlite loss per 100 evaluation steps: {loss_step}\")\n",
    "\n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "            eval_labels.extend(targets)\n",
    "            eval_preds.extend(predictions)\n",
    "\n",
    "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    #print(eval_labels)\n",
    "    #print(eval_preds)\n",
    "\n",
    "    labels = [id2label[id.item()] for id in eval_labels]\n",
    "    predictions = [id2label[id.item()] for id in eval_preds]\n",
    "\n",
    "    #print(labels)\n",
    "    #print(predictions)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "    print(f\"Validation steps: {nb_eval_steps}\")\n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1af54ebb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1af54ebb",
    "outputId": "0171a69f-e3f8-413d-8442-f2c0dc8f0290"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dlite loss per 100 evaluation steps: 0.05078135058283806\n",
      "Validation dlite loss per 100 evaluation steps: 0.03654869038701738\n",
      "Validation dlite loss per 100 evaluation steps: 0.0366139192708328\n",
      "Validation dlite loss per 100 evaluation steps: 0.036856425336021265\n",
      "Validation dlite loss per 100 evaluation steps: 0.036948757087596436\n",
      "Validation dlite loss per 100 evaluation steps: 0.036707947613960956\n",
      "Validation dlite loss per 100 evaluation steps: 0.036755260931802515\n",
      "Validation dlite loss per 100 evaluation steps: 0.03647144908631567\n",
      "Validation dlite loss per 100 evaluation steps: 0.0364145564366239\n",
      "Validation dlite loss per 100 evaluation steps: 0.03611450052999285\n",
      "Validation dlite loss per 100 evaluation steps: 0.036170974101880415\n",
      "Validation dlite loss per 100 evaluation steps: 0.036185257874656214\n",
      "Validation dlite loss per 100 evaluation steps: 0.03592070087005755\n",
      "Validation dlite loss per 100 evaluation steps: 0.03606011568273025\n",
      "Validation dlite loss per 100 evaluation steps: 0.03617684014684323\n",
      "Validation dlite loss per 100 evaluation steps: 0.03665276159078249\n",
      "Validation dlite loss per 100 evaluation steps: 0.036671529644338036\n",
      "Validation dlite loss per 100 evaluation steps: 0.03668579451452866\n",
      "Validation dlite loss per 100 evaluation steps: 0.03646856849492033\n",
      "Validation dlite loss per 100 evaluation steps: 0.03637488338066528\n",
      "Validation dlite loss per 100 evaluation steps: 0.03641745170755613\n",
      "Validation dlite loss per 100 evaluation steps: 0.03637044312826245\n",
      "Validation dlite loss per 100 evaluation steps: 0.03632060707731989\n",
      "Validation dlite loss per 100 evaluation steps: 0.03635658898614323\n",
      "Validation dlite loss per 100 evaluation steps: 0.036324496633599816\n",
      "Validation dlite loss per 100 evaluation steps: 0.03649957616476938\n",
      "Validation dlite loss per 100 evaluation steps: 0.03649148696588456\n",
      "Validation dlite loss per 100 evaluation steps: 0.03659535587436902\n",
      "Validation dlite loss per 100 evaluation steps: 0.03676711609578298\n",
      "Validation dlite loss per 100 evaluation steps: 0.036574247296908766\n",
      "Validation dlite loss per 100 evaluation steps: 0.0366024958947493\n",
      "Validation dlite loss per 100 evaluation steps: 0.036627662915297295\n",
      "Validation dlite loss per 100 evaluation steps: 0.03657193663703916\n",
      "Validation dlite loss per 100 evaluation steps: 0.03666632242518271\n",
      "Validation dlite loss per 100 evaluation steps: 0.03656794270408056\n",
      "Validation dlite loss per 100 evaluation steps: 0.036538780931282946\n",
      "Validation dlite loss per 100 evaluation steps: 0.0365155778811704\n",
      "Validation dlite loss per 100 evaluation steps: 0.036513682391894724\n",
      "Validation dlite loss per 100 evaluation steps: 0.036486194409248905\n",
      "Validation dlite loss per 100 evaluation steps: 0.03648014259945942\n",
      "Validation dlite loss per 100 evaluation steps: 0.036489038069947854\n",
      "Validation dlite loss per 100 evaluation steps: 0.03647654448842917\n",
      "Validation dlite loss per 100 evaluation steps: 0.03645534732275348\n",
      "Validation dlite loss per 100 evaluation steps: 0.03642605366174954\n",
      "Validation dlite loss per 100 evaluation steps: 0.03644513305675043\n",
      "Validation dlite loss per 100 evaluation steps: 0.0364885326681895\n",
      "Validation dlite loss per 100 evaluation steps: 0.036433259787445146\n",
      "Validation dlite loss per 100 evaluation steps: 0.03645512301804693\n",
      "Validation Loss: 0.03642915881808321\n",
      "Validation Accuracy: 0.8292052076605463\n",
      "Validation steps: 4757\n",
      "CPU times: user 1min 47s, sys: 365 ms, total: 1min 47s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "labels, predictions = valid(model, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8334c303",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8334c303",
    "outputId": "cb22f62d-3161-408b-ca4a-4813aaa3e61f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         geo       0.00      0.00      0.00     11232\n",
      "         gpe       0.00      0.00      0.00      3293\n",
      "         org       0.00      0.00      0.00      6531\n",
      "         per       0.00      0.00      0.00      5196\n",
      "         tim       0.00      0.00      0.00      4360\n",
      "\n",
      "   micro avg       0.00      0.00      0.00     30612\n",
      "   macro avg       0.00      0.00      0.00     30612\n",
      "weighted avg       0.00      0.00      0.00     30612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "print(classification_report([labels], [predictions]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a54336ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a54336ab",
    "outputId": "87d4f47a-1367-412f-89f4-b2af79c4f67b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8241245475557106, 0.8241245475557106, 0.8241245475557105, None)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(labels, predictions, average = \"micro\", zero_division = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8741fe98",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8741fe98",
    "outputId": "b04b36c3-ce05-47c1-89c0-69508861919e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9840113225050646, 0.09090909090909091, 0.08214396710416218, None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(labels, predictions, average = \"macro\", zero_division = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "578f4697",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "578f4697",
    "outputId": "f64ee02f-6f8b-4277-fcd6-82d640a49430"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8550567223281941, 0.8241245475557106, 0.7446654569656372, None)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(labels, predictions, average = \"weighted\", zero_division = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2f7d3aeb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f7d3aeb",
    "outputId": "78803a09-79a9-46d7-e5d6-45f46b13dc5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india has a capital called mumbai . on wednesday , the president will give a presentation\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"India has a capital called Mumbai. On Wednesday, the president will give a presentation\"\n",
    "\n",
    "inputs = tokenizer(sentence, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "\n",
    "# move to gpu\n",
    "ids = inputs[\"input_ids\"].to(device)\n",
    "mask = inputs[\"attention_mask\"].to(device)\n",
    "# forward pass\n",
    "outputs = model(ids, mask)\n",
    "logits = outputs[0]\n",
    "\n",
    "active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "token_predictions = [id2label[i] for i in flattened_predictions.cpu().numpy()]\n",
    "wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
    "\n",
    "word_level_predictions = []\n",
    "for pair in wp_preds:\n",
    "  if (pair[0].startswith(\" ##\")) or (pair[0] in ['[CLS]', '[SEP]', '[PAD]']):\n",
    "    # skip prediction\n",
    "    continue\n",
    "  else:\n",
    "    word_level_predictions.append(pair[1])\n",
    "\n",
    "# we join tokens, if they are not special ones\n",
    "str_rep = \" \".join([t[0] for t in wp_preds if t[0] not in ['[CLS]', '[SEP]', '[PAD]']]).replace(\" ##\", \"\")\n",
    "print(str_rep)\n",
    "print(word_level_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1282ab63",
   "metadata": {
    "id": "1282ab63"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2531e79c",
   "metadata": {
    "id": "2531e79c"
   },
   "outputs": [],
   "source": [
    "# KL divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4a852a7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4a852a7d",
    "outputId": "208ce0a4-25f2-46f1-e3f6-436759782438"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (47571, 2)\n",
      "TRAIN Dataset: (38057, 2)\n",
      "TEST Dataset: (9514, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 5e-05\n",
    "MAX_GRAD_NORM = 10\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# tokenization\n",
    "\n",
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Word piece tokenization makes it difficult to match word labels\n",
    "    back up with individual word pieces. This function tokenizes each\n",
    "    word one at a time so that it is easier to preserve the correct\n",
    "    label for each subword. It is, of course, a bit slower in processing\n",
    "    time, but it will help our model achieve higher accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels\n",
    "\n",
    "# Bert imput\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # step 1: tokenize (and adapt corresponding labels)\n",
    "        sentence = self.data.sentence[index]\n",
    "        word_labels = self.data.word_labels[index]\n",
    "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
    "\n",
    "        # step 2: add special tokens (and corresponding labels)\n",
    "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n",
    "        labels.insert(0, \"O\") # add outside label for [CLS] token\n",
    "        labels.insert(-1, \"O\") # add outside label for [SEP] token\n",
    "\n",
    "        # step 3: truncating/padding\n",
    "        maxlen = self.max_len\n",
    "\n",
    "        if (len(tokenized_sentence) > maxlen):\n",
    "          # truncate\n",
    "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
    "          labels = labels[:maxlen]\n",
    "        else:\n",
    "          # pad\n",
    "          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n",
    "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
    "\n",
    "        # step 4: obtain the attention mask\n",
    "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
    "\n",
    "        # step 5: convert tokens to input ids\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "\n",
    "        label_ids = [label2id[label] for label in labels]\n",
    "        # the following line is deprecated\n",
    "        #label_ids = [label if label != 0 else -100 for label in label_ids]\n",
    "\n",
    "        return {\n",
    "              'ids': torch.tensor(ids, dtype=torch.long),\n",
    "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
    "              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# define training size 80/20 split\n",
    "\n",
    "train_size = 0.8\n",
    "train_dataset = data.sample(frac=train_size,random_state=200)\n",
    "test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(data.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)\n",
    "\n",
    "# pyTorch dataloaders\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased',\n",
    "                                                   num_labels=len(id2label),\n",
    "                                                   id2label=id2label,\n",
    "                                                   label2id=label2id)\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "ids = training_set[0][\"ids\"].unsqueeze(0)\n",
    "mask = training_set[0][\"mask\"].unsqueeze(0)\n",
    "targets = training_set[0][\"targets\"].unsqueeze(0)\n",
    "ids = ids.to(device)\n",
    "mask = mask.to(device)\n",
    "targets = targets.to(device)\n",
    "outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "initial_loss = outputs[0]\n",
    "initial_loss\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "25c70987",
   "metadata": {
    "id": "25c70987"
   },
   "outputs": [],
   "source": [
    "# KL implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "04286153",
   "metadata": {
    "id": "04286153"
   },
   "outputs": [],
   "source": [
    "class KLDivergenceLoss(nn.Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(KLDivergenceLoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Convert logits to probabilities using softmax\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # One-hot encode the targets to get true probabilities\n",
    "        true_probs = F.one_hot(targets, num_classes=probs.size(-1)).float()\n",
    "\n",
    "        # Masks for non-zero elements of probs and true_probs\n",
    "        mask_probs = probs > 0\n",
    "        mask_true_probs = true_probs > 0\n",
    "\n",
    "        # Calculate g function for non-zero elements using the mask\n",
    "        kl_values = torch.zeros_like(probs)\n",
    "        kl_values[mask_true_probs] = true_probs[mask_true_probs] * torch.log(true_probs[mask_true_probs]/probs[mask_true_probs])\n",
    "\n",
    "        # Sum over all classes and average over the batch size\n",
    "        loss = kl_values.sum(dim=-1).mean()\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3bc21367",
   "metadata": {
    "id": "3bc21367"
   },
   "outputs": [],
   "source": [
    "kl_loss_fn = KLDivergenceLoss()\n",
    "\n",
    "# Defining the training function on the 80% of the dataset for tuning the bert model\n",
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    for idx, batch in enumerate(training_loader):\n",
    "\n",
    "        ids = batch['ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['mask'].to(device, dtype = torch.long)\n",
    "        targets = batch['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "        loss, tr_logits = outputs.loss, outputs.logits\n",
    "\n",
    "        # implement dlite loss function\n",
    "        flatten_targets = targets.view(-1)\n",
    "        flatten_logits = tr_logits.view(-1, model.num_labels)\n",
    "\n",
    "        loss = kl_loss_fn(flatten_logits, flatten_targets)\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets.size(0)\n",
    "\n",
    "        if idx % 100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training kl loss per 100 training steps: {loss_step}\")\n",
    "\n",
    "        # compute training accuracy\n",
    "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "        tr_preds.extend(predictions)\n",
    "        tr_labels.extend(targets)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n",
    "    print(f\"Training steps: {nb_tr_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "778adc2c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "778adc2c",
    "outputId": "2b248df6-2184-48d8-b859-350d3365a80c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Training dlite loss per 100 training steps: 2.2212939262390137\n",
      "Training dlite loss per 100 training steps: 0.20152779543163754\n",
      "Training dlite loss per 100 training steps: 0.1424976000840317\n",
      "Training dlite loss per 100 training steps: 0.1131605784991985\n",
      "Training dlite loss per 100 training steps: 0.09781182390879235\n",
      "Training dlite loss per 100 training steps: 0.08700993469313992\n",
      "Training dlite loss per 100 training steps: 0.07928984548033412\n",
      "Training dlite loss per 100 training steps: 0.0735936454608877\n",
      "Training dlite loss per 100 training steps: 0.06853542775677264\n",
      "Training dlite loss per 100 training steps: 0.06536193606629762\n",
      "Training dlite loss per 100 training steps: 0.06228359583760616\n",
      "Training dlite loss per 100 training steps: 0.05992395078216076\n",
      "Training dlite loss per 100 training steps: 0.05807913258244484\n",
      "Training dlite loss per 100 training steps: 0.056267312399203316\n",
      "Training dlite loss per 100 training steps: 0.05446469040877046\n",
      "Training dlite loss per 100 training steps: 0.053073030067306834\n",
      "Training dlite loss per 100 training steps: 0.05189398842301513\n",
      "Training dlite loss per 100 training steps: 0.05106407201865076\n",
      "Training dlite loss per 100 training steps: 0.05006045898100566\n",
      "Training dlite loss per 100 training steps: 0.048934948880088\n",
      "Training dlite loss per 100 training steps: 0.048217214143590426\n",
      "Training dlite loss per 100 training steps: 0.04751498718687383\n",
      "Training dlite loss per 100 training steps: 0.04697600245285464\n",
      "Training dlite loss per 100 training steps: 0.04621541610352192\n",
      "Training dlite loss per 100 training steps: 0.045488515415994794\n",
      "Training dlite loss per 100 training steps: 0.04494990078666367\n",
      "Training dlite loss per 100 training steps: 0.0444781202280234\n",
      "Training dlite loss per 100 training steps: 0.043964288056065545\n",
      "Training dlite loss per 100 training steps: 0.043505149391704724\n",
      "Training dlite loss per 100 training steps: 0.04314679413625995\n",
      "Training dlite loss per 100 training steps: 0.04285620481139374\n",
      "Training dlite loss per 100 training steps: 0.042428836787624775\n",
      "Training dlite loss per 100 training steps: 0.04209070790748296\n",
      "Training dlite loss per 100 training steps: 0.04188209145357101\n",
      "Training dlite loss per 100 training steps: 0.04150200295156183\n",
      "Training dlite loss per 100 training steps: 0.04122921620666516\n",
      "Training dlite loss per 100 training steps: 0.04094741480842023\n",
      "Training dlite loss per 100 training steps: 0.04057891379046331\n",
      "Training dlite loss per 100 training steps: 0.04025843658631025\n",
      "Training dlite loss per 100 training steps: 0.03994474599184203\n",
      "Training dlite loss per 100 training steps: 0.03969586753883468\n",
      "Training dlite loss per 100 training steps: 0.03948430408151241\n",
      "Training dlite loss per 100 training steps: 0.03930886945462371\n",
      "Training dlite loss per 100 training steps: 0.03915511458844461\n",
      "Training dlite loss per 100 training steps: 0.03898380026612699\n",
      "Training dlite loss per 100 training steps: 0.03876384436519567\n",
      "Training dlite loss per 100 training steps: 0.0385490880605004\n",
      "Training dlite loss per 100 training steps: 0.038401808343007715\n",
      "Training dlite loss per 100 training steps: 0.038206786503829464\n",
      "Training dlite loss per 100 training steps: 0.03796530202635077\n",
      "Training dlite loss per 100 training steps: 0.03779797374493666\n",
      "Training dlite loss per 100 training steps: 0.03756460925845782\n",
      "Training dlite loss per 100 training steps: 0.03738985811006721\n",
      "Training dlite loss per 100 training steps: 0.03721813441476988\n",
      "Training dlite loss per 100 training steps: 0.037126712177402967\n",
      "Training dlite loss per 100 training steps: 0.03692530555612854\n",
      "Training dlite loss per 100 training steps: 0.03684475115486272\n",
      "Training dlite loss per 100 training steps: 0.03671269747665853\n",
      "Training dlite loss per 100 training steps: 0.03663324389073358\n",
      "Training dlite loss per 100 training steps: 0.036501267874503314\n",
      "Training dlite loss per 100 training steps: 0.03631054623149605\n",
      "Training dlite loss per 100 training steps: 0.036226078976336104\n",
      "Training dlite loss per 100 training steps: 0.03612049533202295\n",
      "Training dlite loss per 100 training steps: 0.03598559013195889\n",
      "Training dlite loss per 100 training steps: 0.03585173280933705\n",
      "Training dlite loss per 100 training steps: 0.03580664798194128\n",
      "Training dlite loss per 100 training steps: 0.03570548003898746\n",
      "Training dlite loss per 100 training steps: 0.03564632939026047\n",
      "Training dlite loss per 100 training steps: 0.035602941758617786\n",
      "Training dlite loss per 100 training steps: 0.0355086323475984\n",
      "Training dlite loss per 100 training steps: 0.03543160673543488\n",
      "Training dlite loss per 100 training steps: 0.03532593236366154\n",
      "Training dlite loss per 100 training steps: 0.035207042950176286\n",
      "Training dlite loss per 100 training steps: 0.0351153935832316\n",
      "Training dlite loss per 100 training steps: 0.03503488516854714\n",
      "Training dlite loss per 100 training steps: 0.03493916388311357\n",
      "Training dlite loss per 100 training steps: 0.03485308945936685\n",
      "Training dlite loss per 100 training steps: 0.03473311939371504\n",
      "Training dlite loss per 100 training steps: 0.03460868247540886\n",
      "Training dlite loss per 100 training steps: 0.03453709159779702\n",
      "Training dlite loss per 100 training steps: 0.03447823678172623\n",
      "Training dlite loss per 100 training steps: 0.03449247921263076\n",
      "Training dlite loss per 100 training steps: 0.034425268619013044\n",
      "Training dlite loss per 100 training steps: 0.03432634685847949\n",
      "Training dlite loss per 100 training steps: 0.03428890264636684\n",
      "Training dlite loss per 100 training steps: 0.03416991269027504\n",
      "Training dlite loss per 100 training steps: 0.03410484643233823\n",
      "Training dlite loss per 100 training steps: 0.034048773682798884\n",
      "Training dlite loss per 100 training steps: 0.03401993462610401\n",
      "Training dlite loss per 100 training steps: 0.03394319078175446\n",
      "Training dlite loss per 100 training steps: 0.033898934268678245\n",
      "Training dlite loss per 100 training steps: 0.033816631494926926\n",
      "Training dlite loss per 100 training steps: 0.0337391009341995\n",
      "Training dlite loss per 100 training steps: 0.03369201465510492\n",
      "Training dlite loss per 100 training steps: 0.03361015070242805\n",
      "Training dlite loss per 100 training steps: 0.033555465099392945\n",
      "Training loss epoch: 0.0335527446257848\n",
      "Training accuracy epoch: 0.9526450707318307\n",
      "Training steps: 9515\n",
      "CPU times: user 19min 4s, sys: 4 s, total: 19min 8s\n",
      "Wall time: 19min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ea3e77df",
   "metadata": {
    "id": "ea3e77df"
   },
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "\n",
    "            ids = batch['ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['mask'].to(device, dtype = torch.long)\n",
    "            targets = batch['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "            loss, eval_logits = outputs.loss, outputs.logits\n",
    "\n",
    "\n",
    "            # implement dlite loss\n",
    "            flatten_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "            flatten_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            loss = kl_loss_fn(flatten_logits, flatten_targets)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += targets.size(0)\n",
    "\n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation kl loss per 100 evaluation steps: {loss_step}\")\n",
    "\n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "            eval_labels.extend(targets)\n",
    "            eval_preds.extend(predictions)\n",
    "\n",
    "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    #print(eval_labels)\n",
    "    #print(eval_preds)\n",
    "\n",
    "    labels = [id2label[id.item()] for id in eval_labels]\n",
    "    predictions = [id2label[id.item()] for id in eval_preds]\n",
    "\n",
    "    #print(labels)\n",
    "    #print(predictions)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "    print(f\"Validation steps: {nb_eval_steps}\")\n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0a2f9aea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0a2f9aea",
    "outputId": "1155d129-061d-489a-c783-6078dba18f6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dlite loss per 100 evaluation steps: 0.007632597349584103\n",
      "Validation dlite loss per 100 evaluation steps: 0.024936844111321663\n",
      "Validation dlite loss per 100 evaluation steps: 0.026968233317886225\n",
      "Validation dlite loss per 100 evaluation steps: 0.026589846328863246\n",
      "Validation dlite loss per 100 evaluation steps: 0.026909879688683393\n",
      "Validation dlite loss per 100 evaluation steps: 0.026705700800517523\n",
      "Validation dlite loss per 100 evaluation steps: 0.02627618377085789\n",
      "Validation dlite loss per 100 evaluation steps: 0.025595445925634203\n",
      "Validation dlite loss per 100 evaluation steps: 0.025263627112083745\n",
      "Validation dlite loss per 100 evaluation steps: 0.025131608209752878\n",
      "Validation dlite loss per 100 evaluation steps: 0.025234122415214867\n",
      "Validation dlite loss per 100 evaluation steps: 0.025027116235100375\n",
      "Validation dlite loss per 100 evaluation steps: 0.025356502731722033\n",
      "Validation dlite loss per 100 evaluation steps: 0.02520338178571108\n",
      "Validation dlite loss per 100 evaluation steps: 0.02542005464640532\n",
      "Validation dlite loss per 100 evaluation steps: 0.0252861764229292\n",
      "Validation dlite loss per 100 evaluation steps: 0.025115758492123347\n",
      "Validation dlite loss per 100 evaluation steps: 0.024885632330397756\n",
      "Validation dlite loss per 100 evaluation steps: 0.025015487709855393\n",
      "Validation dlite loss per 100 evaluation steps: 0.024915746513756335\n",
      "Validation dlite loss per 100 evaluation steps: 0.02490771964817461\n",
      "Validation dlite loss per 100 evaluation steps: 0.02484630973412312\n",
      "Validation dlite loss per 100 evaluation steps: 0.024787941341226992\n",
      "Validation dlite loss per 100 evaluation steps: 0.02497719277304177\n",
      "Validation dlite loss per 100 evaluation steps: 0.024836951645486837\n",
      "Validation dlite loss per 100 evaluation steps: 0.024769700283195598\n",
      "Validation dlite loss per 100 evaluation steps: 0.02486749542469026\n",
      "Validation dlite loss per 100 evaluation steps: 0.024757964577365486\n",
      "Validation dlite loss per 100 evaluation steps: 0.024514772141947642\n",
      "Validation dlite loss per 100 evaluation steps: 0.024747407105911202\n",
      "Validation dlite loss per 100 evaluation steps: 0.024846809805877546\n",
      "Validation dlite loss per 100 evaluation steps: 0.024689235494896693\n",
      "Validation dlite loss per 100 evaluation steps: 0.024782541769157253\n",
      "Validation dlite loss per 100 evaluation steps: 0.024826453907640083\n",
      "Validation dlite loss per 100 evaluation steps: 0.024915407436216088\n",
      "Validation dlite loss per 100 evaluation steps: 0.024849405466722346\n",
      "Validation dlite loss per 100 evaluation steps: 0.024819935326248353\n",
      "Validation dlite loss per 100 evaluation steps: 0.02477539131979869\n",
      "Validation dlite loss per 100 evaluation steps: 0.024853242356028492\n",
      "Validation dlite loss per 100 evaluation steps: 0.02488350199160088\n",
      "Validation dlite loss per 100 evaluation steps: 0.025031981789368343\n",
      "Validation dlite loss per 100 evaluation steps: 0.02511397452457116\n",
      "Validation dlite loss per 100 evaluation steps: 0.025208397083459105\n",
      "Validation dlite loss per 100 evaluation steps: 0.02511051988090689\n",
      "Validation dlite loss per 100 evaluation steps: 0.025145072225247176\n",
      "Validation dlite loss per 100 evaluation steps: 0.025051664811382525\n",
      "Validation dlite loss per 100 evaluation steps: 0.02504355987103121\n",
      "Validation dlite loss per 100 evaluation steps: 0.025035912125009104\n",
      "Validation Loss: 0.02501246035494634\n",
      "Validation Accuracy: 0.9625753835498936\n",
      "Validation steps: 4757\n",
      "CPU times: user 1min 43s, sys: 330 ms, total: 1min 43s\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "labels, predictions = valid(model, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "302b2264",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "302b2264",
    "outputId": "97f879b2-ebbf-43c8-b992-e54cf179308e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         geo       0.82      0.88      0.85     11232\n",
      "         gpe       0.95      0.92      0.94      3293\n",
      "         org       0.70      0.62      0.66      6531\n",
      "         per       0.71      0.84      0.77      5196\n",
      "         tim       0.88      0.82      0.85      4360\n",
      "\n",
      "   micro avg       0.80      0.81      0.80     30612\n",
      "   macro avg       0.81      0.81      0.81     30612\n",
      "weighted avg       0.80      0.81      0.80     30612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "print(classification_report([labels], [predictions]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fd1517dc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd1517dc",
    "outputId": "1556f3c2-7af4-4cc9-8e63-dca7fde382d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9607835363798907, 0.9607835363798907, 0.9607835363798907, None)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(labels, predictions, average = \"micro\", zero_division = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "28a5d897",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28a5d897",
    "outputId": "b0caf8d2-d702-43c5-c9d2-7b7c1f064035"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8544195991976448, 0.7762237811884958, 0.7973727979758309, None)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(labels, predictions, average = \"macro\", zero_division = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9b20a628",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b20a628",
    "outputId": "220635dd-d1cf-4196-96b4-98ebcfbcb130"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.96051291876664, 0.9607835363798907, 0.9601458179989333, None)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(labels, predictions, average = \"weighted\", zero_division = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1498fe",
   "metadata": {
    "id": "0d1498fe"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a234d279",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a234d279",
    "outputId": "a710c56f-6907-4ff2-d4c1-ec478ef36aca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india has a capital called mumbai . on wednesday , the president will give a presentation\n",
      "['B-org', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"India has a capital called Mumbai. On Wednesday, the president will give a presentation\"\n",
    "\n",
    "inputs = tokenizer(sentence, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "\n",
    "# move to gpu\n",
    "ids = inputs[\"input_ids\"].to(device)\n",
    "mask = inputs[\"attention_mask\"].to(device)\n",
    "# forward pass\n",
    "outputs = model(ids, mask)\n",
    "logits = outputs[0]\n",
    "\n",
    "active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "token_predictions = [id2label[i] for i in flattened_predictions.cpu().numpy()]\n",
    "wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
    "\n",
    "word_level_predictions = []\n",
    "for pair in wp_preds:\n",
    "  if (pair[0].startswith(\" ##\")) or (pair[0] in ['[CLS]', '[SEP]', '[PAD]']):\n",
    "    # skip prediction\n",
    "    continue\n",
    "  else:\n",
    "    word_level_predictions.append(pair[1])\n",
    "\n",
    "# we join tokens, if they are not special ones\n",
    "str_rep = \" \".join([t[0] for t in wp_preds if t[0] not in ['[CLS]', '[SEP]', '[PAD]']]).replace(\" ##\", \"\")\n",
    "print(str_rep)\n",
    "print(word_level_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "79f866cb",
   "metadata": {
    "id": "79f866cb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f827354f",
   "metadata": {
    "id": "f827354f"
   },
   "outputs": [],
   "source": [
    "# Dlite 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bb51de62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bb51de62",
    "outputId": "25162271-b85f-4e9a-be7b-f2cdc57a1163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (47571, 2)\n",
      "TRAIN Dataset: (38057, 2)\n",
      "TEST Dataset: (9514, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 5e-05\n",
    "MAX_GRAD_NORM = 10\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# tokenization\n",
    "\n",
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Word piece tokenization makes it difficult to match word labels\n",
    "    back up with individual word pieces. This function tokenizes each\n",
    "    word one at a time so that it is easier to preserve the correct\n",
    "    label for each subword. It is, of course, a bit slower in processing\n",
    "    time, but it will help our model achieve higher accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels\n",
    "\n",
    "# Bert imput\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # step 1: tokenize (and adapt corresponding labels)\n",
    "        sentence = self.data.sentence[index]\n",
    "        word_labels = self.data.word_labels[index]\n",
    "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
    "\n",
    "        # step 2: add special tokens (and corresponding labels)\n",
    "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n",
    "        labels.insert(0, \"O\") # add outside label for [CLS] token\n",
    "        labels.insert(-1, \"O\") # add outside label for [SEP] token\n",
    "\n",
    "        # step 3: truncating/padding\n",
    "        maxlen = self.max_len\n",
    "\n",
    "        if (len(tokenized_sentence) > maxlen):\n",
    "          # truncate\n",
    "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
    "          labels = labels[:maxlen]\n",
    "        else:\n",
    "          # pad\n",
    "          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n",
    "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
    "\n",
    "        # step 4: obtain the attention mask\n",
    "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
    "\n",
    "        # step 5: convert tokens to input ids\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "\n",
    "        label_ids = [label2id[label] for label in labels]\n",
    "        # the following line is deprecated\n",
    "        #label_ids = [label if label != 0 else -100 for label in label_ids]\n",
    "\n",
    "        return {\n",
    "              'ids': torch.tensor(ids, dtype=torch.long),\n",
    "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
    "              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# define training size 80/20 split\n",
    "\n",
    "train_size = 0.8\n",
    "train_dataset = data.sample(frac=train_size,random_state=200)\n",
    "test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(data.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)\n",
    "\n",
    "# pyTorch dataloaders\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased',\n",
    "                                                   num_labels=len(id2label),\n",
    "                                                   id2label=id2label,\n",
    "                                                   label2id=label2id)\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "ids = training_set[0][\"ids\"].unsqueeze(0)\n",
    "mask = training_set[0][\"mask\"].unsqueeze(0)\n",
    "targets = training_set[0][\"targets\"].unsqueeze(0)\n",
    "ids = ids.to(device)\n",
    "mask = mask.to(device)\n",
    "targets = targets.to(device)\n",
    "outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "initial_loss = outputs[0]\n",
    "initial_loss\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4e06870d",
   "metadata": {
    "id": "4e06870d"
   },
   "outputs": [],
   "source": [
    "# DLITE Cube Root Loss function\n",
    "class DLITECubeRootLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DLITECubeRootLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Convert logits to probabilities using softmax\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # One-hot encode the targets to get true probabilities\n",
    "        true_probs = F.one_hot(targets, num_classes=probs.size(-1)).float()\n",
    "\n",
    "        # Masks for non-zero elements of probs and true_probs\n",
    "        mask_probs = probs > 0\n",
    "        mask_true_probs = true_probs > 0\n",
    "\n",
    "        # Calculate g function for non-zero elements using the mask\n",
    "        g_probs = torch.zeros_like(probs)\n",
    "        g_true_probs = torch.zeros_like(true_probs)\n",
    "        g_probs[mask_probs] = probs[mask_probs] * (1 - torch.log(probs[mask_probs]))\n",
    "        g_true_probs[mask_true_probs] = true_probs[mask_true_probs] * (1 - torch.log(true_probs[mask_true_probs]))\n",
    "        g_values = torch.abs(g_probs - g_true_probs)\n",
    "\n",
    "        # Calculate delta_h function for non-zero elements using the mask\n",
    "        delta_h_probs = torch.zeros_like(probs)\n",
    "        delta_h_true_probs = torch.zeros_like(true_probs)\n",
    "        delta_h_probs[mask_probs] = probs[mask_probs]**2 * (1 - 2 * torch.log(probs[mask_probs]))\n",
    "        delta_h_true_probs[mask_true_probs] = true_probs[mask_true_probs]**2 * (1 - 2 * torch.log(true_probs[mask_true_probs]))\n",
    "        delta_h_values = torch.abs(delta_h_probs - delta_h_true_probs) / (2 * (probs + true_probs))\n",
    "\n",
    "        # Compute DLITE loss for each class\n",
    "        dl_values = g_values - delta_h_values\n",
    "\n",
    "        # Sum over all classes and average over the batch size\n",
    "        loss = dl_values.sum(dim=-1).mean() ** 1/3\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7d17f298",
   "metadata": {
    "id": "7d17f298"
   },
   "outputs": [],
   "source": [
    "dl_loss_cube_fn = DLITECubeRootLoss()\n",
    "\n",
    "# Defining the training function on the 80% of the dataset for tuning the bert model\n",
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    for idx, batch in enumerate(training_loader):\n",
    "\n",
    "        ids = batch['ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['mask'].to(device, dtype = torch.long)\n",
    "        targets = batch['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "        loss, tr_logits = outputs.loss, outputs.logits\n",
    "\n",
    "        # implement dlite loss function\n",
    "        flatten_targets = targets.view(-1)\n",
    "        flatten_logits = tr_logits.view(-1, model.num_labels)\n",
    "\n",
    "        loss = dl_loss_cube_fn(flatten_logits, flatten_targets)\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets.size(0)\n",
    "\n",
    "        if idx % 100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training dlite loss per 100 training steps: {loss_step}\")\n",
    "\n",
    "        # compute training accuracy\n",
    "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "        tr_preds.extend(predictions)\n",
    "        tr_labels.extend(targets)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n",
    "    print(f\"Training steps: {nb_tr_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "925a3b01",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "925a3b01",
    "outputId": "613ce666-d019-4484-c85d-321bbb82d553"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Training dlite loss per 100 training steps: 0.23480407893657684\n",
      "Training dlite loss per 100 training steps: 0.02231513557605224\n",
      "Training dlite loss per 100 training steps: 0.017305395563610305\n",
      "Training dlite loss per 100 training steps: 0.015717350071809716\n",
      "Training dlite loss per 100 training steps: 0.014840039298021957\n",
      "Training dlite loss per 100 training steps: 0.0142943076721993\n",
      "Training dlite loss per 100 training steps: 0.013843568010153419\n",
      "Training dlite loss per 100 training steps: 0.013468704333352177\n",
      "Training dlite loss per 100 training steps: 0.013334804680925119\n",
      "Training dlite loss per 100 training steps: 0.013254108624382938\n",
      "Training dlite loss per 100 training steps: 0.013165837708131673\n",
      "Training dlite loss per 100 training steps: 0.013190349434165336\n",
      "Training dlite loss per 100 training steps: 0.0131683416045108\n",
      "Training dlite loss per 100 training steps: 0.013180110525408921\n",
      "Training dlite loss per 100 training steps: 0.013109719098090585\n",
      "Training dlite loss per 100 training steps: 0.013021301678597472\n",
      "Training dlite loss per 100 training steps: 0.012934913419323481\n",
      "Training dlite loss per 100 training steps: 0.012898806671617167\n",
      "Training dlite loss per 100 training steps: 0.012849312498298076\n",
      "Training dlite loss per 100 training steps: 0.01281286192008136\n",
      "Training dlite loss per 100 training steps: 0.012801492389302928\n",
      "Training dlite loss per 100 training steps: 0.012763911056730573\n",
      "Training dlite loss per 100 training steps: 0.01276047969967964\n",
      "Training dlite loss per 100 training steps: 0.012720263584554432\n",
      "Training dlite loss per 100 training steps: 0.012715372925080284\n",
      "Training dlite loss per 100 training steps: 0.012698623563223913\n",
      "Training dlite loss per 100 training steps: 0.012691158063936255\n",
      "Training dlite loss per 100 training steps: 0.012642536234981102\n",
      "Training dlite loss per 100 training steps: 0.01260806653159201\n",
      "Training dlite loss per 100 training steps: 0.01258673550405815\n",
      "Training dlite loss per 100 training steps: 0.012567034680383588\n",
      "Training dlite loss per 100 training steps: 0.012556784245449837\n",
      "Training dlite loss per 100 training steps: 0.012527033879709455\n",
      "Training dlite loss per 100 training steps: 0.012492571785899373\n",
      "Training dlite loss per 100 training steps: 0.01246663921681517\n",
      "Training dlite loss per 100 training steps: 0.012482534986311794\n",
      "Training dlite loss per 100 training steps: 0.012488685035792618\n",
      "Training dlite loss per 100 training steps: 0.012482361918439147\n",
      "Training dlite loss per 100 training steps: 0.012475169230496224\n",
      "Training dlite loss per 100 training steps: 0.012474516992321057\n",
      "Training dlite loss per 100 training steps: 0.01248870177194213\n",
      "Training dlite loss per 100 training steps: 0.012508701035433297\n",
      "Training dlite loss per 100 training steps: 0.012491482981394795\n",
      "Training dlite loss per 100 training steps: 0.012476577166058584\n",
      "Training dlite loss per 100 training steps: 0.012474772656915767\n",
      "Training dlite loss per 100 training steps: 0.012462921752325152\n",
      "Training dlite loss per 100 training steps: 0.012463045697277433\n",
      "Training dlite loss per 100 training steps: 0.012443913116400465\n",
      "Training dlite loss per 100 training steps: 0.012447543910487423\n",
      "Training dlite loss per 100 training steps: 0.012441062552547656\n",
      "Training dlite loss per 100 training steps: 0.012441999237905774\n",
      "Training dlite loss per 100 training steps: 0.012431283892587448\n",
      "Training dlite loss per 100 training steps: 0.012410089562185823\n",
      "Training dlite loss per 100 training steps: 0.012409835312923672\n",
      "Training dlite loss per 100 training steps: 0.012395848205746811\n",
      "Training dlite loss per 100 training steps: 0.012393256950514553\n",
      "Training dlite loss per 100 training steps: 0.01240098623091593\n",
      "Training dlite loss per 100 training steps: 0.012425344875451128\n",
      "Training dlite loss per 100 training steps: 0.012421367204033393\n",
      "Training dlite loss per 100 training steps: 0.012435176023028375\n",
      "Training dlite loss per 100 training steps: 0.012445594956927231\n",
      "Training dlite loss per 100 training steps: 0.01245151019765188\n",
      "Training dlite loss per 100 training steps: 0.012444740531587508\n",
      "Training dlite loss per 100 training steps: 0.012442938203892261\n",
      "Training dlite loss per 100 training steps: 0.01244485330012982\n",
      "Training dlite loss per 100 training steps: 0.012437696162222771\n",
      "Training dlite loss per 100 training steps: 0.012429177514698181\n",
      "Training dlite loss per 100 training steps: 0.012434514553412181\n",
      "Training dlite loss per 100 training steps: 0.01243433367865264\n",
      "Training dlite loss per 100 training steps: 0.01242944080530414\n",
      "Training dlite loss per 100 training steps: 0.012421711702899322\n",
      "Training dlite loss per 100 training steps: 0.012423735045967594\n",
      "Training dlite loss per 100 training steps: 0.012426154054678555\n",
      "Training dlite loss per 100 training steps: 0.01242181875986153\n",
      "Training dlite loss per 100 training steps: 0.012417688399610168\n",
      "Training dlite loss per 100 training steps: 0.012411932120907933\n",
      "Training dlite loss per 100 training steps: 0.012407697572099733\n",
      "Training dlite loss per 100 training steps: 0.012425045901660818\n",
      "Training dlite loss per 100 training steps: 0.012430933133696245\n",
      "Training dlite loss per 100 training steps: 0.012423569685369184\n",
      "Training dlite loss per 100 training steps: 0.012424689893430927\n",
      "Training dlite loss per 100 training steps: 0.012421201502793461\n",
      "Training dlite loss per 100 training steps: 0.012421926107779754\n",
      "Training dlite loss per 100 training steps: 0.012423966461201766\n",
      "Training dlite loss per 100 training steps: 0.012424253255127965\n",
      "Training dlite loss per 100 training steps: 0.012419095755231814\n",
      "Training dlite loss per 100 training steps: 0.012425790598055047\n",
      "Training dlite loss per 100 training steps: 0.012426121120768272\n",
      "Training dlite loss per 100 training steps: 0.012438501738657732\n",
      "Training dlite loss per 100 training steps: 0.012436195049585748\n",
      "Training dlite loss per 100 training steps: 0.012435096827004421\n",
      "Training dlite loss per 100 training steps: 0.012431518950240153\n",
      "Training dlite loss per 100 training steps: 0.012426037582344998\n",
      "Training dlite loss per 100 training steps: 0.012419204089454712\n",
      "Training dlite loss per 100 training steps: 0.012417779106013712\n",
      "Training dlite loss per 100 training steps: 0.0124185768187093\n",
      "Training loss epoch: 0.01241802619613295\n",
      "Training accuracy epoch: 0.8243861359043618\n",
      "Training steps: 9515\n",
      "CPU times: user 19min 17s, sys: 4.18 s, total: 19min 21s\n",
      "Wall time: 19min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6f778ec2",
   "metadata": {
    "id": "6f778ec2"
   },
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "\n",
    "            ids = batch['ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['mask'].to(device, dtype = torch.long)\n",
    "            targets = batch['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "            loss, eval_logits = outputs.loss, outputs.logits\n",
    "\n",
    "\n",
    "            # implement dlite loss\n",
    "            flatten_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "            flatten_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            loss = dl_loss_cube_fn(flatten_logits, flatten_targets)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += targets.size(0)\n",
    "\n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation dlite loss per 100 evaluation steps: {loss_step}\")\n",
    "\n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "            eval_labels.extend(targets)\n",
    "            eval_preds.extend(predictions)\n",
    "\n",
    "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    #print(eval_labels)\n",
    "    #print(eval_preds)\n",
    "\n",
    "    labels = [id2label[id.item()] for id in eval_labels]\n",
    "    predictions = [id2label[id.item()] for id in eval_preds]\n",
    "\n",
    "    #print(labels)\n",
    "    #print(predictions)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "    print(f\"Validation steps: {nb_eval_steps}\")\n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "896a9286",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "896a9286",
    "outputId": "489fae45-911b-463f-d786-a8dd9d45f5c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dlite loss per 100 evaluation steps: 0.0026042242534458637\n",
      "Validation dlite loss per 100 evaluation steps: 0.012788826672548896\n",
      "Validation dlite loss per 100 evaluation steps: 0.012716414135403692\n",
      "Validation dlite loss per 100 evaluation steps: 0.01230711514396899\n",
      "Validation dlite loss per 100 evaluation steps: 0.012231838822574618\n",
      "Validation dlite loss per 100 evaluation steps: 0.012072258413593918\n",
      "Validation dlite loss per 100 evaluation steps: 0.012056777077650698\n",
      "Validation dlite loss per 100 evaluation steps: 0.011952839467747133\n",
      "Validation dlite loss per 100 evaluation steps: 0.011739931457615245\n",
      "Validation dlite loss per 100 evaluation steps: 0.011828630627045868\n",
      "Validation dlite loss per 100 evaluation steps: 0.011834568578748872\n",
      "Validation dlite loss per 100 evaluation steps: 0.011915116588842164\n",
      "Validation dlite loss per 100 evaluation steps: 0.011862992956443056\n",
      "Validation dlite loss per 100 evaluation steps: 0.011893944570936352\n",
      "Validation dlite loss per 100 evaluation steps: 0.011929771628398159\n",
      "Validation dlite loss per 100 evaluation steps: 0.011913981206407511\n",
      "Validation dlite loss per 100 evaluation steps: 0.011974986310359734\n",
      "Validation dlite loss per 100 evaluation steps: 0.011968345617023524\n",
      "Validation dlite loss per 100 evaluation steps: 0.011910387996275022\n",
      "Validation dlite loss per 100 evaluation steps: 0.011908529020705915\n",
      "Validation dlite loss per 100 evaluation steps: 0.012012922500553216\n",
      "Validation dlite loss per 100 evaluation steps: 0.011993345590996103\n",
      "Validation dlite loss per 100 evaluation steps: 0.012003352160288996\n",
      "Validation dlite loss per 100 evaluation steps: 0.012066813125226266\n",
      "Validation dlite loss per 100 evaluation steps: 0.012089195538521096\n",
      "Validation dlite loss per 100 evaluation steps: 0.012060328736423221\n",
      "Validation dlite loss per 100 evaluation steps: 0.012038687687386675\n",
      "Validation dlite loss per 100 evaluation steps: 0.01199309916131506\n",
      "Validation dlite loss per 100 evaluation steps: 0.012060938385189821\n",
      "Validation dlite loss per 100 evaluation steps: 0.012078319081353485\n",
      "Validation dlite loss per 100 evaluation steps: 0.012097578631723737\n",
      "Validation dlite loss per 100 evaluation steps: 0.012127772873283534\n",
      "Validation dlite loss per 100 evaluation steps: 0.012132080919720557\n",
      "Validation dlite loss per 100 evaluation steps: 0.01214756702868579\n",
      "Validation dlite loss per 100 evaluation steps: 0.012122325768384984\n",
      "Validation dlite loss per 100 evaluation steps: 0.012125304498319972\n",
      "Validation dlite loss per 100 evaluation steps: 0.012144027705973915\n",
      "Validation dlite loss per 100 evaluation steps: 0.012142037258174776\n",
      "Validation dlite loss per 100 evaluation steps: 0.012103839857249808\n",
      "Validation dlite loss per 100 evaluation steps: 0.012125345022365269\n",
      "Validation dlite loss per 100 evaluation steps: 0.012119414621825145\n",
      "Validation dlite loss per 100 evaluation steps: 0.012101073292590704\n",
      "Validation dlite loss per 100 evaluation steps: 0.012105301357408296\n",
      "Validation dlite loss per 100 evaluation steps: 0.01214505606261301\n",
      "Validation dlite loss per 100 evaluation steps: 0.012165844238329717\n",
      "Validation dlite loss per 100 evaluation steps: 0.012153308520043362\n",
      "Validation dlite loss per 100 evaluation steps: 0.012150090712366618\n",
      "Validation dlite loss per 100 evaluation steps: 0.01215642711604442\n",
      "Validation Loss: 0.012143063726347755\n",
      "Validation Accuracy: 0.8291057558011954\n",
      "Validation steps: 4757\n",
      "CPU times: user 1min 45s, sys: 381 ms, total: 1min 46s\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "labels, predictions = valid(model, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2fd9426a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2fd9426a",
    "outputId": "0ff5f084-3f82-4387-cb59-048e20d43488"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         geo       0.00      0.00      0.00     11232\n",
      "         gpe       0.00      0.00      0.00      3293\n",
      "         org       0.00      0.00      0.00      6531\n",
      "         per       0.00      0.00      0.00      5196\n",
      "         tim       0.00      0.00      0.00      4360\n",
      "\n",
      "   micro avg       0.00      0.00      0.00     30612\n",
      "   macro avg       0.00      0.00      0.00     30612\n",
      "weighted avg       0.00      0.00      0.00     30612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "print(classification_report([labels], [predictions]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "249391a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "249391a8",
    "outputId": "acc4a453-308e-47c8-a480-a485e26e7a90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8241245475557106, 0.8241245475557106, 0.8241245475557105, None)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(labels, predictions, average = \"micro\", zero_division = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "17fe101a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17fe101a",
    "outputId": "52676ee1-d303-4b61-8571-081866050b8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9840113225050646, 0.09090909090909091, 0.08214396710416218, None)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(labels, predictions, average = \"macro\", zero_division = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1635e8ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1635e8ed",
    "outputId": "c0cd7e81-dcb9-45bc-a14d-c9d132b79463"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8550567223281941, 0.8241245475557106, 0.7446654569656372, None)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(labels, predictions, average = \"weighted\", zero_division = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "277f2b37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "277f2b37",
    "outputId": "df8776fc-01a4-42ce-82c8-f9d9499d5d85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india has a capital called mumbai . on wednesday , the president will give a presentation\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"India has a capital called Mumbai. On Wednesday, the president will give a presentation\"\n",
    "\n",
    "inputs = tokenizer(sentence, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "\n",
    "# move to gpu\n",
    "ids = inputs[\"input_ids\"].to(device)\n",
    "mask = inputs[\"attention_mask\"].to(device)\n",
    "# forward pass\n",
    "outputs = model(ids, mask)\n",
    "logits = outputs[0]\n",
    "\n",
    "active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "token_predictions = [id2label[i] for i in flattened_predictions.cpu().numpy()]\n",
    "wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
    "\n",
    "word_level_predictions = []\n",
    "for pair in wp_preds:\n",
    "  if (pair[0].startswith(\" ##\")) or (pair[0] in ['[CLS]', '[SEP]', '[PAD]']):\n",
    "    # skip prediction\n",
    "    continue\n",
    "  else:\n",
    "    word_level_predictions.append(pair[1])\n",
    "\n",
    "# we join tokens, if they are not special ones\n",
    "str_rep = \" \".join([t[0] for t in wp_preds if t[0] not in ['[CLS]', '[SEP]', '[PAD]']]).replace(\" ##\", \"\")\n",
    "print(str_rep)\n",
    "print(word_level_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0b316609",
   "metadata": {
    "id": "0b316609"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b013592c",
   "metadata": {
    "id": "b013592c"
   },
   "outputs": [],
   "source": [
    "# LIT implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "369bb63e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "369bb63e",
    "outputId": "cc448c3b-88d3-4dd7-adad-98b1dfc0421a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (47571, 2)\n",
      "TRAIN Dataset: (38057, 2)\n",
      "TEST Dataset: (9514, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 5e-05\n",
    "MAX_GRAD_NORM = 10\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# tokenization\n",
    "\n",
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Word piece tokenization makes it difficult to match word labels\n",
    "    back up with individual word pieces. This function tokenizes each\n",
    "    word one at a time so that it is easier to preserve the correct\n",
    "    label for each subword. It is, of course, a bit slower in processing\n",
    "    time, but it will help our model achieve higher accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels\n",
    "\n",
    "# Bert imput\n",
    "\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # step 1: tokenize (and adapt corresponding labels)\n",
    "        sentence = self.data.sentence[index]\n",
    "        word_labels = self.data.word_labels[index]\n",
    "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
    "\n",
    "        # step 2: add special tokens (and corresponding labels)\n",
    "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n",
    "        labels.insert(0, \"O\") # add outside label for [CLS] token\n",
    "        labels.insert(-1, \"O\") # add outside label for [SEP] token\n",
    "\n",
    "        # step 3: truncating/padding\n",
    "        maxlen = self.max_len\n",
    "\n",
    "        if (len(tokenized_sentence) > maxlen):\n",
    "          # truncate\n",
    "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
    "          labels = labels[:maxlen]\n",
    "        else:\n",
    "          # pad\n",
    "          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n",
    "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
    "\n",
    "        # step 4: obtain the attention mask\n",
    "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
    "\n",
    "        # step 5: convert tokens to input ids\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "\n",
    "        label_ids = [label2id[label] for label in labels]\n",
    "        # the following line is deprecated\n",
    "        #label_ids = [label if label != 0 else -100 for label in label_ids]\n",
    "\n",
    "        return {\n",
    "              'ids': torch.tensor(ids, dtype=torch.long),\n",
    "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
    "              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# define training size 80/20 split\n",
    "\n",
    "train_size = 0.8\n",
    "train_dataset = data.sample(frac=train_size,random_state=200)\n",
    "test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(data.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)\n",
    "\n",
    "# pyTorch dataloaders\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased',\n",
    "                                                   num_labels=len(id2label),\n",
    "                                                   id2label=id2label,\n",
    "                                                   label2id=label2id)\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "ids = training_set[0][\"ids\"].unsqueeze(0)\n",
    "mask = training_set[0][\"mask\"].unsqueeze(0)\n",
    "targets = training_set[0][\"targets\"].unsqueeze(0)\n",
    "ids = ids.to(device)\n",
    "mask = mask.to(device)\n",
    "targets = targets.to(device)\n",
    "outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "initial_loss = outputs[0]\n",
    "initial_loss\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "63a41faf",
   "metadata": {
    "id": "63a41faf"
   },
   "outputs": [],
   "source": [
    "# LIT Loss function\n",
    "class LITLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LITLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Convert logits to probabilities using softmax\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # One-hot encode the targets to get true probabilities\n",
    "        true_probs = F.one_hot(targets, num_classes=probs.size(-1)).float()\n",
    "\n",
    "        # Masks for non-zero elements of probs and true_probs\n",
    "        mask_probs = probs > 0\n",
    "        mask_true_probs = true_probs > 0\n",
    "\n",
    "        # Calculate g function for non-zero elements using the mask\n",
    "        g_probs = torch.zeros_like(probs)\n",
    "        g_true_probs = torch.zeros_like(true_probs)\n",
    "        g_probs[mask_probs] = probs[mask_probs] * (1 - torch.log(probs[mask_probs]))\n",
    "        g_true_probs[mask_true_probs] = true_probs[mask_true_probs] * (1 - torch.log(true_probs[mask_true_probs]))\n",
    "        g_values = torch.abs(g_probs - g_true_probs)\n",
    "\n",
    "\n",
    "        # Sum over all classes and average over the batch size\n",
    "        loss = g_values.sum(dim=-1).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fae227bf",
   "metadata": {
    "id": "fae227bf"
   },
   "outputs": [],
   "source": [
    "lit_loss_fn = LITLoss()\n",
    "\n",
    "# Defining the training function on the 80% of the dataset for tuning the bert model\n",
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    for idx, batch in enumerate(training_loader):\n",
    "\n",
    "        ids = batch['ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['mask'].to(device, dtype = torch.long)\n",
    "        targets = batch['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "        loss, tr_logits = outputs.loss, outputs.logits\n",
    "\n",
    "        # implement dlite loss function\n",
    "        flatten_targets = targets.view(-1)\n",
    "        flatten_logits = tr_logits.view(-1, model.num_labels)\n",
    "\n",
    "        loss = lit_loss_fn(flatten_logits, flatten_targets)\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets.size(0)\n",
    "\n",
    "        if idx % 100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training dlite loss per 100 training steps: {loss_step}\")\n",
    "\n",
    "        # compute training accuracy\n",
    "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "        tr_preds.extend(predictions)\n",
    "        tr_labels.extend(targets)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n",
    "    print(f\"Training steps: {nb_tr_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8c8a50bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c8a50bb",
    "outputId": "d2019ea7-f38c-4a13-9826-0e6179b4942a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Training dlite loss per 100 training steps: 3.733372688293457\n",
      "Training dlite loss per 100 training steps: 0.22335822909775346\n",
      "Training dlite loss per 100 training steps: 0.15422709554952768\n",
      "Training dlite loss per 100 training steps: 0.12651470846703827\n",
      "Training dlite loss per 100 training steps: 0.11468221188088568\n",
      "Training dlite loss per 100 training steps: 0.10673201963246917\n",
      "Training dlite loss per 100 training steps: 0.10034291545791511\n",
      "Training dlite loss per 100 training steps: 0.09683792364631465\n",
      "Training dlite loss per 100 training steps: 0.09429537052228773\n",
      "Training dlite loss per 100 training steps: 0.09239889854644337\n",
      "Training dlite loss per 100 training steps: 0.09018131541602264\n",
      "Training dlite loss per 100 training steps: 0.08917213134270087\n",
      "Training dlite loss per 100 training steps: 0.08770477056216765\n",
      "Training dlite loss per 100 training steps: 0.08655226493097175\n",
      "Training dlite loss per 100 training steps: 0.08554752791165138\n",
      "Training dlite loss per 100 training steps: 0.08410441867895808\n",
      "Training dlite loss per 100 training steps: 0.08351074688453321\n",
      "Training dlite loss per 100 training steps: 0.08280172462755543\n",
      "Training dlite loss per 100 training steps: 0.08274071133582553\n",
      "Training dlite loss per 100 training steps: 0.08206667985159881\n",
      "Training dlite loss per 100 training steps: 0.08200191413577151\n",
      "Training dlite loss per 100 training steps: 0.08143881621795163\n",
      "Training dlite loss per 100 training steps: 0.08134515996649395\n",
      "Training dlite loss per 100 training steps: 0.08103680780675725\n",
      "Training dlite loss per 100 training steps: 0.08060082406573027\n",
      "Training dlite loss per 100 training steps: 0.0800900299723291\n",
      "Training dlite loss per 100 training steps: 0.07993961706663912\n",
      "Training dlite loss per 100 training steps: 0.07972632461690035\n",
      "Training dlite loss per 100 training steps: 0.07969121046329229\n",
      "Training dlite loss per 100 training steps: 0.0794765429614094\n",
      "Training dlite loss per 100 training steps: 0.07924478709365633\n",
      "Training dlite loss per 100 training steps: 0.07925204757943971\n",
      "Training dlite loss per 100 training steps: 0.07906347822346131\n",
      "Training dlite loss per 100 training steps: 0.07895012617987486\n",
      "Training dlite loss per 100 training steps: 0.07879394799281927\n",
      "Training dlite loss per 100 training steps: 0.07890768857701515\n",
      "Training dlite loss per 100 training steps: 0.07859523006607526\n",
      "Training dlite loss per 100 training steps: 0.07834601949187216\n",
      "Training dlite loss per 100 training steps: 0.07806361263512981\n",
      "Training dlite loss per 100 training steps: 0.0778797404095576\n",
      "Training dlite loss per 100 training steps: 0.07774113108302703\n",
      "Training dlite loss per 100 training steps: 0.07755589344312766\n",
      "Training dlite loss per 100 training steps: 0.07752727595398434\n",
      "Training dlite loss per 100 training steps: 0.07735554560713608\n",
      "Training dlite loss per 100 training steps: 0.07722087536874349\n",
      "Training dlite loss per 100 training steps: 0.07708781762206665\n",
      "Training dlite loss per 100 training steps: 0.07710399491700287\n",
      "Training dlite loss per 100 training steps: 0.07693748298541511\n",
      "Training dlite loss per 100 training steps: 0.07694630533695386\n",
      "Training dlite loss per 100 training steps: 0.07693402298828168\n",
      "Training dlite loss per 100 training steps: 0.07682535629278865\n",
      "Training dlite loss per 100 training steps: 0.0768748540271578\n",
      "Training dlite loss per 100 training steps: 0.07683155481125717\n",
      "Training dlite loss per 100 training steps: 0.07675081745841027\n",
      "Training dlite loss per 100 training steps: 0.07668173550127876\n",
      "Training dlite loss per 100 training steps: 0.07672663775383883\n",
      "Training dlite loss per 100 training steps: 0.07669251114541376\n",
      "Training dlite loss per 100 training steps: 0.07669588578917846\n",
      "Training dlite loss per 100 training steps: 0.07653213687165607\n",
      "Training dlite loss per 100 training steps: 0.07646263226571576\n",
      "Training dlite loss per 100 training steps: 0.07646833991550529\n",
      "Training dlite loss per 100 training steps: 0.07639958175414861\n",
      "Training dlite loss per 100 training steps: 0.07643319363590004\n",
      "Training dlite loss per 100 training steps: 0.07629586884658193\n",
      "Training dlite loss per 100 training steps: 0.07619578209516346\n",
      "Training dlite loss per 100 training steps: 0.07619430688095068\n",
      "Training dlite loss per 100 training steps: 0.0761342859637838\n",
      "Training dlite loss per 100 training steps: 0.0760929566713452\n",
      "Training dlite loss per 100 training steps: 0.07604364885357145\n",
      "Training dlite loss per 100 training steps: 0.0759238782022382\n",
      "Training dlite loss per 100 training steps: 0.07597212207119289\n",
      "Training dlite loss per 100 training steps: 0.07601900370468118\n",
      "Training dlite loss per 100 training steps: 0.07596476717029102\n",
      "Training dlite loss per 100 training steps: 0.07601259872912247\n",
      "Training dlite loss per 100 training steps: 0.0759979100650561\n",
      "Training dlite loss per 100 training steps: 0.07595184363746589\n",
      "Training dlite loss per 100 training steps: 0.07593679381357273\n",
      "Training dlite loss per 100 training steps: 0.07594901607746633\n",
      "Training dlite loss per 100 training steps: 0.07592336759590095\n",
      "Training dlite loss per 100 training steps: 0.07583903856898924\n",
      "Training dlite loss per 100 training steps: 0.07582223687112812\n",
      "Training dlite loss per 100 training steps: 0.07581404553973477\n",
      "Training dlite loss per 100 training steps: 0.07581748404147527\n",
      "Training dlite loss per 100 training steps: 0.0759078945780396\n",
      "Training dlite loss per 100 training steps: 0.075928730070582\n",
      "Training dlite loss per 100 training steps: 0.07592196328463582\n",
      "Training dlite loss per 100 training steps: 0.07588446953218152\n",
      "Training dlite loss per 100 training steps: 0.07583077660597463\n",
      "Training dlite loss per 100 training steps: 0.07577031364873527\n",
      "Training dlite loss per 100 training steps: 0.07577308688161254\n",
      "Training dlite loss per 100 training steps: 0.07572849369784684\n",
      "Training dlite loss per 100 training steps: 0.07570719858457928\n",
      "Training dlite loss per 100 training steps: 0.0756502791376027\n",
      "Training dlite loss per 100 training steps: 0.0756491804422397\n",
      "Training dlite loss per 100 training steps: 0.07567261983672084\n",
      "Training dlite loss per 100 training steps: 0.07567665265157297\n",
      "Training loss epoch: 0.07564536104995452\n",
      "Training accuracy epoch: 0.8246610620994616\n",
      "Training steps: 9515\n",
      "CPU times: user 19min 9s, sys: 4.09 s, total: 19min 13s\n",
      "Wall time: 19min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f1cebf3c",
   "metadata": {
    "id": "f1cebf3c"
   },
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "\n",
    "            ids = batch['ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['mask'].to(device, dtype = torch.long)\n",
    "            targets = batch['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "            loss, eval_logits = outputs.loss, outputs.logits\n",
    "\n",
    "\n",
    "            # implement dlite loss\n",
    "            flatten_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "            flatten_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            loss = lit_loss_fn(flatten_logits, flatten_targets)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += targets.size(0)\n",
    "\n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation dlite loss per 100 evaluation steps: {loss_step}\")\n",
    "\n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
    "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "            eval_labels.extend(targets)\n",
    "            eval_preds.extend(predictions)\n",
    "\n",
    "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    #print(eval_labels)\n",
    "    #print(eval_preds)\n",
    "\n",
    "    labels = [id2label[id.item()] for id in eval_labels]\n",
    "    predictions = [id2label[id.item()] for id in eval_preds]\n",
    "\n",
    "    #print(labels)\n",
    "    #print(predictions)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "    print(f\"Validation steps: {nb_eval_steps}\")\n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "43e1df93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43e1df93",
    "outputId": "f38864e4-1beb-4fc7-8c4f-557d0772b34b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dlite loss per 100 evaluation steps: 0.08593813329935074\n",
      "Validation dlite loss per 100 evaluation steps: 0.06992637758960574\n",
      "Validation dlite loss per 100 evaluation steps: 0.06938029961096351\n",
      "Validation dlite loss per 100 evaluation steps: 0.06706874204203409\n",
      "Validation dlite loss per 100 evaluation steps: 0.06879349689560606\n",
      "Validation dlite loss per 100 evaluation steps: 0.06814559536563539\n",
      "Validation dlite loss per 100 evaluation steps: 0.06863624245348723\n",
      "Validation dlite loss per 100 evaluation steps: 0.06965559222045878\n",
      "Validation dlite loss per 100 evaluation steps: 0.07010831285390416\n",
      "Validation dlite loss per 100 evaluation steps: 0.07018307110537843\n",
      "Validation dlite loss per 100 evaluation steps: 0.07078922123784773\n",
      "Validation dlite loss per 100 evaluation steps: 0.07107948363752684\n",
      "Validation dlite loss per 100 evaluation steps: 0.07231667339844428\n",
      "Validation dlite loss per 100 evaluation steps: 0.07237885341828164\n",
      "Validation dlite loss per 100 evaluation steps: 0.07227044201367235\n",
      "Validation dlite loss per 100 evaluation steps: 0.07220770502843488\n",
      "Validation dlite loss per 100 evaluation steps: 0.07247974936384469\n",
      "Validation dlite loss per 100 evaluation steps: 0.07244423410017133\n",
      "Validation dlite loss per 100 evaluation steps: 0.07235627050501318\n",
      "Validation dlite loss per 100 evaluation steps: 0.07270085828766112\n",
      "Validation dlite loss per 100 evaluation steps: 0.07244878571337035\n",
      "Validation dlite loss per 100 evaluation steps: 0.07267436160408602\n",
      "Validation dlite loss per 100 evaluation steps: 0.0726522704258377\n",
      "Validation dlite loss per 100 evaluation steps: 0.07282223405773225\n",
      "Validation dlite loss per 100 evaluation steps: 0.07282510890778124\n",
      "Validation dlite loss per 100 evaluation steps: 0.07319323266076491\n",
      "Validation dlite loss per 100 evaluation steps: 0.07349400263020683\n",
      "Validation dlite loss per 100 evaluation steps: 0.07332706478007735\n",
      "Validation dlite loss per 100 evaluation steps: 0.07331708425554608\n",
      "Validation dlite loss per 100 evaluation steps: 0.0732162285619212\n",
      "Validation dlite loss per 100 evaluation steps: 0.07333816810385312\n",
      "Validation dlite loss per 100 evaluation steps: 0.07360592336443035\n",
      "Validation dlite loss per 100 evaluation steps: 0.0734566836838863\n",
      "Validation dlite loss per 100 evaluation steps: 0.07342772129363387\n",
      "Validation dlite loss per 100 evaluation steps: 0.07332006292542162\n",
      "Validation dlite loss per 100 evaluation steps: 0.07323417524251463\n",
      "Validation dlite loss per 100 evaluation steps: 0.07318126173817796\n",
      "Validation dlite loss per 100 evaluation steps: 0.07319031329175686\n",
      "Validation dlite loss per 100 evaluation steps: 0.07315778097618421\n",
      "Validation dlite loss per 100 evaluation steps: 0.07308285730873704\n",
      "Validation dlite loss per 100 evaluation steps: 0.07301167892684121\n",
      "Validation dlite loss per 100 evaluation steps: 0.07302779281745854\n",
      "Validation dlite loss per 100 evaluation steps: 0.07296317350534604\n",
      "Validation dlite loss per 100 evaluation steps: 0.07298874804501619\n",
      "Validation dlite loss per 100 evaluation steps: 0.07288357334744255\n",
      "Validation dlite loss per 100 evaluation steps: 0.07284902960968193\n",
      "Validation dlite loss per 100 evaluation steps: 0.07290428345964504\n",
      "Validation dlite loss per 100 evaluation steps: 0.07283420745053593\n",
      "Validation Loss: 0.07285872550944006\n",
      "Validation Accuracy: 0.828783557099785\n",
      "Validation steps: 4757\n",
      "CPU times: user 1min 42s, sys: 400 ms, total: 1min 43s\n",
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "labels, predictions = valid(model, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "187ccfcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "187ccfcd",
    "outputId": "1eed2c59-f461-400a-ec62-e690942518eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         geo       0.00      0.00      0.00     11232\n",
      "         gpe       0.00      0.00      0.00      3293\n",
      "         org       0.00      0.00      0.00      6531\n",
      "         per       0.00      0.00      0.00      5196\n",
      "         tim       0.00      0.00      0.00      4360\n",
      "\n",
      "   micro avg       0.00      0.00      0.00     30612\n",
      "   macro avg       0.00      0.00      0.00     30612\n",
      "weighted avg       0.00      0.00      0.00     30612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "print(classification_report([labels], [predictions]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0118e9713f624e96a48d579636a4b988": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a50cf9045a9d4713b1b882b8fe5983aa",
       "IPY_MODEL_2656e3b279814c3ba96c2d887ed0a23c",
       "IPY_MODEL_9e1e8507a3604db8b41e721f42a92a22"
      ],
      "layout": "IPY_MODEL_d26fd5ed228047ed8d1a3e1f372cb023"
     }
    },
    "0e89d5a0770f41559be9b818ed986db9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb80aac8df054449a9d9e813d7ac46ef",
      "placeholder": "​",
      "style": "IPY_MODEL_72bdce3d60ea49acabf03d6fcf45458a",
      "value": "config.json: 100%"
     }
    },
    "138f78a807c44077acc47b16fc42dd67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2232ac4603f840689b772854343f55eb",
       "IPY_MODEL_80fd709f663446abbe9a9045349ae5f2",
       "IPY_MODEL_e3398dc1bdc149a1ae5a09aa13eb625a"
      ],
      "layout": "IPY_MODEL_13fae4cb29134d62966f3cf140caa4a8"
     }
    },
    "13fae4cb29134d62966f3cf140caa4a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2232ac4603f840689b772854343f55eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8589f00ef3c848bb9630ef30418d1ea5",
      "placeholder": "​",
      "style": "IPY_MODEL_2988641d13d246ccb678c118b1e7aac2",
      "value": "vocab.txt: 100%"
     }
    },
    "2656e3b279814c3ba96c2d887ed0a23c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7cbec94204f248f596e5e85d4cb5a68b",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8c81a1bf403740c8b42665fcccfc2887",
      "value": 466062
     }
    },
    "2988641d13d246ccb678c118b1e7aac2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "388035b119b64f7c8d525316299cdfd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d04b634bf96c4a53b4860cd7721dedd2",
      "placeholder": "​",
      "style": "IPY_MODEL_5cec4229c71b48eab696c84790f199d4",
      "value": " 570/570 [00:00&lt;00:00, 39.6kB/s]"
     }
    },
    "40555c8120ed42599a152208926a34d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "462792fae6074789bdc7e675f820a4c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0e89d5a0770f41559be9b818ed986db9",
       "IPY_MODEL_e8fdfdf53c26418485c5f05c6029a111",
       "IPY_MODEL_388035b119b64f7c8d525316299cdfd5"
      ],
      "layout": "IPY_MODEL_d235ea9a08fd4056a73784f34be49966"
     }
    },
    "4ee4ac924bce4a8a8c10de3b03187056": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56f660b673744d85a2cc9897465cd65e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5cec4229c71b48eab696c84790f199d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5d2ca44bcf1549a6881634d6af96c405": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5df80a5fca0247f7aeae095bcb557783": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e35936100c4479c9f3094778aa32aaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_deeb7c7b77ad4501bd2d0a58f0602111",
      "placeholder": "​",
      "style": "IPY_MODEL_797c2bf7975f4166bd82c48b67121fbd",
      "value": " 440M/440M [00:05&lt;00:00, 66.2MB/s]"
     }
    },
    "68f0a32e8add4924a23e6244420a4097": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "696102f8d24f4bffb765a618f45642b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6bbc04eda753406cb7804b2373e64e47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72bdce3d60ea49acabf03d6fcf45458a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "73038743885d457f83ced11dcd108488": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56f660b673744d85a2cc9897465cd65e",
      "max": 440449768,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f9e395910dbe41b583b4c1456838913e",
      "value": 440449768
     }
    },
    "75c43918b8974816aa5613c12935b789": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4780b187f3e4ad09cebd74744943ffa",
      "placeholder": "​",
      "style": "IPY_MODEL_e93905cc931c4c7cb6012450f40a2a20",
      "value": " 28.0/28.0 [00:00&lt;00:00, 268B/s]"
     }
    },
    "797c2bf7975f4166bd82c48b67121fbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7cbec94204f248f596e5e85d4cb5a68b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d807fede1d142828a8e8a1326736d75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b890ba1ede044bafb3e6be5dcbd7ac8e",
       "IPY_MODEL_73038743885d457f83ced11dcd108488",
       "IPY_MODEL_5e35936100c4479c9f3094778aa32aaf"
      ],
      "layout": "IPY_MODEL_d5578e892d134774a360965cb533818c"
     }
    },
    "7da4f585b97d4121a1c231d368d02c2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7eb5064ad48d44c9babdc53c9efe0e97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "80fd709f663446abbe9a9045349ae5f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9edcf5221340497ba4e0d0c486207f3b",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7da4f585b97d4121a1c231d368d02c2c",
      "value": 231508
     }
    },
    "8589f00ef3c848bb9630ef30418d1ea5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c81a1bf403740c8b42665fcccfc2887": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "90e33b49bec747d5a4b3ee38d72e4b4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "91e8394b20094101a4125c21a50939da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e1e8507a3604db8b41e721f42a92a22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d2ca44bcf1549a6881634d6af96c405",
      "placeholder": "​",
      "style": "IPY_MODEL_90e33b49bec747d5a4b3ee38d72e4b4a",
      "value": " 466k/466k [00:00&lt;00:00, 798kB/s]"
     }
    },
    "9edcf5221340497ba4e0d0c486207f3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a50cf9045a9d4713b1b882b8fe5983aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e23c972ae1234ef693acf50f5635fe58",
      "placeholder": "​",
      "style": "IPY_MODEL_f478c0d581d148b4bc28cf562f861ddc",
      "value": "tokenizer.json: 100%"
     }
    },
    "b890ba1ede044bafb3e6be5dcbd7ac8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_696102f8d24f4bffb765a618f45642b2",
      "placeholder": "​",
      "style": "IPY_MODEL_be66705cc9fc41238670ecf5dcfab3b0",
      "value": "model.safetensors: 100%"
     }
    },
    "be66705cc9fc41238670ecf5dcfab3b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c1e690cbf82942d997c62e48249ed74c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d4594f8894db4360ac0298a466b5dc23",
      "placeholder": "​",
      "style": "IPY_MODEL_5df80a5fca0247f7aeae095bcb557783",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "c90782f047654ba9acdddfb97cd2b5d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_91e8394b20094101a4125c21a50939da",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_68f0a32e8add4924a23e6244420a4097",
      "value": 28
     }
    },
    "cb80aac8df054449a9d9e813d7ac46ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d04b634bf96c4a53b4860cd7721dedd2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d235ea9a08fd4056a73784f34be49966": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d26fd5ed228047ed8d1a3e1f372cb023": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4594f8894db4360ac0298a466b5dc23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5578e892d134774a360965cb533818c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "deeb7c7b77ad4501bd2d0a58f0602111": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e23c972ae1234ef693acf50f5635fe58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3398dc1bdc149a1ae5a09aa13eb625a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6bbc04eda753406cb7804b2373e64e47",
      "placeholder": "​",
      "style": "IPY_MODEL_e516609cfff940f78c7eee96ab2b1464",
      "value": " 232k/232k [00:00&lt;00:00, 592kB/s]"
     }
    },
    "e4780b187f3e4ad09cebd74744943ffa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e516609cfff940f78c7eee96ab2b1464": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e8fdfdf53c26418485c5f05c6029a111": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ee4ac924bce4a8a8c10de3b03187056",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7eb5064ad48d44c9babdc53c9efe0e97",
      "value": 570
     }
    },
    "e93905cc931c4c7cb6012450f40a2a20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f478c0d581d148b4bc28cf562f861ddc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f8235fe44b404c95a24dadc7e3e6f703": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c1e690cbf82942d997c62e48249ed74c",
       "IPY_MODEL_c90782f047654ba9acdddfb97cd2b5d1",
       "IPY_MODEL_75c43918b8974816aa5613c12935b789"
      ],
      "layout": "IPY_MODEL_40555c8120ed42599a152208926a34d5"
     }
    },
    "f9e395910dbe41b583b4c1456838913e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
