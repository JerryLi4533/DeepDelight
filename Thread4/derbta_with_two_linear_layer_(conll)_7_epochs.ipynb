{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": [],
        "id": "PEBdvK_nGr3w"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import classification_report\n",
        "import random ,json\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, Dataset\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hskApG2OGr3x"
      },
      "source": [
        "## Setting Basic Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ZlnriKCUGr3y"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    batch_size = 4\n",
        "    epochs = 7\n",
        "    lr = 1e-5\n",
        "    seed = 123\n",
        "\n",
        "\n",
        "    # if Linear layer is empty, then not using the linear layer. Setting the linear layer based on layers number required. (can customized)\n",
        "    linear_layer = [512, 384, 256, 128, 128, 128, 128, 128]\n",
        "    # Setting the linear layer number\n",
        "    linear_layer_num = 2\n",
        "    # if lstm layer is 0, then not using the lstm layer. Setting the lstm layer based on layers number required\n",
        "    lstm_layer_num = 0\n",
        "    bi_lstm=False\n",
        "\n",
        "\n",
        "    # Internet resource; download from Internet\n",
        "    # model_name = \"microsoft/deberta-v3-base\"\n",
        "\n",
        "    model_name = \"microsoft/deberta-base\"\n",
        "\n",
        "    hidden_size=768\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_data_name = \"conll2003\"\n",
        "\n",
        "    @classmethod\n",
        "    def describe(cls):\n",
        "        parm = {\"train_data_name\": cls.train_data_name,\n",
        "                \"encoder_name\": cls.model_name,\n",
        "                \"batch_size\": cls.batch_size,\n",
        "                \"epochs\": cls.epochs,\n",
        "                \"lr\": cls.lr,\n",
        "                \"seed\": cls.seed,\n",
        "                \"bi_lstm\": cls.bi_lstm,\n",
        "                \"lstm_layer_num\": cls.lstm_layer_num,\n",
        "                \"linear_layer\": cls.linear_layer,\n",
        "                \"linear_layer_num\":cls.linear_layer_num}\n",
        "        return json.dumps(parm , ensure_ascii=False, indent=2)\n",
        "\n",
        "random.seed(Config.seed)\n",
        "np.random.seed(Config.seed)\n",
        "torch.manual_seed(Config.seed)\n",
        "torch.cuda.manual_seed_all(Config.seed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## given configuration result"
      ],
      "metadata": {
        "id": "qQGJgAySrwEr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BObln4RIGr3y",
        "outputId": "11d4e3b8-3208-485e-8198-b478e6ecca12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"train_data_name\": \"conll2003\",\n",
            "  \"encoder_name\": \"microsoft/deberta-base\",\n",
            "  \"batch_size\": 4,\n",
            "  \"epochs\": 7,\n",
            "  \"lr\": 1e-05,\n",
            "  \"seed\": 123,\n",
            "  \"bi_lstm\": false,\n",
            "  \"lstm_layer_num\": 0,\n",
            "  \"linear_layer\": [\n",
            "    512,\n",
            "    384,\n",
            "    256,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128\n",
            "  ],\n",
            "  \"linear_layer_num\": 2\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(Config.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9ig_SZXGr3z"
      },
      "source": [
        "## Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGZ_d-HrGr3z",
        "outputId": "2bf0ec5d-ff43-4efd-a10a-74d22d001b43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14986, 2) (3465, 2) (3683, 2)\n"
          ]
        }
      ],
      "source": [
        "def read_conll2003(file_path):\n",
        "    data = []\n",
        "    sample = []\n",
        "    for idx, line in enumerate(open(file_path)):\n",
        "        if idx == 0:\n",
        "            continue\n",
        "        line = line.strip()\n",
        "        if line == \"\":\n",
        "            if len(sample) != 0:\n",
        "                data.append(sample)\n",
        "            sample = []\n",
        "        else:\n",
        "            line = line.split()\n",
        "            assert len(line) == 4\n",
        "            sample.append([line[0], line[-1]])\n",
        "    if len(sample) != 0:\n",
        "        data.append(sample)\n",
        "    data = [{\"word\": [i[0] for i in sample], \"tag\": [i[1] for i in sample]} for sample in data]\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Setting the chosen dataset, conll2003 or ner_datasetreference.\n",
        "if Config.train_data_name == \"conll2003\":\n",
        "    train_path = os.path.join(Config.train_data_name, 'train.txt')\n",
        "    dev_path = os.path.join(Config.train_data_name, 'valid.txt')\n",
        "    test_path = os.path.join(Config.train_data_name, 'test.txt')\n",
        "    train_df = read_conll2003(train_path)\n",
        "    valid_df = read_conll2003(dev_path)\n",
        "    test_df = read_conll2003(test_path)\n",
        "    print(train_df.shape, valid_df.shape, test_df.shape)\n",
        "elif Config.train_data_name == \"ner_datasetreference\":\n",
        "    df = pd.read_csv(\"ner_datasetreference.csv\", encoding='iso-8859-1')\n",
        "    data = []\n",
        "    word, tag = [], []\n",
        "    for i,j,k in zip(df['Sentence #'], df['Word'], df['Tag']):\n",
        "        if not pd.isnull(i):\n",
        "            assert i.startswith('Sentence')\n",
        "            if len(word) > 0:\n",
        "                data.append({\"word\":word, \"tag\":tag})\n",
        "            word, tag = [], []\n",
        "        if isinstance(j, str) and isinstance(k, str):\n",
        "            # remove 'art', 'eve', 'nat' label for better macro results\n",
        "            if any( t in k for t in ['art', 'eve', 'nat']):\n",
        "                continue\n",
        "            word.append(j)\n",
        "            tag.append(k)\n",
        "    if len(word) > 0:\n",
        "        data.append({\"word\":word, \"tag\":tag})\n",
        "        word, tag = [], []\n",
        "    print(data[0], data[-1])\n",
        "    df = pd.DataFrame(data)\n",
        "    train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
        "    valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "    print(df.shape, train_df.shape, valid_df.shape, test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "BYJxdEf9Gr30",
        "outputId": "b35b03aa-4849-4ae1-c610-e9b0aacc924e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    word  \\\n",
              "0      [EU, rejects, German, call, to, boycott, Briti...   \n",
              "1                                     [Peter, Blackburn]   \n",
              "2                                 [BRUSSELS, 1996-08-22]   \n",
              "3      [The, European, Commission, said, on, Thursday...   \n",
              "4      [Germany, 's, representative, to, the, Europea...   \n",
              "...                                                  ...   \n",
              "14981                                    [on, Friday, :]   \n",
              "14982                                    [Division, two]   \n",
              "14983                          [Plymouth, 2, Preston, 1]   \n",
              "14984                                  [Division, three]   \n",
              "14985                           [Swansea, 1, Lincoln, 2]   \n",
              "\n",
              "                                                     tag  \n",
              "0              [B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]  \n",
              "1                                         [B-PER, I-PER]  \n",
              "2                                             [B-LOC, O]  \n",
              "3      [O, B-ORG, I-ORG, O, O, O, O, O, O, B-MISC, O,...  \n",
              "4      [B-LOC, O, O, O, O, B-ORG, I-ORG, O, O, O, B-P...  \n",
              "...                                                  ...  \n",
              "14981                                          [O, O, O]  \n",
              "14982                                             [O, O]  \n",
              "14983                               [B-ORG, O, B-ORG, O]  \n",
              "14984                                             [O, O]  \n",
              "14985                               [B-ORG, O, B-ORG, O]  \n",
              "\n",
              "[14986 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-36ddf98b-40da-4078-ae9a-97cf8d76cc9f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[EU, rejects, German, call, to, boycott, Briti...</td>\n",
              "      <td>[B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Peter, Blackburn]</td>\n",
              "      <td>[B-PER, I-PER]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[BRUSSELS, 1996-08-22]</td>\n",
              "      <td>[B-LOC, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[The, European, Commission, said, on, Thursday...</td>\n",
              "      <td>[O, B-ORG, I-ORG, O, O, O, O, O, O, B-MISC, O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Germany, 's, representative, to, the, Europea...</td>\n",
              "      <td>[B-LOC, O, O, O, O, B-ORG, I-ORG, O, O, O, B-P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14981</th>\n",
              "      <td>[on, Friday, :]</td>\n",
              "      <td>[O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14982</th>\n",
              "      <td>[Division, two]</td>\n",
              "      <td>[O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14983</th>\n",
              "      <td>[Plymouth, 2, Preston, 1]</td>\n",
              "      <td>[B-ORG, O, B-ORG, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14984</th>\n",
              "      <td>[Division, three]</td>\n",
              "      <td>[O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14985</th>\n",
              "      <td>[Swansea, 1, Lincoln, 2]</td>\n",
              "      <td>[B-ORG, O, B-ORG, O]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14986 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-36ddf98b-40da-4078-ae9a-97cf8d76cc9f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-36ddf98b-40da-4078-ae9a-97cf8d76cc9f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-36ddf98b-40da-4078-ae9a-97cf8d76cc9f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b11756bb-bce1-44f0-8540-177f88a789d1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b11756bb-bce1-44f0-8540-177f88a789d1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b11756bb-bce1-44f0-8540-177f88a789d1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_42874ca6-009b-4432-a08e-41c19a2ef7bb\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('train_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_42874ca6-009b-4432-a08e-41c19a2ef7bb button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('train_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df",
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 14986,\n  \"fields\": [\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tag\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "qNmakWvLGr30",
        "outputId": "cdc8cbd8-ee7c-4a5b-8f1b-aabc6fc40e0d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   word  \\\n",
              "0     [SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...   \n",
              "1                                        [Nadim, Ladki]   \n",
              "2       [AL-AIN, ,, United, Arab, Emirates, 1996-12-06]   \n",
              "3     [Japan, began, the, defence, of, their, Asian,...   \n",
              "4     [But, China, saw, their, luck, desert, them, i...   \n",
              "...                                                 ...   \n",
              "3678  [That, is, why, this, is, so, emotional, a, ni...   \n",
              "3679  [\", It, was, the, joy, that, we, all, had, ove...   \n",
              "3680  [Charlton, managed, Ireland, for, 93, matches,...   \n",
              "3681  [He, guided, Ireland, to, two, successive, Wor...   \n",
              "3682  [The, lanky, former, Leeds, United, defender, ...   \n",
              "\n",
              "                                                    tag  \n",
              "0          [O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O]  \n",
              "1                                        [B-PER, I-PER]  \n",
              "2                    [B-LOC, O, B-LOC, I-LOC, I-LOC, O]  \n",
              "3     [B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...  \n",
              "4     [O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...  \n",
              "...                                                 ...  \n",
              "3678  [O, O, O, O, O, O, O, O, O, O, O, O, O, B-PER,...  \n",
              "3679  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "3680  [B-PER, O, B-LOC, O, O, O, O, O, O, O, O, O, O...  \n",
              "3681  [O, O, B-LOC, O, O, O, B-MISC, I-MISC, O, O, O...  \n",
              "3682  [O, O, O, B-ORG, I-ORG, O, O, O, O, O, B-LOC, ...  \n",
              "\n",
              "[3683 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-18d9a7c5-30e8-48be-8f63-8f910600fb43\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...</td>\n",
              "      <td>[O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Nadim, Ladki]</td>\n",
              "      <td>[B-PER, I-PER]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[AL-AIN, ,, United, Arab, Emirates, 1996-12-06]</td>\n",
              "      <td>[B-LOC, O, B-LOC, I-LOC, I-LOC, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Japan, began, the, defence, of, their, Asian,...</td>\n",
              "      <td>[B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[But, China, saw, their, luck, desert, them, i...</td>\n",
              "      <td>[O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3678</th>\n",
              "      <td>[That, is, why, this, is, so, emotional, a, ni...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, B-PER,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3679</th>\n",
              "      <td>[\", It, was, the, joy, that, we, all, had, ove...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3680</th>\n",
              "      <td>[Charlton, managed, Ireland, for, 93, matches,...</td>\n",
              "      <td>[B-PER, O, B-LOC, O, O, O, O, O, O, O, O, O, O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3681</th>\n",
              "      <td>[He, guided, Ireland, to, two, successive, Wor...</td>\n",
              "      <td>[O, O, B-LOC, O, O, O, B-MISC, I-MISC, O, O, O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3682</th>\n",
              "      <td>[The, lanky, former, Leeds, United, defender, ...</td>\n",
              "      <td>[O, O, O, B-ORG, I-ORG, O, O, O, O, O, B-LOC, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3683 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-18d9a7c5-30e8-48be-8f63-8f910600fb43')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-18d9a7c5-30e8-48be-8f63-8f910600fb43 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-18d9a7c5-30e8-48be-8f63-8f910600fb43');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-faa1f2a9-20f3-4f32-9961-bc1cfe434637\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-faa1f2a9-20f3-4f32-9961-bc1cfe434637')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-faa1f2a9-20f3-4f32-9961-bc1cfe434637 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_c8ac1c1a-f016-424d-8f91-08a617c73e9a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('test_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c8ac1c1a-f016-424d-8f91-08a617c73e9a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('test_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_df",
              "summary": "{\n  \"name\": \"test_df\",\n  \"rows\": 3683,\n  \"fields\": [\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tag\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "wdv02qIvGr30",
        "outputId": "0196bd3a-dbc9-4666-a3cd-a06fe03e03dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   word  \\\n",
              "0     [CRICKET, -, LEICESTERSHIRE, TAKE, OVER, AT, T...   \n",
              "1                                  [LONDON, 1996-08-30]   \n",
              "2     [West, Indian, all-rounder, Phil, Simmons, too...   \n",
              "3     [Their, stay, on, top, ,, though, ,, may, be, ...   \n",
              "4     [After, bowling, Somerset, out, for, 83, on, t...   \n",
              "...                                                 ...   \n",
              "3460  [But, the, prices, may, move, in, a, close, ra...   \n",
              "3461  [Brokers, said, blue, chips, like, IDLC, ,, Ba...   \n",
              "3462  [They, said, there, was, still, demand, for, b...   \n",
              "3463  [The, DSE, all, share, price, index, closed, 2...   \n",
              "3464                [--, Dhaka, Newsroom, 880-2-506363]   \n",
              "\n",
              "                                                    tag  \n",
              "0                 [O, O, B-ORG, O, O, O, O, O, O, O, O]  \n",
              "1                                            [B-LOC, O]  \n",
              "2     [B-MISC, I-MISC, O, B-PER, I-PER, O, O, O, O, ...  \n",
              "3     [O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG,...  \n",
              "4     [O, O, B-ORG, O, O, O, O, O, O, O, O, B-LOC, I...  \n",
              "...                                                 ...  \n",
              "3460   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
              "3461  [O, O, O, O, O, B-ORG, O, B-ORG, I-ORG, O, B-O...  \n",
              "3462  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "3463  [O, B-ORG, O, O, O, O, O, O, O, O, O, O, O, O,...  \n",
              "3464                               [O, B-ORG, I-ORG, O]  \n",
              "\n",
              "[3465 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ccd45bcd-7bc8-4dff-97fc-5caec8632976\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CRICKET, -, LEICESTERSHIRE, TAKE, OVER, AT, T...</td>\n",
              "      <td>[O, O, B-ORG, O, O, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[LONDON, 1996-08-30]</td>\n",
              "      <td>[B-LOC, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[West, Indian, all-rounder, Phil, Simmons, too...</td>\n",
              "      <td>[B-MISC, I-MISC, O, B-PER, I-PER, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Their, stay, on, top, ,, though, ,, may, be, ...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[After, bowling, Somerset, out, for, 83, on, t...</td>\n",
              "      <td>[O, O, B-ORG, O, O, O, O, O, O, O, O, B-LOC, I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3460</th>\n",
              "      <td>[But, the, prices, may, move, in, a, close, ra...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3461</th>\n",
              "      <td>[Brokers, said, blue, chips, like, IDLC, ,, Ba...</td>\n",
              "      <td>[O, O, O, O, O, B-ORG, O, B-ORG, I-ORG, O, B-O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3462</th>\n",
              "      <td>[They, said, there, was, still, demand, for, b...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3463</th>\n",
              "      <td>[The, DSE, all, share, price, index, closed, 2...</td>\n",
              "      <td>[O, B-ORG, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3464</th>\n",
              "      <td>[--, Dhaka, Newsroom, 880-2-506363]</td>\n",
              "      <td>[O, B-ORG, I-ORG, O]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3465 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ccd45bcd-7bc8-4dff-97fc-5caec8632976')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ccd45bcd-7bc8-4dff-97fc-5caec8632976 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ccd45bcd-7bc8-4dff-97fc-5caec8632976');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d10e7887-44c9-4150-bf18-f5c123bf5adf\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d10e7887-44c9-4150-bf18-f5c123bf5adf')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d10e7887-44c9-4150-bf18-f5c123bf5adf button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_7a725a0e-3594-465a-98d2-232ed1a47628\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('valid_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_7a725a0e-3594-465a-98d2-232ed1a47628 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('valid_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "valid_df",
              "summary": "{\n  \"name\": \"valid_df\",\n  \"rows\": 3465,\n  \"fields\": [\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tag\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "valid_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5vlJtg-Gr31",
        "outputId": "1f3df14d-0b5a-4dd9-efa0-75b417602fd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ner category ['LOC', 'MISC', 'ORG', 'PER'] .\n",
            "\n",
            "label list ['O', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC', 'B-ORG', 'I-ORG', 'B-PER', 'I-PER'] .\n",
            "\n",
            "label2id {'O': 0, 'B-LOC': 1, 'I-LOC': 2, 'B-MISC': 3, 'I-MISC': 4, 'B-ORG': 5, 'I-ORG': 6, 'B-PER': 7, 'I-PER': 8} .\n",
            "\n",
            "id2label {0: 'O', 1: 'B-LOC', 2: 'I-LOC', 3: 'B-MISC', 4: 'I-MISC', 5: 'B-ORG', 6: 'I-ORG', 7: 'B-PER', 8: 'I-PER'}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def collect_label(df_list):\n",
        "    ret = set()\n",
        "    for df in df_list:\n",
        "        for labels in df['tag']:\n",
        "            for l in labels:\n",
        "                if l == \"O\":\n",
        "                    continue\n",
        "                assert l.startswith(\"B-\") or l.startswith(\"I-\")\n",
        "                ret.add(l[2:])\n",
        "    return sorted(list(ret))\n",
        "\n",
        "ner_category = collect_label([train_df, valid_df, test_df])\n",
        "label_list = []\n",
        "for l in ner_category:\n",
        "    label_list.append(\"B-\" + l)\n",
        "    label_list.append(\"I-\" + l)\n",
        "label_list = ['O'] + label_list\n",
        "label2id = dict([(v, idx) for idx, v in enumerate(label_list)])\n",
        "id2label = dict([(idx, v) for idx, v in enumerate(label_list)])\n",
        "print(f\"ner category {ner_category} .\\n\\nlabel list {label_list} .\\n\\nlabel2id {label2id} .\\n\\nid2label {id2label}\\n\\n\")\n",
        "label_list = label_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLLN7xMQGr31"
      },
      "source": [
        "## Import Reberta Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290,
          "referenced_widgets": [
            "0127a4d474ed4d04a1a64ddbac8e3055",
            "558cd186fc6c47f5a62de3d3f2b1243e",
            "45ca6b93a1f34eb2952f0b8a1a2b30e3",
            "6318c45215274aab93562697f5af7d14",
            "9b499b7adee74b0fbf160f7ed3fd83c5",
            "301a28c81fba4a1a93c55d8deac3d9f8",
            "cabcba8534fb4826b15aa1b4b6b79d83",
            "83468ddec8a6451faaa90790f848849f",
            "743aaa1f68584d17a00b293d20ec571d",
            "bf65d25d68ab4154af7177ba09b33f55",
            "35b6ef271c6444e2b1c3e0b15be38d4c",
            "ca634021192b431fbcd969199c784f36",
            "85c21f4e95cc4a75b56cb37b83d854c2",
            "9323f740402c4dbc99731bf17a1b4888",
            "095034b99ded45c5b920dcf82e760937",
            "051b2c7b6518400d8e5dedcd22b2611b",
            "37fc3aa0f426460e9b7ea73817b85c4f",
            "7d637801cade4a29ae6e0f85ea824725",
            "d3256ab20d314f1b8b2846e6fb665b45",
            "989bede1a8124febbe8b9ab3931c8cc3",
            "2267fab0630b4ad28b859b8a4356d0f1",
            "ac2618da9b1b4f04b6c53bbc275fdb26",
            "cf96a3feb3f5419ebdf0fc9b7a4d4f9b",
            "f3d0640daeb14b2bb3f722ab4d3ef656",
            "08303bf7dea045ee8b815a3e58a68fe8",
            "02b8a9b538d540e69bda82b0c5f0843a",
            "ff28928c34d5420b9f49b8df4f02f2cb",
            "97195ad2e3b640cb8d4b4f6f13e6b037",
            "c44c1c63fe144b54b7e72e7ae0fe9ce8",
            "e70b2ad48eeb4bb891ab152bf61df125",
            "2291b57456de4c4791af1655c1d6befd",
            "e919f4ce90574462845871a3c5c0f1ae",
            "c757662152224d969921af41728fb531",
            "abb4dc5fbc784f57b6459074385a8451",
            "eb8f35431bca4cc48f5a8fc64f2aa4f3",
            "86a84530a19d4fb7bc12251dce5123a4",
            "e3fe8ad0cadc462898f3a1990d463afe",
            "d67b6d0c47004031bca1f5373293e964",
            "a51811b109bd40c4a17b3d6a223c521e",
            "e56003a0abca408c8e1b59080d0cefe9",
            "c10162c02cdb460c9b860b3c7ca5b786",
            "03c46eb119eb420ba150e87a970c443b",
            "2a90e894d8544345a4a0d7628864c1ad",
            "8f4dbd2d81094767b0c09ced78f95c68"
          ]
        },
        "id": "Sy1aItuvGr31",
        "outputId": "725199f8-7f1f-43e0-e505-3ee562066da7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0127a4d474ed4d04a1a64ddbac8e3055"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/474 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca634021192b431fbcd969199c784f36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf96a3feb3f5419ebdf0fc9b7a4d4f9b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abb4dc5fbc784f57b6459074385a8451"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(Config.model_name, add_prefix_space=True)\n",
        "print(tokenizer.is_fast)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvXSJptjGr31"
      },
      "source": [
        "## tokenize and build Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": [],
        "id": "G49tIcsZGr31"
      },
      "outputs": [],
      "source": [
        "def align(tag, word_ids):\n",
        "    aligned_tag = []\n",
        "    i = 0\n",
        "    while i < len(word_ids):\n",
        "        if word_ids[i] is None:\n",
        "            aligned_tag.append(None)\n",
        "            i += 1\n",
        "        elif tag[word_ids[i]] == \"O\":\n",
        "            aligned_tag.append(tag[word_ids[i]])\n",
        "            i += 1\n",
        "        elif tag[word_ids[i]].startswith(\"B-\"):\n",
        "            n = 0\n",
        "            while (i+n) < len(word_ids) and word_ids[i]  == word_ids[i+n]:\n",
        "                n += 1\n",
        "            aligned_tag.append(tag[word_ids[i]])\n",
        "            if n > 1:\n",
        "                aligned_tag.extend([\"I-\" + tag[word_ids[i]][2:] ] * (n-1))\n",
        "            i = i + n\n",
        "        else:\n",
        "            aligned_tag.append(tag[word_ids[i]])\n",
        "            i += 1\n",
        "    return aligned_tag\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LldB9JC9Gr31",
        "outputId": "6c62a3b0-bf69-435f-c0ed-277678b29391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', '1996-08-22', '1996-08-22', 'I'] ['O', 'B-LOC', 'B-ORG', 'O']\n",
            "   tokens   tags  word-index\n",
            "0   [CLS]   None         NaN\n",
            "1      ĠI      O         0.0\n",
            "2   Ġ1996  B-LOC         1.0\n",
            "3       -  I-LOC         1.0\n",
            "4      08  I-LOC         1.0\n",
            "5       -  I-LOC         1.0\n",
            "6      22  I-LOC         1.0\n",
            "7   Ġ1996  B-ORG         2.0\n",
            "8       -  I-ORG         2.0\n",
            "9      08  I-ORG         2.0\n",
            "10      -  I-ORG         2.0\n",
            "11     22  I-ORG         2.0\n",
            "12     ĠI      O         3.0\n",
            "13  [SEP]   None         NaN\n"
          ]
        }
      ],
      "source": [
        "#words = train_df.iloc[2][\"word\"]\n",
        "#tag = train_df.iloc[2][\"label\"]\n",
        "words = ['I', '1996-08-22', '1996-08-22', 'I']\n",
        "tag = [\"O\", \"B-LOC\", \"B-ORG\", \"O\"]\n",
        "print(words, tag)\n",
        "s = tokenizer(words, truncation=True, is_split_into_words=True)\n",
        "word_ids = s.word_ids()\n",
        "# align tokens and words\n",
        "tokens = tokenizer.convert_ids_to_tokens(s['input_ids'])\n",
        "tags = align(tag, s.word_ids())\n",
        "print(pd.DataFrame(list(zip(tokens, tags, word_ids)), columns=['tokens', 'tags', 'word-index']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": [],
        "id": "Y3i02DNiGr31"
      },
      "outputs": [],
      "source": [
        "def preprocess(x):\n",
        "    word = x['word']\n",
        "    r = tokenizer(word, truncation=True, is_split_into_words=True)\n",
        "    word_ids = r.word_ids()\n",
        "    tokens = tokenizer.convert_ids_to_tokens(r['input_ids'])\n",
        "    align_label = align(x['tag'], word_ids)\n",
        "    return tokens, align_label, r['input_ids'], [label2id[i] if i is not None else -100  for i in align_label], word_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": [],
        "id": "rmnu0LUoGr31"
      },
      "outputs": [],
      "source": [
        "train_df[['token', 'label', 'id', 'label_id', 'word_ids']] = train_df.apply(lambda x: pd.Series(preprocess(x)), axis=1)\n",
        "valid_df[['token', 'label', 'id', 'label_id', 'word_ids']] = valid_df.apply(lambda x: pd.Series(preprocess(x)), axis=1)\n",
        "test_df[['token', 'label', 'id', 'label_id', 'word_ids']] = test_df.apply(lambda x: pd.Series(preprocess(x)), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "id": "LHAT0xb_Gr32",
        "outputId": "1c996dd3-342e-4b45-b2b5-5d15de54df19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   word  \\\n",
              "0     [SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...   \n",
              "1                                        [Nadim, Ladki]   \n",
              "2       [AL-AIN, ,, United, Arab, Emirates, 1996-12-06]   \n",
              "3     [Japan, began, the, defence, of, their, Asian,...   \n",
              "4     [But, China, saw, their, luck, desert, them, i...   \n",
              "...                                                 ...   \n",
              "3678  [That, is, why, this, is, so, emotional, a, ni...   \n",
              "3679  [\", It, was, the, joy, that, we, all, had, ove...   \n",
              "3680  [Charlton, managed, Ireland, for, 93, matches,...   \n",
              "3681  [He, guided, Ireland, to, two, successive, Wor...   \n",
              "3682  [The, lanky, former, Leeds, United, defender, ...   \n",
              "\n",
              "                                                    tag  \\\n",
              "0          [O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O]   \n",
              "1                                        [B-PER, I-PER]   \n",
              "2                    [B-LOC, O, B-LOC, I-LOC, I-LOC, O]   \n",
              "3     [B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...   \n",
              "4     [O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...   \n",
              "...                                                 ...   \n",
              "3678  [O, O, O, O, O, O, O, O, O, O, O, O, O, B-PER,...   \n",
              "3679  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "3680  [B-PER, O, B-LOC, O, O, O, O, O, O, O, O, O, O...   \n",
              "3681  [O, O, B-LOC, O, O, O, B-MISC, I-MISC, O, O, O...   \n",
              "3682  [O, O, O, B-ORG, I-ORG, O, O, O, O, O, B-LOC, ...   \n",
              "\n",
              "                                                  token  \\\n",
              "0     [[CLS], ĠSO, CC, ER, Ġ-, ĠJ, AP, AN, ĠGET, ĠL,...   \n",
              "1                    [[CLS], ĠNad, im, ĠLad, ki, [SEP]]   \n",
              "2     [[CLS], ĠAL, -, AIN, Ġ,, ĠUnited, ĠArab, ĠEmir...   \n",
              "3     [[CLS], ĠJapan, Ġbegan, Ġthe, Ġdefence, Ġof, Ġ...   \n",
              "4     [[CLS], ĠBut, ĠChina, Ġsaw, Ġtheir, Ġluck, Ġde...   \n",
              "...                                                 ...   \n",
              "3678  [[CLS], ĠThat, Ġis, Ġwhy, Ġthis, Ġis, Ġso, Ġem...   \n",
              "3679  [[CLS], Ġ\", ĠIt, Ġwas, Ġthe, Ġjoy, Ġthat, Ġwe,...   \n",
              "3680  [[CLS], ĠCharl, ton, Ġmanaged, ĠIreland, Ġfor,...   \n",
              "3681  [[CLS], ĠHe, Ġguided, ĠIreland, Ġto, Ġtwo, Ġsu...   \n",
              "3682  [[CLS], ĠThe, Ġl, anky, Ġformer, ĠLeeds, ĠUnit...   \n",
              "\n",
              "                                                  label  \\\n",
              "0     [None, O, O, O, O, B-LOC, I-LOC, I-LOC, O, O, ...   \n",
              "1              [None, B-PER, I-PER, I-PER, I-PER, None]   \n",
              "2     [None, B-LOC, I-LOC, I-LOC, O, B-LOC, I-LOC, I...   \n",
              "3     [None, B-LOC, O, O, O, O, O, B-MISC, I-MISC, O...   \n",
              "4     [None, O, B-LOC, O, O, O, O, O, O, O, O, O, O,...   \n",
              "...                                                 ...   \n",
              "3678  [None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "3679  [None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "3680  [None, B-PER, I-PER, O, B-LOC, O, O, O, O, O, ...   \n",
              "3681  [None, O, O, B-LOC, O, O, O, B-MISC, I-MISC, O...   \n",
              "3682  [None, O, O, O, O, B-ORG, I-ORG, O, O, O, O, O...   \n",
              "\n",
              "                                                     id  \\\n",
              "0     [1, 13910, 3376, 2076, 111, 344, 591, 1889, 77...   \n",
              "1                        [1, 7157, 757, 17804, 3144, 2]   \n",
              "2     [1, 6019, 12, 33178, 2156, 315, 4681, 8313, 80...   \n",
              "3     [1, 1429, 880, 5, 2994, 9, 49, 3102, 968, 1270...   \n",
              "4     [1, 125, 436, 794, 49, 6620, 10348, 106, 11, 5...   \n",
              "...                                                 ...   \n",
              "3678  [1, 280, 16, 596, 42, 16, 98, 3722, 10, 363, 1...   \n",
              "3679  [1, 22, 85, 21, 5, 5823, 14, 52, 70, 56, 81, 5...   \n",
              "3680  [1, 15959, 1054, 2312, 2487, 13, 8060, 2856, 2...   \n",
              "3681  [1, 91, 10346, 2487, 7, 80, 12565, 623, 968, 6...   \n",
              "3682  [1, 20, 784, 30451, 320, 9245, 315, 5142, 222,...   \n",
              "\n",
              "                                               label_id  \\\n",
              "0     [-100, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, ...   \n",
              "1                              [-100, 7, 8, 8, 8, -100]   \n",
              "2      [-100, 1, 2, 2, 0, 1, 2, 2, 0, 0, 0, 0, 0, -100]   \n",
              "3     [-100, 1, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, ...   \n",
              "4     [-100, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "...                                                 ...   \n",
              "3678  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "3679  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "3680  [-100, 7, 8, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "3681  [-100, 0, 0, 1, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, ...   \n",
              "3682  [-100, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 0, 1, 0, ...   \n",
              "\n",
              "                                               word_ids  \n",
              "0     [None, 0, 0, 0, 1, 2, 2, 2, 3, 4, 4, 4, 5, 6, ...  \n",
              "1                              [None, 0, 0, 1, 1, None]  \n",
              "2      [None, 0, 0, 0, 1, 2, 3, 4, 5, 5, 5, 5, 5, None]  \n",
              "3     [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...  \n",
              "4     [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...  \n",
              "...                                                 ...  \n",
              "3678  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...  \n",
              "3679  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...  \n",
              "3680  [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...  \n",
              "3681  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...  \n",
              "3682  [None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...  \n",
              "\n",
              "[3683 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c15c4b5d-bb4b-4fcb-8c05-8e667fad74a5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "      <th>token</th>\n",
              "      <th>label</th>\n",
              "      <th>id</th>\n",
              "      <th>label_id</th>\n",
              "      <th>word_ids</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...</td>\n",
              "      <td>[O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O]</td>\n",
              "      <td>[[CLS], ĠSO, CC, ER, Ġ-, ĠJ, AP, AN, ĠGET, ĠL,...</td>\n",
              "      <td>[None, O, O, O, O, B-LOC, I-LOC, I-LOC, O, O, ...</td>\n",
              "      <td>[1, 13910, 3376, 2076, 111, 344, 591, 1889, 77...</td>\n",
              "      <td>[-100, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[None, 0, 0, 0, 1, 2, 2, 2, 3, 4, 4, 4, 5, 6, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Nadim, Ladki]</td>\n",
              "      <td>[B-PER, I-PER]</td>\n",
              "      <td>[[CLS], ĠNad, im, ĠLad, ki, [SEP]]</td>\n",
              "      <td>[None, B-PER, I-PER, I-PER, I-PER, None]</td>\n",
              "      <td>[1, 7157, 757, 17804, 3144, 2]</td>\n",
              "      <td>[-100, 7, 8, 8, 8, -100]</td>\n",
              "      <td>[None, 0, 0, 1, 1, None]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[AL-AIN, ,, United, Arab, Emirates, 1996-12-06]</td>\n",
              "      <td>[B-LOC, O, B-LOC, I-LOC, I-LOC, O]</td>\n",
              "      <td>[[CLS], ĠAL, -, AIN, Ġ,, ĠUnited, ĠArab, ĠEmir...</td>\n",
              "      <td>[None, B-LOC, I-LOC, I-LOC, O, B-LOC, I-LOC, I...</td>\n",
              "      <td>[1, 6019, 12, 33178, 2156, 315, 4681, 8313, 80...</td>\n",
              "      <td>[-100, 1, 2, 2, 0, 1, 2, 2, 0, 0, 0, 0, 0, -100]</td>\n",
              "      <td>[None, 0, 0, 0, 1, 2, 3, 4, 5, 5, 5, 5, 5, None]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Japan, began, the, defence, of, their, Asian,...</td>\n",
              "      <td>[B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...</td>\n",
              "      <td>[[CLS], ĠJapan, Ġbegan, Ġthe, Ġdefence, Ġof, Ġ...</td>\n",
              "      <td>[None, B-LOC, O, O, O, O, O, B-MISC, I-MISC, O...</td>\n",
              "      <td>[1, 1429, 880, 5, 2994, 9, 49, 3102, 968, 1270...</td>\n",
              "      <td>[-100, 1, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[But, China, saw, their, luck, desert, them, i...</td>\n",
              "      <td>[O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
              "      <td>[[CLS], ĠBut, ĠChina, Ġsaw, Ġtheir, Ġluck, Ġde...</td>\n",
              "      <td>[None, O, B-LOC, O, O, O, O, O, O, O, O, O, O,...</td>\n",
              "      <td>[1, 125, 436, 794, 49, 6620, 10348, 106, 11, 5...</td>\n",
              "      <td>[-100, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3678</th>\n",
              "      <td>[That, is, why, this, is, so, emotional, a, ni...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, B-PER,...</td>\n",
              "      <td>[[CLS], ĠThat, Ġis, Ġwhy, Ġthis, Ġis, Ġso, Ġem...</td>\n",
              "      <td>[None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>[1, 280, 16, 596, 42, 16, 98, 3722, 10, 363, 1...</td>\n",
              "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3679</th>\n",
              "      <td>[\", It, was, the, joy, that, we, all, had, ove...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>[[CLS], Ġ\", ĠIt, Ġwas, Ġthe, Ġjoy, Ġthat, Ġwe,...</td>\n",
              "      <td>[None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>[1, 22, 85, 21, 5, 5823, 14, 52, 70, 56, 81, 5...</td>\n",
              "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3680</th>\n",
              "      <td>[Charlton, managed, Ireland, for, 93, matches,...</td>\n",
              "      <td>[B-PER, O, B-LOC, O, O, O, O, O, O, O, O, O, O...</td>\n",
              "      <td>[[CLS], ĠCharl, ton, Ġmanaged, ĠIreland, Ġfor,...</td>\n",
              "      <td>[None, B-PER, I-PER, O, B-LOC, O, O, O, O, O, ...</td>\n",
              "      <td>[1, 15959, 1054, 2312, 2487, 13, 8060, 2856, 2...</td>\n",
              "      <td>[-100, 7, 8, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3681</th>\n",
              "      <td>[He, guided, Ireland, to, two, successive, Wor...</td>\n",
              "      <td>[O, O, B-LOC, O, O, O, B-MISC, I-MISC, O, O, O...</td>\n",
              "      <td>[[CLS], ĠHe, Ġguided, ĠIreland, Ġto, Ġtwo, Ġsu...</td>\n",
              "      <td>[None, O, O, B-LOC, O, O, O, B-MISC, I-MISC, O...</td>\n",
              "      <td>[1, 91, 10346, 2487, 7, 80, 12565, 623, 968, 6...</td>\n",
              "      <td>[-100, 0, 0, 1, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3682</th>\n",
              "      <td>[The, lanky, former, Leeds, United, defender, ...</td>\n",
              "      <td>[O, O, O, B-ORG, I-ORG, O, O, O, O, O, B-LOC, ...</td>\n",
              "      <td>[[CLS], ĠThe, Ġl, anky, Ġformer, ĠLeeds, ĠUnit...</td>\n",
              "      <td>[None, O, O, O, O, B-ORG, I-ORG, O, O, O, O, O...</td>\n",
              "      <td>[1, 20, 784, 30451, 320, 9245, 315, 5142, 222,...</td>\n",
              "      <td>[-100, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
              "      <td>[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3683 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c15c4b5d-bb4b-4fcb-8c05-8e667fad74a5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c15c4b5d-bb4b-4fcb-8c05-8e667fad74a5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c15c4b5d-bb4b-4fcb-8c05-8e667fad74a5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1ccbd704-6986-4731-9746-4b4a10ded3d8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1ccbd704-6986-4731-9746-4b4a10ded3d8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1ccbd704-6986-4731-9746-4b4a10ded3d8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_33d7fb80-bb0e-43f7-b28f-63290b88276f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('test_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_33d7fb80-bb0e-43f7-b28f-63290b88276f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('test_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_df",
              "summary": "{\n  \"name\": \"test_df\",\n  \"rows\": 3683,\n  \"fields\": [\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tag\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"token\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_id\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word_ids\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWRSGLs4Gr32"
      },
      "source": [
        "## Building Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": [],
        "id": "CNHgo_2cGr32"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NerDataset(Dataset):\n",
        "    def __init__(self, df, device):\n",
        "        self.data = df.to_dict(orient='records')\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.data[item]\n",
        "\n",
        "    def collate_to_max_length(self, batch):\n",
        "        max_seq_length = max([len(s['id']) for s in batch])\n",
        "        batch = sorted(batch, key=lambda x: -len(x['id']))\n",
        "        seq_length = torch.tensor([len(x['id']) for x in batch])\n",
        "        input_ids = torch.tensor([x[\"id\"] + [0] * (max_seq_length - len(x['id'])) for x in batch]).to(self.device)\n",
        "        labels = torch.tensor([x[\"label_id\"] + [-100] * (max_seq_length - len(x['label_id'])) for x in batch]).to(self.device)\n",
        "        return {\"id\": input_ids, \"label_id\": labels, 'seq_length':seq_length, \"sample\":batch}\n",
        "\n",
        "\n",
        "dataset_train = NerDataset(train_df, Config.device)\n",
        "\n",
        "train_dataloader = DataLoader(dataset_train,\n",
        "                              sampler=RandomSampler(dataset_train),\n",
        "                              batch_size=Config.batch_size,\n",
        "                              drop_last=False,\n",
        "                              collate_fn=dataset_train.collate_to_max_length)\n",
        "\n",
        "\n",
        "\n",
        "dataset_valid = NerDataset(valid_df, Config.device)\n",
        "\n",
        "valid_dataloader = DataLoader(dataset_valid,\n",
        "                              sampler=RandomSampler(dataset_valid),\n",
        "                              batch_size=Config.batch_size,\n",
        "                              drop_last=False,\n",
        "                              collate_fn=dataset_valid.collate_to_max_length)\n",
        "\n",
        "\n",
        "dataset_test = NerDataset(test_df, Config.device)\n",
        "\n",
        "test_dataloader = DataLoader(dataset_test,\n",
        "                              sampler=RandomSampler(dataset_test),\n",
        "                              batch_size=Config.batch_size,\n",
        "                              drop_last=False,\n",
        "                              collate_fn=dataset_test.collate_to_max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5B33GkMGr32"
      },
      "source": [
        "## Building Custom loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Jp_D66RpGr32"
      },
      "outputs": [],
      "source": [
        "\n",
        "class L1_Loss:\n",
        "    def __init__(self):\n",
        "        self.l1_loss = nn.L1Loss()\n",
        "    def loss(self, target, logit, label_num):\n",
        "\n",
        "        target = target.view(-1)\n",
        "        logit = logit.view(-1, label_num)\n",
        "        mask = target.ne(-100).to(logit.device)\n",
        "        logit = torch.masked_select(logit, mask.unsqueeze(-1).expand_as(logit)).reshape(-1, label_num)\n",
        "        target = torch.masked_select(target, mask)\n",
        "\n",
        "        target = F.one_hot(target, num_classes=label_num)\n",
        "        return self.l1_loss(logit, target.float())\n",
        "\n",
        "\n",
        "class L2_Loss:\n",
        "    def __init__(self):\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "    def loss(self, target, logit,label_num):\n",
        "        target = target.view(-1)\n",
        "        logit = logit.view(-1, label_num)\n",
        "        mask = target.ne(-100).to(logit.device)\n",
        "        logit = torch.masked_select(logit, mask.unsqueeze(-1).expand_as(logit)).reshape(-1, label_num)\n",
        "        target = torch.masked_select(target, mask)\n",
        "\n",
        "        target = F.one_hot(target, num_classes=label_num)\n",
        "        loss = self.mse_loss(logit, target.float())\n",
        "        return loss\n",
        "\n",
        "class CE_Loss:\n",
        "    def __init__(self):\n",
        "        self.ce_loss = nn.CrossEntropyLoss(ignore_index=-100, reduce='mean')\n",
        "    def loss(self, target, logit, label_num):\n",
        "        return self.ce_loss(logit.reshape(-1, label_num), target.reshape(-1) )\n",
        "\n",
        "class KLDivergenceLoss:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def loss(self, target, logit, label_num):\n",
        "        target = target.view(-1)\n",
        "        logit = logit.view(-1, label_num)\n",
        "\n",
        "        mask = target.ne(-100).to(logit.device)\n",
        "        logit = torch.masked_select(logit, mask.unsqueeze(-1).expand_as(logit)).reshape(-1, label_num)\n",
        "        target = torch.masked_select(target, mask)\n",
        "\n",
        "        probs = F.softmax(logit, dim=-1)\n",
        "\n",
        "        # One-hot encode the targets to get true probabilities\n",
        "        true_probs = F.one_hot(target, num_classes=label_num).float()\n",
        "\n",
        "        mask_true_probs = true_probs > 0\n",
        "\n",
        "        # Calculate g function for non-zero elements using the mask\n",
        "        kl_values = torch.zeros_like(probs)\n",
        "        kl_values[mask_true_probs] = true_probs[mask_true_probs] * torch.log(true_probs[mask_true_probs]/probs[mask_true_probs])\n",
        "\n",
        "        # Sum over all classes and average over the batch size\n",
        "        loss = kl_values.sum(dim=-1).mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "# DLITE Loss function\n",
        "class DLITELoss:\n",
        "    def __init__(self):\n",
        "        super(DLITELoss, self).__init__()\n",
        "\n",
        "    def loss(self, targets, logits, label_num, epsilon=1e-10):\n",
        "        targets = targets.view(-1)\n",
        "        logits = logits.view(-1, label_num)\n",
        "\n",
        "        mask = targets.ne(-100).to(logits.device)\n",
        "        logits = torch.masked_select(logits, mask.unsqueeze(-1).expand_as(logits)).reshape(-1, label_num)\n",
        "        targets = torch.masked_select(targets, mask)\n",
        "\n",
        "        # Convert logits to probabilities using softmax\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # One-hot encode the targets to get true probabilities\n",
        "        true_probs = F.one_hot(targets, num_classes=probs.size(-1)).float()\n",
        "\n",
        "        # Define the g function\n",
        "        g_values = torch.abs(probs * (1 - torch.log(probs + epsilon)) - true_probs * (1 - torch.log(true_probs + epsilon)))\n",
        "\n",
        "        # Define the delta_h function\n",
        "        delta_h_values = torch.abs(probs**2 * (1 - 2 * torch.log(probs + epsilon)) - true_probs**2 * (1 - 2 * torch.log(true_probs + epsilon))) / (2 * (probs + true_probs))\n",
        "\n",
        "        # Compute DLITE loss for each class\n",
        "        dl_values = g_values - delta_h_values\n",
        "\n",
        "        # Sum over all classes and average over batch size\n",
        "        loss = dl_values.sum(dim=-1).mean()\n",
        "\n",
        "        return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Custom Layer"
      ],
      "metadata": {
        "id": "R0x9wjATtD7Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": [],
        "id": "5bf_rR5pGr32"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LSTMEncoder(nn.Module):\n",
        "    \"\"\"lstm encoder\n",
        "    \"\"\"\n",
        "    def __init__(self, config, hidden_size):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "        self.lstm = torch.nn.LSTM(hidden_size, hidden_size,\n",
        "                                  num_layers=config.lstm_layer_num, bidirectional=config.bi_lstm,\n",
        "                                  batch_first=True)\n",
        "\n",
        "    def forward(self, hidden_state, seq_length):\n",
        "        sequence_output = pack_padded_sequence(hidden_state, seq_length, batch_first=True)\n",
        "        sequence_output, (h_n, c_n) = self.lstm(sequence_output)\n",
        "        sequence_output, _ = pad_packed_sequence(sequence_output, batch_first=True)\n",
        "        return sequence_output\n",
        "\n",
        "\n",
        "\n",
        "class Ner_Model(nn.Module):\n",
        "    def __init__(self,config, label_num, loss_name):\n",
        "        super(Ner_Model, self).__init__()\n",
        "        self.config = config\n",
        "        # deberat model\n",
        "        self.model = transformers.AutoModel.from_pretrained(config.model_name)\n",
        "\n",
        "        hidden_size = config.hidden_size\n",
        "\n",
        "        # using linear layer\n",
        "        linear_layer = []\n",
        "        if self.config.linear_layer_num  > 0:\n",
        "            for out_dim in config.linear_layer[0:config.linear_layer_num]:\n",
        "                linear_layer.append( nn.Linear(in_features=hidden_size, out_features=out_dim) )\n",
        "                linear_layer.append( nn.ReLU() )\n",
        "                hidden_size = out_dim\n",
        "\n",
        "            self.linear_model = nn.Sequential(*linear_layer)\n",
        "\n",
        "        # whether to use lstm layer\n",
        "        if config.lstm_layer_num > 0:\n",
        "            self.lstm = LSTMEncoder(config,hidden_size)\n",
        "\n",
        "        # identify label number\n",
        "        self.label_num = label_num\n",
        "\n",
        "        # whether to use bi-lstm layer\n",
        "        if config.bi_lstm and config.lstm_layer_num > 0:\n",
        "            hidden_size = hidden_size * 2\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_size, label_num)\n",
        "\n",
        "        if loss_name == 'ce':\n",
        "            self.loss_func = CE_Loss()\n",
        "        elif loss_name == 'l1':\n",
        "            self.loss_func = L1_Loss()\n",
        "        elif loss_name == 'l2':\n",
        "            self.loss_func = L2_Loss()\n",
        "        elif loss_name == 'kl':\n",
        "            self.loss_func = KLDivergenceLoss()\n",
        "        elif loss_name == 'dlite':\n",
        "            self.loss_func = DLITELoss()\n",
        "        else:\n",
        "            assert 1==0\n",
        "\n",
        "        print(\"model configuration\")\n",
        "        print(\"%\" * 20)\n",
        "        print(self)\n",
        "        print(\"%\" * 20)\n",
        "\n",
        "    def forward(self, input_ids, seq_length, attention_mask, labels):\n",
        "        output = self.model(input_ids, attention_mask)\n",
        "        sequence_output = output[0]\n",
        "        if self.config.linear_layer_num > 0:\n",
        "            sequence_output = self.linear_model(sequence_output)\n",
        "\n",
        "        if self.config.lstm_layer_num > 0:\n",
        "            sequence_output = self.lstm(sequence_output, seq_length)\n",
        "\n",
        "        logit = self.classifier(sequence_output)\n",
        "        loss = self.loss_func.loss(labels, logit, len(label2id))\n",
        "        return loss, logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": [],
        "id": "8F51tFmwGr32"
      },
      "outputs": [],
      "source": [
        "# Building optimizer\n",
        "def get_optimizer(model, config):\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.1},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "\n",
        "    optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                      betas=(0.9, 0.98),\n",
        "                      lr=config.lr)\n",
        "    return optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uXq2R_yGr32"
      },
      "source": [
        "## Defining the training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": [],
        "id": "bDIJoYpWGr33"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate(model, data_loader, mode=\"Validation\"):\n",
        "    ground_truth, predict = [], []\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples = 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(data_loader):\n",
        "            attention_mask = batch[\"id\"].ne(0)\n",
        "            targets = batch['label_id']\n",
        "            loss, logit = model(batch[\"id\"], batch['seq_length'], attention_mask=attention_mask,\n",
        "                                             labels=targets)\n",
        "            eval_loss += loss.cpu().item()\n",
        "            if (step+1) % 100==0:\n",
        "                loss_step = eval_loss / (step+1)\n",
        "                print(f\"{mode} loss per 100 evaluation steps: {loss_step}\")\n",
        "\n",
        "            # compute training accuracy\n",
        "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = logit.view(-1, len(label2id)) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            active_accuracy = flattened_targets.ne(-100) # active accuracy is also of shape (batch_size * seq_len,)\n",
        "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "            eval_preds.extend(predictions.tolist())\n",
        "            eval_labels.extend(targets.tolist())\n",
        "\n",
        "    eval_loss = eval_loss / (step+1)\n",
        "    eval_accuracy = eval_accuracy / (step+1)\n",
        "    eval_labels,eval_preds = [id2label[i] for i in eval_labels], [id2label[i] for i in eval_preds]\n",
        "\n",
        "\n",
        "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(eval_labels, eval_preds, average='micro')\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(eval_labels, eval_preds, average='macro')\n",
        "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(eval_labels, eval_preds,average='weighted')\n",
        "\n",
        "    p_r_f1 = [[round(precision_micro,4), round(recall_micro,4), round(f1_micro,4)],\n",
        "              [round(precision_macro,4), round(recall_macro,4), round(f1_macro,4)],\n",
        "              [round(precision_weighted,4), round(recall_weighted,4), round(f1_weighted,4)]]\n",
        "\n",
        "    p_r_f1 = pd.DataFrame(p_r_f1, columns=['precision', 'recall', 'f1'], index=['micro', 'macro', 'weighted'])\n",
        "\n",
        "    print(f\"{mode} Loss: {eval_loss}\")\n",
        "    print(f\"{mode} Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    p_r_f1_each_label = classification_report(eval_labels, eval_preds)\n",
        "    print(f\"{mode} P-R-F1 for each label: \\n{p_r_f1_each_label}\")\n",
        "    print(f\"{mode} P-R-F1 tor all label: \\n{p_r_f1}\")\n",
        "    print(f\"{mode} steps: {(step+1)}\")\n",
        "    return eval_loss, p_r_f1, p_r_f1_each_label\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMapwwmhGr33"
      },
      "source": [
        "## Running under 5 custom loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "DzSeh-WWGr33"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "loss_list = ['l1', 'l2', 'ce', 'kl', 'dlite']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b41425bb20e44c828a1ad53159045ba6",
            "87d37cfa8e7e4fb7a4e65485ba744d4d",
            "6e24fa9dcae841ce9632d81eec70dc7d",
            "dca0ba7b84ed4f3988219b431f5bd941",
            "978f50ab9bed43188f7d585f037a7950",
            "58e5b9a6fd0e4c57aff8ebb4c62f975a",
            "29c8a38d87f54502ae782eed00417bf7",
            "800eff5c6d7344edaa682c274c3d8bc0",
            "8e4fdf274c154d60a525239131852f4d",
            "b7630259ba4d46e69808b211ba732b30",
            "6615d34abe47424798691a942c3792c6"
          ]
        },
        "id": "n6U0QqPKGr33",
        "outputId": "056844b8-7c64-48b8-ddda-c92dfb17b110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "loss_name: l1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/559M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b41425bb20e44c828a1ad53159045ba6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model configuration\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Ner_Model(\n",
            "  (model): DebertaModel(\n",
            "    (embeddings): DebertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "      (LayerNorm): DebertaLayerNorm()\n",
            "      (dropout): StableDropout()\n",
            "    )\n",
            "    (encoder): DebertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x DebertaLayer(\n",
            "          (attention): DebertaAttention(\n",
            "            (self): DisentangledSelfAttention(\n",
            "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
            "              (pos_dropout): StableDropout()\n",
            "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "            (output): DebertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): DebertaLayerNorm()\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "          )\n",
            "          (intermediate): DebertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): DebertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): DebertaLayerNorm()\n",
            "            (dropout): StableDropout()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (rel_embeddings): Embedding(1024, 768)\n",
            "    )\n",
            "  )\n",
            "  (linear_model): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=384, bias=True)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (classifier): Linear(in_features=384, out_features=9, bias=True)\n",
            ")\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.09016069363802671\n",
            "Training loss per 100 training steps: 0.06695134508423507\n",
            "Training loss per 100 training steps: 0.056195854895437755\n",
            "Training loss per 100 training steps: 0.0497762214159593\n",
            "Training loss per 100 training steps: 0.04599042132310569\n",
            "Training loss per 100 training steps: 0.04290924383715416\n",
            "Training loss per 100 training steps: 0.04061529534270188\n",
            "Training loss per 100 training steps: 0.03872850931889843\n",
            "Training loss per 100 training steps: 0.037097839555547886\n",
            "Training loss per 100 training steps: 0.03579140048637055\n",
            "Training loss per 100 training steps: 0.03494218209313906\n",
            "Training loss per 100 training steps: 0.03391770016984083\n",
            "Training loss per 100 training steps: 0.03303660039037753\n",
            "Training loss per 100 training steps: 0.032324562624562535\n",
            "Training loss per 100 training steps: 0.03157136011403054\n",
            "Training loss per 100 training steps: 0.030838418272614944\n",
            "Training loss per 100 training steps: 0.030139897608828237\n",
            "Training loss per 100 training steps: 0.029281442896462978\n",
            "Training loss per 100 training steps: 0.028585883376531694\n",
            "Training loss per 100 training steps: 0.027754218347370626\n",
            "Training loss per 100 training steps: 0.026964169657966566\n",
            "Training loss per 100 training steps: 0.026322537563074466\n",
            "Training loss per 100 training steps: 0.025642661355466216\n",
            "Training loss per 100 training steps: 0.025067510759448245\n",
            "Training loss per 100 training steps: 0.024515493838721886\n",
            "Training loss per 100 training steps: 0.023989720290499884\n",
            "Training loss per 100 training steps: 0.023550503917029817\n",
            "Training loss per 100 training steps: 0.0231352223326186\n",
            "Training loss per 100 training steps: 0.022732696372670412\n",
            "Training loss per 100 training steps: 0.022316330513257224\n",
            "Training loss per 100 training steps: 0.021919290084887535\n",
            "Training loss per 100 training steps: 0.021582386298432538\n",
            "Training loss per 100 training steps: 0.02128820360186651\n",
            "Training loss per 100 training steps: 0.021045370156369994\n",
            "Training loss per 100 training steps: 0.020730682938525986\n",
            "Training loss per 100 training steps: 0.020440873679989535\n",
            "Training loss per 100 training steps: 0.020204876609448644\n",
            "Training loss epoch: 0.020083917635291652\n",
            "Training accuracy epoch: 0.9307683515051365\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.010112687192158774\n",
            "Validation loss per 100 evaluation steps: 0.010120981221552939\n",
            "Validation loss per 100 evaluation steps: 0.009765440401388333\n",
            "Validation loss per 100 evaluation steps: 0.009608350918861107\n",
            "Validation loss per 100 evaluation steps: 0.009351176326395944\n",
            "Validation loss per 100 evaluation steps: 0.009592353977398792\n",
            "Validation loss per 100 evaluation steps: 0.009512732807197609\n",
            "Validation loss per 100 evaluation steps: 0.009474878065302618\n",
            "Validation Loss: 0.009548352392067576\n",
            "Validation Accuracy: 0.968859138695015\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.92      0.97      0.94      1837\n",
            "      B-MISC       0.66      0.79      0.72       922\n",
            "       B-ORG       0.83      0.94      0.88      1341\n",
            "       B-PER       0.99      0.93      0.96      1842\n",
            "       I-LOC       0.89      0.95      0.92      1801\n",
            "      I-MISC       0.60      0.38      0.47       935\n",
            "       I-ORG       0.81      0.93      0.86      2319\n",
            "       I-PER       0.99      0.93      0.96      4219\n",
            "           O       1.00      0.99      0.99     51041\n",
            "\n",
            "    accuracy                           0.97     66257\n",
            "   macro avg       0.85      0.87      0.86     66257\n",
            "weighted avg       0.97      0.97      0.97     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9698  0.9698  0.9698\n",
            "macro        0.8541  0.8673  0.8561\n",
            "weighted     0.9704  0.9698  0.9694\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.014584356911946088\n",
            "Test loss per 100 evaluation steps: 0.012833900239784271\n",
            "Test loss per 100 evaluation steps: 0.012306558002213327\n",
            "Test loss per 100 evaluation steps: 0.012488004175975221\n",
            "Test loss per 100 evaluation steps: 0.012104219404864124\n",
            "Test loss per 100 evaluation steps: 0.012140468165065007\n",
            "Test loss per 100 evaluation steps: 0.012180625634251296\n",
            "Test loss per 100 evaluation steps: 0.012345023673624382\n",
            "Test loss per 100 evaluation steps: 0.012234609634809506\n",
            "Test Loss: 0.012218674101872343\n",
            "Test Accuracy: 0.952032530676201\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.91      0.92      0.91      1668\n",
            "      B-MISC       0.53      0.78      0.63       702\n",
            "       B-ORG       0.79      0.92      0.85      1661\n",
            "       B-PER       0.99      0.86      0.92      1617\n",
            "       I-LOC       0.85      0.89      0.87      1394\n",
            "      I-MISC       0.38      0.33      0.35       736\n",
            "       I-ORG       0.76      0.95      0.84      2804\n",
            "       I-PER       0.99      0.87      0.92      3810\n",
            "           O       1.00      0.98      0.99     47094\n",
            "\n",
            "    accuracy                           0.96     61486\n",
            "   macro avg       0.80      0.83      0.81     61486\n",
            "weighted avg       0.96      0.96      0.96     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9553  0.9553  0.9553\n",
            "macro        0.7992  0.8331  0.8107\n",
            "weighted     0.9604  0.9553  0.9565\n",
            "Test steps: 921\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.01062950492487289\n",
            "Training loss per 100 training steps: 0.010932027213275432\n",
            "Training loss per 100 training steps: 0.010203525440301746\n",
            "Training loss per 100 training steps: 0.00981217682652641\n",
            "Training loss per 100 training steps: 0.0098783516776748\n",
            "Training loss per 100 training steps: 0.009938673157788193\n",
            "Training loss per 100 training steps: 0.009648474542773329\n",
            "Training loss per 100 training steps: 0.009411603104017558\n",
            "Training loss per 100 training steps: 0.009291377150414821\n",
            "Training loss per 100 training steps: 0.00903651102882577\n",
            "Training loss per 100 training steps: 0.008933752104375426\n",
            "Training loss per 100 training steps: 0.008901352258496142\n",
            "Training loss per 100 training steps: 0.00889281569639794\n",
            "Training loss per 100 training steps: 0.009002813595829398\n",
            "Training loss per 100 training steps: 0.009041061963575582\n",
            "Training loss per 100 training steps: 0.008970696620053786\n",
            "Training loss per 100 training steps: 0.008775539778768743\n",
            "Training loss per 100 training steps: 0.008722173392598051\n",
            "Training loss per 100 training steps: 0.008681695167018149\n",
            "Training loss per 100 training steps: 0.00858364666227135\n",
            "Training loss per 100 training steps: 0.008560234578187755\n",
            "Training loss per 100 training steps: 0.008466724052162714\n",
            "Training loss per 100 training steps: 0.008381805308365627\n",
            "Training loss per 100 training steps: 0.008304287541905068\n",
            "Training loss per 100 training steps: 0.008292766406759619\n",
            "Training loss per 100 training steps: 0.008169006839587102\n",
            "Training loss per 100 training steps: 0.008128830875438224\n",
            "Training loss per 100 training steps: 0.008044901283948483\n",
            "Training loss per 100 training steps: 0.00802273324339506\n",
            "Training loss per 100 training steps: 0.00794719457932903\n",
            "Training loss per 100 training steps: 0.00785664667322811\n",
            "Training loss per 100 training steps: 0.007864973251325864\n",
            "Training loss per 100 training steps: 0.007845178023562766\n",
            "Training loss per 100 training steps: 0.007776446520857623\n",
            "Training loss per 100 training steps: 0.007727666222523632\n",
            "Training loss per 100 training steps: 0.007712328609599758\n",
            "Training loss per 100 training steps: 0.007648509896338313\n",
            "Training loss epoch: 0.007649035376814386\n",
            "Training accuracy epoch: 0.9760517347833024\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.006728373971418478\n",
            "Validation loss per 100 evaluation steps: 0.005909972993540577\n",
            "Validation loss per 100 evaluation steps: 0.005529273471523387\n",
            "Validation loss per 100 evaluation steps: 0.005464431504224194\n",
            "Validation loss per 100 evaluation steps: 0.00562303819990484\n",
            "Validation loss per 100 evaluation steps: 0.005645760352344951\n",
            "Validation loss per 100 evaluation steps: 0.005619576081974499\n",
            "Validation loss per 100 evaluation steps: 0.005641261072778434\n",
            "Validation Loss: 0.005690731619972942\n",
            "Validation Accuracy: 0.9812169984956636\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.96      0.97      0.96      1837\n",
            "      B-MISC       0.92      0.87      0.89       922\n",
            "       B-ORG       0.90      0.95      0.93      1341\n",
            "       B-PER       0.99      0.95      0.97      1842\n",
            "       I-LOC       0.93      0.96      0.95      1801\n",
            "      I-MISC       0.78      0.80      0.79       935\n",
            "       I-ORG       0.91      0.92      0.91      2319\n",
            "       I-PER       0.99      0.96      0.97      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.98     66257\n",
            "   macro avg       0.93      0.93      0.93     66257\n",
            "weighted avg       0.98      0.98      0.98     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9823  0.9823  0.9823\n",
            "macro        0.9298  0.9307  0.9300\n",
            "weighted     0.9825  0.9823  0.9824\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.008266762174316681\n",
            "Test loss per 100 evaluation steps: 0.008874499679077417\n",
            "Test loss per 100 evaluation steps: 0.009100456072677237\n",
            "Test loss per 100 evaluation steps: 0.009211296809007763\n",
            "Test loss per 100 evaluation steps: 0.008929484849621076\n",
            "Test loss per 100 evaluation steps: 0.008679633261781419\n",
            "Test loss per 100 evaluation steps: 0.00862140550066085\n",
            "Test loss per 100 evaluation steps: 0.00866245623012219\n",
            "Test loss per 100 evaluation steps: 0.008744816351196883\n",
            "Test Loss: 0.008750760291588526\n",
            "Test Accuracy: 0.9672702888106967\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.92      0.93      0.93      1668\n",
            "      B-MISC       0.82      0.79      0.80       702\n",
            "       B-ORG       0.87      0.92      0.90      1661\n",
            "       B-PER       0.98      0.93      0.96      1617\n",
            "       I-LOC       0.86      0.92      0.89      1394\n",
            "      I-MISC       0.56      0.68      0.62       736\n",
            "       I-ORG       0.86      0.94      0.90      2804\n",
            "       I-PER       0.99      0.94      0.96      3810\n",
            "           O       0.99      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.87      0.89      0.88     61486\n",
            "weighted avg       0.97      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9700  0.9700  0.9700\n",
            "macro        0.8728  0.8945  0.8824\n",
            "weighted     0.9720  0.9700  0.9708\n",
            "Test steps: 921\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.00631867581862025\n",
            "Training loss per 100 training steps: 0.005640433225780725\n",
            "Training loss per 100 training steps: 0.005974140308292893\n",
            "Training loss per 100 training steps: 0.006009047950792592\n",
            "Training loss per 100 training steps: 0.005836880316259339\n",
            "Training loss per 100 training steps: 0.005833170760500555\n",
            "Training loss per 100 training steps: 0.005902043711394071\n",
            "Training loss per 100 training steps: 0.005834652084013214\n",
            "Training loss per 100 training steps: 0.005674580516901591\n",
            "Training loss per 100 training steps: 0.005850816764577758\n",
            "Training loss per 100 training steps: 0.005853877866598354\n",
            "Training loss per 100 training steps: 0.005877025278411262\n",
            "Training loss per 100 training steps: 0.005820839011979002\n",
            "Training loss per 100 training steps: 0.005872901147938267\n",
            "Training loss per 100 training steps: 0.005870511914875048\n",
            "Training loss per 100 training steps: 0.0057863282251128115\n",
            "Training loss per 100 training steps: 0.005770036385887686\n",
            "Training loss per 100 training steps: 0.005718502506335628\n",
            "Training loss per 100 training steps: 0.005683093156426606\n",
            "Training loss per 100 training steps: 0.005622297680034535\n",
            "Training loss per 100 training steps: 0.005643525814826023\n",
            "Training loss per 100 training steps: 0.005626372300294778\n",
            "Training loss per 100 training steps: 0.005628072512386691\n",
            "Training loss per 100 training steps: 0.005662893692763949\n",
            "Training loss per 100 training steps: 0.005649731245054863\n",
            "Training loss per 100 training steps: 0.005605122081581682\n",
            "Training loss per 100 training steps: 0.0056078798240223144\n",
            "Training loss per 100 training steps: 0.005613835631666006\n",
            "Training loss per 100 training steps: 0.005642256594588177\n",
            "Training loss per 100 training steps: 0.005653566567401867\n",
            "Training loss per 100 training steps: 0.005590310589290194\n",
            "Training loss per 100 training steps: 0.005607332084437076\n",
            "Training loss per 100 training steps: 0.005592257344679004\n",
            "Training loss per 100 training steps: 0.0055909091766464854\n",
            "Training loss per 100 training steps: 0.00558734119354215\n",
            "Training loss per 100 training steps: 0.005587094397447395\n",
            "Training loss per 100 training steps: 0.0055917767202245654\n",
            "Training loss epoch: 0.005610867911852011\n",
            "Training accuracy epoch: 0.9818064595873601\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.004792090140981599\n",
            "Validation loss per 100 evaluation steps: 0.004692618610279169\n",
            "Validation loss per 100 evaluation steps: 0.004950905098424604\n",
            "Validation loss per 100 evaluation steps: 0.004823522763035726\n",
            "Validation loss per 100 evaluation steps: 0.005078607595176436\n",
            "Validation loss per 100 evaluation steps: 0.005211403829453048\n",
            "Validation loss per 100 evaluation steps: 0.005269367634651384\n",
            "Validation loss per 100 evaluation steps: 0.005217913804517593\n",
            "Validation Loss: 0.005252976745903876\n",
            "Validation Accuracy: 0.9812140250245412\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.96      0.95      0.96      1837\n",
            "      B-MISC       0.95      0.85      0.90       922\n",
            "       B-ORG       0.90      0.93      0.91      1341\n",
            "       B-PER       0.95      0.99      0.97      1842\n",
            "       I-LOC       0.96      0.91      0.93      1801\n",
            "      I-MISC       0.93      0.76      0.83       935\n",
            "       I-ORG       0.89      0.89      0.89      2319\n",
            "       I-PER       0.95      0.99      0.97      4219\n",
            "           O       0.99      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.98     66257\n",
            "   macro avg       0.94      0.92      0.93     66257\n",
            "weighted avg       0.98      0.98      0.98     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9818  0.9818  0.9818\n",
            "macro        0.9424  0.9183  0.9289\n",
            "weighted     0.9818  0.9818  0.9816\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.005123385175829753\n",
            "Test loss per 100 evaluation steps: 0.007257415615313221\n",
            "Test loss per 100 evaluation steps: 0.0077570189597706\n",
            "Test loss per 100 evaluation steps: 0.007808741940825712\n",
            "Test loss per 100 evaluation steps: 0.007851623870024924\n",
            "Test loss per 100 evaluation steps: 0.007683780147975388\n",
            "Test loss per 100 evaluation steps: 0.007812944893417547\n",
            "Test loss per 100 evaluation steps: 0.007793149645076483\n",
            "Test loss per 100 evaluation steps: 0.007780286322231405\n",
            "Test Loss: 0.007693959322131576\n",
            "Test Accuracy: 0.9702419988924634\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.93      0.93      0.93      1668\n",
            "      B-MISC       0.85      0.79      0.82       702\n",
            "       B-ORG       0.90      0.90      0.90      1661\n",
            "       B-PER       0.92      0.97      0.95      1617\n",
            "       I-LOC       0.89      0.90      0.89      1394\n",
            "      I-MISC       0.68      0.60      0.64       736\n",
            "       I-ORG       0.90      0.91      0.90      2804\n",
            "       I-PER       0.93      0.99      0.96      3810\n",
            "           O       0.99      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.89      0.88      0.89     61486\n",
            "weighted avg       0.97      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9719  0.9719  0.9719\n",
            "macro        0.8885  0.8848  0.8860\n",
            "weighted     0.9717  0.9719  0.9717\n",
            "Test steps: 921\n",
            "Training epoch: 4\n",
            "Training loss per 100 training steps: 0.005092682265676558\n",
            "Training loss per 100 training steps: 0.005029557979141828\n",
            "Training loss per 100 training steps: 0.005381894232899261\n",
            "Training loss per 100 training steps: 0.005345520558475983\n",
            "Training loss per 100 training steps: 0.005355438598897308\n",
            "Training loss per 100 training steps: 0.005247299116163049\n",
            "Training loss per 100 training steps: 0.005140025115688332\n",
            "Training loss per 100 training steps: 0.005099535595654743\n",
            "Training loss per 100 training steps: 0.005076498522798324\n",
            "Training loss per 100 training steps: 0.0049317113737342875\n",
            "Training loss per 100 training steps: 0.00498698827360799\n",
            "Training loss per 100 training steps: 0.004968806029501139\n",
            "Training loss per 100 training steps: 0.004948388566621221\n",
            "Training loss per 100 training steps: 0.004913099047865087\n",
            "Training loss per 100 training steps: 0.004980108147312421\n",
            "Training loss per 100 training steps: 0.004959661364537169\n",
            "Training loss per 100 training steps: 0.004938178714244928\n",
            "Training loss per 100 training steps: 0.004919788387245111\n",
            "Training loss per 100 training steps: 0.004828468943949127\n",
            "Training loss per 100 training steps: 0.00482502204591583\n",
            "Training loss per 100 training steps: 0.00477325989927132\n",
            "Training loss per 100 training steps: 0.004863258082914399\n",
            "Training loss per 100 training steps: 0.004858951439066162\n",
            "Training loss per 100 training steps: 0.004829203999409704\n",
            "Training loss per 100 training steps: 0.0048332664841902444\n",
            "Training loss per 100 training steps: 0.004782944023362898\n",
            "Training loss per 100 training steps: 0.004801248092790721\n",
            "Training loss per 100 training steps: 0.004802938153484969\n",
            "Training loss per 100 training steps: 0.004773873341659181\n",
            "Training loss per 100 training steps: 0.004779116925453612\n",
            "Training loss per 100 training steps: 0.0047819706741457566\n",
            "Training loss per 100 training steps: 0.004753281140710897\n",
            "Training loss per 100 training steps: 0.0047632592663726285\n",
            "Training loss per 100 training steps: 0.0047718624693088175\n",
            "Training loss per 100 training steps: 0.004768802186341158\n",
            "Training loss per 100 training steps: 0.0047734081295251\n",
            "Training loss per 100 training steps: 0.004762962500106958\n",
            "Training loss epoch: 0.00475949808273509\n",
            "Training accuracy epoch: 0.9845324895316682\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.0030519790935795753\n",
            "Validation loss per 100 evaluation steps: 0.004255571296671406\n",
            "Validation loss per 100 evaluation steps: 0.0040522720296091086\n",
            "Validation loss per 100 evaluation steps: 0.004269722537137568\n",
            "Validation loss per 100 evaluation steps: 0.0042338629638543354\n",
            "Validation loss per 100 evaluation steps: 0.004465614216654406\n",
            "Validation loss per 100 evaluation steps: 0.0044539082738005425\n",
            "Validation loss per 100 evaluation steps: 0.004576612856290012\n",
            "Validation Loss: 0.004506825425694595\n",
            "Validation Accuracy: 0.9844512938534614\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.96      0.96      1837\n",
            "      B-MISC       0.93      0.88      0.90       922\n",
            "       B-ORG       0.91      0.95      0.93      1341\n",
            "       B-PER       0.96      0.98      0.97      1842\n",
            "       I-LOC       0.98      0.93      0.96      1801\n",
            "      I-MISC       0.87      0.80      0.83       935\n",
            "       I-ORG       0.92      0.93      0.92      2319\n",
            "       I-PER       0.97      0.99      0.98      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.98     66257\n",
            "   macro avg       0.94      0.93      0.94     66257\n",
            "weighted avg       0.98      0.98      0.98     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9849  0.9849  0.9849\n",
            "macro        0.9445  0.9346  0.9392\n",
            "weighted     0.9848  0.9849  0.9848\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.009329826937173493\n",
            "Test loss per 100 evaluation steps: 0.007658599660499021\n",
            "Test loss per 100 evaluation steps: 0.007226092503600133\n",
            "Test loss per 100 evaluation steps: 0.006882689055128139\n",
            "Test loss per 100 evaluation steps: 0.00700862471020082\n",
            "Test loss per 100 evaluation steps: 0.007237405681565482\n",
            "Test loss per 100 evaluation steps: 0.007153310527501162\n",
            "Test loss per 100 evaluation steps: 0.007304493739320605\n",
            "Test loss per 100 evaluation steps: 0.007313985589005622\n",
            "Test Loss: 0.007322589231493966\n",
            "Test Accuracy: 0.9717972046884725\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.95      0.92      0.93      1668\n",
            "      B-MISC       0.84      0.80      0.82       702\n",
            "       B-ORG       0.88      0.92      0.90      1661\n",
            "       B-PER       0.95      0.97      0.96      1617\n",
            "       I-LOC       0.91      0.89      0.90      1394\n",
            "      I-MISC       0.64      0.64      0.64       736\n",
            "       I-ORG       0.88      0.94      0.91      2804\n",
            "       I-PER       0.96      0.98      0.97      3810\n",
            "           O       0.99      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.89      0.89      0.89     61486\n",
            "weighted avg       0.97      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9732  0.9732  0.9732\n",
            "macro        0.8887  0.8937  0.8909\n",
            "weighted     0.9736  0.9732  0.9734\n",
            "Test steps: 921\n",
            "Training epoch: 5\n",
            "Training loss per 100 training steps: 0.004652557455119677\n",
            "Training loss per 100 training steps: 0.00429253327602055\n",
            "Training loss per 100 training steps: 0.004638826940984776\n",
            "Training loss per 100 training steps: 0.005126651266036788\n",
            "Training loss per 100 training steps: 0.004924410823383368\n",
            "Training loss per 100 training steps: 0.004737002257897984\n",
            "Training loss per 100 training steps: 0.004738314527619098\n",
            "Training loss per 100 training steps: 0.00464226631767815\n",
            "Training loss per 100 training steps: 0.004578443436605286\n",
            "Training loss per 100 training steps: 0.004457890576595673\n",
            "Training loss per 100 training steps: 0.004381136413139757\n",
            "Training loss per 100 training steps: 0.004386149793839043\n",
            "Training loss per 100 training steps: 0.004386978787683452\n",
            "Training loss per 100 training steps: 0.004406909964870595\n",
            "Training loss per 100 training steps: 0.00438184290996287\n",
            "Training loss per 100 training steps: 0.004324295553051343\n",
            "Training loss per 100 training steps: 0.0043341301289825315\n",
            "Training loss per 100 training steps: 0.004286629977674844\n",
            "Training loss per 100 training steps: 0.004247919882519023\n",
            "Training loss per 100 training steps: 0.004208700565824983\n",
            "Training loss per 100 training steps: 0.004159347024535583\n",
            "Training loss per 100 training steps: 0.004116291060808263\n",
            "Training loss per 100 training steps: 0.004095316193453745\n",
            "Training loss per 100 training steps: 0.0041289451444875645\n",
            "Training loss per 100 training steps: 0.004115685384860263\n",
            "Training loss per 100 training steps: 0.004161749430864942\n",
            "Training loss per 100 training steps: 0.00413020149895197\n",
            "Training loss per 100 training steps: 0.004132937441920928\n",
            "Training loss per 100 training steps: 0.004098218683422351\n",
            "Training loss per 100 training steps: 0.0040885823101270945\n",
            "Training loss per 100 training steps: 0.004099998078271446\n",
            "Training loss per 100 training steps: 0.0041284582287698865\n",
            "Training loss per 100 training steps: 0.004113073668691372\n",
            "Training loss per 100 training steps: 0.004107240522973691\n",
            "Training loss per 100 training steps: 0.0041042038951667826\n",
            "Training loss per 100 training steps: 0.004073333088534936\n",
            "Training loss per 100 training steps: 0.004033138023247325\n",
            "Training loss epoch: 0.004043545532200925\n",
            "Training accuracy epoch: 0.987089044637355\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.005152875336352736\n",
            "Validation loss per 100 evaluation steps: 0.005680754743225407\n",
            "Validation loss per 100 evaluation steps: 0.005255790508817882\n",
            "Validation loss per 100 evaluation steps: 0.00498749543519807\n",
            "Validation loss per 100 evaluation steps: 0.004864885200979188\n",
            "Validation loss per 100 evaluation steps: 0.004691748150895971\n",
            "Validation loss per 100 evaluation steps: 0.004620311142560759\n",
            "Validation loss per 100 evaluation steps: 0.004607910523554892\n",
            "Validation Loss: 0.004566740874572192\n",
            "Validation Accuracy: 0.9840703337167587\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.95      0.96      1837\n",
            "      B-MISC       0.93      0.88      0.91       922\n",
            "       B-ORG       0.91      0.95      0.93      1341\n",
            "       B-PER       0.96      0.99      0.97      1842\n",
            "       I-LOC       0.98      0.91      0.94      1801\n",
            "      I-MISC       0.92      0.81      0.86       935\n",
            "       I-ORG       0.90      0.95      0.92      2319\n",
            "       I-PER       0.95      0.99      0.97      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.98     66257\n",
            "   macro avg       0.95      0.94      0.94     66257\n",
            "weighted avg       0.99      0.98      0.98     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9848  0.9848  0.9848\n",
            "macro        0.9462  0.9373  0.9409\n",
            "weighted     0.9850  0.9848  0.9847\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.0073284389544278385\n",
            "Test loss per 100 evaluation steps: 0.00753873022767948\n",
            "Test loss per 100 evaluation steps: 0.007282419559390595\n",
            "Test loss per 100 evaluation steps: 0.007364336122118402\n",
            "Test loss per 100 evaluation steps: 0.007322474533575587\n",
            "Test loss per 100 evaluation steps: 0.007351623429567553\n",
            "Test loss per 100 evaluation steps: 0.007523448223863462\n",
            "Test loss per 100 evaluation steps: 0.0076752465201570886\n",
            "Test loss per 100 evaluation steps: 0.007375882923434903\n",
            "Test Loss: 0.007539678536012696\n",
            "Test Accuracy: 0.9707556286153424\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.95      0.93      0.94      1668\n",
            "      B-MISC       0.85      0.81      0.83       702\n",
            "       B-ORG       0.89      0.93      0.91      1661\n",
            "       B-PER       0.92      0.97      0.95      1617\n",
            "       I-LOC       0.91      0.90      0.90      1394\n",
            "      I-MISC       0.70      0.67      0.69       736\n",
            "       I-ORG       0.88      0.95      0.91      2804\n",
            "       I-PER       0.92      0.98      0.95      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.89      0.90      0.90     61486\n",
            "weighted avg       0.97      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9723  0.9723  0.9723\n",
            "macro        0.8905  0.9033  0.8964\n",
            "weighted     0.9730  0.9723  0.9724\n",
            "Test steps: 921\n",
            "Training epoch: 6\n",
            "Training loss per 100 training steps: 0.0054292089847149325\n",
            "Training loss per 100 training steps: 0.004981283368251752\n",
            "Training loss per 100 training steps: 0.004378740358515642\n",
            "Training loss per 100 training steps: 0.004102884317107964\n",
            "Training loss per 100 training steps: 0.003890679803560488\n",
            "Training loss per 100 training steps: 0.004027050879182449\n",
            "Training loss per 100 training steps: 0.004193713917214024\n",
            "Training loss per 100 training steps: 0.004239685000720783\n",
            "Training loss per 100 training steps: 0.004243399986816156\n",
            "Training loss per 100 training steps: 0.004251212155912071\n",
            "Training loss per 100 training steps: 0.0041170813255964524\n",
            "Training loss per 100 training steps: 0.004016098746530285\n",
            "Training loss per 100 training steps: 0.004028236464342067\n",
            "Training loss per 100 training steps: 0.004017828896820512\n",
            "Training loss per 100 training steps: 0.003987429691730843\n",
            "Training loss per 100 training steps: 0.003933465246791457\n",
            "Training loss per 100 training steps: 0.004019847161030424\n",
            "Training loss per 100 training steps: 0.003977919035306614\n",
            "Training loss per 100 training steps: 0.004029909307244357\n",
            "Training loss per 100 training steps: 0.003999496978722163\n",
            "Training loss per 100 training steps: 0.00395835295804621\n",
            "Training loss per 100 training steps: 0.0039694601266554936\n",
            "Training loss per 100 training steps: 0.003941773095373166\n",
            "Training loss per 100 training steps: 0.003906442568077182\n",
            "Training loss per 100 training steps: 0.003876387105253525\n",
            "Training loss per 100 training steps: 0.0038613363301332215\n",
            "Training loss per 100 training steps: 0.0038544415752187855\n",
            "Training loss per 100 training steps: 0.003865279331948841\n",
            "Training loss per 100 training steps: 0.0038393673065202794\n",
            "Training loss per 100 training steps: 0.0038584826771111695\n",
            "Training loss per 100 training steps: 0.003839502491999508\n",
            "Training loss per 100 training steps: 0.0038225322008383954\n",
            "Training loss per 100 training steps: 0.003810616896856421\n",
            "Training loss per 100 training steps: 0.003802127542311449\n",
            "Training loss per 100 training steps: 0.0037911825937002766\n",
            "Training loss per 100 training steps: 0.003740159482848766\n",
            "Training loss per 100 training steps: 0.003764972552504529\n",
            "Training loss epoch: 0.0037534837051856344\n",
            "Training accuracy epoch: 0.9878666930612889\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.005911382525227964\n",
            "Validation loss per 100 evaluation steps: 0.004852667619998101\n",
            "Validation loss per 100 evaluation steps: 0.004944904116758456\n",
            "Validation loss per 100 evaluation steps: 0.004990543777385028\n",
            "Validation loss per 100 evaluation steps: 0.004946523885591887\n",
            "Validation loss per 100 evaluation steps: 0.004773478949500714\n",
            "Validation loss per 100 evaluation steps: 0.004742700122047349\n",
            "Validation loss per 100 evaluation steps: 0.004656215252107358\n",
            "Validation Loss: 0.004620422861047923\n",
            "Validation Accuracy: 0.9839810468460068\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.94      0.98      0.96      1837\n",
            "      B-MISC       0.94      0.84      0.89       922\n",
            "       B-ORG       0.94      0.93      0.94      1341\n",
            "       B-PER       0.98      0.96      0.97      1842\n",
            "       I-LOC       0.93      0.96      0.95      1801\n",
            "      I-MISC       0.90      0.76      0.82       935\n",
            "       I-ORG       0.92      0.94      0.93      2319\n",
            "       I-PER       0.99      0.97      0.98      4219\n",
            "           O       0.99      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.98     66257\n",
            "   macro avg       0.95      0.93      0.94     66257\n",
            "weighted avg       0.98      0.98      0.98     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9842  0.9842  0.9842\n",
            "macro        0.9485  0.9271  0.9368\n",
            "weighted     0.9840  0.9842  0.9840\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.006870264029712417\n",
            "Test loss per 100 evaluation steps: 0.00636451416881755\n",
            "Test loss per 100 evaluation steps: 0.007142795970818649\n",
            "Test loss per 100 evaluation steps: 0.007522070225386415\n",
            "Test loss per 100 evaluation steps: 0.007655554728000425\n",
            "Test loss per 100 evaluation steps: 0.007648211946846762\n",
            "Test loss per 100 evaluation steps: 0.007622553994213896\n",
            "Test loss per 100 evaluation steps: 0.007863199816347333\n",
            "Test loss per 100 evaluation steps: 0.007942100865701731\n",
            "Test Loss: 0.007932849108743563\n",
            "Test Accuracy: 0.9694136713727716\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.89      0.94      0.92      1668\n",
            "      B-MISC       0.86      0.78      0.82       702\n",
            "       B-ORG       0.88      0.91      0.89      1661\n",
            "       B-PER       0.98      0.93      0.95      1617\n",
            "       I-LOC       0.81      0.92      0.86      1394\n",
            "      I-MISC       0.72      0.60      0.65       736\n",
            "       I-ORG       0.86      0.93      0.89      2804\n",
            "       I-PER       0.97      0.95      0.96      3810\n",
            "           O       0.99      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.89      0.88      0.88     61486\n",
            "weighted avg       0.97      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9706  0.9706  0.9706\n",
            "macro        0.8854  0.8814  0.8819\n",
            "weighted     0.9711  0.9706  0.9706\n",
            "Test steps: 921\n",
            "Training epoch: 7\n",
            "Training loss per 100 training steps: 0.0038888197686173954\n",
            "Training loss per 100 training steps: 0.0038157857418991627\n",
            "Training loss per 100 training steps: 0.0039848417493825155\n",
            "Training loss per 100 training steps: 0.003961311520106392\n",
            "Training loss per 100 training steps: 0.0038875988179352134\n",
            "Training loss per 100 training steps: 0.0037674700339751627\n",
            "Training loss per 100 training steps: 0.0037698783224498454\n",
            "Training loss per 100 training steps: 0.0037833019594836516\n",
            "Training loss per 100 training steps: 0.003743669325906214\n",
            "Training loss per 100 training steps: 0.003809978784120176\n",
            "Training loss per 100 training steps: 0.003732211096159352\n",
            "Training loss per 100 training steps: 0.0037566582892516937\n",
            "Training loss per 100 training steps: 0.0037145348976796062\n",
            "Training loss per 100 training steps: 0.0036219507237962847\n",
            "Training loss per 100 training steps: 0.0035007420496161405\n",
            "Training loss per 100 training steps: 0.003545572495622764\n",
            "Training loss per 100 training steps: 0.0035002976492122637\n",
            "Training loss per 100 training steps: 0.003538350315648131\n",
            "Training loss per 100 training steps: 0.0035016528333806874\n",
            "Training loss per 100 training steps: 0.0035068957357434556\n",
            "Training loss per 100 training steps: 0.0035028137103793036\n",
            "Training loss per 100 training steps: 0.0034751715045157733\n",
            "Training loss per 100 training steps: 0.0034506894431188297\n",
            "Training loss per 100 training steps: 0.003454281146478024\n",
            "Training loss per 100 training steps: 0.0034856000717380085\n",
            "Training loss per 100 training steps: 0.003474529734454476\n",
            "Training loss per 100 training steps: 0.0034679551211215937\n",
            "Training loss per 100 training steps: 0.003437993979314342\n",
            "Training loss per 100 training steps: 0.003458687774576086\n",
            "Training loss per 100 training steps: 0.003461580058268737\n",
            "Training loss per 100 training steps: 0.0034350986646454512\n",
            "Training loss per 100 training steps: 0.0034423722014071245\n",
            "Training loss per 100 training steps: 0.0034589269646147095\n",
            "Training loss per 100 training steps: 0.003439022502219921\n",
            "Training loss per 100 training steps: 0.0034248130408564715\n",
            "Training loss per 100 training steps: 0.0034104080360622094\n",
            "Training loss per 100 training steps: 0.0033908032898309743\n",
            "Training loss epoch: 0.003394908060476682\n",
            "Training accuracy epoch: 0.9890118188312748\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.005370045800227672\n",
            "Validation loss per 100 evaluation steps: 0.004740064282232197\n",
            "Validation loss per 100 evaluation steps: 0.004939189307236423\n",
            "Validation loss per 100 evaluation steps: 0.005049128801838379\n",
            "Validation loss per 100 evaluation steps: 0.004842250523448456\n",
            "Validation loss per 100 evaluation steps: 0.004660829459753586\n",
            "Validation loss per 100 evaluation steps: 0.004634971401413038\n",
            "Validation loss per 100 evaluation steps: 0.004503764511719055\n",
            "Validation Loss: 0.004541005329172542\n",
            "Validation Accuracy: 0.9841519830639178\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.98      0.95      0.97      1837\n",
            "      B-MISC       0.93      0.89      0.91       922\n",
            "       B-ORG       0.90      0.95      0.92      1341\n",
            "       B-PER       0.98      0.97      0.98      1842\n",
            "       I-LOC       0.97      0.91      0.94      1801\n",
            "      I-MISC       0.89      0.80      0.84       935\n",
            "       I-ORG       0.89      0.94      0.91      2319\n",
            "       I-PER       0.99      0.97      0.98      4219\n",
            "           O       0.99      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.98     66257\n",
            "   macro avg       0.95      0.93      0.94     66257\n",
            "weighted avg       0.98      0.98      0.98     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9842  0.9842  0.9842\n",
            "macro        0.9468  0.9314  0.9385\n",
            "weighted     0.9843  0.9842  0.9842\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.0054620830380008555\n",
            "Test loss per 100 evaluation steps: 0.005739878429449163\n",
            "Test loss per 100 evaluation steps: 0.006791180616904361\n",
            "Test loss per 100 evaluation steps: 0.0068312327070452735\n",
            "Test loss per 100 evaluation steps: 0.006973556068725884\n",
            "Test loss per 100 evaluation steps: 0.006970097330825714\n",
            "Test loss per 100 evaluation steps: 0.007032964323963305\n",
            "Test loss per 100 evaluation steps: 0.006897899530849827\n",
            "Test loss per 100 evaluation steps: 0.00696137516235467\n",
            "Test Loss: 0.0070406150378197856\n",
            "Test Accuracy: 0.9730910114222348\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.94      0.93      0.93      1668\n",
            "      B-MISC       0.86      0.83      0.85       702\n",
            "       B-ORG       0.87      0.93      0.90      1661\n",
            "       B-PER       0.97      0.93      0.95      1617\n",
            "       I-LOC       0.91      0.90      0.90      1394\n",
            "      I-MISC       0.76      0.68      0.72       736\n",
            "       I-ORG       0.86      0.94      0.90      2804\n",
            "       I-PER       0.97      0.95      0.96      3810\n",
            "           O       0.99      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.90      0.90      0.90     61486\n",
            "weighted avg       0.97      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9745  0.9745  0.9745\n",
            "macro        0.9036  0.8974  0.8999\n",
            "weighted     0.9748  0.9745  0.9745\n",
            "Test steps: 921\n",
            "====================================================================================================\n",
            "loss_name: l2\n",
            "model configuration\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Ner_Model(\n",
            "  (model): DebertaModel(\n",
            "    (embeddings): DebertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "      (LayerNorm): DebertaLayerNorm()\n",
            "      (dropout): StableDropout()\n",
            "    )\n",
            "    (encoder): DebertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x DebertaLayer(\n",
            "          (attention): DebertaAttention(\n",
            "            (self): DisentangledSelfAttention(\n",
            "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
            "              (pos_dropout): StableDropout()\n",
            "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "            (output): DebertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): DebertaLayerNorm()\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "          )\n",
            "          (intermediate): DebertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): DebertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): DebertaLayerNorm()\n",
            "            (dropout): StableDropout()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (rel_embeddings): Embedding(1024, 768)\n",
            "    )\n",
            "  )\n",
            "  (linear_model): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=384, bias=True)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (classifier): Linear(in_features=384, out_features=9, bias=True)\n",
            ")\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.0392971508577466\n",
            "Training loss per 100 training steps: 0.028174848259659484\n",
            "Training loss per 100 training steps: 0.023445859525818378\n",
            "Training loss per 100 training steps: 0.01987903044639097\n",
            "Training loss per 100 training steps: 0.01802109150023898\n",
            "Training loss per 100 training steps: 0.016488680425197043\n",
            "Training loss per 100 training steps: 0.015433586831952978\n",
            "Training loss per 100 training steps: 0.014218273370643146\n",
            "Training loss per 100 training steps: 0.013318504623651582\n",
            "Training loss per 100 training steps: 0.012571321809344227\n",
            "Training loss per 100 training steps: 0.012058994369105097\n",
            "Training loss per 100 training steps: 0.011594762406978891\n",
            "Training loss per 100 training steps: 0.011126428934893249\n",
            "Training loss per 100 training steps: 0.010737734763575386\n",
            "Training loss per 100 training steps: 0.010396290486237072\n",
            "Training loss per 100 training steps: 0.010009529550675324\n",
            "Training loss per 100 training steps: 0.009675081321838662\n",
            "Training loss per 100 training steps: 0.009386968923059208\n",
            "Training loss per 100 training steps: 0.00915266952851533\n",
            "Training loss per 100 training steps: 0.008893934283780254\n",
            "Training loss per 100 training steps: 0.008640436493567124\n",
            "Training loss per 100 training steps: 0.00844455435966004\n",
            "Training loss per 100 training steps: 0.008254636939337891\n",
            "Training loss per 100 training steps: 0.008069522066361969\n",
            "Training loss per 100 training steps: 0.007894404649970238\n",
            "Training loss per 100 training steps: 0.0077006054089752765\n",
            "Training loss per 100 training steps: 0.007523074867955681\n",
            "Training loss per 100 training steps: 0.007383183202865829\n",
            "Training loss per 100 training steps: 0.007244521614632285\n",
            "Training loss per 100 training steps: 0.007087636936662117\n",
            "Training loss per 100 training steps: 0.0069807141586047255\n",
            "Training loss per 100 training steps: 0.006887244799196423\n",
            "Training loss per 100 training steps: 0.006779446758385651\n",
            "Training loss per 100 training steps: 0.006670139855000075\n",
            "Training loss per 100 training steps: 0.006595766868345631\n",
            "Training loss per 100 training steps: 0.0065402366658165395\n",
            "Training loss per 100 training steps: 0.006502925781809412\n",
            "Training loss epoch: 0.00645939568750047\n",
            "Training accuracy epoch: 0.9648287593024301\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.004421577764987888\n",
            "Validation loss per 100 evaluation steps: 0.004534418371895299\n",
            "Validation loss per 100 evaluation steps: 0.004870293654409276\n",
            "Validation loss per 100 evaluation steps: 0.004571471213712357\n",
            "Validation loss per 100 evaluation steps: 0.004137910263503727\n",
            "Validation loss per 100 evaluation steps: 0.003985987134522778\n",
            "Validation loss per 100 evaluation steps: 0.004175693497576763\n",
            "Validation loss per 100 evaluation steps: 0.004151148711300721\n",
            "Validation Loss: 0.004071598331303867\n",
            "Validation Accuracy: 0.9789587345994153\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.95      0.96      1837\n",
            "      B-MISC       0.90      0.90      0.90       922\n",
            "       B-ORG       0.86      0.96      0.91      1341\n",
            "       B-PER       0.99      0.92      0.96      1842\n",
            "       I-LOC       0.95      0.93      0.94      1801\n",
            "      I-MISC       0.76      0.90      0.82       935\n",
            "       I-ORG       0.82      0.94      0.88      2319\n",
            "       I-PER       0.99      0.93      0.96      4219\n",
            "           O       1.00      0.99      1.00     51041\n",
            "\n",
            "    accuracy                           0.98     66257\n",
            "   macro avg       0.92      0.94      0.92     66257\n",
            "weighted avg       0.98      0.98      0.98     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9793  0.9793  0.9793\n",
            "macro        0.9158  0.9356  0.9241\n",
            "weighted     0.9811  0.9793  0.9798\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.006625163150838489\n",
            "Test loss per 100 evaluation steps: 0.006265892026831353\n",
            "Test loss per 100 evaluation steps: 0.006605647355985032\n",
            "Test loss per 100 evaluation steps: 0.007178737287567856\n",
            "Test loss per 100 evaluation steps: 0.007111134815248079\n",
            "Test loss per 100 evaluation steps: 0.0070811040264015905\n",
            "Test loss per 100 evaluation steps: 0.007237124594071897\n",
            "Test loss per 100 evaluation steps: 0.007283310058883216\n",
            "Test loss per 100 evaluation steps: 0.007132289454356295\n",
            "Test Loss: 0.007097791925411393\n",
            "Test Accuracy: 0.9634575728849231\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.94      0.92      0.93      1668\n",
            "      B-MISC       0.79      0.82      0.80       702\n",
            "       B-ORG       0.82      0.94      0.88      1661\n",
            "       B-PER       0.99      0.89      0.93      1617\n",
            "       I-LOC       0.91      0.90      0.90      1394\n",
            "      I-MISC       0.54      0.71      0.61       736\n",
            "       I-ORG       0.78      0.96      0.86      2804\n",
            "       I-PER       0.99      0.90      0.94      3810\n",
            "           O       1.00      0.98      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.86      0.89      0.87     61486\n",
            "weighted avg       0.97      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9654  0.9654  0.9654\n",
            "macro        0.8614  0.8906  0.8724\n",
            "weighted     0.9702  0.9654  0.9669\n",
            "Test steps: 921\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.003503293196117738\n",
            "Training loss per 100 training steps: 0.0030442503742597183\n",
            "Training loss per 100 training steps: 0.0027758107944585693\n",
            "Training loss per 100 training steps: 0.0028757809885610187\n",
            "Training loss per 100 training steps: 0.002977472550221137\n",
            "Training loss per 100 training steps: 0.003089304742546422\n",
            "Training loss per 100 training steps: 0.0030704315477383458\n",
            "Training loss per 100 training steps: 0.0031667596696752297\n",
            "Training loss per 100 training steps: 0.0031728662459580745\n",
            "Training loss per 100 training steps: 0.003099066813425452\n",
            "Training loss per 100 training steps: 0.0030629290648787654\n",
            "Training loss per 100 training steps: 0.002951543011886315\n",
            "Training loss per 100 training steps: 0.0029703930934696335\n",
            "Training loss per 100 training steps: 0.0029311712464238685\n",
            "Training loss per 100 training steps: 0.002918083712242757\n",
            "Training loss per 100 training steps: 0.0028655465374254163\n",
            "Training loss per 100 training steps: 0.0028932882000770083\n",
            "Training loss per 100 training steps: 0.00287453900374304\n",
            "Training loss per 100 training steps: 0.002831055556363074\n",
            "Training loss per 100 training steps: 0.0028271674407496904\n",
            "Training loss per 100 training steps: 0.002858295406049798\n",
            "Training loss per 100 training steps: 0.0028267277798369665\n",
            "Training loss per 100 training steps: 0.0028200934704893923\n",
            "Training loss per 100 training steps: 0.002789378121503129\n",
            "Training loss per 100 training steps: 0.0028007402337971144\n",
            "Training loss per 100 training steps: 0.002809185057180562\n",
            "Training loss per 100 training steps: 0.002794241687884938\n",
            "Training loss per 100 training steps: 0.0027768291226142796\n",
            "Training loss per 100 training steps: 0.0027515706814238944\n",
            "Training loss per 100 training steps: 0.002729085993444945\n",
            "Training loss per 100 training steps: 0.0027051087618301314\n",
            "Training loss per 100 training steps: 0.002676278880545624\n",
            "Training loss per 100 training steps: 0.0026544178193260892\n",
            "Training loss per 100 training steps: 0.002648618189742753\n",
            "Training loss per 100 training steps: 0.0026398255394004602\n",
            "Training loss per 100 training steps: 0.0026254696676490615\n",
            "Training loss per 100 training steps: 0.0026282402735464654\n",
            "Training loss epoch: 0.002621851570518208\n",
            "Training accuracy epoch: 0.9859312243521337\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.002026711731978139\n",
            "Validation loss per 100 evaluation steps: 0.002526295327115804\n",
            "Validation loss per 100 evaluation steps: 0.0025383108781291716\n",
            "Validation loss per 100 evaluation steps: 0.0024672460533838605\n",
            "Validation loss per 100 evaluation steps: 0.0026224193798407213\n",
            "Validation loss per 100 evaluation steps: 0.0025673421406130124\n",
            "Validation loss per 100 evaluation steps: 0.0027481623514514857\n",
            "Validation loss per 100 evaluation steps: 0.002759226936716459\n",
            "Validation Loss: 0.0027111633162380353\n",
            "Validation Accuracy: 0.9854646663167491\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.96      0.98      0.97      1837\n",
            "      B-MISC       0.87      0.94      0.90       922\n",
            "       B-ORG       0.95      0.94      0.95      1341\n",
            "       B-PER       0.99      0.97      0.98      1842\n",
            "       I-LOC       0.94      0.97      0.95      1801\n",
            "      I-MISC       0.77      0.90      0.83       935\n",
            "       I-ORG       0.95      0.93      0.94      2319\n",
            "       I-PER       0.99      0.97      0.98      4219\n",
            "           O       1.00      0.99      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.93      0.96      0.94     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9859  0.9859  0.9859\n",
            "macro        0.9337  0.9554  0.9437\n",
            "weighted     0.9867  0.9859  0.9862\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.006760197086005064\n",
            "Test loss per 100 evaluation steps: 0.0055831020078221626\n",
            "Test loss per 100 evaluation steps: 0.005396050462586573\n",
            "Test loss per 100 evaluation steps: 0.00537567048463643\n",
            "Test loss per 100 evaluation steps: 0.005224726075357467\n",
            "Test loss per 100 evaluation steps: 0.005437921936481871\n",
            "Test loss per 100 evaluation steps: 0.005657097335889765\n",
            "Test loss per 100 evaluation steps: 0.005465366693449596\n",
            "Test loss per 100 evaluation steps: 0.00575994111250111\n",
            "Test Loss: 0.0058143272914384595\n",
            "Test Accuracy: 0.9703780754556179\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.91      0.94      0.93      1668\n",
            "      B-MISC       0.74      0.87      0.80       702\n",
            "       B-ORG       0.91      0.91      0.91      1661\n",
            "       B-PER       0.98      0.95      0.96      1617\n",
            "       I-LOC       0.83      0.93      0.88      1394\n",
            "      I-MISC       0.57      0.76      0.65       736\n",
            "       I-ORG       0.91      0.93      0.92      2804\n",
            "       I-PER       0.99      0.96      0.98      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.87      0.92      0.89     61486\n",
            "weighted avg       0.98      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9729  0.9729  0.9729\n",
            "macro        0.8703  0.9165  0.8908\n",
            "weighted     0.9758  0.9729  0.9740\n",
            "Test steps: 921\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.0012083379298746877\n",
            "Training loss per 100 training steps: 0.0013828413798091788\n",
            "Training loss per 100 training steps: 0.0016534379226929255\n",
            "Training loss per 100 training steps: 0.0017410953412081653\n",
            "Training loss per 100 training steps: 0.001832311710317299\n",
            "Training loss per 100 training steps: 0.00166841995728646\n",
            "Training loss per 100 training steps: 0.0016730092480663318\n",
            "Training loss per 100 training steps: 0.0016903814445754506\n",
            "Training loss per 100 training steps: 0.001693426694313934\n",
            "Training loss per 100 training steps: 0.001700367959418145\n",
            "Training loss per 100 training steps: 0.0017236048256199865\n",
            "Training loss per 100 training steps: 0.0017154151913018722\n",
            "Training loss per 100 training steps: 0.0016806780517771795\n",
            "Training loss per 100 training steps: 0.0016570912477683513\n",
            "Training loss per 100 training steps: 0.0016467346667862633\n",
            "Training loss per 100 training steps: 0.0016663127647279907\n",
            "Training loss per 100 training steps: 0.0016843044196962985\n",
            "Training loss per 100 training steps: 0.0016792223217806975\n",
            "Training loss per 100 training steps: 0.0017024376386220146\n",
            "Training loss per 100 training steps: 0.001721740067839164\n",
            "Training loss per 100 training steps: 0.0017323531010056503\n",
            "Training loss per 100 training steps: 0.001714631731846649\n",
            "Training loss per 100 training steps: 0.0017259632086951688\n",
            "Training loss per 100 training steps: 0.0017200329960845313\n",
            "Training loss per 100 training steps: 0.0017193583725136706\n",
            "Training loss per 100 training steps: 0.001738453891557583\n",
            "Training loss per 100 training steps: 0.0017342242940085945\n",
            "Training loss per 100 training steps: 0.001725978649517726\n",
            "Training loss per 100 training steps: 0.001734894953838977\n",
            "Training loss per 100 training steps: 0.0017397305893876668\n",
            "Training loss per 100 training steps: 0.001755945736987351\n",
            "Training loss per 100 training steps: 0.0017496700295203027\n",
            "Training loss per 100 training steps: 0.001754741026940213\n",
            "Training loss per 100 training steps: 0.0017442463093311745\n",
            "Training loss per 100 training steps: 0.0017510152193922943\n",
            "Training loss per 100 training steps: 0.0017492492706724584\n",
            "Training loss per 100 training steps: 0.001743703067306981\n",
            "Training loss epoch: 0.001750233281184276\n",
            "Training accuracy epoch: 0.99067426571187\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.001744268035854475\n",
            "Validation loss per 100 evaluation steps: 0.0021772609848630963\n",
            "Validation loss per 100 evaluation steps: 0.002102845410330095\n",
            "Validation loss per 100 evaluation steps: 0.0020364105770795506\n",
            "Validation loss per 100 evaluation steps: 0.0021396212610125076\n",
            "Validation loss per 100 evaluation steps: 0.0021447857443793812\n",
            "Validation loss per 100 evaluation steps: 0.0022208411113693468\n",
            "Validation loss per 100 evaluation steps: 0.00231180161918644\n",
            "Validation Loss: 0.0022375098688436025\n",
            "Validation Accuracy: 0.9885773696201138\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.98      0.97      1837\n",
            "      B-MISC       0.91      0.94      0.93       922\n",
            "       B-ORG       0.95      0.96      0.95      1341\n",
            "       B-PER       0.98      0.98      0.98      1842\n",
            "       I-LOC       0.98      0.97      0.97      1801\n",
            "      I-MISC       0.86      0.90      0.88       935\n",
            "       I-ORG       0.95      0.94      0.95      2319\n",
            "       I-PER       0.99      0.98      0.98      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.95      0.96      0.96     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9891  0.9891  0.9891\n",
            "macro        0.9542  0.9602  0.9571\n",
            "weighted     0.9892  0.9891  0.9892\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.004452214480279508\n",
            "Test loss per 100 evaluation steps: 0.004797653109590101\n",
            "Test loss per 100 evaluation steps: 0.004833514718549547\n",
            "Test loss per 100 evaluation steps: 0.004896462038444725\n",
            "Test loss per 100 evaluation steps: 0.004717362840604437\n",
            "Test loss per 100 evaluation steps: 0.004715212214377213\n",
            "Test loss per 100 evaluation steps: 0.004670971794179033\n",
            "Test loss per 100 evaluation steps: 0.004698218350119418\n",
            "Test loss per 100 evaluation steps: 0.004853022819079848\n",
            "Test Loss: 0.00487509581654934\n",
            "Test Accuracy: 0.9751579087750589\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.95      0.93      0.94      1668\n",
            "      B-MISC       0.83      0.86      0.85       702\n",
            "       B-ORG       0.89      0.94      0.92      1661\n",
            "       B-PER       0.97      0.96      0.97      1617\n",
            "       I-LOC       0.92      0.89      0.91      1394\n",
            "      I-MISC       0.67      0.71      0.69       736\n",
            "       I-ORG       0.89      0.95      0.92      2804\n",
            "       I-PER       0.96      0.98      0.97      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.98     61486\n",
            "   macro avg       0.90      0.91      0.91     61486\n",
            "weighted avg       0.98      0.98      0.98     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9761  0.9761  0.9761\n",
            "macro        0.8973  0.9134  0.9050\n",
            "weighted     0.9769  0.9761  0.9764\n",
            "Test steps: 921\n",
            "Training epoch: 4\n",
            "Training loss per 100 training steps: 0.0013902397046695115\n",
            "Training loss per 100 training steps: 0.0014152272377623376\n",
            "Training loss per 100 training steps: 0.0015076180348387424\n",
            "Training loss per 100 training steps: 0.0014036783176402422\n",
            "Training loss per 100 training steps: 0.0014167925642705086\n",
            "Training loss per 100 training steps: 0.001369842089266058\n",
            "Training loss per 100 training steps: 0.0014609467072101065\n",
            "Training loss per 100 training steps: 0.0014177560340647233\n",
            "Training loss per 100 training steps: 0.0014483405962214925\n",
            "Training loss per 100 training steps: 0.0014402813868664452\n",
            "Training loss per 100 training steps: 0.0014349183210478342\n",
            "Training loss per 100 training steps: 0.0013851268399101475\n",
            "Training loss per 100 training steps: 0.0013639209833162078\n",
            "Training loss per 100 training steps: 0.001341262753751055\n",
            "Training loss per 100 training steps: 0.0013529304274112898\n",
            "Training loss per 100 training steps: 0.0013572987796305824\n",
            "Training loss per 100 training steps: 0.001372612524072097\n",
            "Training loss per 100 training steps: 0.0013482336141522765\n",
            "Training loss per 100 training steps: 0.0013411322571492317\n",
            "Training loss per 100 training steps: 0.0013316685168138064\n",
            "Training loss per 100 training steps: 0.001334898339569225\n",
            "Training loss per 100 training steps: 0.0013382033731663384\n",
            "Training loss per 100 training steps: 0.0013198072985573083\n",
            "Training loss per 100 training steps: 0.0013187478170652866\n",
            "Training loss per 100 training steps: 0.001335904342046706\n",
            "Training loss per 100 training steps: 0.0013530960070910134\n",
            "Training loss per 100 training steps: 0.0013543197971537463\n",
            "Training loss per 100 training steps: 0.0013203889497896983\n",
            "Training loss per 100 training steps: 0.0013193065788947996\n",
            "Training loss per 100 training steps: 0.0013399139799776093\n",
            "Training loss per 100 training steps: 0.0013417381602439783\n",
            "Training loss per 100 training steps: 0.0013466258196771718\n",
            "Training loss per 100 training steps: 0.0013368590640493493\n",
            "Training loss per 100 training steps: 0.0013398444950598787\n",
            "Training loss per 100 training steps: 0.0013463404369283255\n",
            "Training loss per 100 training steps: 0.001362474366097154\n",
            "Training loss per 100 training steps: 0.0013560024997902498\n",
            "Training loss epoch: 0.0013537458477607285\n",
            "Training accuracy epoch: 0.9929514740536672\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.0020656953248908393\n",
            "Validation loss per 100 evaluation steps: 0.0022872822146609907\n",
            "Validation loss per 100 evaluation steps: 0.0022406152266088004\n",
            "Validation loss per 100 evaluation steps: 0.0022786322095805643\n",
            "Validation loss per 100 evaluation steps: 0.0022666369663811563\n",
            "Validation loss per 100 evaluation steps: 0.0022597473007203916\n",
            "Validation loss per 100 evaluation steps: 0.002199420952401202\n",
            "Validation loss per 100 evaluation steps: 0.0021840370464519766\n",
            "Validation Loss: 0.0021246525869219796\n",
            "Validation Accuracy: 0.9891593335802891\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.96      0.97      0.97      1837\n",
            "      B-MISC       0.95      0.93      0.94       922\n",
            "       B-ORG       0.94      0.96      0.95      1341\n",
            "       B-PER       0.98      0.98      0.98      1842\n",
            "       I-LOC       0.94      0.96      0.95      1801\n",
            "      I-MISC       0.94      0.86      0.90       935\n",
            "       I-ORG       0.95      0.95      0.95      2319\n",
            "       I-PER       0.99      0.99      0.99      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.96      0.96      0.96     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9892  0.9892  0.9892\n",
            "macro        0.9602  0.9560  0.9579\n",
            "weighted     0.9892  0.9892  0.9892\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.005118992961106414\n",
            "Test loss per 100 evaluation steps: 0.005479610516074445\n",
            "Test loss per 100 evaluation steps: 0.005712172316172352\n",
            "Test loss per 100 evaluation steps: 0.005769035988508904\n",
            "Test loss per 100 evaluation steps: 0.0057372285041956276\n",
            "Test loss per 100 evaluation steps: 0.005593961136224834\n",
            "Test loss per 100 evaluation steps: 0.005509616602291122\n",
            "Test loss per 100 evaluation steps: 0.005576356667797597\n",
            "Test loss per 100 evaluation steps: 0.005500655002367694\n",
            "Test Loss: 0.005469937838700317\n",
            "Test Accuracy: 0.9719812579966475\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.93      0.93      0.93      1668\n",
            "      B-MISC       0.83      0.83      0.83       702\n",
            "       B-ORG       0.90      0.93      0.91      1661\n",
            "       B-PER       0.97      0.96      0.97      1617\n",
            "       I-LOC       0.86      0.93      0.89      1394\n",
            "      I-MISC       0.67      0.63      0.65       736\n",
            "       I-ORG       0.91      0.94      0.92      2804\n",
            "       I-PER       0.96      0.98      0.97      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.89      0.90      0.90     61486\n",
            "weighted avg       0.98      0.97      0.98     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9750  0.9750  0.9750\n",
            "macro        0.8905  0.9033  0.8966\n",
            "weighted     0.9753  0.9750  0.9751\n",
            "Test steps: 921\n",
            "Training epoch: 5\n",
            "Training loss per 100 training steps: 0.0016155365704980794\n",
            "Training loss per 100 training steps: 0.0013661177775202305\n",
            "Training loss per 100 training steps: 0.0013082832205236628\n",
            "Training loss per 100 training steps: 0.0012100233107480562\n",
            "Training loss per 100 training steps: 0.0011320849820076547\n",
            "Training loss per 100 training steps: 0.001097560612251982\n",
            "Training loss per 100 training steps: 0.0010235865434619232\n",
            "Training loss per 100 training steps: 0.0010133685750906808\n",
            "Training loss per 100 training steps: 0.001021451500386724\n",
            "Training loss per 100 training steps: 0.0010137437577504899\n",
            "Training loss per 100 training steps: 0.0010296011177425416\n",
            "Training loss per 100 training steps: 0.0010201210689767019\n",
            "Training loss per 100 training steps: 0.0009979115803567906\n",
            "Training loss per 100 training steps: 0.0010741804715436436\n",
            "Training loss per 100 training steps: 0.001069085882535243\n",
            "Training loss per 100 training steps: 0.001058281854546408\n",
            "Training loss per 100 training steps: 0.001028008015743099\n",
            "Training loss per 100 training steps: 0.0010359323567753463\n",
            "Training loss per 100 training steps: 0.0010189250167389361\n",
            "Training loss per 100 training steps: 0.0010021550474452851\n",
            "Training loss per 100 training steps: 0.0010242722610265835\n",
            "Training loss per 100 training steps: 0.0010272979570411487\n",
            "Training loss per 100 training steps: 0.0010222211594274363\n",
            "Training loss per 100 training steps: 0.001008443494893451\n",
            "Training loss per 100 training steps: 0.0010230827308900189\n",
            "Training loss per 100 training steps: 0.0010144514690413277\n",
            "Training loss per 100 training steps: 0.0010083533841093919\n",
            "Training loss per 100 training steps: 0.0010179217359817423\n",
            "Training loss per 100 training steps: 0.0010035880757845131\n",
            "Training loss per 100 training steps: 0.0009992356617240148\n",
            "Training loss per 100 training steps: 0.000980826076166493\n",
            "Training loss per 100 training steps: 0.0009680640839584954\n",
            "Training loss per 100 training steps: 0.0009823639228408974\n",
            "Training loss per 100 training steps: 0.0009746806442400584\n",
            "Training loss per 100 training steps: 0.0009660548465082164\n",
            "Training loss per 100 training steps: 0.0009774073039904528\n",
            "Training loss per 100 training steps: 0.0009828716564716407\n",
            "Training loss epoch: 0.0009908465563338038\n",
            "Training accuracy epoch: 0.9949122901439794\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.0027415381035189056\n",
            "Validation loss per 100 evaluation steps: 0.0028607008328708616\n",
            "Validation loss per 100 evaluation steps: 0.0027316260880888878\n",
            "Validation loss per 100 evaluation steps: 0.0027196407464316506\n",
            "Validation loss per 100 evaluation steps: 0.00267965290565553\n",
            "Validation loss per 100 evaluation steps: 0.002537668071090593\n",
            "Validation loss per 100 evaluation steps: 0.002564389347033804\n",
            "Validation loss per 100 evaluation steps: 0.0025046762275314906\n",
            "Validation Loss: 0.0025730164422343494\n",
            "Validation Accuracy: 0.9871883123960473\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.97      0.97      1837\n",
            "      B-MISC       0.89      0.95      0.92       922\n",
            "       B-ORG       0.95      0.93      0.94      1341\n",
            "       B-PER       0.98      0.98      0.98      1842\n",
            "       I-LOC       0.97      0.95      0.96      1801\n",
            "      I-MISC       0.83      0.92      0.87       935\n",
            "       I-ORG       0.94      0.92      0.93      2319\n",
            "       I-PER       0.99      0.98      0.99      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.95      0.96      0.95     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9876  0.9876  0.9876\n",
            "macro        0.9472  0.9574  0.9519\n",
            "weighted     0.9879  0.9876  0.9877\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.006223231148196646\n",
            "Test loss per 100 evaluation steps: 0.005709436278557405\n",
            "Test loss per 100 evaluation steps: 0.0051155460756065925\n",
            "Test loss per 100 evaluation steps: 0.005285245593290711\n",
            "Test loss per 100 evaluation steps: 0.005250202667710255\n",
            "Test loss per 100 evaluation steps: 0.004990648090967322\n",
            "Test loss per 100 evaluation steps: 0.005025829693622654\n",
            "Test loss per 100 evaluation steps: 0.005179197724107781\n",
            "Test loss per 100 evaluation steps: 0.005247362931277167\n",
            "Test Loss: 0.005246530368873628\n",
            "Test Accuracy: 0.9742545961643633\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.94      0.94      0.94      1668\n",
            "      B-MISC       0.78      0.90      0.83       702\n",
            "       B-ORG       0.91      0.91      0.91      1661\n",
            "       B-PER       0.97      0.96      0.97      1617\n",
            "       I-LOC       0.93      0.90      0.91      1394\n",
            "      I-MISC       0.60      0.82      0.69       736\n",
            "       I-ORG       0.91      0.94      0.92      2804\n",
            "       I-PER       0.98      0.98      0.98      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.98     61486\n",
            "   macro avg       0.89      0.93      0.91     61486\n",
            "weighted avg       0.98      0.98      0.98     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9763  0.9763  0.9763\n",
            "macro        0.8916  0.9265  0.9067\n",
            "weighted     0.9784  0.9763  0.9771\n",
            "Test steps: 921\n",
            "Training epoch: 6\n",
            "Training loss per 100 training steps: 0.0011384157631073321\n",
            "Training loss per 100 training steps: 0.000942908628699115\n",
            "Training loss per 100 training steps: 0.0009076801922644033\n",
            "Training loss per 100 training steps: 0.0009554938824135206\n",
            "Training loss per 100 training steps: 0.0010178724851248261\n",
            "Training loss per 100 training steps: 0.0009789218689972283\n",
            "Training loss per 100 training steps: 0.0009527032907005507\n",
            "Training loss per 100 training steps: 0.0009366854084328224\n",
            "Training loss per 100 training steps: 0.0009125967628683106\n",
            "Training loss per 100 training steps: 0.0009135026862209088\n",
            "Training loss per 100 training steps: 0.0009051775109587239\n",
            "Training loss per 100 training steps: 0.0008871861428644933\n",
            "Training loss per 100 training steps: 0.0008864253296853628\n",
            "Training loss per 100 training steps: 0.0008926356197145781\n",
            "Training loss per 100 training steps: 0.0009077635117716151\n",
            "Training loss per 100 training steps: 0.0008828693047445313\n",
            "Training loss per 100 training steps: 0.0009120034150285338\n",
            "Training loss per 100 training steps: 0.0008829731718889081\n",
            "Training loss per 100 training steps: 0.0008777240871485877\n",
            "Training loss per 100 training steps: 0.0008745145307925668\n",
            "Training loss per 100 training steps: 0.0008862248186869693\n",
            "Training loss per 100 training steps: 0.0009109348142456755\n",
            "Training loss per 100 training steps: 0.0009031514668909633\n",
            "Training loss per 100 training steps: 0.0009230045986100777\n",
            "Training loss per 100 training steps: 0.0009013068506495983\n",
            "Training loss per 100 training steps: 0.0009078979916573469\n",
            "Training loss per 100 training steps: 0.0009053606809705646\n",
            "Training loss per 100 training steps: 0.0009022670476401475\n",
            "Training loss per 100 training steps: 0.000896653316344782\n",
            "Training loss per 100 training steps: 0.0008877147717989829\n",
            "Training loss per 100 training steps: 0.0009007432256277987\n",
            "Training loss per 100 training steps: 0.0008927615293679025\n",
            "Training loss per 100 training steps: 0.0008959132969444413\n",
            "Training loss per 100 training steps: 0.0008862286575518899\n",
            "Training loss per 100 training steps: 0.0008865758786795205\n",
            "Training loss per 100 training steps: 0.0008867351871985344\n",
            "Training loss per 100 training steps: 0.0008772735831529939\n",
            "Training loss epoch: 0.0008732958828335524\n",
            "Training accuracy epoch: 0.9955939989653216\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.0033030206387047656\n",
            "Validation loss per 100 evaluation steps: 0.0026727117851714863\n",
            "Validation loss per 100 evaluation steps: 0.0027700361631589963\n",
            "Validation loss per 100 evaluation steps: 0.002761848355557959\n",
            "Validation loss per 100 evaluation steps: 0.0025551719677459916\n",
            "Validation loss per 100 evaluation steps: 0.002485247414518502\n",
            "Validation loss per 100 evaluation steps: 0.002344807203018198\n",
            "Validation loss per 100 evaluation steps: 0.0023194987027181924\n",
            "Validation Loss: 0.0023144846059611705\n",
            "Validation Accuracy: 0.9883885841392235\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.98      0.97      1837\n",
            "      B-MISC       0.92      0.95      0.93       922\n",
            "       B-ORG       0.96      0.94      0.95      1341\n",
            "       B-PER       0.97      0.99      0.98      1842\n",
            "       I-LOC       0.97      0.97      0.97      1801\n",
            "      I-MISC       0.85      0.94      0.89       935\n",
            "       I-ORG       0.96      0.94      0.95      2319\n",
            "       I-PER       0.98      0.99      0.98      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.95      0.97      0.96     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9893  0.9893  0.9893\n",
            "macro        0.9533  0.9658  0.9592\n",
            "weighted     0.9895  0.9893  0.9893\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.0063755310392480165\n",
            "Test loss per 100 evaluation steps: 0.005734202654875844\n",
            "Test loss per 100 evaluation steps: 0.00543117960465679\n",
            "Test loss per 100 evaluation steps: 0.005492789527970672\n",
            "Test loss per 100 evaluation steps: 0.005332429282716476\n",
            "Test loss per 100 evaluation steps: 0.005264530102634429\n",
            "Test loss per 100 evaluation steps: 0.00537138951925434\n",
            "Test loss per 100 evaluation steps: 0.00529296318972456\n",
            "Test loss per 100 evaluation steps: 0.005312441931024902\n",
            "Test Loss: 0.00531091671369519\n",
            "Test Accuracy: 0.9735019823275608\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.93      0.95      0.94      1668\n",
            "      B-MISC       0.80      0.86      0.83       702\n",
            "       B-ORG       0.92      0.92      0.92      1661\n",
            "       B-PER       0.96      0.97      0.97      1617\n",
            "       I-LOC       0.89      0.93      0.91      1394\n",
            "      I-MISC       0.60      0.75      0.67       736\n",
            "       I-ORG       0.91      0.94      0.92      2804\n",
            "       I-PER       0.96      0.99      0.98      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.98     61486\n",
            "   macro avg       0.89      0.92      0.90     61486\n",
            "weighted avg       0.98      0.98      0.98     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9750  0.9750  0.9750\n",
            "macro        0.8857  0.9204  0.9019\n",
            "weighted     0.9767  0.9750  0.9757\n",
            "Test steps: 921\n",
            "Training epoch: 7\n",
            "Training loss per 100 training steps: 0.0003063218088936992\n",
            "Training loss per 100 training steps: 0.0006039819061561502\n",
            "Training loss per 100 training steps: 0.000680661268491652\n",
            "Training loss per 100 training steps: 0.0007271901541946591\n",
            "Training loss per 100 training steps: 0.0007744972176451483\n",
            "Training loss per 100 training steps: 0.0008199662693239892\n",
            "Training loss per 100 training steps: 0.0007741239723678258\n",
            "Training loss per 100 training steps: 0.0007638363565985174\n",
            "Training loss per 100 training steps: 0.0007851599454392676\n",
            "Training loss per 100 training steps: 0.0007958817420594641\n",
            "Training loss per 100 training steps: 0.0007846966925568598\n",
            "Training loss per 100 training steps: 0.000774365193790345\n",
            "Training loss per 100 training steps: 0.0007574174695897595\n",
            "Training loss per 100 training steps: 0.000761135178423891\n",
            "Training loss per 100 training steps: 0.0007509543746552178\n",
            "Training loss per 100 training steps: 0.0007205951227274454\n",
            "Training loss per 100 training steps: 0.0007225059248974182\n",
            "Training loss per 100 training steps: 0.0007239244901529673\n",
            "Training loss per 100 training steps: 0.0007099068415051366\n",
            "Training loss per 100 training steps: 0.0007005554542035953\n",
            "Training loss per 100 training steps: 0.0006908922490161526\n",
            "Training loss per 100 training steps: 0.0006852408345239589\n",
            "Training loss per 100 training steps: 0.0007041581274849863\n",
            "Training loss per 100 training steps: 0.0007039967162856442\n",
            "Training loss per 100 training steps: 0.0007084195992404603\n",
            "Training loss per 100 training steps: 0.0007198852316191908\n",
            "Training loss per 100 training steps: 0.0007239634414159574\n",
            "Training loss per 100 training steps: 0.000727656514719927\n",
            "Training loss per 100 training steps: 0.0007468335306267865\n",
            "Training loss per 100 training steps: 0.0007495196233232946\n",
            "Training loss per 100 training steps: 0.0007403750026627119\n",
            "Training loss per 100 training steps: 0.0007397221657974029\n",
            "Training loss per 100 training steps: 0.0007317332614206861\n",
            "Training loss per 100 training steps: 0.0007255552574706733\n",
            "Training loss per 100 training steps: 0.0007248488204340252\n",
            "Training loss per 100 training steps: 0.000723259580825331\n",
            "Training loss per 100 training steps: 0.0007417981317276777\n",
            "Training loss epoch: 0.0007427420719503409\n",
            "Training accuracy epoch: 0.9963279810470024\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.0021293988423622067\n",
            "Validation loss per 100 evaluation steps: 0.0021609143640966976\n",
            "Validation loss per 100 evaluation steps: 0.0023922256842267113\n",
            "Validation loss per 100 evaluation steps: 0.0025837756188707315\n",
            "Validation loss per 100 evaluation steps: 0.0026456559623329666\n",
            "Validation loss per 100 evaluation steps: 0.002602348256193636\n",
            "Validation loss per 100 evaluation steps: 0.0025169930904433255\n",
            "Validation loss per 100 evaluation steps: 0.002520199224245516\n",
            "Validation Loss: 0.002440851265898703\n",
            "Validation Accuracy: 0.9875445813677004\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.98      0.97      1837\n",
            "      B-MISC       0.93      0.93      0.93       922\n",
            "       B-ORG       0.92      0.96      0.94      1341\n",
            "       B-PER       0.99      0.97      0.98      1842\n",
            "       I-LOC       0.97      0.98      0.97      1801\n",
            "      I-MISC       0.86      0.90      0.88       935\n",
            "       I-ORG       0.92      0.95      0.93      2319\n",
            "       I-PER       0.99      0.97      0.98      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.95      0.96      0.95     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9880  0.9880  0.9880\n",
            "macro        0.9489  0.9588  0.9537\n",
            "weighted     0.9882  0.9880  0.9881\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.00622353300895611\n",
            "Test loss per 100 evaluation steps: 0.006230877928564951\n",
            "Test loss per 100 evaluation steps: 0.005907969591197192\n",
            "Test loss per 100 evaluation steps: 0.005697298984039208\n",
            "Test loss per 100 evaluation steps: 0.005680175302213684\n",
            "Test loss per 100 evaluation steps: 0.006008301902980595\n",
            "Test loss per 100 evaluation steps: 0.005776011185053644\n",
            "Test loss per 100 evaluation steps: 0.005767036930458289\n",
            "Test loss per 100 evaluation steps: 0.00586381642059324\n",
            "Test Loss: 0.00586892641766703\n",
            "Test Accuracy: 0.9717143408263779\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.94      0.94      0.94      1668\n",
            "      B-MISC       0.83      0.85      0.84       702\n",
            "       B-ORG       0.88      0.94      0.91      1661\n",
            "       B-PER       0.97      0.94      0.96      1617\n",
            "       I-LOC       0.88      0.94      0.91      1394\n",
            "      I-MISC       0.66      0.73      0.69       736\n",
            "       I-ORG       0.86      0.96      0.91      2804\n",
            "       I-PER       0.97      0.95      0.96      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.89      0.91      0.90     61486\n",
            "weighted avg       0.98      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9740   0.974  0.9740\n",
            "macro        0.8876   0.914  0.9001\n",
            "weighted     0.9755   0.974  0.9746\n",
            "Test steps: 921\n",
            "====================================================================================================\n",
            "loss_name: ce\n",
            "model configuration\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Ner_Model(\n",
            "  (model): DebertaModel(\n",
            "    (embeddings): DebertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "      (LayerNorm): DebertaLayerNorm()\n",
            "      (dropout): StableDropout()\n",
            "    )\n",
            "    (encoder): DebertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x DebertaLayer(\n",
            "          (attention): DebertaAttention(\n",
            "            (self): DisentangledSelfAttention(\n",
            "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
            "              (pos_dropout): StableDropout()\n",
            "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "            (output): DebertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): DebertaLayerNorm()\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "          )\n",
            "          (intermediate): DebertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): DebertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): DebertaLayerNorm()\n",
            "            (dropout): StableDropout()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (rel_embeddings): Embedding(1024, 768)\n",
            "    )\n",
            "  )\n",
            "  (linear_model): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=384, bias=True)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (classifier): Linear(in_features=384, out_features=9, bias=True)\n",
            ")\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 1.355984319448471\n",
            "Training loss per 100 training steps: 0.9811865419894457\n",
            "Training loss per 100 training steps: 0.8039958883076906\n",
            "Training loss per 100 training steps: 0.6906253723986446\n",
            "Training loss per 100 training steps: 0.6069741239249706\n",
            "Training loss per 100 training steps: 0.5448697881028056\n",
            "Training loss per 100 training steps: 0.4920452127765332\n",
            "Training loss per 100 training steps: 0.4505947787279729\n",
            "Training loss per 100 training steps: 0.4199929560907185\n",
            "Training loss per 100 training steps: 0.39370985858514906\n",
            "Training loss per 100 training steps: 0.37424438340187244\n",
            "Training loss per 100 training steps: 0.35283005821615615\n",
            "Training loss per 100 training steps: 0.33435695310105357\n",
            "Training loss per 100 training steps: 0.317761128580231\n",
            "Training loss per 100 training steps: 0.30438678778878725\n",
            "Training loss per 100 training steps: 0.29054408306623375\n",
            "Training loss per 100 training steps: 0.2792609577157932\n",
            "Training loss per 100 training steps: 0.26811084233796767\n",
            "Training loss per 100 training steps: 0.25824141454408067\n",
            "Training loss per 100 training steps: 0.2500654850388237\n",
            "Training loss per 100 training steps: 0.24317921959454128\n",
            "Training loss per 100 training steps: 0.23718880936558973\n",
            "Training loss per 100 training steps: 0.23135171567385956\n",
            "Training loss per 100 training steps: 0.22578302976212095\n",
            "Training loss per 100 training steps: 0.22062437145200092\n",
            "Training loss per 100 training steps: 0.21544215623866947\n",
            "Training loss per 100 training steps: 0.21115013761044893\n",
            "Training loss per 100 training steps: 0.2066435376960934\n",
            "Training loss per 100 training steps: 0.20162242523430654\n",
            "Training loss per 100 training steps: 0.19724547947758886\n",
            "Training loss per 100 training steps: 0.19265141423414237\n",
            "Training loss per 100 training steps: 0.18914258390539543\n",
            "Training loss per 100 training steps: 0.18613614525510153\n",
            "Training loss per 100 training steps: 0.18255784745822892\n",
            "Training loss per 100 training steps: 0.17957896635613205\n",
            "Training loss per 100 training steps: 0.17632327075483367\n",
            "Training loss per 100 training steps: 0.17367142225656582\n",
            "Training loss epoch: 0.1723465174085801\n",
            "Training accuracy epoch: 0.9528648152466519\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.04597607558534946\n",
            "Validation loss per 100 evaluation steps: 0.05361872265319107\n",
            "Validation loss per 100 evaluation steps: 0.056113438200748836\n",
            "Validation loss per 100 evaluation steps: 0.05578753273395705\n",
            "Validation loss per 100 evaluation steps: 0.05520896530384198\n",
            "Validation loss per 100 evaluation steps: 0.053858863434312905\n",
            "Validation loss per 100 evaluation steps: 0.052451356244127965\n",
            "Validation loss per 100 evaluation steps: 0.05113258261344526\n",
            "Validation Loss: 0.05435559294942024\n",
            "Validation Accuracy: 0.9850593830097325\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.96      0.97      1837\n",
            "      B-MISC       0.90      0.91      0.90       922\n",
            "       B-ORG       0.96      0.91      0.94      1341\n",
            "       B-PER       0.96      0.99      0.97      1842\n",
            "       I-LOC       0.95      0.95      0.95      1801\n",
            "      I-MISC       0.83      0.80      0.82       935\n",
            "       I-ORG       0.97      0.89      0.93      2319\n",
            "       I-PER       0.97      0.99      0.98      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.94      0.93      0.94     66257\n",
            "weighted avg       0.98      0.99      0.98     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9850  0.9850  0.9850\n",
            "macro        0.9438  0.9348  0.9390\n",
            "weighted     0.9849  0.9850  0.9849\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.0924384804617148\n",
            "Test loss per 100 evaluation steps: 0.09230682885987335\n",
            "Test loss per 100 evaluation steps: 0.11220183087483747\n",
            "Test loss per 100 evaluation steps: 0.112650633115627\n",
            "Test loss per 100 evaluation steps: 0.10579128615377703\n",
            "Test loss per 100 evaluation steps: 0.10101698544545798\n",
            "Test loss per 100 evaluation steps: 0.10425068845036939\n",
            "Test loss per 100 evaluation steps: 0.10392914511245181\n",
            "Test loss per 100 evaluation steps: 0.10093748790195807\n",
            "Test Loss: 0.10111559442598772\n",
            "Test Accuracy: 0.9736866936019997\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.93      0.94      0.93      1668\n",
            "      B-MISC       0.81      0.85      0.83       702\n",
            "       B-ORG       0.93      0.89      0.91      1661\n",
            "       B-PER       0.95      0.98      0.96      1617\n",
            "       I-LOC       0.88      0.93      0.90      1394\n",
            "      I-MISC       0.66      0.72      0.69       736\n",
            "       I-ORG       0.93      0.90      0.92      2804\n",
            "       I-PER       0.95      0.99      0.97      3810\n",
            "           O       0.99      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.98     61486\n",
            "   macro avg       0.89      0.91      0.90     61486\n",
            "weighted avg       0.98      0.98      0.98     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9754  0.9754  0.9754\n",
            "macro        0.8941  0.9092  0.9013\n",
            "weighted     0.9760  0.9754  0.9756\n",
            "Test steps: 921\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.09270859374548308\n",
            "Training loss per 100 training steps: 0.061327628339931835\n",
            "Training loss per 100 training steps: 0.05761410416230016\n",
            "Training loss per 100 training steps: 0.054072163383025326\n",
            "Training loss per 100 training steps: 0.050944516470714005\n",
            "Training loss per 100 training steps: 0.05291070501955498\n",
            "Training loss per 100 training steps: 0.05272743592606275\n",
            "Training loss per 100 training steps: 0.05564256420593665\n",
            "Training loss per 100 training steps: 0.05453064688243709\n",
            "Training loss per 100 training steps: 0.05471741821318574\n",
            "Training loss per 100 training steps: 0.05764337375658215\n",
            "Training loss per 100 training steps: 0.05688629996976185\n",
            "Training loss per 100 training steps: 0.057110849566692753\n",
            "Training loss per 100 training steps: 0.05570938661113489\n",
            "Training loss per 100 training steps: 0.05651122620881263\n",
            "Training loss per 100 training steps: 0.05652683690162121\n",
            "Training loss per 100 training steps: 0.05589353615552468\n",
            "Training loss per 100 training steps: 0.05425194015171049\n",
            "Training loss per 100 training steps: 0.05412522072392963\n",
            "Training loss per 100 training steps: 0.053744451413404025\n",
            "Training loss per 100 training steps: 0.05353920989238153\n",
            "Training loss per 100 training steps: 0.052654705699857335\n",
            "Training loss per 100 training steps: 0.05202296581960147\n",
            "Training loss per 100 training steps: 0.05249631561046044\n",
            "Training loss per 100 training steps: 0.05209139875255642\n",
            "Training loss per 100 training steps: 0.05198337515277112\n",
            "Training loss per 100 training steps: 0.052341200463022146\n",
            "Training loss per 100 training steps: 0.05205363209380656\n",
            "Training loss per 100 training steps: 0.05176586294539196\n",
            "Training loss per 100 training steps: 0.051714436554005566\n",
            "Training loss per 100 training steps: 0.05152073123002699\n",
            "Training loss per 100 training steps: 0.051212645184537\n",
            "Training loss per 100 training steps: 0.05117195932703026\n",
            "Training loss per 100 training steps: 0.05112995713689409\n",
            "Training loss per 100 training steps: 0.050972819979073915\n",
            "Training loss per 100 training steps: 0.05041312634742806\n",
            "Training loss per 100 training steps: 0.050230207581504094\n",
            "Training loss epoch: 0.04986813548926678\n",
            "Training accuracy epoch: 0.9863630003848373\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.052691377996234225\n",
            "Validation loss per 100 evaluation steps: 0.04898094791627955\n",
            "Validation loss per 100 evaluation steps: 0.04706276554274761\n",
            "Validation loss per 100 evaluation steps: 0.04474563639079861\n",
            "Validation loss per 100 evaluation steps: 0.04774318159150425\n",
            "Validation loss per 100 evaluation steps: 0.04731622953942861\n",
            "Validation loss per 100 evaluation steps: 0.05019390202405962\n",
            "Validation loss per 100 evaluation steps: 0.05278875227795652\n",
            "Validation Loss: 0.05380585318789232\n",
            "Validation Accuracy: 0.9863952687329938\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.96      0.97      0.97      1837\n",
            "      B-MISC       0.90      0.94      0.92       922\n",
            "       B-ORG       0.95      0.94      0.94      1341\n",
            "       B-PER       0.98      0.98      0.98      1842\n",
            "       I-LOC       0.95      0.95      0.95      1801\n",
            "      I-MISC       0.82      0.91      0.86       935\n",
            "       I-ORG       0.93      0.95      0.94      2319\n",
            "       I-PER       0.98      0.99      0.99      4219\n",
            "           O       1.00      0.99      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.94      0.96      0.95     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9871  0.9871  0.9871\n",
            "macro        0.9416  0.9583  0.9496\n",
            "weighted     0.9874  0.9871  0.9872\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.16028515984100522\n",
            "Test loss per 100 evaluation steps: 0.12125610939066973\n",
            "Test loss per 100 evaluation steps: 0.12669507586668866\n",
            "Test loss per 100 evaluation steps: 0.1281743228126288\n",
            "Test loss per 100 evaluation steps: 0.12274649530800526\n",
            "Test loss per 100 evaluation steps: 0.11823397789291144\n",
            "Test loss per 100 evaluation steps: 0.12124615807098703\n",
            "Test loss per 100 evaluation steps: 0.11987911517710018\n",
            "Test loss per 100 evaluation steps: 0.1229108997964714\n",
            "Test Loss: 0.12626745526361088\n",
            "Test Accuracy: 0.9720317555243545\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.92      0.94      0.93      1668\n",
            "      B-MISC       0.75      0.88      0.81       702\n",
            "       B-ORG       0.92      0.90      0.91      1661\n",
            "       B-PER       0.97      0.97      0.97      1617\n",
            "       I-LOC       0.86      0.93      0.89      1394\n",
            "      I-MISC       0.58      0.76      0.66       736\n",
            "       I-ORG       0.91      0.94      0.92      2804\n",
            "       I-PER       0.97      0.98      0.98      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.87      0.92      0.90     61486\n",
            "weighted avg       0.98      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9739  0.9739  0.9739\n",
            "macro        0.8748  0.9199  0.8952\n",
            "weighted     0.9764  0.9739  0.9748\n",
            "Test steps: 921\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.04108718198083807\n",
            "Training loss per 100 training steps: 0.04323690488105058\n",
            "Training loss per 100 training steps: 0.04189572185801808\n",
            "Training loss per 100 training steps: 0.04251680165951257\n",
            "Training loss per 100 training steps: 0.038856504919647705\n",
            "Training loss per 100 training steps: 0.03843959316354206\n",
            "Training loss per 100 training steps: 0.038290836504103413\n",
            "Training loss per 100 training steps: 0.03579392879977604\n",
            "Training loss per 100 training steps: 0.03372829726284383\n",
            "Training loss per 100 training steps: 0.03496087549500226\n",
            "Training loss per 100 training steps: 0.0346873642514137\n",
            "Training loss per 100 training steps: 0.034558089507833456\n",
            "Training loss per 100 training steps: 0.03448766078776456\n",
            "Training loss per 100 training steps: 0.034436651549570214\n",
            "Training loss per 100 training steps: 0.03426825842068259\n",
            "Training loss per 100 training steps: 0.033721181751288895\n",
            "Training loss per 100 training steps: 0.033289695322767814\n",
            "Training loss per 100 training steps: 0.0329750259618312\n",
            "Training loss per 100 training steps: 0.03311521238516434\n",
            "Training loss per 100 training steps: 0.033555712508510624\n",
            "Training loss per 100 training steps: 0.03308246124924543\n",
            "Training loss per 100 training steps: 0.032917348466263235\n",
            "Training loss per 100 training steps: 0.033549253659451925\n",
            "Training loss per 100 training steps: 0.033217878059525294\n",
            "Training loss per 100 training steps: 0.03310701755722985\n",
            "Training loss per 100 training steps: 0.03325948942228794\n",
            "Training loss per 100 training steps: 0.03323732787440854\n",
            "Training loss per 100 training steps: 0.03376277462589185\n",
            "Training loss per 100 training steps: 0.033622768153571965\n",
            "Training loss per 100 training steps: 0.0331163502929121\n",
            "Training loss per 100 training steps: 0.03278416410665452\n",
            "Training loss per 100 training steps: 0.03258752475812344\n",
            "Training loss per 100 training steps: 0.0323516989137076\n",
            "Training loss per 100 training steps: 0.031911518139839994\n",
            "Training loss per 100 training steps: 0.03215118545088418\n",
            "Training loss per 100 training steps: 0.03224832682732085\n",
            "Training loss per 100 training steps: 0.03213102699415538\n",
            "Training loss epoch: 0.03228395404846151\n",
            "Training accuracy epoch: 0.9916705926237318\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.05048991214542184\n",
            "Validation loss per 100 evaluation steps: 0.056658619522640945\n",
            "Validation loss per 100 evaluation steps: 0.057968827561368626\n",
            "Validation loss per 100 evaluation steps: 0.0541623576861457\n",
            "Validation loss per 100 evaluation steps: 0.05762450379881193\n",
            "Validation loss per 100 evaluation steps: 0.057860145712669085\n",
            "Validation loss per 100 evaluation steps: 0.0595622167946671\n",
            "Validation loss per 100 evaluation steps: 0.05994584107215815\n",
            "Validation Loss: 0.05971756360796623\n",
            "Validation Accuracy: 0.98536716345273\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.97      0.97      1837\n",
            "      B-MISC       0.90      0.95      0.93       922\n",
            "       B-ORG       0.97      0.94      0.95      1341\n",
            "       B-PER       0.97      0.99      0.98      1842\n",
            "       I-LOC       0.97      0.96      0.96      1801\n",
            "      I-MISC       0.68      0.96      0.80       935\n",
            "       I-ORG       0.96      0.92      0.94      2319\n",
            "       I-PER       0.97      0.99      0.98      4219\n",
            "           O       1.00      0.99      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.93      0.96      0.95     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9853  0.9853  0.9853\n",
            "macro        0.9330  0.9630  0.9451\n",
            "weighted     0.9872  0.9853  0.9859\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.1354063290990598\n",
            "Test loss per 100 evaluation steps: 0.16692491804824386\n",
            "Test loss per 100 evaluation steps: 0.15657629802667847\n",
            "Test loss per 100 evaluation steps: 0.16230661708426852\n",
            "Test loss per 100 evaluation steps: 0.16750630404289404\n",
            "Test loss per 100 evaluation steps: 0.16156793834618535\n",
            "Test loss per 100 evaluation steps: 0.16292969021620854\n",
            "Test loss per 100 evaluation steps: 0.1559394703370981\n",
            "Test loss per 100 evaluation steps: 0.16155891154056007\n",
            "Test Loss: 0.16286562188183182\n",
            "Test Accuracy: 0.9677054783680314\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.93      0.93      0.93      1668\n",
            "      B-MISC       0.79      0.86      0.82       702\n",
            "       B-ORG       0.92      0.90      0.91      1661\n",
            "       B-PER       0.96      0.97      0.96      1617\n",
            "       I-LOC       0.87      0.91      0.89      1394\n",
            "      I-MISC       0.46      0.81      0.59       736\n",
            "       I-ORG       0.92      0.92      0.92      2804\n",
            "       I-PER       0.96      0.99      0.97      3810\n",
            "           O       1.00      0.98      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.87      0.92      0.89     61486\n",
            "weighted avg       0.98      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9706  0.9706  0.9706\n",
            "macro        0.8684  0.9193  0.8884\n",
            "weighted     0.9757  0.9706  0.9725\n",
            "Test steps: 921\n",
            "Training epoch: 4\n",
            "Training loss per 100 training steps: 0.0317364611197263\n",
            "Training loss per 100 training steps: 0.02334417995021795\n",
            "Training loss per 100 training steps: 0.023397489613998915\n",
            "Training loss per 100 training steps: 0.02844663631020012\n",
            "Training loss per 100 training steps: 0.027747663982998348\n",
            "Training loss per 100 training steps: 0.02601530253023763\n",
            "Training loss per 100 training steps: 0.024176776591214418\n",
            "Training loss per 100 training steps: 0.023656737036944833\n",
            "Training loss per 100 training steps: 0.02263460196363869\n",
            "Training loss per 100 training steps: 0.023337710151834473\n",
            "Training loss per 100 training steps: 0.022659677912670304\n",
            "Training loss per 100 training steps: 0.022632745057265615\n",
            "Training loss per 100 training steps: 0.02279754645239826\n",
            "Training loss per 100 training steps: 0.02255930634794952\n",
            "Training loss per 100 training steps: 0.022725725453788377\n",
            "Training loss per 100 training steps: 0.02367661667910852\n",
            "Training loss per 100 training steps: 0.023336396871784338\n",
            "Training loss per 100 training steps: 0.023771439921228092\n",
            "Training loss per 100 training steps: 0.02425850669134601\n",
            "Training loss per 100 training steps: 0.02398291465026341\n",
            "Training loss per 100 training steps: 0.023659516388052863\n",
            "Training loss per 100 training steps: 0.023871937697132868\n",
            "Training loss per 100 training steps: 0.02357136126588043\n",
            "Training loss per 100 training steps: 0.02286389126018548\n",
            "Training loss per 100 training steps: 0.022844759853874713\n",
            "Training loss per 100 training steps: 0.022491917758341234\n",
            "Training loss per 100 training steps: 0.022452061388349074\n",
            "Training loss per 100 training steps: 0.022422513068881926\n",
            "Training loss per 100 training steps: 0.023189577900966783\n",
            "Training loss per 100 training steps: 0.023104696487523446\n",
            "Training loss per 100 training steps: 0.022992264141982646\n",
            "Training loss per 100 training steps: 0.022964823491559514\n",
            "Training loss per 100 training steps: 0.022746526819866466\n",
            "Training loss per 100 training steps: 0.022309337060471935\n",
            "Training loss per 100 training steps: 0.022398116943235697\n",
            "Training loss per 100 training steps: 0.0221336386711083\n",
            "Training loss per 100 training steps: 0.02253344974058613\n",
            "Training loss epoch: 0.022635310931690317\n",
            "Training accuracy epoch: 0.9942243160167525\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.05298087484043208\n",
            "Validation loss per 100 evaluation steps: 0.05123872873693472\n",
            "Validation loss per 100 evaluation steps: 0.04612177378076012\n",
            "Validation loss per 100 evaluation steps: 0.04686839437095841\n",
            "Validation loss per 100 evaluation steps: 0.04957384775931132\n",
            "Validation loss per 100 evaluation steps: 0.05255242112513467\n",
            "Validation loss per 100 evaluation steps: 0.05266559178721634\n",
            "Validation loss per 100 evaluation steps: 0.05290084540786665\n",
            "Validation Loss: 0.052914880120661444\n",
            "Validation Accuracy: 0.9864183590619959\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.97      0.97      1837\n",
            "      B-MISC       0.95      0.91      0.93       922\n",
            "       B-ORG       0.90      0.96      0.93      1341\n",
            "       B-PER       0.98      0.98      0.98      1842\n",
            "       I-LOC       0.96      0.94      0.95      1801\n",
            "      I-MISC       0.88      0.86      0.87       935\n",
            "       I-ORG       0.88      0.96      0.92      2319\n",
            "       I-PER       0.99      0.99      0.99      4219\n",
            "           O       1.00      0.99      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.94      0.95      0.95     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9866  0.9866  0.9866\n",
            "macro        0.9449  0.9528  0.9485\n",
            "weighted     0.9870  0.9866  0.9867\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.11871265651556313\n",
            "Test loss per 100 evaluation steps: 0.14749171602441494\n",
            "Test loss per 100 evaluation steps: 0.15806008380922626\n",
            "Test loss per 100 evaluation steps: 0.1565326035857288\n",
            "Test loss per 100 evaluation steps: 0.14885957420515478\n",
            "Test loss per 100 evaluation steps: 0.14469480318497518\n",
            "Test loss per 100 evaluation steps: 0.14127911043018684\n",
            "Test loss per 100 evaluation steps: 0.14533584349501324\n",
            "Test loss per 100 evaluation steps: 0.1439580274984231\n",
            "Test Loss: 0.14370567934650894\n",
            "Test Accuracy: 0.970345294297264\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.93      0.94      0.94      1668\n",
            "      B-MISC       0.85      0.83      0.84       702\n",
            "       B-ORG       0.87      0.94      0.91      1661\n",
            "       B-PER       0.97      0.96      0.96      1617\n",
            "       I-LOC       0.89      0.93      0.91      1394\n",
            "      I-MISC       0.64      0.70      0.67       736\n",
            "       I-ORG       0.85      0.95      0.90      2804\n",
            "       I-PER       0.97      0.98      0.97      3810\n",
            "           O       1.00      0.98      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.88      0.91      0.90     61486\n",
            "weighted avg       0.97      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9731  0.9731  0.9731\n",
            "macro        0.8850  0.9149  0.8992\n",
            "weighted     0.9748  0.9731  0.9737\n",
            "Test steps: 921\n",
            "Training epoch: 5\n",
            "Training loss per 100 training steps: 0.017431586434104248\n",
            "Training loss per 100 training steps: 0.016493341305613286\n",
            "Training loss per 100 training steps: 0.016861837231505583\n",
            "Training loss per 100 training steps: 0.017514128443472145\n",
            "Training loss per 100 training steps: 0.01698717801325256\n",
            "Training loss per 100 training steps: 0.01913495937958942\n",
            "Training loss per 100 training steps: 0.017591403410593297\n",
            "Training loss per 100 training steps: 0.018546423439474893\n",
            "Training loss per 100 training steps: 0.018386323815777563\n",
            "Training loss per 100 training steps: 0.01770911241561771\n",
            "Training loss per 100 training steps: 0.017330158515062727\n",
            "Training loss per 100 training steps: 0.016761557855616047\n",
            "Training loss per 100 training steps: 0.01665193525063212\n",
            "Training loss per 100 training steps: 0.01657204702730139\n",
            "Training loss per 100 training steps: 0.017377710577263013\n",
            "Training loss per 100 training steps: 0.01793716378759882\n",
            "Training loss per 100 training steps: 0.018020173727236397\n",
            "Training loss per 100 training steps: 0.018107126290970985\n",
            "Training loss per 100 training steps: 0.018306137244452608\n",
            "Training loss per 100 training steps: 0.01802176301740201\n",
            "Training loss per 100 training steps: 0.017868803566531522\n",
            "Training loss per 100 training steps: 0.0176978935309696\n",
            "Training loss per 100 training steps: 0.01746913685758757\n",
            "Training loss per 100 training steps: 0.017352330078222167\n",
            "Training loss per 100 training steps: 0.017139068220362968\n",
            "Training loss per 100 training steps: 0.016721341528177967\n",
            "Training loss per 100 training steps: 0.01657709808978325\n",
            "Training loss per 100 training steps: 0.016242995951733324\n",
            "Training loss per 100 training steps: 0.015893764577792572\n",
            "Training loss per 100 training steps: 0.015850297092191493\n",
            "Training loss per 100 training steps: 0.015939168622502504\n",
            "Training loss per 100 training steps: 0.016313895659361605\n",
            "Training loss per 100 training steps: 0.016258964723275506\n",
            "Training loss per 100 training steps: 0.01630912332682355\n",
            "Training loss per 100 training steps: 0.016392417736667993\n",
            "Training loss per 100 training steps: 0.016370701136838257\n",
            "Training loss per 100 training steps: 0.01639250948066055\n",
            "Training loss epoch: 0.01649799015655352\n",
            "Training accuracy epoch: 0.9961925566236175\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.03191857412566605\n",
            "Validation loss per 100 evaluation steps: 0.04513411921607258\n",
            "Validation loss per 100 evaluation steps: 0.04821176070027529\n",
            "Validation loss per 100 evaluation steps: 0.04855058077620015\n",
            "Validation loss per 100 evaluation steps: 0.046744674344921806\n",
            "Validation loss per 100 evaluation steps: 0.04771027117393108\n",
            "Validation loss per 100 evaluation steps: 0.04644000658748025\n",
            "Validation loss per 100 evaluation steps: 0.048739228559115874\n",
            "Validation Loss: 0.04878353034062545\n",
            "Validation Accuracy: 0.9896030581251583\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.98      0.97      0.98      1837\n",
            "      B-MISC       0.93      0.95      0.94       922\n",
            "       B-ORG       0.94      0.96      0.95      1341\n",
            "       B-PER       0.98      0.98      0.98      1842\n",
            "       I-LOC       0.98      0.97      0.97      1801\n",
            "      I-MISC       0.89      0.88      0.89       935\n",
            "       I-ORG       0.94      0.96      0.95      2319\n",
            "       I-PER       0.99      0.99      0.99      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.96      0.96      0.96     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9896  0.9896  0.9896\n",
            "macro        0.9573  0.9613  0.9593\n",
            "weighted     0.9897  0.9896  0.9897\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.15517419706062355\n",
            "Test loss per 100 evaluation steps: 0.1445981634248801\n",
            "Test loss per 100 evaluation steps: 0.12754174560817166\n",
            "Test loss per 100 evaluation steps: 0.11875865341853115\n",
            "Test loss per 100 evaluation steps: 0.13760584153089803\n",
            "Test loss per 100 evaluation steps: 0.13137400895612095\n",
            "Test loss per 100 evaluation steps: 0.13349122960710053\n",
            "Test loss per 100 evaluation steps: 0.13480855636870956\n",
            "Test loss per 100 evaluation steps: 0.13512716512732773\n",
            "Test Loss: 0.1359361989299021\n",
            "Test Accuracy: 0.9733614055682177\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.95      0.93      0.94      1668\n",
            "      B-MISC       0.79      0.88      0.83       702\n",
            "       B-ORG       0.89      0.93      0.91      1661\n",
            "       B-PER       0.97      0.96      0.97      1617\n",
            "       I-LOC       0.91      0.90      0.90      1394\n",
            "      I-MISC       0.65      0.76      0.70       736\n",
            "       I-ORG       0.88      0.95      0.92      2804\n",
            "       I-PER       0.97      0.97      0.97      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.98     61486\n",
            "   macro avg       0.89      0.92      0.90     61486\n",
            "weighted avg       0.98      0.98      0.98     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9757  0.9757  0.9757\n",
            "macro        0.8914  0.9180  0.9038\n",
            "weighted     0.9771  0.9757  0.9763\n",
            "Test steps: 921\n",
            "Training epoch: 6\n",
            "Training loss per 100 training steps: 0.01610162597949966\n",
            "Training loss per 100 training steps: 0.011002822901318723\n",
            "Training loss per 100 training steps: 0.011464313354348027\n",
            "Training loss per 100 training steps: 0.010460885908487398\n",
            "Training loss per 100 training steps: 0.012488515388482483\n",
            "Training loss per 100 training steps: 0.012588454491466715\n",
            "Training loss per 100 training steps: 0.013486512368817784\n",
            "Training loss per 100 training steps: 0.013102529690297616\n",
            "Training loss per 100 training steps: 0.012656395745291068\n",
            "Training loss per 100 training steps: 0.012603285999817672\n",
            "Training loss per 100 training steps: 0.012592851454664924\n",
            "Training loss per 100 training steps: 0.012390428548386201\n",
            "Training loss per 100 training steps: 0.01224062464554943\n",
            "Training loss per 100 training steps: 0.012899247967222567\n",
            "Training loss per 100 training steps: 0.012999330238441568\n",
            "Training loss per 100 training steps: 0.012521636665162532\n",
            "Training loss per 100 training steps: 0.01227282050291679\n",
            "Training loss per 100 training steps: 0.012178203697088368\n",
            "Training loss per 100 training steps: 0.012752178741075591\n",
            "Training loss per 100 training steps: 0.012386052356165238\n",
            "Training loss per 100 training steps: 0.01257572336656137\n",
            "Training loss per 100 training steps: 0.012824104737562514\n",
            "Training loss per 100 training steps: 0.012790467361943308\n",
            "Training loss per 100 training steps: 0.01270082876157024\n",
            "Training loss per 100 training steps: 0.012595552443468478\n",
            "Training loss per 100 training steps: 0.012437975606366168\n",
            "Training loss per 100 training steps: 0.012410364690183794\n",
            "Training loss per 100 training steps: 0.012375769637629413\n",
            "Training loss per 100 training steps: 0.012255240690081902\n",
            "Training loss per 100 training steps: 0.012630405848143103\n",
            "Training loss per 100 training steps: 0.012569031208510545\n",
            "Training loss per 100 training steps: 0.012834865425970178\n",
            "Training loss per 100 training steps: 0.012559537368128848\n",
            "Training loss per 100 training steps: 0.01254584152503412\n",
            "Training loss per 100 training steps: 0.012666624413234136\n",
            "Training loss per 100 training steps: 0.012690440787981363\n",
            "Training loss per 100 training steps: 0.012598609710594491\n",
            "Training loss epoch: 0.012558843591950015\n",
            "Training accuracy epoch: 0.9969856663768698\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.049279228009108916\n",
            "Validation loss per 100 evaluation steps: 0.055439382158738226\n",
            "Validation loss per 100 evaluation steps: 0.05258596272741973\n",
            "Validation loss per 100 evaluation steps: 0.05748042305146555\n",
            "Validation loss per 100 evaluation steps: 0.059409369447988865\n",
            "Validation loss per 100 evaluation steps: 0.0593064791750506\n",
            "Validation loss per 100 evaluation steps: 0.05526152431400208\n",
            "Validation loss per 100 evaluation steps: 0.05749902064560729\n",
            "Validation Loss: 0.058545060299645055\n",
            "Validation Accuracy: 0.9889913361999229\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.98      0.98      0.98      1837\n",
            "      B-MISC       0.94      0.92      0.93       922\n",
            "       B-ORG       0.93      0.96      0.95      1341\n",
            "       B-PER       0.98      0.98      0.98      1842\n",
            "       I-LOC       0.97      0.97      0.97      1801\n",
            "      I-MISC       0.90      0.85      0.88       935\n",
            "       I-ORG       0.94      0.97      0.95      2319\n",
            "       I-PER       0.99      0.98      0.99      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.96      0.96      0.96     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9893  0.9893  0.9893\n",
            "macro        0.9585  0.9576  0.9579\n",
            "weighted     0.9893  0.9893  0.9893\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.16680209503578225\n",
            "Test loss per 100 evaluation steps: 0.14956444969906443\n",
            "Test loss per 100 evaluation steps: 0.15690655847484475\n",
            "Test loss per 100 evaluation steps: 0.15521777050978472\n",
            "Test loss per 100 evaluation steps: 0.1503191010538794\n",
            "Test loss per 100 evaluation steps: 0.14967027338005665\n",
            "Test loss per 100 evaluation steps: 0.15037448218695057\n",
            "Test loss per 100 evaluation steps: 0.15161664392128274\n",
            "Test loss per 100 evaluation steps: 0.15009642767242237\n",
            "Test Loss: 0.1532335498176104\n",
            "Test Accuracy: 0.9747216335932919\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.94      0.94      0.94      1668\n",
            "      B-MISC       0.83      0.84      0.84       702\n",
            "       B-ORG       0.89      0.94      0.91      1661\n",
            "       B-PER       0.97      0.96      0.97      1617\n",
            "       I-LOC       0.88      0.93      0.90      1394\n",
            "      I-MISC       0.67      0.70      0.68       736\n",
            "       I-ORG       0.90      0.95      0.92      2804\n",
            "       I-PER       0.98      0.98      0.98      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.98     61486\n",
            "   macro avg       0.89      0.91      0.90     61486\n",
            "weighted avg       0.98      0.98      0.98     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9764  0.9764  0.9764\n",
            "macro        0.8944  0.9150  0.9044\n",
            "weighted     0.9772  0.9764  0.9767\n",
            "Test steps: 921\n",
            "Training epoch: 7\n",
            "Training loss per 100 training steps: 0.006209949936137491\n",
            "Training loss per 100 training steps: 0.006498390796646163\n",
            "Training loss per 100 training steps: 0.008084390834728766\n",
            "Training loss per 100 training steps: 0.008912106467762441\n",
            "Training loss per 100 training steps: 0.009760540367309658\n",
            "Training loss per 100 training steps: 0.008991714602102548\n",
            "Training loss per 100 training steps: 0.008637569679590992\n",
            "Training loss per 100 training steps: 0.008300197235886344\n",
            "Training loss per 100 training steps: 0.008171399667752566\n",
            "Training loss per 100 training steps: 0.00803076959338705\n",
            "Training loss per 100 training steps: 0.008209850318141683\n",
            "Training loss per 100 training steps: 0.008057851378452293\n",
            "Training loss per 100 training steps: 0.008148341857507917\n",
            "Training loss per 100 training steps: 0.008169062930866987\n",
            "Training loss per 100 training steps: 0.008359019066029457\n",
            "Training loss per 100 training steps: 0.008159865443976172\n",
            "Training loss per 100 training steps: 0.008111101775324748\n",
            "Training loss per 100 training steps: 0.008241954900763631\n",
            "Training loss per 100 training steps: 0.008214792766648795\n",
            "Training loss per 100 training steps: 0.008052373713092947\n",
            "Training loss per 100 training steps: 0.008364201580616282\n",
            "Training loss per 100 training steps: 0.008658082850061378\n",
            "Training loss per 100 training steps: 0.008554161390629816\n",
            "Training loss per 100 training steps: 0.008433503013280112\n",
            "Training loss per 100 training steps: 0.008230187197047416\n",
            "Training loss per 100 training steps: 0.008416105744281244\n",
            "Training loss per 100 training steps: 0.008584204095674\n",
            "Training loss per 100 training steps: 0.00840577636036057\n",
            "Training loss per 100 training steps: 0.008160042567013958\n",
            "Training loss per 100 training steps: 0.008245859916115933\n",
            "Training loss per 100 training steps: 0.00832031482242714\n",
            "Training loss per 100 training steps: 0.008895645160420856\n",
            "Training loss per 100 training steps: 0.008694626156519034\n",
            "Training loss per 100 training steps: 0.008627001894048776\n",
            "Training loss per 100 training steps: 0.008809130147177841\n",
            "Training loss per 100 training steps: 0.009244849546505899\n",
            "Training loss per 100 training steps: 0.009235743206923944\n",
            "Training loss epoch: 0.00923540160943297\n",
            "Training accuracy epoch: 0.9978229122037492\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.08619315766660293\n",
            "Validation loss per 100 evaluation steps: 0.08741468970250026\n",
            "Validation loss per 100 evaluation steps: 0.0764293971883247\n",
            "Validation loss per 100 evaluation steps: 0.07292134859011867\n",
            "Validation loss per 100 evaluation steps: 0.07267045847009831\n",
            "Validation loss per 100 evaluation steps: 0.07481542662387104\n",
            "Validation loss per 100 evaluation steps: 0.06887661716002835\n",
            "Validation loss per 100 evaluation steps: 0.06773486479366853\n",
            "Validation Loss: 0.06693863203616583\n",
            "Validation Accuracy: 0.9884701772623263\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.97      0.97      1837\n",
            "      B-MISC       0.92      0.94      0.93       922\n",
            "       B-ORG       0.94      0.96      0.95      1341\n",
            "       B-PER       0.99      0.97      0.98      1842\n",
            "       I-LOC       0.96      0.97      0.96      1801\n",
            "      I-MISC       0.87      0.89      0.88       935\n",
            "       I-ORG       0.93      0.96      0.94      2319\n",
            "       I-PER       0.99      0.97      0.98      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.95      0.96      0.96     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9884  0.9884  0.9884\n",
            "macro        0.9508  0.9599  0.9553\n",
            "weighted     0.9886  0.9884  0.9885\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.13261759563463785\n",
            "Test loss per 100 evaluation steps: 0.18161483397515896\n",
            "Test loss per 100 evaluation steps: 0.1746103287733634\n",
            "Test loss per 100 evaluation steps: 0.1792790467394775\n",
            "Test loss per 100 evaluation steps: 0.17696699839038593\n",
            "Test loss per 100 evaluation steps: 0.1815508828751869\n",
            "Test loss per 100 evaluation steps: 0.18568813304507525\n",
            "Test loss per 100 evaluation steps: 0.19014340365329643\n",
            "Test loss per 100 evaluation steps: 0.1873355519504346\n",
            "Test Loss: 0.18711004065046005\n",
            "Test Accuracy: 0.9709598369495407\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.93      0.94      0.94      1668\n",
            "      B-MISC       0.82      0.85      0.83       702\n",
            "       B-ORG       0.87      0.92      0.90      1661\n",
            "       B-PER       0.98      0.93      0.96      1617\n",
            "       I-LOC       0.87      0.93      0.90      1394\n",
            "      I-MISC       0.65      0.71      0.68       736\n",
            "       I-ORG       0.85      0.95      0.90      2804\n",
            "       I-PER       0.98      0.94      0.96      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.88      0.91      0.89     61486\n",
            "weighted avg       0.97      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9724  0.9724  0.9724\n",
            "macro        0.8829  0.9067  0.8940\n",
            "weighted     0.9740  0.9724  0.9730\n",
            "Test steps: 921\n",
            "====================================================================================================\n",
            "loss_name: kl\n",
            "model configuration\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Ner_Model(\n",
            "  (model): DebertaModel(\n",
            "    (embeddings): DebertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "      (LayerNorm): DebertaLayerNorm()\n",
            "      (dropout): StableDropout()\n",
            "    )\n",
            "    (encoder): DebertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x DebertaLayer(\n",
            "          (attention): DebertaAttention(\n",
            "            (self): DisentangledSelfAttention(\n",
            "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
            "              (pos_dropout): StableDropout()\n",
            "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "            (output): DebertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): DebertaLayerNorm()\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "          )\n",
            "          (intermediate): DebertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): DebertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): DebertaLayerNorm()\n",
            "            (dropout): StableDropout()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (rel_embeddings): Embedding(1024, 768)\n",
            "    )\n",
            "  )\n",
            "  (linear_model): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=384, bias=True)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (classifier): Linear(in_features=384, out_features=9, bias=True)\n",
            ")\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 1.1767484188079833\n",
            "Training loss per 100 training steps: 0.8917207848094404\n",
            "Training loss per 100 training steps: 0.7346138525133331\n",
            "Training loss per 100 training steps: 0.6324248754698784\n",
            "Training loss per 100 training steps: 0.5596611495316028\n",
            "Training loss per 100 training steps: 0.5061514236715933\n",
            "Training loss per 100 training steps: 0.46224860461295714\n",
            "Training loss per 100 training steps: 0.4260816278337734\n",
            "Training loss per 100 training steps: 0.39705272236373274\n",
            "Training loss per 100 training steps: 0.37399852598411965\n",
            "Training loss per 100 training steps: 0.3520696441581557\n",
            "Training loss per 100 training steps: 0.3343039526734113\n",
            "Training loss per 100 training steps: 0.3170943796189609\n",
            "Training loss per 100 training steps: 0.30274032306549736\n",
            "Training loss per 100 training steps: 0.2914248355849801\n",
            "Training loss per 100 training steps: 0.2794771406400105\n",
            "Training loss per 100 training steps: 0.2692306098234429\n",
            "Training loss per 100 training steps: 0.2592074675205448\n",
            "Training loss per 100 training steps: 0.25045491235679945\n",
            "Training loss per 100 training steps: 0.24255151278775883\n",
            "Training loss per 100 training steps: 0.23584009996033273\n",
            "Training loss per 100 training steps: 0.22972345006123016\n",
            "Training loss per 100 training steps: 0.22351273192771023\n",
            "Training loss per 100 training steps: 0.217792342727577\n",
            "Training loss per 100 training steps: 0.2128550427384209\n",
            "Training loss per 100 training steps: 0.2081939133140706\n",
            "Training loss per 100 training steps: 0.20343740604347893\n",
            "Training loss per 100 training steps: 0.1988100081171641\n",
            "Training loss per 100 training steps: 0.19400945634773836\n",
            "Training loss per 100 training steps: 0.19028448863700032\n",
            "Training loss per 100 training steps: 0.1867210459130846\n",
            "Training loss per 100 training steps: 0.18277108681573737\n",
            "Training loss per 100 training steps: 0.17959598887914607\n",
            "Training loss per 100 training steps: 0.17629789037614554\n",
            "Training loss per 100 training steps: 0.172888980426942\n",
            "Training loss per 100 training steps: 0.17007198435191337\n",
            "Training loss per 100 training steps: 0.16774449345959364\n",
            "Training loss epoch: 0.16635786083771795\n",
            "Training accuracy epoch: 0.9554281579471918\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.04868813161272555\n",
            "Validation loss per 100 evaluation steps: 0.049592296500049995\n",
            "Validation loss per 100 evaluation steps: 0.052468361723100924\n",
            "Validation loss per 100 evaluation steps: 0.054823391200989134\n",
            "Validation loss per 100 evaluation steps: 0.05306251920893555\n",
            "Validation loss per 100 evaluation steps: 0.0528275941775064\n",
            "Validation loss per 100 evaluation steps: 0.05224540877615384\n",
            "Validation loss per 100 evaluation steps: 0.0514879825645221\n",
            "Validation Loss: 0.05109661999698006\n",
            "Validation Accuracy: 0.9860562010497366\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.96      0.97      0.97      1837\n",
            "      B-MISC       0.90      0.92      0.91       922\n",
            "       B-ORG       0.95      0.93      0.94      1341\n",
            "       B-PER       0.98      0.98      0.98      1842\n",
            "       I-LOC       0.94      0.98      0.96      1801\n",
            "      I-MISC       0.84      0.82      0.83       935\n",
            "       I-ORG       0.94      0.92      0.93      2319\n",
            "       I-PER       0.99      0.98      0.98      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.94      0.94      0.94     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9860   0.986  0.9860\n",
            "macro        0.9438   0.944  0.9438\n",
            "weighted     0.9860   0.986  0.9860\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.10643039835267701\n",
            "Test loss per 100 evaluation steps: 0.09613226601184578\n",
            "Test loss per 100 evaluation steps: 0.10130321551909825\n",
            "Test loss per 100 evaluation steps: 0.10129125340856263\n",
            "Test loss per 100 evaluation steps: 0.10265276498487219\n",
            "Test loss per 100 evaluation steps: 0.10120167869744667\n",
            "Test loss per 100 evaluation steps: 0.10270835472936077\n",
            "Test loss per 100 evaluation steps: 0.10431466472291503\n",
            "Test loss per 100 evaluation steps: 0.10723046673746366\n",
            "Test Loss: 0.10735968750270526\n",
            "Test Accuracy: 0.9719653196494272\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.93      0.93      0.93      1668\n",
            "      B-MISC       0.80      0.85      0.83       702\n",
            "       B-ORG       0.89      0.91      0.90      1661\n",
            "       B-PER       0.97      0.95      0.96      1617\n",
            "       I-LOC       0.87      0.92      0.89      1394\n",
            "      I-MISC       0.64      0.70      0.67       736\n",
            "       I-ORG       0.88      0.93      0.90      2804\n",
            "       I-PER       0.98      0.96      0.97      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.88      0.91      0.89     61486\n",
            "weighted avg       0.97      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9735  0.9735  0.9735\n",
            "macro        0.8836  0.9051  0.8939\n",
            "weighted     0.9746  0.9735  0.9740\n",
            "Test steps: 921\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.03330599101143889\n",
            "Training loss per 100 training steps: 0.04325000039592851\n",
            "Training loss per 100 training steps: 0.04332877370102021\n",
            "Training loss per 100 training steps: 0.05027950462608714\n",
            "Training loss per 100 training steps: 0.05193629805656383\n",
            "Training loss per 100 training steps: 0.05071934147286811\n",
            "Training loss per 100 training steps: 0.051644181935906606\n",
            "Training loss per 100 training steps: 0.051880418531764005\n",
            "Training loss per 100 training steps: 0.052913363036220996\n",
            "Training loss per 100 training steps: 0.0532039761688211\n",
            "Training loss per 100 training steps: 0.05584115246273789\n",
            "Training loss per 100 training steps: 0.05486414778822412\n",
            "Training loss per 100 training steps: 0.05454833770251403\n",
            "Training loss per 100 training steps: 0.0532556412903276\n",
            "Training loss per 100 training steps: 0.05200289338846536\n",
            "Training loss per 100 training steps: 0.05127344354540583\n",
            "Training loss per 100 training steps: 0.05182234132540008\n",
            "Training loss per 100 training steps: 0.05190670181984994\n",
            "Training loss per 100 training steps: 0.05200381280858174\n",
            "Training loss per 100 training steps: 0.052128778151913135\n",
            "Training loss per 100 training steps: 0.05305490622172892\n",
            "Training loss per 100 training steps: 0.05225289096741604\n",
            "Training loss per 100 training steps: 0.051754507625598266\n",
            "Training loss per 100 training steps: 0.05164879767365468\n",
            "Training loss per 100 training steps: 0.05147937170980149\n",
            "Training loss per 100 training steps: 0.05078958673005848\n",
            "Training loss per 100 training steps: 0.050714322786570176\n",
            "Training loss per 100 training steps: 0.05094103092519488\n",
            "Training loss per 100 training steps: 0.050681740453146634\n",
            "Training loss per 100 training steps: 0.050148737003759984\n",
            "Training loss per 100 training steps: 0.049656373010258256\n",
            "Training loss per 100 training steps: 0.049509994883633224\n",
            "Training loss per 100 training steps: 0.04997520662562844\n",
            "Training loss per 100 training steps: 0.049864055714458705\n",
            "Training loss per 100 training steps: 0.049267559513848806\n",
            "Training loss per 100 training steps: 0.04949065070970391\n",
            "Training loss per 100 training steps: 0.0491510619779001\n",
            "Training loss epoch: 0.04896818227172394\n",
            "Training accuracy epoch: 0.9866703314752434\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.03824323604334495\n",
            "Validation loss per 100 evaluation steps: 0.05009785284222744\n",
            "Validation loss per 100 evaluation steps: 0.053701848996812865\n",
            "Validation loss per 100 evaluation steps: 0.05510909817403444\n",
            "Validation loss per 100 evaluation steps: 0.05375267220006208\n",
            "Validation loss per 100 evaluation steps: 0.05154377055691536\n",
            "Validation loss per 100 evaluation steps: 0.049827655651544255\n",
            "Validation loss per 100 evaluation steps: 0.04941211027877216\n",
            "Validation Loss: 0.04933647640111425\n",
            "Validation Accuracy: 0.9868987427272835\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.96      0.97      1837\n",
            "      B-MISC       0.90      0.93      0.91       922\n",
            "       B-ORG       0.93      0.96      0.94      1341\n",
            "       B-PER       0.98      0.98      0.98      1842\n",
            "       I-LOC       0.97      0.95      0.96      1801\n",
            "      I-MISC       0.83      0.86      0.84       935\n",
            "       I-ORG       0.93      0.94      0.94      2319\n",
            "       I-PER       0.99      0.99      0.99      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.94      0.95      0.95     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9872  0.9872  0.9872\n",
            "macro        0.9447  0.9525  0.9485\n",
            "weighted     0.9874  0.9872  0.9872\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.12905559161241398\n",
            "Test loss per 100 evaluation steps: 0.12620368848831276\n",
            "Test loss per 100 evaluation steps: 0.12298323011336228\n",
            "Test loss per 100 evaluation steps: 0.11333383012475678\n",
            "Test loss per 100 evaluation steps: 0.10940397131425561\n",
            "Test loss per 100 evaluation steps: 0.11055673153566507\n",
            "Test loss per 100 evaluation steps: 0.11515347914046807\n",
            "Test loss per 100 evaluation steps: 0.11272347061967594\n",
            "Test loss per 100 evaluation steps: 0.11265930346109801\n",
            "Test Loss: 0.11234880114809977\n",
            "Test Accuracy: 0.9739163393826501\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.95      0.93      0.94      1668\n",
            "      B-MISC       0.81      0.84      0.83       702\n",
            "       B-ORG       0.89      0.94      0.91      1661\n",
            "       B-PER       0.97      0.96      0.97      1617\n",
            "       I-LOC       0.91      0.90      0.90      1394\n",
            "      I-MISC       0.61      0.71      0.66       736\n",
            "       I-ORG       0.90      0.96      0.93      2804\n",
            "       I-PER       0.97      0.98      0.98      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.98     61486\n",
            "   macro avg       0.89      0.91      0.90     61486\n",
            "weighted avg       0.98      0.98      0.98     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9757  0.9757  0.9757\n",
            "macro        0.8904  0.9126  0.9009\n",
            "weighted     0.9769  0.9757  0.9762\n",
            "Test steps: 921\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.023448386902164203\n",
            "Training loss per 100 training steps: 0.020825682301147026\n",
            "Training loss per 100 training steps: 0.023969156241655583\n",
            "Training loss per 100 training steps: 0.023602088467559953\n",
            "Training loss per 100 training steps: 0.02604515210464888\n",
            "Training loss per 100 training steps: 0.025392630752382198\n",
            "Training loss per 100 training steps: 0.027981665696393716\n",
            "Training loss per 100 training steps: 0.027386765030541937\n",
            "Training loss per 100 training steps: 0.028080333336850647\n",
            "Training loss per 100 training steps: 0.029025136838852633\n",
            "Training loss per 100 training steps: 0.02845799761219413\n",
            "Training loss per 100 training steps: 0.02999904313438492\n",
            "Training loss per 100 training steps: 0.02925899988593021\n",
            "Training loss per 100 training steps: 0.029735710398524264\n",
            "Training loss per 100 training steps: 0.030582065083151973\n",
            "Training loss per 100 training steps: 0.029977385805107132\n",
            "Training loss per 100 training steps: 0.029682158305061903\n",
            "Training loss per 100 training steps: 0.029072694692329402\n",
            "Training loss per 100 training steps: 0.029349513914768116\n",
            "Training loss per 100 training steps: 0.02944323632197302\n",
            "Training loss per 100 training steps: 0.0291549264534617\n",
            "Training loss per 100 training steps: 0.029389629425645995\n",
            "Training loss per 100 training steps: 0.028648215331244422\n",
            "Training loss per 100 training steps: 0.029299227664049188\n",
            "Training loss per 100 training steps: 0.029313546292429965\n",
            "Training loss per 100 training steps: 0.02904411565464985\n",
            "Training loss per 100 training steps: 0.02843635532616005\n",
            "Training loss per 100 training steps: 0.02870560161559037\n",
            "Training loss per 100 training steps: 0.029673768009044327\n",
            "Training loss per 100 training steps: 0.030094273606862166\n",
            "Training loss per 100 training steps: 0.030189695517449676\n",
            "Training loss per 100 training steps: 0.03050489525466105\n",
            "Training loss per 100 training steps: 0.030242671824710146\n",
            "Training loss per 100 training steps: 0.030301882776666924\n",
            "Training loss per 100 training steps: 0.03060871664561585\n",
            "Training loss per 100 training steps: 0.030356610333620766\n",
            "Training loss per 100 training steps: 0.0303128112936827\n",
            "Training loss epoch: 0.030239087768553045\n",
            "Training accuracy epoch: 0.9921535600357017\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.05723823796441138\n",
            "Validation loss per 100 evaluation steps: 0.057006091190123696\n",
            "Validation loss per 100 evaluation steps: 0.05801444070549527\n",
            "Validation loss per 100 evaluation steps: 0.0585435378527427\n",
            "Validation loss per 100 evaluation steps: 0.0540221854811025\n",
            "Validation loss per 100 evaluation steps: 0.05209723539386687\n",
            "Validation loss per 100 evaluation steps: 0.05279468615799747\n",
            "Validation loss per 100 evaluation steps: 0.051731368257064786\n",
            "Validation Loss: 0.05257906480179191\n",
            "Validation Accuracy: 0.9877411231911766\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.98      0.97      0.97      1837\n",
            "      B-MISC       0.89      0.95      0.92       922\n",
            "       B-ORG       0.97      0.93      0.95      1341\n",
            "       B-PER       0.97      0.99      0.98      1842\n",
            "       I-LOC       0.98      0.96      0.97      1801\n",
            "      I-MISC       0.84      0.89      0.87       935\n",
            "       I-ORG       0.97      0.91      0.94      2319\n",
            "       I-PER       0.97      1.00      0.98      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.95      0.96      0.95     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9883  0.9883  0.9883\n",
            "macro        0.9519  0.9551  0.9531\n",
            "weighted     0.9885  0.9883  0.9883\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.18332865236376164\n",
            "Test loss per 100 evaluation steps: 0.15106360616659914\n",
            "Test loss per 100 evaluation steps: 0.12732426447601636\n",
            "Test loss per 100 evaluation steps: 0.11899449683105104\n",
            "Test loss per 100 evaluation steps: 0.12407567315433335\n",
            "Test loss per 100 evaluation steps: 0.12425052589102961\n",
            "Test loss per 100 evaluation steps: 0.11948664962195835\n",
            "Test loss per 100 evaluation steps: 0.1203654283391893\n",
            "Test loss per 100 evaluation steps: 0.11877839998186675\n",
            "Test Loss: 0.11977399578879022\n",
            "Test Accuracy: 0.9758330291710846\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.93      0.94      0.94      1668\n",
            "      B-MISC       0.80      0.89      0.84       702\n",
            "       B-ORG       0.93      0.90      0.92      1661\n",
            "       B-PER       0.95      0.98      0.97      1617\n",
            "       I-LOC       0.90      0.92      0.91      1394\n",
            "      I-MISC       0.66      0.77      0.71       736\n",
            "       I-ORG       0.94      0.91      0.93      2804\n",
            "       I-PER       0.95      0.99      0.97      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.98     61486\n",
            "   macro avg       0.90      0.92      0.91     61486\n",
            "weighted avg       0.98      0.98      0.98     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9767  0.9767  0.9767\n",
            "macro        0.8954  0.9201  0.9069\n",
            "weighted     0.9777  0.9767  0.9771\n",
            "Test steps: 921\n",
            "Training epoch: 4\n",
            "Training loss per 100 training steps: 0.02846686321863672\n",
            "Training loss per 100 training steps: 0.022460370217086165\n",
            "Training loss per 100 training steps: 0.021446207015881857\n",
            "Training loss per 100 training steps: 0.026253885206806445\n",
            "Training loss per 100 training steps: 0.025942200410339865\n",
            "Training loss per 100 training steps: 0.025718640685457407\n",
            "Training loss per 100 training steps: 0.025911853097443652\n",
            "Training loss per 100 training steps: 0.02484461848128376\n",
            "Training loss per 100 training steps: 0.023880952216333955\n",
            "Training loss per 100 training steps: 0.023168708895322198\n",
            "Training loss per 100 training steps: 0.023262812321160956\n",
            "Training loss per 100 training steps: 0.0226295334395733\n",
            "Training loss per 100 training steps: 0.02343716224808584\n",
            "Training loss per 100 training steps: 0.02317631875311625\n",
            "Training loss per 100 training steps: 0.02292196709406562\n",
            "Training loss per 100 training steps: 0.022759921263568687\n",
            "Training loss per 100 training steps: 0.022401469581810114\n",
            "Training loss per 100 training steps: 0.022384442419998877\n",
            "Training loss per 100 training steps: 0.022101919576751627\n",
            "Training loss per 100 training steps: 0.02274093772726701\n",
            "Training loss per 100 training steps: 0.022651440585579535\n",
            "Training loss per 100 training steps: 0.022510111575188188\n",
            "Training loss per 100 training steps: 0.02219986319150374\n",
            "Training loss per 100 training steps: 0.021813409591766232\n",
            "Training loss per 100 training steps: 0.021662101005309523\n",
            "Training loss per 100 training steps: 0.021410234587761947\n",
            "Training loss per 100 training steps: 0.021164037536168002\n",
            "Training loss per 100 training steps: 0.02187883140805947\n",
            "Training loss per 100 training steps: 0.021770116470502372\n",
            "Training loss per 100 training steps: 0.021250624570777896\n",
            "Training loss per 100 training steps: 0.021564016906191214\n",
            "Training loss per 100 training steps: 0.02151671378134381\n",
            "Training loss per 100 training steps: 0.021391923422652327\n",
            "Training loss per 100 training steps: 0.02145099779194692\n",
            "Training loss per 100 training steps: 0.021557890354090756\n",
            "Training loss per 100 training steps: 0.021752805118872957\n",
            "Training loss per 100 training steps: 0.02184082593083666\n",
            "Training loss epoch: 0.021723366025891956\n",
            "Training accuracy epoch: 0.9943437681146033\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.07161321771092843\n",
            "Validation loss per 100 evaluation steps: 0.06298014229109868\n",
            "Validation loss per 100 evaluation steps: 0.0579625033663433\n",
            "Validation loss per 100 evaluation steps: 0.05482897518097616\n",
            "Validation loss per 100 evaluation steps: 0.05121147806394947\n",
            "Validation loss per 100 evaluation steps: 0.04749525848431404\n",
            "Validation loss per 100 evaluation steps: 0.04652519618360332\n",
            "Validation loss per 100 evaluation steps: 0.04687768063109388\n",
            "Validation Loss: 0.047127805530084416\n",
            "Validation Accuracy: 0.9891411276585885\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.98      0.98      1837\n",
            "      B-MISC       0.92      0.93      0.93       922\n",
            "       B-ORG       0.96      0.95      0.95      1341\n",
            "       B-PER       0.98      0.99      0.98      1842\n",
            "       I-LOC       0.97      0.98      0.97      1801\n",
            "      I-MISC       0.88      0.87      0.88       935\n",
            "       I-ORG       0.97      0.92      0.95      2319\n",
            "       I-PER       0.98      0.99      0.99      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.96      0.96      0.96     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9893  0.9893  0.9893\n",
            "macro        0.9588  0.9563  0.9574\n",
            "weighted     0.9893  0.9893  0.9893\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.14526236213496305\n",
            "Test loss per 100 evaluation steps: 0.12478411595802755\n",
            "Test loss per 100 evaluation steps: 0.12157097960264461\n",
            "Test loss per 100 evaluation steps: 0.12297836763341366\n",
            "Test loss per 100 evaluation steps: 0.12833498609970775\n",
            "Test loss per 100 evaluation steps: 0.12267922829778097\n",
            "Test loss per 100 evaluation steps: 0.12026764942905564\n",
            "Test loss per 100 evaluation steps: 0.12093966285507804\n",
            "Test loss per 100 evaluation steps: 0.11860418034257236\n",
            "Test Loss: 0.11771557302925867\n",
            "Test Accuracy: 0.9768453536821103\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.95      0.93      0.94      1668\n",
            "      B-MISC       0.83      0.87      0.85       702\n",
            "       B-ORG       0.91      0.93      0.92      1661\n",
            "       B-PER       0.96      0.97      0.97      1617\n",
            "       I-LOC       0.92      0.92      0.92      1394\n",
            "      I-MISC       0.67      0.76      0.71       736\n",
            "       I-ORG       0.93      0.94      0.93      2804\n",
            "       I-PER       0.97      0.99      0.98      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.98     61486\n",
            "   macro avg       0.90      0.92      0.91     61486\n",
            "weighted avg       0.98      0.98      0.98     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9783  0.9783  0.9783\n",
            "macro        0.9032  0.9218  0.9121\n",
            "weighted     0.9791  0.9783  0.9786\n",
            "Test steps: 921\n",
            "Training epoch: 5\n",
            "Training loss per 100 training steps: 0.010546830233943183\n",
            "Training loss per 100 training steps: 0.017698848717991496\n",
            "Training loss per 100 training steps: 0.016943268752511356\n",
            "Training loss per 100 training steps: 0.018217064136151748\n",
            "Training loss per 100 training steps: 0.01928788124250423\n",
            "Training loss per 100 training steps: 0.018138767370370866\n",
            "Training loss per 100 training steps: 0.017640202463436644\n",
            "Training loss per 100 training steps: 0.01690038291055089\n",
            "Training loss per 100 training steps: 0.017369371642515844\n",
            "Training loss per 100 training steps: 0.01835085941759462\n",
            "Training loss per 100 training steps: 0.017259468551776742\n",
            "Training loss per 100 training steps: 0.016394071833856286\n",
            "Training loss per 100 training steps: 0.01666766441240235\n",
            "Training loss per 100 training steps: 0.016882472706543663\n",
            "Training loss per 100 training steps: 0.016738189530474\n",
            "Training loss per 100 training steps: 0.016319057318985416\n",
            "Training loss per 100 training steps: 0.016241294822374425\n",
            "Training loss per 100 training steps: 0.015846172371935204\n",
            "Training loss per 100 training steps: 0.016065830540464\n",
            "Training loss per 100 training steps: 0.015748025158506606\n",
            "Training loss per 100 training steps: 0.015577918731840846\n",
            "Training loss per 100 training steps: 0.015647558391199873\n",
            "Training loss per 100 training steps: 0.015372575695275136\n",
            "Training loss per 100 training steps: 0.01576374314822033\n",
            "Training loss per 100 training steps: 0.015847222793836407\n",
            "Training loss per 100 training steps: 0.015863967997648405\n",
            "Training loss per 100 training steps: 0.015463249490140798\n",
            "Training loss per 100 training steps: 0.015386328510780914\n",
            "Training loss per 100 training steps: 0.015488935299092881\n",
            "Training loss per 100 training steps: 0.015544480963229943\n",
            "Training loss per 100 training steps: 0.015547054229819262\n",
            "Training loss per 100 training steps: 0.01536153145932019\n",
            "Training loss per 100 training steps: 0.015284782556521378\n",
            "Training loss per 100 training steps: 0.015212908874184852\n",
            "Training loss per 100 training steps: 0.01521097783057017\n",
            "Training loss per 100 training steps: 0.01506651092142369\n",
            "Training loss per 100 training steps: 0.015115459783478523\n",
            "Training loss epoch: 0.015058697906597423\n",
            "Training accuracy epoch: 0.9961907907895965\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.04525755449878489\n",
            "Validation loss per 100 evaluation steps: 0.048087776503307395\n",
            "Validation loss per 100 evaluation steps: 0.04939973636472132\n",
            "Validation loss per 100 evaluation steps: 0.05337875321723914\n",
            "Validation loss per 100 evaluation steps: 0.052415545773281336\n",
            "Validation loss per 100 evaluation steps: 0.04868120970057135\n",
            "Validation loss per 100 evaluation steps: 0.050780132533478274\n",
            "Validation loss per 100 evaluation steps: 0.051509280059494816\n",
            "Validation Loss: 0.05150014461319036\n",
            "Validation Accuracy: 0.9894840460481015\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.98      0.98      0.98      1837\n",
            "      B-MISC       0.91      0.95      0.93       922\n",
            "       B-ORG       0.97      0.93      0.95      1341\n",
            "       B-PER       0.97      0.99      0.98      1842\n",
            "       I-LOC       0.98      0.97      0.98      1801\n",
            "      I-MISC       0.85      0.92      0.88       935\n",
            "       I-ORG       0.96      0.94      0.95      2319\n",
            "       I-PER       0.98      0.99      0.99      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.96      0.96      0.96     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9893  0.9893  0.9893\n",
            "macro        0.9552  0.9617  0.9582\n",
            "weighted     0.9894  0.9893  0.9893\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.11614575902749494\n",
            "Test loss per 100 evaluation steps: 0.11437343346380657\n",
            "Test loss per 100 evaluation steps: 0.12985897140608965\n",
            "Test loss per 100 evaluation steps: 0.1440027232808916\n",
            "Test loss per 100 evaluation steps: 0.15052663432922417\n",
            "Test loss per 100 evaluation steps: 0.14320541921389426\n",
            "Test loss per 100 evaluation steps: 0.14106675820114364\n",
            "Test loss per 100 evaluation steps: 0.14578311157551072\n",
            "Test loss per 100 evaluation steps: 0.14537543751140522\n",
            "Test Loss: 0.14377915987810547\n",
            "Test Accuracy: 0.9756272221126849\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.95      0.94      0.94      1668\n",
            "      B-MISC       0.81      0.88      0.84       702\n",
            "       B-ORG       0.92      0.91      0.92      1661\n",
            "       B-PER       0.95      0.97      0.96      1617\n",
            "       I-LOC       0.93      0.92      0.93      1394\n",
            "      I-MISC       0.62      0.80      0.70       736\n",
            "       I-ORG       0.91      0.94      0.93      2804\n",
            "       I-PER       0.97      0.98      0.98      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.98     61486\n",
            "   macro avg       0.90      0.93      0.91     61486\n",
            "weighted avg       0.98      0.98      0.98     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9770  0.9770  0.9770\n",
            "macro        0.8963  0.9261  0.9097\n",
            "weighted     0.9786  0.9770  0.9776\n",
            "Test steps: 921\n",
            "Training epoch: 6\n",
            "Training loss per 100 training steps: 0.0038192949956282972\n",
            "Training loss per 100 training steps: 0.015377746041376667\n",
            "Training loss per 100 training steps: 0.022059445025976555\n",
            "Training loss per 100 training steps: 0.01902245444705841\n",
            "Training loss per 100 training steps: 0.01821402960299747\n",
            "Training loss per 100 training steps: 0.016961290148428815\n",
            "Training loss per 100 training steps: 0.015173403854153418\n",
            "Training loss per 100 training steps: 0.014448098425302192\n",
            "Training loss per 100 training steps: 0.013652546663261596\n",
            "Training loss per 100 training steps: 0.013752180218544253\n",
            "Training loss per 100 training steps: 0.013482426414536184\n",
            "Training loss per 100 training steps: 0.01290307649639279\n",
            "Training loss per 100 training steps: 0.01229785068219791\n",
            "Training loss per 100 training steps: 0.012178376276767721\n",
            "Training loss per 100 training steps: 0.012316686679143762\n",
            "Training loss per 100 training steps: 0.012073753509336314\n",
            "Training loss per 100 training steps: 0.012354441671586489\n",
            "Training loss per 100 training steps: 0.012357772836085384\n",
            "Training loss per 100 training steps: 0.013590339011089591\n",
            "Training loss per 100 training steps: 0.013561530061536587\n",
            "Training loss per 100 training steps: 0.013081425975471704\n",
            "Training loss per 100 training steps: 0.012718253904051836\n",
            "Training loss per 100 training steps: 0.012741756142936538\n",
            "Training loss per 100 training steps: 0.012304477835929408\n",
            "Training loss per 100 training steps: 0.012358719703309725\n",
            "Training loss per 100 training steps: 0.01237437684457693\n",
            "Training loss per 100 training steps: 0.012323433085415909\n",
            "Training loss per 100 training steps: 0.012436320287322198\n",
            "Training loss per 100 training steps: 0.012301213058581981\n",
            "Training loss per 100 training steps: 0.012193841318125882\n",
            "Training loss per 100 training steps: 0.012068672533785121\n",
            "Training loss per 100 training steps: 0.012007497324068623\n",
            "Training loss per 100 training steps: 0.012052678271055066\n",
            "Training loss per 100 training steps: 0.012385085802878648\n",
            "Training loss per 100 training steps: 0.012423492271956483\n",
            "Training loss per 100 training steps: 0.012540113602994753\n",
            "Training loss per 100 training steps: 0.01243281552373225\n",
            "Training loss epoch: 0.012333128205436318\n",
            "Training accuracy epoch: 0.996987197676446\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.07126380917149618\n",
            "Validation loss per 100 evaluation steps: 0.06883744933929847\n",
            "Validation loss per 100 evaluation steps: 0.07099867309949787\n",
            "Validation loss per 100 evaluation steps: 0.07137384717541295\n",
            "Validation loss per 100 evaluation steps: 0.06633477584599222\n",
            "Validation loss per 100 evaluation steps: 0.06561150216685367\n",
            "Validation loss per 100 evaluation steps: 0.06173448635323114\n",
            "Validation loss per 100 evaluation steps: 0.058732360256253176\n",
            "Validation Loss: 0.06220850899725149\n",
            "Validation Accuracy: 0.9887442710137904\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.98      0.97      0.97      1837\n",
            "      B-MISC       0.91      0.94      0.92       922\n",
            "       B-ORG       0.95      0.94      0.95      1341\n",
            "       B-PER       0.98      0.99      0.98      1842\n",
            "       I-LOC       0.97      0.96      0.97      1801\n",
            "      I-MISC       0.86      0.88      0.87       935\n",
            "       I-ORG       0.95      0.94      0.95      2319\n",
            "       I-PER       0.99      0.99      0.99      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.95      0.96      0.95     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9889  0.9889  0.9889\n",
            "macro        0.9536  0.9560  0.9548\n",
            "weighted     0.9890  0.9889  0.9889\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.14116701542898227\n",
            "Test loss per 100 evaluation steps: 0.15312746215833614\n",
            "Test loss per 100 evaluation steps: 0.14248628115018316\n",
            "Test loss per 100 evaluation steps: 0.1442250489226808\n",
            "Test loss per 100 evaluation steps: 0.1382338212827126\n",
            "Test loss per 100 evaluation steps: 0.14450242420570247\n",
            "Test loss per 100 evaluation steps: 0.1556747077489243\n",
            "Test loss per 100 evaluation steps: 0.15642924119958593\n",
            "Test loss per 100 evaluation steps: 0.1524321439938088\n",
            "Test Loss: 0.15265704038477843\n",
            "Test Accuracy: 0.9747652115155615\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.95      0.94      0.94      1668\n",
            "      B-MISC       0.81      0.87      0.84       702\n",
            "       B-ORG       0.90      0.92      0.91      1661\n",
            "       B-PER       0.97      0.97      0.97      1617\n",
            "       I-LOC       0.93      0.91      0.92      1394\n",
            "      I-MISC       0.67      0.76      0.71       736\n",
            "       I-ORG       0.90      0.95      0.92      2804\n",
            "       I-PER       0.97      0.98      0.97      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.98     61486\n",
            "   macro avg       0.90      0.92      0.91     61486\n",
            "weighted avg       0.98      0.98      0.98     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9767  0.9767  0.9767\n",
            "macro        0.8997  0.9203  0.9094\n",
            "weighted     0.9776  0.9767  0.9770\n",
            "Test steps: 921\n",
            "Training epoch: 7\n",
            "Training loss per 100 training steps: 0.0071457922632907865\n",
            "Training loss per 100 training steps: 0.012535846541541105\n",
            "Training loss per 100 training steps: 0.010273081347004337\n",
            "Training loss per 100 training steps: 0.01162914423976872\n",
            "Training loss per 100 training steps: 0.010870465711785073\n",
            "Training loss per 100 training steps: 0.010315910690175466\n",
            "Training loss per 100 training steps: 0.010210434251657716\n",
            "Training loss per 100 training steps: 0.010625135450106881\n",
            "Training loss per 100 training steps: 0.010531266514328511\n",
            "Training loss per 100 training steps: 0.010306375431766355\n",
            "Training loss per 100 training steps: 0.01016082473504825\n",
            "Training loss per 100 training steps: 0.010145877492665628\n",
            "Training loss per 100 training steps: 0.010586068454728705\n",
            "Training loss per 100 training steps: 0.010061476305642308\n",
            "Training loss per 100 training steps: 0.010140083727290099\n",
            "Training loss per 100 training steps: 0.010326677262503666\n",
            "Training loss per 100 training steps: 0.010237600115175851\n",
            "Training loss per 100 training steps: 0.010227555687479505\n",
            "Training loss per 100 training steps: 0.009847915637423715\n",
            "Training loss per 100 training steps: 0.009753581898632775\n",
            "Training loss per 100 training steps: 0.009943969944503287\n",
            "Training loss per 100 training steps: 0.010090194305684525\n",
            "Training loss per 100 training steps: 0.009954598045783313\n",
            "Training loss per 100 training steps: 0.0096273639889381\n",
            "Training loss per 100 training steps: 0.009670302920668655\n",
            "Training loss per 100 training steps: 0.00946025651276841\n",
            "Training loss per 100 training steps: 0.009241597889844915\n",
            "Training loss per 100 training steps: 0.0090665895974811\n",
            "Training loss per 100 training steps: 0.009564057085592045\n",
            "Training loss per 100 training steps: 0.009739858822217836\n",
            "Training loss per 100 training steps: 0.009779427218980258\n",
            "Training loss per 100 training steps: 0.00967319562559041\n",
            "Training loss per 100 training steps: 0.009715493326177762\n",
            "Training loss per 100 training steps: 0.009723036313737072\n",
            "Training loss per 100 training steps: 0.010050904357956435\n",
            "Training loss per 100 training steps: 0.01005709788615238\n",
            "Training loss per 100 training steps: 0.009981530542119086\n",
            "Training loss epoch: 0.010052612935677809\n",
            "Training accuracy epoch: 0.9975573477840987\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.10058558025481033\n",
            "Validation loss per 100 evaluation steps: 0.07674961770252593\n",
            "Validation loss per 100 evaluation steps: 0.07176717073087274\n",
            "Validation loss per 100 evaluation steps: 0.07312253713450217\n",
            "Validation loss per 100 evaluation steps: 0.07002215094371422\n",
            "Validation loss per 100 evaluation steps: 0.06549441965524239\n",
            "Validation loss per 100 evaluation steps: 0.06709913172153911\n",
            "Validation loss per 100 evaluation steps: 0.06596548221626336\n",
            "Validation Loss: 0.06414731048230356\n",
            "Validation Accuracy: 0.989127024042233\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.98      0.98      1837\n",
            "      B-MISC       0.90      0.95      0.93       922\n",
            "       B-ORG       0.96      0.94      0.95      1341\n",
            "       B-PER       0.98      0.99      0.98      1842\n",
            "       I-LOC       0.96      0.98      0.97      1801\n",
            "      I-MISC       0.83      0.91      0.87       935\n",
            "       I-ORG       0.96      0.94      0.95      2319\n",
            "       I-PER       0.99      0.99      0.99      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.95      0.96      0.96     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9894  0.9894  0.9894\n",
            "macro        0.9505  0.9633  0.9566\n",
            "weighted     0.9896  0.9894  0.9895\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.17040132813509445\n",
            "Test loss per 100 evaluation steps: 0.18040586497853384\n",
            "Test loss per 100 evaluation steps: 0.17499906994137443\n",
            "Test loss per 100 evaluation steps: 0.18746129445054976\n",
            "Test loss per 100 evaluation steps: 0.1706699913354896\n",
            "Test loss per 100 evaluation steps: 0.1697682508602596\n",
            "Test loss per 100 evaluation steps: 0.16872590614136762\n",
            "Test loss per 100 evaluation steps: 0.170526529391542\n",
            "Test loss per 100 evaluation steps: 0.17522286123627712\n",
            "Test Loss: 0.1751254678954092\n",
            "Test Accuracy: 0.9744214209258238\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.93      0.94      0.93      1668\n",
            "      B-MISC       0.81      0.88      0.84       702\n",
            "       B-ORG       0.91      0.91      0.91      1661\n",
            "       B-PER       0.97      0.97      0.97      1617\n",
            "       I-LOC       0.86      0.94      0.90      1394\n",
            "      I-MISC       0.61      0.76      0.68       736\n",
            "       I-ORG       0.91      0.93      0.92      2804\n",
            "       I-PER       0.97      0.98      0.98      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.98     61486\n",
            "   macro avg       0.89      0.92      0.90     61486\n",
            "weighted avg       0.98      0.98      0.98     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9752  0.9752  0.9752\n",
            "macro        0.8850  0.9231  0.9027\n",
            "weighted     0.9771  0.9752  0.9760\n",
            "Test steps: 921\n",
            "====================================================================================================\n",
            "loss_name: dlite\n",
            "model configuration\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Ner_Model(\n",
            "  (model): DebertaModel(\n",
            "    (embeddings): DebertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "      (LayerNorm): DebertaLayerNorm()\n",
            "      (dropout): StableDropout()\n",
            "    )\n",
            "    (encoder): DebertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x DebertaLayer(\n",
            "          (attention): DebertaAttention(\n",
            "            (self): DisentangledSelfAttention(\n",
            "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
            "              (pos_dropout): StableDropout()\n",
            "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "            (output): DebertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): DebertaLayerNorm()\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "          )\n",
            "          (intermediate): DebertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): DebertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): DebertaLayerNorm()\n",
            "            (dropout): StableDropout()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (rel_embeddings): Embedding(1024, 768)\n",
            "    )\n",
            "  )\n",
            "  (linear_model): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=384, bias=True)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (classifier): Linear(in_features=384, out_features=9, bias=True)\n",
            ")\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.39271477080881595\n",
            "Training loss per 100 training steps: 0.3217502337694168\n",
            "Training loss per 100 training steps: 0.28658653508871795\n",
            "Training loss per 100 training steps: 0.2523313945310656\n",
            "Training loss per 100 training steps: 0.2236389668061165\n",
            "Training loss per 100 training steps: 0.20319474370829993\n",
            "Training loss per 100 training steps: 0.18561266703023907\n",
            "Training loss per 100 training steps: 0.16985499392256315\n",
            "Training loss per 100 training steps: 0.1574742290541245\n",
            "Training loss per 100 training steps: 0.14774460593430558\n",
            "Training loss per 100 training steps: 0.1389871371593009\n",
            "Training loss per 100 training steps: 0.13132876948337072\n",
            "Training loss per 100 training steps: 0.1249029566682517\n",
            "Training loss per 100 training steps: 0.11935885110836743\n",
            "Training loss per 100 training steps: 0.11402398196460369\n",
            "Training loss per 100 training steps: 0.10898977680768439\n",
            "Training loss per 100 training steps: 0.10449909369368791\n",
            "Training loss per 100 training steps: 0.10039214407164966\n",
            "Training loss per 100 training steps: 0.09628137833539911\n",
            "Training loss per 100 training steps: 0.093040848560222\n",
            "Training loss per 100 training steps: 0.08952951408832416\n",
            "Training loss per 100 training steps: 0.08674427429648858\n",
            "Training loss per 100 training steps: 0.08403274186905353\n",
            "Training loss per 100 training steps: 0.08156320064195977\n",
            "Training loss per 100 training steps: 0.07978723203590635\n",
            "Training loss per 100 training steps: 0.07814157491975912\n",
            "Training loss per 100 training steps: 0.07612125405768354\n",
            "Training loss per 100 training steps: 0.07423659586485752\n",
            "Training loss per 100 training steps: 0.07274883748642204\n",
            "Training loss per 100 training steps: 0.07116282443709497\n",
            "Training loss per 100 training steps: 0.06955189293670569\n",
            "Training loss per 100 training steps: 0.06824929162840362\n",
            "Training loss per 100 training steps: 0.06659624011694784\n",
            "Training loss per 100 training steps: 0.06527949276202943\n",
            "Training loss per 100 training steps: 0.06403985804191377\n",
            "Training loss per 100 training steps: 0.06276031101836199\n",
            "Training loss per 100 training steps: 0.061603104641041705\n",
            "Training loss epoch: 0.06114741061995279\n",
            "Training accuracy epoch: 0.9406986844986361\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.01775096418337853\n",
            "Validation loss per 100 evaluation steps: 0.01768706747982833\n",
            "Validation loss per 100 evaluation steps: 0.017124237615941337\n",
            "Validation loss per 100 evaluation steps: 0.01681874914373566\n",
            "Validation loss per 100 evaluation steps: 0.016385337097304727\n",
            "Validation loss per 100 evaluation steps: 0.01714820824994907\n",
            "Validation loss per 100 evaluation steps: 0.016899480156220566\n",
            "Validation loss per 100 evaluation steps: 0.01681280417552671\n",
            "Validation Loss: 0.01680039301196447\n",
            "Validation Accuracy: 0.9813184752409183\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.95      0.96      1837\n",
            "      B-MISC       0.89      0.89      0.89       922\n",
            "       B-ORG       0.91      0.92      0.91      1341\n",
            "       B-PER       0.98      0.97      0.97      1842\n",
            "       I-LOC       0.97      0.92      0.94      1801\n",
            "      I-MISC       0.76      0.85      0.80       935\n",
            "       I-ORG       0.88      0.91      0.90      2319\n",
            "       I-PER       0.98      0.98      0.98      4219\n",
            "           O       1.00      0.99      1.00     51041\n",
            "\n",
            "    accuracy                           0.98     66257\n",
            "   macro avg       0.93      0.93      0.93     66257\n",
            "weighted avg       0.98      0.98      0.98     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9820  0.9820  0.9820\n",
            "macro        0.9255  0.9334  0.9290\n",
            "weighted     0.9826  0.9820  0.9822\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.03139867425310513\n",
            "Test loss per 100 evaluation steps: 0.029346488765718277\n",
            "Test loss per 100 evaluation steps: 0.028329151405841912\n",
            "Test loss per 100 evaluation steps: 0.02957220292457123\n",
            "Test loss per 100 evaluation steps: 0.029189919403468594\n",
            "Test loss per 100 evaluation steps: 0.028534318302316328\n",
            "Test loss per 100 evaluation steps: 0.027876214996840645\n",
            "Test loss per 100 evaluation steps: 0.028541068540552033\n",
            "Test loss per 100 evaluation steps: 0.028081374407048415\n",
            "Test Loss: 0.028021392983078548\n",
            "Test Accuracy: 0.9699421980713515\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.94      0.92      0.93      1668\n",
            "      B-MISC       0.78      0.82      0.80       702\n",
            "       B-ORG       0.89      0.90      0.89      1661\n",
            "       B-PER       0.97      0.96      0.96      1617\n",
            "       I-LOC       0.92      0.90      0.91      1394\n",
            "      I-MISC       0.55      0.70      0.62       736\n",
            "       I-ORG       0.86      0.93      0.89      2804\n",
            "       I-PER       0.97      0.98      0.97      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.88      0.90      0.89     61486\n",
            "weighted avg       0.97      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9714  0.9714  0.9714\n",
            "macro        0.8754  0.8982  0.8856\n",
            "weighted     0.9734  0.9714  0.9722\n",
            "Test steps: 921\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.01811393057971145\n",
            "Training loss per 100 training steps: 0.01712107496954559\n",
            "Training loss per 100 training steps: 0.018346025460098947\n",
            "Training loss per 100 training steps: 0.01628461364055056\n",
            "Training loss per 100 training steps: 0.017542766987124196\n",
            "Training loss per 100 training steps: 0.01770794686595006\n",
            "Training loss per 100 training steps: 0.01707967444564539\n",
            "Training loss per 100 training steps: 0.018005978019745045\n",
            "Training loss per 100 training steps: 0.01766064972811465\n",
            "Training loss per 100 training steps: 0.01775360383261341\n",
            "Training loss per 100 training steps: 0.017395370755554144\n",
            "Training loss per 100 training steps: 0.01703897860263396\n",
            "Training loss per 100 training steps: 0.01695785332234104\n",
            "Training loss per 100 training steps: 0.01697909556607296\n",
            "Training loss per 100 training steps: 0.01747149792316486\n",
            "Training loss per 100 training steps: 0.017269906926629233\n",
            "Training loss per 100 training steps: 0.017428101281759475\n",
            "Training loss per 100 training steps: 0.01751027687699257\n",
            "Training loss per 100 training steps: 0.017313334655539286\n",
            "Training loss per 100 training steps: 0.01717094086937277\n",
            "Training loss per 100 training steps: 0.01701724989165043\n",
            "Training loss per 100 training steps: 0.01709529493934042\n",
            "Training loss per 100 training steps: 0.017144367300942695\n",
            "Training loss per 100 training steps: 0.017021855174338043\n",
            "Training loss per 100 training steps: 0.01705394305155387\n",
            "Training loss per 100 training steps: 0.016965812199596256\n",
            "Training loss per 100 training steps: 0.01689499316721426\n",
            "Training loss per 100 training steps: 0.016979806280404058\n",
            "Training loss per 100 training steps: 0.016996775837791387\n",
            "Training loss per 100 training steps: 0.016978314975610677\n",
            "Training loss per 100 training steps: 0.016810654968903105\n",
            "Training loss per 100 training steps: 0.016802535562466688\n",
            "Training loss per 100 training steps: 0.016820904692746642\n",
            "Training loss per 100 training steps: 0.0167918893593588\n",
            "Training loss per 100 training steps: 0.016640728522101588\n",
            "Training loss per 100 training steps: 0.01655501162315924\n",
            "Training loss per 100 training steps: 0.01669123967277264\n",
            "Training loss epoch: 0.016684219248921037\n",
            "Training accuracy epoch: 0.9820306531511398\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.012763143035949725\n",
            "Validation loss per 100 evaluation steps: 0.011662743593490177\n",
            "Validation loss per 100 evaluation steps: 0.012283536279608901\n",
            "Validation loss per 100 evaluation steps: 0.012664412335774387\n",
            "Validation loss per 100 evaluation steps: 0.012461375858193605\n",
            "Validation loss per 100 evaluation steps: 0.012772249461500754\n",
            "Validation loss per 100 evaluation steps: 0.012823480129152033\n",
            "Validation loss per 100 evaluation steps: 0.01256865399381013\n",
            "Validation Loss: 0.012930883368200755\n",
            "Validation Accuracy: 0.9861395110383986\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.96      0.96      1837\n",
            "      B-MISC       0.94      0.91      0.92       922\n",
            "       B-ORG       0.91      0.95      0.93      1341\n",
            "       B-PER       0.98      0.98      0.98      1842\n",
            "       I-LOC       0.96      0.94      0.95      1801\n",
            "      I-MISC       0.88      0.85      0.87       935\n",
            "       I-ORG       0.90      0.95      0.92      2319\n",
            "       I-PER       0.98      0.98      0.98      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.95      0.95      0.95     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9858  0.9858  0.9858\n",
            "macro        0.9462  0.9459  0.9459\n",
            "weighted     0.9860  0.9858  0.9859\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.025297203396185068\n",
            "Test loss per 100 evaluation steps: 0.025388226608311016\n",
            "Test loss per 100 evaluation steps: 0.0242570270346611\n",
            "Test loss per 100 evaluation steps: 0.025689940324809868\n",
            "Test loss per 100 evaluation steps: 0.026189865835112867\n",
            "Test loss per 100 evaluation steps: 0.026076197923184736\n",
            "Test loss per 100 evaluation steps: 0.025627865944344585\n",
            "Test loss per 100 evaluation steps: 0.026365243714702728\n",
            "Test loss per 100 evaluation steps: 0.02662065219210995\n",
            "Test Loss: 0.02666586120063391\n",
            "Test Accuracy: 0.9714695306995325\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.94      0.92      0.93      1668\n",
            "      B-MISC       0.83      0.81      0.82       702\n",
            "       B-ORG       0.88      0.92      0.90      1661\n",
            "       B-PER       0.97      0.96      0.96      1617\n",
            "       I-LOC       0.89      0.89      0.89      1394\n",
            "      I-MISC       0.64      0.64      0.64       736\n",
            "       I-ORG       0.87      0.94      0.90      2804\n",
            "       I-PER       0.97      0.98      0.98      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.89      0.90      0.89     61486\n",
            "weighted avg       0.97      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9733  0.9733  0.9733\n",
            "macro        0.8858  0.8960  0.8906\n",
            "weighted     0.9740  0.9733  0.9735\n",
            "Test steps: 921\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.013685085736742621\n",
            "Training loss per 100 training steps: 0.011298704299179008\n",
            "Training loss per 100 training steps: 0.010628439716541227\n",
            "Training loss per 100 training steps: 0.012090891120456035\n",
            "Training loss per 100 training steps: 0.012482491583075898\n",
            "Training loss per 100 training steps: 0.01309558202071609\n",
            "Training loss per 100 training steps: 0.013500080288146948\n",
            "Training loss per 100 training steps: 0.012971215224068829\n",
            "Training loss per 100 training steps: 0.012697008961108622\n",
            "Training loss per 100 training steps: 0.012629712456352535\n",
            "Training loss per 100 training steps: 0.012570777309476233\n",
            "Training loss per 100 training steps: 0.012617364562900852\n",
            "Training loss per 100 training steps: 0.012583414176841381\n",
            "Training loss per 100 training steps: 0.012082038526039339\n",
            "Training loss per 100 training steps: 0.012057082893098292\n",
            "Training loss per 100 training steps: 0.01218038443598914\n",
            "Training loss per 100 training steps: 0.011894230676150126\n",
            "Training loss per 100 training steps: 0.012015316407174497\n",
            "Training loss per 100 training steps: 0.012117120571314637\n",
            "Training loss per 100 training steps: 0.012089692592894608\n",
            "Training loss per 100 training steps: 0.012004621613910525\n",
            "Training loss per 100 training steps: 0.011909893842569503\n",
            "Training loss per 100 training steps: 0.011892704602930439\n",
            "Training loss per 100 training steps: 0.011892831056928609\n",
            "Training loss per 100 training steps: 0.011763736716098356\n",
            "Training loss per 100 training steps: 0.011795714279015697\n",
            "Training loss per 100 training steps: 0.011813916592209522\n",
            "Training loss per 100 training steps: 0.011858249215801331\n",
            "Training loss per 100 training steps: 0.011871889900532536\n",
            "Training loss per 100 training steps: 0.011840040074777713\n",
            "Training loss per 100 training steps: 0.01184448372515861\n",
            "Training loss per 100 training steps: 0.011941607684548786\n",
            "Training loss per 100 training steps: 0.011927224050411744\n",
            "Training loss per 100 training steps: 0.011889461497390745\n",
            "Training loss per 100 training steps: 0.011944114990052185\n",
            "Training loss per 100 training steps: 0.012008209911198265\n",
            "Training loss per 100 training steps: 0.011948507212889396\n",
            "Training loss epoch: 0.011900882679816706\n",
            "Training accuracy epoch: 0.9872703638383425\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.011481084576294051\n",
            "Validation loss per 100 evaluation steps: 0.010443577595515308\n",
            "Validation loss per 100 evaluation steps: 0.01150713835943255\n",
            "Validation loss per 100 evaluation steps: 0.012256329465149065\n",
            "Validation loss per 100 evaluation steps: 0.012077278973958983\n",
            "Validation loss per 100 evaluation steps: 0.011483079903638328\n",
            "Validation loss per 100 evaluation steps: 0.011217217101166495\n",
            "Validation loss per 100 evaluation steps: 0.011104793169719898\n",
            "Validation Loss: 0.011124886256303323\n",
            "Validation Accuracy: 0.9878401887334632\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.98      0.97      1837\n",
            "      B-MISC       0.92      0.92      0.92       922\n",
            "       B-ORG       0.95      0.94      0.94      1341\n",
            "       B-PER       0.97      0.98      0.98      1842\n",
            "       I-LOC       0.97      0.97      0.97      1801\n",
            "      I-MISC       0.87      0.86      0.86       935\n",
            "       I-ORG       0.96      0.93      0.94      2319\n",
            "       I-PER       0.98      0.99      0.98      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.95      0.95      0.95     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9878  0.9878  0.9878\n",
            "macro        0.9532  0.9518  0.9524\n",
            "weighted     0.9878  0.9878  0.9878\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.03352063205743889\n",
            "Test loss per 100 evaluation steps: 0.030497041480200552\n",
            "Test loss per 100 evaluation steps: 0.027002933736625892\n",
            "Test loss per 100 evaluation steps: 0.027195003776049588\n",
            "Test loss per 100 evaluation steps: 0.02759086201985201\n",
            "Test loss per 100 evaluation steps: 0.027398072038268613\n",
            "Test loss per 100 evaluation steps: 0.02724326074094506\n",
            "Test loss per 100 evaluation steps: 0.02694579085905616\n",
            "Test loss per 100 evaluation steps: 0.026622170521779176\n",
            "Test Loss: 0.026314896043604322\n",
            "Test Accuracy: 0.9726889776208052\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.94      0.94      0.94      1668\n",
            "      B-MISC       0.82      0.84      0.83       702\n",
            "       B-ORG       0.90      0.92      0.91      1661\n",
            "       B-PER       0.97      0.97      0.97      1617\n",
            "       I-LOC       0.90      0.91      0.91      1394\n",
            "      I-MISC       0.64      0.69      0.66       736\n",
            "       I-ORG       0.90      0.93      0.92      2804\n",
            "       I-PER       0.96      0.98      0.97      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.89      0.91      0.90     61486\n",
            "weighted avg       0.98      0.97      0.98     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9749  0.9749  0.9749\n",
            "macro        0.8920  0.9082  0.8999\n",
            "weighted     0.9756  0.9749  0.9752\n",
            "Test steps: 921\n",
            "Training epoch: 4\n",
            "Training loss per 100 training steps: 0.005810268424393143\n",
            "Training loss per 100 training steps: 0.007825618218987529\n",
            "Training loss per 100 training steps: 0.008334872611214147\n",
            "Training loss per 100 training steps: 0.009556376106634162\n",
            "Training loss per 100 training steps: 0.010113125069779925\n",
            "Training loss per 100 training steps: 0.009894106870004483\n",
            "Training loss per 100 training steps: 0.009501524202717389\n",
            "Training loss per 100 training steps: 0.009138205046174902\n",
            "Training loss per 100 training steps: 0.009106511684872204\n",
            "Training loss per 100 training steps: 0.008961354775318113\n",
            "Training loss per 100 training steps: 0.009015169327999294\n",
            "Training loss per 100 training steps: 0.009021634686804798\n",
            "Training loss per 100 training steps: 0.009164163406572138\n",
            "Training loss per 100 training steps: 0.009514183812057127\n",
            "Training loss per 100 training steps: 0.009420235928516074\n",
            "Training loss per 100 training steps: 0.009246016559719692\n",
            "Training loss per 100 training steps: 0.009199422414991899\n",
            "Training loss per 100 training steps: 0.009100766650586947\n",
            "Training loss per 100 training steps: 0.009136501142645677\n",
            "Training loss per 100 training steps: 0.008991453119282596\n",
            "Training loss per 100 training steps: 0.009066975789335283\n",
            "Training loss per 100 training steps: 0.00921951354558034\n",
            "Training loss per 100 training steps: 0.009299918813210841\n",
            "Training loss per 100 training steps: 0.009374396718408633\n",
            "Training loss per 100 training steps: 0.009481088038028293\n",
            "Training loss per 100 training steps: 0.009475894617716344\n",
            "Training loss per 100 training steps: 0.009319409086246852\n",
            "Training loss per 100 training steps: 0.00929991491911559\n",
            "Training loss per 100 training steps: 0.009378259459746742\n",
            "Training loss per 100 training steps: 0.00946985280075008\n",
            "Training loss per 100 training steps: 0.009646007444374875\n",
            "Training loss per 100 training steps: 0.009543896527938073\n",
            "Training loss per 100 training steps: 0.00961545496390031\n",
            "Training loss per 100 training steps: 0.009660848112693443\n",
            "Training loss per 100 training steps: 0.009718060895737056\n",
            "Training loss per 100 training steps: 0.009711628435769625\n",
            "Training loss per 100 training steps: 0.00971534343947693\n",
            "Training loss epoch: 0.009644674581408769\n",
            "Training accuracy epoch: 0.9896353792270657\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.011662203611795121\n",
            "Validation loss per 100 evaluation steps: 0.010455230588304403\n",
            "Validation loss per 100 evaluation steps: 0.009390608824314767\n",
            "Validation loss per 100 evaluation steps: 0.010734517201636323\n",
            "Validation loss per 100 evaluation steps: 0.010441766338474281\n",
            "Validation loss per 100 evaluation steps: 0.010425851805633405\n",
            "Validation loss per 100 evaluation steps: 0.010715114935642629\n",
            "Validation loss per 100 evaluation steps: 0.0112297098221282\n",
            "Validation Loss: 0.01108087245625071\n",
            "Validation Accuracy: 0.988095843018688\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.97      0.97      1837\n",
            "      B-MISC       0.92      0.93      0.93       922\n",
            "       B-ORG       0.95      0.95      0.95      1341\n",
            "       B-PER       0.98      0.99      0.98      1842\n",
            "       I-LOC       0.97      0.96      0.97      1801\n",
            "      I-MISC       0.88      0.87      0.87       935\n",
            "       I-ORG       0.95      0.93      0.94      2319\n",
            "       I-PER       0.98      0.99      0.98      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.95      0.95      0.95     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9883  0.9883  0.9883\n",
            "macro        0.9549  0.9535  0.9542\n",
            "weighted     0.9883  0.9883  0.9883\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.021158956280532947\n",
            "Test loss per 100 evaluation steps: 0.02307867201594661\n",
            "Test loss per 100 evaluation steps: 0.024625550025348653\n",
            "Test loss per 100 evaluation steps: 0.024106505550537064\n",
            "Test loss per 100 evaluation steps: 0.023176016978810138\n",
            "Test loss per 100 evaluation steps: 0.02297571982120947\n",
            "Test loss per 100 evaluation steps: 0.024154508672303662\n",
            "Test loss per 100 evaluation steps: 0.024815852602654617\n",
            "Test loss per 100 evaluation steps: 0.02482902006652921\n",
            "Test Loss: 0.024386438183533553\n",
            "Test Accuracy: 0.9745953063134545\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.94      0.93      0.94      1668\n",
            "      B-MISC       0.82      0.86      0.84       702\n",
            "       B-ORG       0.90      0.93      0.91      1661\n",
            "       B-PER       0.97      0.97      0.97      1617\n",
            "       I-LOC       0.89      0.91      0.90      1394\n",
            "      I-MISC       0.69      0.73      0.71       736\n",
            "       I-ORG       0.91      0.94      0.92      2804\n",
            "       I-PER       0.96      0.98      0.97      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.98     61486\n",
            "   macro avg       0.90      0.92      0.91     61486\n",
            "weighted avg       0.98      0.98      0.98     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9765  0.9765  0.9765\n",
            "macro        0.8968  0.9158  0.9061\n",
            "weighted     0.9771  0.9765  0.9768\n",
            "Test steps: 921\n",
            "Training epoch: 5\n",
            "Training loss per 100 training steps: 0.008082276298421221\n",
            "Training loss per 100 training steps: 0.0074848098158088305\n",
            "Training loss per 100 training steps: 0.0067771014416581465\n",
            "Training loss per 100 training steps: 0.006460459776075282\n",
            "Training loss per 100 training steps: 0.006704822117727531\n",
            "Training loss per 100 training steps: 0.00676077512468235\n",
            "Training loss per 100 training steps: 0.0069535362574160734\n",
            "Training loss per 100 training steps: 0.006741668296924032\n",
            "Training loss per 100 training steps: 0.006800350988645442\n",
            "Training loss per 100 training steps: 0.006855775553623232\n",
            "Training loss per 100 training steps: 0.006860568717174157\n",
            "Training loss per 100 training steps: 0.006783310831718978\n",
            "Training loss per 100 training steps: 0.007086024847062049\n",
            "Training loss per 100 training steps: 0.00733921090682555\n",
            "Training loss per 100 training steps: 0.007548890422758897\n",
            "Training loss per 100 training steps: 0.00761500981391233\n",
            "Training loss per 100 training steps: 0.007511002960336521\n",
            "Training loss per 100 training steps: 0.007441348878215529\n",
            "Training loss per 100 training steps: 0.0075983389827938425\n",
            "Training loss per 100 training steps: 0.007598223089468462\n",
            "Training loss per 100 training steps: 0.007506200386606132\n",
            "Training loss per 100 training steps: 0.0074342199742731\n",
            "Training loss per 100 training steps: 0.007277276281609538\n",
            "Training loss per 100 training steps: 0.007261498951357079\n",
            "Training loss per 100 training steps: 0.007354338063798718\n",
            "Training loss per 100 training steps: 0.007423816192385617\n",
            "Training loss per 100 training steps: 0.007405216772265986\n",
            "Training loss per 100 training steps: 0.007487161001035178\n",
            "Training loss per 100 training steps: 0.0075020925057740166\n",
            "Training loss per 100 training steps: 0.007467133029362818\n",
            "Training loss per 100 training steps: 0.007424161756217241\n",
            "Training loss per 100 training steps: 0.007463300214234199\n",
            "Training loss per 100 training steps: 0.007525998153470789\n",
            "Training loss per 100 training steps: 0.007423538598144249\n",
            "Training loss per 100 training steps: 0.007437983473758664\n",
            "Training loss per 100 training steps: 0.007421709694306757\n",
            "Training loss per 100 training steps: 0.00741564577130526\n",
            "Training loss epoch: 0.007511526207337237\n",
            "Training accuracy epoch: 0.9918477199110505\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.013932408177888647\n",
            "Validation loss per 100 evaluation steps: 0.010999769911974227\n",
            "Validation loss per 100 evaluation steps: 0.011807328100433855\n",
            "Validation loss per 100 evaluation steps: 0.012240689508432183\n",
            "Validation loss per 100 evaluation steps: 0.012210712288315108\n",
            "Validation loss per 100 evaluation steps: 0.012196978738084284\n",
            "Validation loss per 100 evaluation steps: 0.012333050355813302\n",
            "Validation loss per 100 evaluation steps: 0.012380217989068498\n",
            "Validation Loss: 0.012234829446723564\n",
            "Validation Accuracy: 0.9867845302595055\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.96      0.98      0.97      1837\n",
            "      B-MISC       0.92      0.94      0.93       922\n",
            "       B-ORG       0.97      0.90      0.93      1341\n",
            "       B-PER       0.95      0.99      0.97      1842\n",
            "       I-LOC       0.96      0.96      0.96      1801\n",
            "      I-MISC       0.87      0.89      0.88       935\n",
            "       I-ORG       0.97      0.90      0.94      2319\n",
            "       I-PER       0.96      0.99      0.98      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.95      0.95      0.95     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9873  0.9873  0.9873\n",
            "macro        0.9531  0.9501  0.9512\n",
            "weighted     0.9874  0.9873  0.9872\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.023773918113270155\n",
            "Test loss per 100 evaluation steps: 0.025213246479155488\n",
            "Test loss per 100 evaluation steps: 0.0258961786808815\n",
            "Test loss per 100 evaluation steps: 0.024179727845376534\n",
            "Test loss per 100 evaluation steps: 0.02505629683511279\n",
            "Test loss per 100 evaluation steps: 0.02642230310968292\n",
            "Test loss per 100 evaluation steps: 0.026753048195622568\n",
            "Test loss per 100 evaluation steps: 0.027549376006845774\n",
            "Test loss per 100 evaluation steps: 0.026405839270639782\n",
            "Test Loss: 0.026483880048098422\n",
            "Test Accuracy: 0.9719805399670494\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.93      0.94      0.94      1668\n",
            "      B-MISC       0.81      0.87      0.84       702\n",
            "       B-ORG       0.94      0.86      0.90      1661\n",
            "       B-PER       0.92      0.98      0.95      1617\n",
            "       I-LOC       0.90      0.90      0.90      1394\n",
            "      I-MISC       0.66      0.78      0.71       736\n",
            "       I-ORG       0.94      0.89      0.91      2804\n",
            "       I-PER       0.94      0.99      0.96      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.89      0.91      0.90     61486\n",
            "weighted avg       0.98      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9747  0.9747  0.9747\n",
            "macro        0.8929  0.9112  0.9009\n",
            "weighted     0.9757  0.9747  0.9750\n",
            "Test steps: 921\n",
            "Training epoch: 6\n",
            "Training loss per 100 training steps: 0.0035053116159178897\n",
            "Training loss per 100 training steps: 0.005188510981779473\n",
            "Training loss per 100 training steps: 0.005228947096632055\n",
            "Training loss per 100 training steps: 0.005026133480572668\n",
            "Training loss per 100 training steps: 0.004813856116643592\n",
            "Training loss per 100 training steps: 0.005265505641381007\n",
            "Training loss per 100 training steps: 0.005218600684769399\n",
            "Training loss per 100 training steps: 0.00544782586005212\n",
            "Training loss per 100 training steps: 0.005864218393989265\n",
            "Training loss per 100 training steps: 0.005795806524849411\n",
            "Training loss per 100 training steps: 0.005782441673238891\n",
            "Training loss per 100 training steps: 0.005630605113274451\n",
            "Training loss per 100 training steps: 0.005507263428504836\n",
            "Training loss per 100 training steps: 0.005607636637330294\n",
            "Training loss per 100 training steps: 0.005612127901387301\n",
            "Training loss per 100 training steps: 0.005664222728275909\n",
            "Training loss per 100 training steps: 0.005741102437379481\n",
            "Training loss per 100 training steps: 0.005896224646834014\n",
            "Training loss per 100 training steps: 0.006027032618589129\n",
            "Training loss per 100 training steps: 0.005995054986366142\n",
            "Training loss per 100 training steps: 0.006000061212245641\n",
            "Training loss per 100 training steps: 0.006178721249535112\n",
            "Training loss per 100 training steps: 0.006060577182994294\n",
            "Training loss per 100 training steps: 0.006041127377477456\n",
            "Training loss per 100 training steps: 0.006009285463446554\n",
            "Training loss per 100 training steps: 0.006050282734673719\n",
            "Training loss per 100 training steps: 0.005984537093800394\n",
            "Training loss per 100 training steps: 0.006159738325876844\n",
            "Training loss per 100 training steps: 0.0061065419999429505\n",
            "Training loss per 100 training steps: 0.005996983290900024\n",
            "Training loss per 100 training steps: 0.005999407159656621\n",
            "Training loss per 100 training steps: 0.006155489140828401\n",
            "Training loss per 100 training steps: 0.006110192696617415\n",
            "Training loss per 100 training steps: 0.006072783339160725\n",
            "Training loss per 100 training steps: 0.006059426699353383\n",
            "Training loss per 100 training steps: 0.0060415771685588815\n",
            "Training loss per 100 training steps: 0.005978429584361253\n",
            "Training loss epoch: 0.006009103447178051\n",
            "Training accuracy epoch: 0.9936807232203024\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.009398839723037894\n",
            "Validation loss per 100 evaluation steps: 0.011300336060491247\n",
            "Validation loss per 100 evaluation steps: 0.010231923350841044\n",
            "Validation loss per 100 evaluation steps: 0.011191387167712038\n",
            "Validation loss per 100 evaluation steps: 0.011162469051769222\n",
            "Validation loss per 100 evaluation steps: 0.011229930253912765\n",
            "Validation loss per 100 evaluation steps: 0.01114070976178758\n",
            "Validation loss per 100 evaluation steps: 0.01080250053257501\n",
            "Validation Loss: 0.010982719549810671\n",
            "Validation Accuracy: 0.9881922486705709\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.99      0.95      0.97      1837\n",
            "      B-MISC       0.92      0.93      0.93       922\n",
            "       B-ORG       0.93      0.95      0.94      1341\n",
            "       B-PER       0.97      0.99      0.98      1842\n",
            "       I-LOC       0.98      0.95      0.96      1801\n",
            "      I-MISC       0.87      0.91      0.89       935\n",
            "       I-ORG       0.94      0.93      0.94      2319\n",
            "       I-PER       0.97      0.99      0.98      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.95      0.96      0.95     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9882  0.9882  0.9882\n",
            "macro        0.9528  0.9560  0.9543\n",
            "weighted     0.9884  0.9882  0.9883\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.01858654658654359\n",
            "Test loss per 100 evaluation steps: 0.025461156363722922\n",
            "Test loss per 100 evaluation steps: 0.023759231549447007\n",
            "Test loss per 100 evaluation steps: 0.02242718680622957\n",
            "Test loss per 100 evaluation steps: 0.02314066645603617\n",
            "Test loss per 100 evaluation steps: 0.023462250790611705\n",
            "Test loss per 100 evaluation steps: 0.023289556078545024\n",
            "Test loss per 100 evaluation steps: 0.023653932746679266\n",
            "Test loss per 100 evaluation steps: 0.024206143142691778\n",
            "Test Loss: 0.023891015063929523\n",
            "Test Accuracy: 0.9749784232860423\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.91      0.94      1668\n",
            "      B-MISC       0.83      0.87      0.85       702\n",
            "       B-ORG       0.89      0.94      0.91      1661\n",
            "       B-PER       0.96      0.97      0.97      1617\n",
            "       I-LOC       0.94      0.89      0.91      1394\n",
            "      I-MISC       0.62      0.77      0.69       736\n",
            "       I-ORG       0.90      0.94      0.92      2804\n",
            "       I-PER       0.96      0.99      0.97      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.98     61486\n",
            "   macro avg       0.89      0.92      0.91     61486\n",
            "weighted avg       0.98      0.98      0.98     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9758  0.9758  0.9758\n",
            "macro        0.8945  0.9189  0.9053\n",
            "weighted     0.9774  0.9758  0.9764\n",
            "Test steps: 921\n",
            "Training epoch: 7\n",
            "Training loss per 100 training steps: 0.005174930178562818\n",
            "Training loss per 100 training steps: 0.006526990019804941\n",
            "Training loss per 100 training steps: 0.007274759943674288\n",
            "Training loss per 100 training steps: 0.006431119094005595\n",
            "Training loss per 100 training steps: 0.006070476755532241\n",
            "Training loss per 100 training steps: 0.005478034242641741\n",
            "Training loss per 100 training steps: 0.006069730381640852\n",
            "Training loss per 100 training steps: 0.006248765481288174\n",
            "Training loss per 100 training steps: 0.006154898299891821\n",
            "Training loss per 100 training steps: 0.006314477449610308\n",
            "Training loss per 100 training steps: 0.0063377990754240315\n",
            "Training loss per 100 training steps: 0.006242611663471299\n",
            "Training loss per 100 training steps: 0.006073806289475757\n",
            "Training loss per 100 training steps: 0.006139181675186721\n",
            "Training loss per 100 training steps: 0.00604290823662878\n",
            "Training loss per 100 training steps: 0.005992113574057037\n",
            "Training loss per 100 training steps: 0.006002900233231718\n",
            "Training loss per 100 training steps: 0.00606260859672378\n",
            "Training loss per 100 training steps: 0.005990098399192604\n",
            "Training loss per 100 training steps: 0.0061476629845923295\n",
            "Training loss per 100 training steps: 0.006167958322315436\n",
            "Training loss per 100 training steps: 0.006116201132580375\n",
            "Training loss per 100 training steps: 0.006094825012083161\n",
            "Training loss per 100 training steps: 0.005979700539433767\n",
            "Training loss per 100 training steps: 0.005809291465404624\n",
            "Training loss per 100 training steps: 0.005738379913723566\n",
            "Training loss per 100 training steps: 0.005720644739950021\n",
            "Training loss per 100 training steps: 0.005711744887792339\n",
            "Training loss per 100 training steps: 0.005719714192824523\n",
            "Training loss per 100 training steps: 0.005675518014854703\n",
            "Training loss per 100 training steps: 0.005641718514155743\n",
            "Training loss per 100 training steps: 0.005588103594986356\n",
            "Training loss per 100 training steps: 0.005719950680373477\n",
            "Training loss per 100 training steps: 0.005737243837153493\n",
            "Training loss per 100 training steps: 0.0057353742028756595\n",
            "Training loss per 100 training steps: 0.005723669016305411\n",
            "Training loss per 100 training steps: 0.0056436445702408074\n",
            "Training loss epoch: 0.005598513861738188\n",
            "Training accuracy epoch: 0.9941163393903933\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.010158249027224429\n",
            "Validation loss per 100 evaluation steps: 0.010671038954570803\n",
            "Validation loss per 100 evaluation steps: 0.010263524658788489\n",
            "Validation loss per 100 evaluation steps: 0.010578022003853604\n",
            "Validation loss per 100 evaluation steps: 0.010430754045055722\n",
            "Validation loss per 100 evaluation steps: 0.010676337932831785\n",
            "Validation loss per 100 evaluation steps: 0.010647101602183276\n",
            "Validation loss per 100 evaluation steps: 0.010601297052677872\n",
            "Validation Loss: 0.010831879007470348\n",
            "Validation Accuracy: 0.9883425738364288\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.98      0.97      1837\n",
            "      B-MISC       0.93      0.93      0.93       922\n",
            "       B-ORG       0.95      0.94      0.94      1341\n",
            "       B-PER       0.98      0.98      0.98      1842\n",
            "       I-LOC       0.96      0.97      0.97      1801\n",
            "      I-MISC       0.89      0.84      0.87       935\n",
            "       I-ORG       0.94      0.94      0.94      2319\n",
            "       I-PER       0.98      0.99      0.98      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.96      0.95      0.95     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9879  0.9879  0.9879\n",
            "macro        0.9551  0.9505  0.9527\n",
            "weighted     0.9878  0.9879  0.9878\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.030569743091006102\n",
            "Test loss per 100 evaluation steps: 0.027577530145730975\n",
            "Test loss per 100 evaluation steps: 0.024632111095139634\n",
            "Test loss per 100 evaluation steps: 0.025699658085606883\n",
            "Test loss per 100 evaluation steps: 0.024150811628891405\n",
            "Test loss per 100 evaluation steps: 0.024385135576619536\n",
            "Test loss per 100 evaluation steps: 0.024437697048647837\n",
            "Test loss per 100 evaluation steps: 0.02413397543193913\n",
            "Test loss per 100 evaluation steps: 0.02583087680101408\n",
            "Test Loss: 0.025924879542680506\n",
            "Test Accuracy: 0.9731446865509177\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.94      0.94      0.94      1668\n",
            "      B-MISC       0.84      0.85      0.84       702\n",
            "       B-ORG       0.90      0.92      0.91      1661\n",
            "       B-PER       0.96      0.97      0.96      1617\n",
            "       I-LOC       0.89      0.93      0.91      1394\n",
            "      I-MISC       0.67      0.69      0.68       736\n",
            "       I-ORG       0.89      0.94      0.91      2804\n",
            "       I-PER       0.96      0.98      0.97      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.98     61486\n",
            "   macro avg       0.89      0.91      0.90     61486\n",
            "weighted avg       0.98      0.98      0.98     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9753  0.9753  0.9753\n",
            "macro        0.8933  0.9123  0.9026\n",
            "weighted     0.9759  0.9753  0.9755\n",
            "Test steps: 921\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "def train(config,loss_name):\n",
        "    print(\"=\" * 100)\n",
        "    print(f\"loss_name: {loss_name}\")\n",
        "    model = Ner_Model(config, len(label2id), loss_name).to(config.device)\n",
        "    optimizer = get_optimizer(model, config)\n",
        "\n",
        "    valid_each_label_p_r_f1_list = []\n",
        "    valid_p_r_f1_list = []\n",
        "    test_each_label_p_r_f1_list = []\n",
        "    test_p_r_f1_list = []\n",
        "\n",
        "    valid_loss_list = []\n",
        "    test_loss_list = []\n",
        "\n",
        "    model.train()\n",
        "    interval = 100\n",
        "    for epoch in range(config.epochs):\n",
        "        print(f\"Training epoch: {epoch + 1}\")\n",
        "        tr_preds,tr_labels = [], []\n",
        "        total_loss = 0.0\n",
        "        tr_accuracy = 0.0\n",
        "        # print(\"\\n\" + \"-\" * 30 + \"\\n\")\n",
        "        # print(f\"epoch: {epoch},  train dataloader size: {len(train_dataloader)}\")\n",
        "        # print(f\"epoch: {epoch},  valid dataloader size: {len(valid_dataloader)}\")\n",
        "        # print(f\"epoch: {epoch},  test dataloader size: {len(test_dataloader)}\")\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            attention_mask = batch[\"id\"].ne(0)\n",
        "            targets = batch['label_id']\n",
        "            loss, logit= model(batch[\"id\"], batch['seq_length'], attention_mask=attention_mask,\n",
        "                                             labels=targets)\n",
        "\n",
        "            # compute training accuracy\n",
        "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = logit.view(-1, len(label2id)) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            active_accuracy = flattened_targets.ne(-100) # active accuracy is also of shape (batch_size * seq_len,)\n",
        "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "            tr_accuracy += tmp_tr_accuracy\n",
        "            tr_preds.extend(predictions)\n",
        "            tr_labels.extend(targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            if (step + 1) % interval == 0:\n",
        "                print(f\"Training loss per 100 training steps: {total_loss / (step+1)}\")\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        print(f\"Training loss epoch: {total_loss / (step+1)}\")\n",
        "        print(f\"Training accuracy epoch: {tr_accuracy / (step+1)}\")\n",
        "        print(f\"Training steps: {step+1}\")\n",
        "        print(\"\\n\\n\")\n",
        "        model.eval()\n",
        "\n",
        "\n",
        "        valid_loss, valid_p_r_f1,  valid_each_label_p_r_f1 = evaluate(model,valid_dataloader, \"Validation\")\n",
        "        valid_loss_list.append(valid_loss)\n",
        "        valid_p_r_f1_list.append(valid_p_r_f1)\n",
        "        valid_each_label_p_r_f1_list.append(valid_each_label_p_r_f1)\n",
        "\n",
        "        print(\"\\n\\n\")\n",
        "        test_loss, test_p_r_f1,test_each_label_p_r_f1  = evaluate(model,test_dataloader, \"Test\")\n",
        "        test_loss_list.append(test_loss)\n",
        "        test_p_r_f1_list.append(test_p_r_f1)\n",
        "        test_each_label_p_r_f1_list.append(test_each_label_p_r_f1)\n",
        "\n",
        "\n",
        "        #print(f\"epoch: {epoch}, train_loss: {train_loss}, \\n{train_p_r_f1}\")\n",
        "        #print(f\"epoch: {epoch}, valid_loss: {valid_loss}, \\n{valid_p_r_f1}\")\n",
        "        #print(f\"epoch: {epoch}, test_loss: {test_loss},  \\n {test_p_r_f1}\")\n",
        "        model.train()\n",
        "    return   {\n",
        "              \"valid_loss_list\":valid_loss_list,\n",
        "              \"test_loss_list\":test_loss_list,\n",
        "\n",
        "              \"valid_p_r_f1_list\":valid_p_r_f1_list,\n",
        "              \"valid_each_label_p_r_f1_list\":valid_each_label_p_r_f1_list,\n",
        "\n",
        "              \"test_p_r_f1_list\":test_p_r_f1_list,\n",
        "              \"test_each_label_p_r_f1_list\": test_each_label_p_r_f1_list}\n",
        "\n",
        "\n",
        "result = {}\n",
        "for loss_name in ['l1', 'l2', 'ce', 'kl', 'dlite']:\n",
        "    r = train(Config, loss_name)\n",
        "    result[loss_name] = r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": [],
        "id": "PS1O-MxfGr33"
      },
      "source": [
        "## Result Comparison after cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ciR0WOCJGr33"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(\"result.pkl\", \"wb\") as f:\n",
        "    pickle.dump(result, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYodcFxGGr33"
      },
      "source": [
        "#### Overall Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgKkKD11Gr33",
        "outputId": "be42c111-adf5-4b5d-983b-e3ab62d8e46e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "micro\n",
            "    loss  precision  recall      f1\n",
            "0     l1     0.9745  0.9745  0.9745\n",
            "1     l2     0.9740  0.9740  0.9740\n",
            "2     ce     0.9724  0.9724  0.9724\n",
            "3     kl     0.9752  0.9752  0.9752\n",
            "4  dlite     0.9753  0.9753  0.9753\n",
            "====================================================================================================\n",
            "macro\n",
            "    loss  precision  recall      f1\n",
            "0     l1     0.9036  0.8974  0.8999\n",
            "1     l2     0.8876  0.9140  0.9001\n",
            "2     ce     0.8829  0.9067  0.8940\n",
            "3     kl     0.8850  0.9231  0.9027\n",
            "4  dlite     0.8933  0.9123  0.9026\n",
            "====================================================================================================\n",
            "weighted\n",
            "    loss  precision  recall      f1\n",
            "0     l1     0.9748  0.9745  0.9745\n",
            "1     l2     0.9755  0.9740  0.9746\n",
            "2     ce     0.9740  0.9724  0.9730\n",
            "3     kl     0.9771  0.9752  0.9760\n",
            "4  dlite     0.9759  0.9753  0.9755\n"
          ]
        }
      ],
      "source": [
        "columns = ['loss', 'precision', 'recall', 'f1']\n",
        "for t in ['micro', 'macro', 'weighted']:\n",
        "    df = []\n",
        "    for loss_name in loss_list:\n",
        "        row = {'loss': loss_name}\n",
        "        row['precision'] = result[loss_name]['test_p_r_f1_list'][-1].loc[t, 'precision']\n",
        "        row['recall'] = result[loss_name]['test_p_r_f1_list'][-1].loc[t, 'recall']\n",
        "        row['f1'] = result[loss_name]['test_p_r_f1_list'][-1].loc[t, 'f1']\n",
        "        df.append(row)\n",
        "    print(\"=\"*100)\n",
        "    print(t)\n",
        "    print(pd.DataFrame(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKeZYzqvGr33"
      },
      "source": [
        "#### Each label Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejx5I88-Gr34",
        "outputId": "c192d522-b4f1-49f3-e625-fed3c1d5f5c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test dataset\n",
            "--------------------------------------------------\n",
            "l1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.94      0.93      0.93      1668\n",
            "      B-MISC       0.86      0.83      0.85       702\n",
            "       B-ORG       0.87      0.93      0.90      1661\n",
            "       B-PER       0.97      0.93      0.95      1617\n",
            "       I-LOC       0.91      0.90      0.90      1394\n",
            "      I-MISC       0.76      0.68      0.72       736\n",
            "       I-ORG       0.86      0.94      0.90      2804\n",
            "       I-PER       0.97      0.95      0.96      3810\n",
            "           O       0.99      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.90      0.90      0.90     61486\n",
            "weighted avg       0.97      0.97      0.97     61486\n",
            "\n",
            "--------------------------------------------------\n",
            "l2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.94      0.94      0.94      1668\n",
            "      B-MISC       0.83      0.85      0.84       702\n",
            "       B-ORG       0.88      0.94      0.91      1661\n",
            "       B-PER       0.97      0.94      0.96      1617\n",
            "       I-LOC       0.88      0.94      0.91      1394\n",
            "      I-MISC       0.66      0.73      0.69       736\n",
            "       I-ORG       0.86      0.96      0.91      2804\n",
            "       I-PER       0.97      0.95      0.96      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.89      0.91      0.90     61486\n",
            "weighted avg       0.98      0.97      0.97     61486\n",
            "\n",
            "--------------------------------------------------\n",
            "ce\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.93      0.94      0.94      1668\n",
            "      B-MISC       0.82      0.85      0.83       702\n",
            "       B-ORG       0.87      0.92      0.90      1661\n",
            "       B-PER       0.98      0.93      0.96      1617\n",
            "       I-LOC       0.87      0.93      0.90      1394\n",
            "      I-MISC       0.65      0.71      0.68       736\n",
            "       I-ORG       0.85      0.95      0.90      2804\n",
            "       I-PER       0.98      0.94      0.96      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.88      0.91      0.89     61486\n",
            "weighted avg       0.97      0.97      0.97     61486\n",
            "\n",
            "--------------------------------------------------\n",
            "kl\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.93      0.94      0.93      1668\n",
            "      B-MISC       0.81      0.88      0.84       702\n",
            "       B-ORG       0.91      0.91      0.91      1661\n",
            "       B-PER       0.97      0.97      0.97      1617\n",
            "       I-LOC       0.86      0.94      0.90      1394\n",
            "      I-MISC       0.61      0.76      0.68       736\n",
            "       I-ORG       0.91      0.93      0.92      2804\n",
            "       I-PER       0.97      0.98      0.98      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.98     61486\n",
            "   macro avg       0.89      0.92      0.90     61486\n",
            "weighted avg       0.98      0.98      0.98     61486\n",
            "\n",
            "--------------------------------------------------\n",
            "dlite\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.94      0.94      0.94      1668\n",
            "      B-MISC       0.84      0.85      0.84       702\n",
            "       B-ORG       0.90      0.92      0.91      1661\n",
            "       B-PER       0.96      0.97      0.96      1617\n",
            "       I-LOC       0.89      0.93      0.91      1394\n",
            "      I-MISC       0.67      0.69      0.68       736\n",
            "       I-ORG       0.89      0.94      0.91      2804\n",
            "       I-PER       0.96      0.98      0.97      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.98     61486\n",
            "   macro avg       0.89      0.91      0.90     61486\n",
            "weighted avg       0.98      0.98      0.98     61486\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"test dataset\")\n",
        "for loss_name in loss_list:\n",
        "    print(\"-\"*50)\n",
        "    print(loss_name)\n",
        "    print(result[loss_name]['test_each_label_p_r_f1_list'][-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqfxwsPKGr38"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0127a4d474ed4d04a1a64ddbac8e3055": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_558cd186fc6c47f5a62de3d3f2b1243e",
              "IPY_MODEL_45ca6b93a1f34eb2952f0b8a1a2b30e3",
              "IPY_MODEL_6318c45215274aab93562697f5af7d14"
            ],
            "layout": "IPY_MODEL_9b499b7adee74b0fbf160f7ed3fd83c5"
          }
        },
        "558cd186fc6c47f5a62de3d3f2b1243e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_301a28c81fba4a1a93c55d8deac3d9f8",
            "placeholder": "​",
            "style": "IPY_MODEL_cabcba8534fb4826b15aa1b4b6b79d83",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "45ca6b93a1f34eb2952f0b8a1a2b30e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83468ddec8a6451faaa90790f848849f",
            "max": 52,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_743aaa1f68584d17a00b293d20ec571d",
            "value": 52
          }
        },
        "6318c45215274aab93562697f5af7d14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf65d25d68ab4154af7177ba09b33f55",
            "placeholder": "​",
            "style": "IPY_MODEL_35b6ef271c6444e2b1c3e0b15be38d4c",
            "value": " 52.0/52.0 [00:00&lt;00:00, 4.27kB/s]"
          }
        },
        "9b499b7adee74b0fbf160f7ed3fd83c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "301a28c81fba4a1a93c55d8deac3d9f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cabcba8534fb4826b15aa1b4b6b79d83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83468ddec8a6451faaa90790f848849f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "743aaa1f68584d17a00b293d20ec571d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf65d25d68ab4154af7177ba09b33f55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35b6ef271c6444e2b1c3e0b15be38d4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca634021192b431fbcd969199c784f36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85c21f4e95cc4a75b56cb37b83d854c2",
              "IPY_MODEL_9323f740402c4dbc99731bf17a1b4888",
              "IPY_MODEL_095034b99ded45c5b920dcf82e760937"
            ],
            "layout": "IPY_MODEL_051b2c7b6518400d8e5dedcd22b2611b"
          }
        },
        "85c21f4e95cc4a75b56cb37b83d854c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37fc3aa0f426460e9b7ea73817b85c4f",
            "placeholder": "​",
            "style": "IPY_MODEL_7d637801cade4a29ae6e0f85ea824725",
            "value": "config.json: 100%"
          }
        },
        "9323f740402c4dbc99731bf17a1b4888": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3256ab20d314f1b8b2846e6fb665b45",
            "max": 474,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_989bede1a8124febbe8b9ab3931c8cc3",
            "value": 474
          }
        },
        "095034b99ded45c5b920dcf82e760937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2267fab0630b4ad28b859b8a4356d0f1",
            "placeholder": "​",
            "style": "IPY_MODEL_ac2618da9b1b4f04b6c53bbc275fdb26",
            "value": " 474/474 [00:00&lt;00:00, 42.5kB/s]"
          }
        },
        "051b2c7b6518400d8e5dedcd22b2611b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37fc3aa0f426460e9b7ea73817b85c4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d637801cade4a29ae6e0f85ea824725": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3256ab20d314f1b8b2846e6fb665b45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "989bede1a8124febbe8b9ab3931c8cc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2267fab0630b4ad28b859b8a4356d0f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac2618da9b1b4f04b6c53bbc275fdb26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf96a3feb3f5419ebdf0fc9b7a4d4f9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3d0640daeb14b2bb3f722ab4d3ef656",
              "IPY_MODEL_08303bf7dea045ee8b815a3e58a68fe8",
              "IPY_MODEL_02b8a9b538d540e69bda82b0c5f0843a"
            ],
            "layout": "IPY_MODEL_ff28928c34d5420b9f49b8df4f02f2cb"
          }
        },
        "f3d0640daeb14b2bb3f722ab4d3ef656": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97195ad2e3b640cb8d4b4f6f13e6b037",
            "placeholder": "​",
            "style": "IPY_MODEL_c44c1c63fe144b54b7e72e7ae0fe9ce8",
            "value": "vocab.json: 100%"
          }
        },
        "08303bf7dea045ee8b815a3e58a68fe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e70b2ad48eeb4bb891ab152bf61df125",
            "max": 898825,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2291b57456de4c4791af1655c1d6befd",
            "value": 898825
          }
        },
        "02b8a9b538d540e69bda82b0c5f0843a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e919f4ce90574462845871a3c5c0f1ae",
            "placeholder": "​",
            "style": "IPY_MODEL_c757662152224d969921af41728fb531",
            "value": " 899k/899k [00:00&lt;00:00, 2.02MB/s]"
          }
        },
        "ff28928c34d5420b9f49b8df4f02f2cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97195ad2e3b640cb8d4b4f6f13e6b037": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c44c1c63fe144b54b7e72e7ae0fe9ce8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e70b2ad48eeb4bb891ab152bf61df125": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2291b57456de4c4791af1655c1d6befd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e919f4ce90574462845871a3c5c0f1ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c757662152224d969921af41728fb531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abb4dc5fbc784f57b6459074385a8451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb8f35431bca4cc48f5a8fc64f2aa4f3",
              "IPY_MODEL_86a84530a19d4fb7bc12251dce5123a4",
              "IPY_MODEL_e3fe8ad0cadc462898f3a1990d463afe"
            ],
            "layout": "IPY_MODEL_d67b6d0c47004031bca1f5373293e964"
          }
        },
        "eb8f35431bca4cc48f5a8fc64f2aa4f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a51811b109bd40c4a17b3d6a223c521e",
            "placeholder": "​",
            "style": "IPY_MODEL_e56003a0abca408c8e1b59080d0cefe9",
            "value": "merges.txt: 100%"
          }
        },
        "86a84530a19d4fb7bc12251dce5123a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c10162c02cdb460c9b860b3c7ca5b786",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03c46eb119eb420ba150e87a970c443b",
            "value": 456318
          }
        },
        "e3fe8ad0cadc462898f3a1990d463afe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a90e894d8544345a4a0d7628864c1ad",
            "placeholder": "​",
            "style": "IPY_MODEL_8f4dbd2d81094767b0c09ced78f95c68",
            "value": " 456k/456k [00:00&lt;00:00, 2.13MB/s]"
          }
        },
        "d67b6d0c47004031bca1f5373293e964": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a51811b109bd40c4a17b3d6a223c521e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e56003a0abca408c8e1b59080d0cefe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c10162c02cdb460c9b860b3c7ca5b786": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03c46eb119eb420ba150e87a970c443b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2a90e894d8544345a4a0d7628864c1ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f4dbd2d81094767b0c09ced78f95c68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b41425bb20e44c828a1ad53159045ba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_87d37cfa8e7e4fb7a4e65485ba744d4d",
              "IPY_MODEL_6e24fa9dcae841ce9632d81eec70dc7d",
              "IPY_MODEL_dca0ba7b84ed4f3988219b431f5bd941"
            ],
            "layout": "IPY_MODEL_978f50ab9bed43188f7d585f037a7950"
          }
        },
        "87d37cfa8e7e4fb7a4e65485ba744d4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58e5b9a6fd0e4c57aff8ebb4c62f975a",
            "placeholder": "​",
            "style": "IPY_MODEL_29c8a38d87f54502ae782eed00417bf7",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "6e24fa9dcae841ce9632d81eec70dc7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_800eff5c6d7344edaa682c274c3d8bc0",
            "max": 558614189,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e4fdf274c154d60a525239131852f4d",
            "value": 558614189
          }
        },
        "dca0ba7b84ed4f3988219b431f5bd941": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7630259ba4d46e69808b211ba732b30",
            "placeholder": "​",
            "style": "IPY_MODEL_6615d34abe47424798691a942c3792c6",
            "value": " 559M/559M [00:35&lt;00:00, 14.4MB/s]"
          }
        },
        "978f50ab9bed43188f7d585f037a7950": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58e5b9a6fd0e4c57aff8ebb4c62f975a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29c8a38d87f54502ae782eed00417bf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "800eff5c6d7344edaa682c274c3d8bc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e4fdf274c154d60a525239131852f4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7630259ba4d46e69808b211ba732b30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6615d34abe47424798691a942c3792c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}