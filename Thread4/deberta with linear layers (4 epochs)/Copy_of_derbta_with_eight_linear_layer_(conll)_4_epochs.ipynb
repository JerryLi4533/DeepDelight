{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEBdvK_nGr3w"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import classification_report\n",
        "import random ,json\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, Dataset\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hskApG2OGr3x"
      },
      "source": [
        "## Setting Basic Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlnriKCUGr3y"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    batch_size = 4\n",
        "    epochs = 4\n",
        "    lr = 1e-5\n",
        "    seed = 123\n",
        "\n",
        "\n",
        "    # if Linear layer is empty, then not using the linear layer. Setting the linear layer based on layers number required. (can customized)\n",
        "    linear_layer = [512, 384, 256, 128, 128, 128, 128, 128]\n",
        "    # Setting the linear layer number\n",
        "    linear_layer_num = 8\n",
        "    # if lstm layer is 0, then not using the lstm layer. Setting the lstm layer based on layers number required\n",
        "    lstm_layer_num = 0\n",
        "    bi_lstm=True\n",
        "\n",
        "\n",
        "    # Internet resource; download from Internet\n",
        "    # model_name = \"microsoft/deberta-v3-base\"\n",
        "\n",
        "    model_name = \"microsoft/deberta-base\"\n",
        "\n",
        "    hidden_size=768\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_data_name = \"conll2003\"\n",
        "\n",
        "    @classmethod\n",
        "    def describe(cls):\n",
        "        parm = {\"train_data_name\": cls.train_data_name,\n",
        "                \"encoder_name\": cls.model_name,\n",
        "                \"batch_size\": cls.batch_size,\n",
        "                \"epochs\": cls.epochs,\n",
        "                \"lr\": cls.lr,\n",
        "                \"seed\": cls.seed,\n",
        "                \"bi_lstm\": cls.bi_lstm,\n",
        "                \"lstm_layer_num\": cls.lstm_layer_num,\n",
        "                \"linear_layer\": cls.linear_layer,\n",
        "                \"linear_layer_num\":cls.linear_layer_num}\n",
        "        return json.dumps(parm , ensure_ascii=False, indent=2)\n",
        "\n",
        "random.seed(Config.seed)\n",
        "np.random.seed(Config.seed)\n",
        "torch.manual_seed(Config.seed)\n",
        "torch.cuda.manual_seed_all(Config.seed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQGJgAySrwEr"
      },
      "source": [
        "## given configuration result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BObln4RIGr3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8354e4c2-ecbb-4b3d-a26c-507fbd1934d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"train_data_name\": \"conll2003\",\n",
            "  \"encoder_name\": \"microsoft/deberta-base\",\n",
            "  \"batch_size\": 4,\n",
            "  \"epochs\": 4,\n",
            "  \"lr\": 1e-05,\n",
            "  \"seed\": 123,\n",
            "  \"bi_lstm\": true,\n",
            "  \"lstm_layer_num\": 0,\n",
            "  \"linear_layer\": [\n",
            "    512,\n",
            "    384,\n",
            "    256,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128,\n",
            "    128\n",
            "  ],\n",
            "  \"linear_layer_num\": 8\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(Config.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9ig_SZXGr3z"
      },
      "source": [
        "## Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGZ_d-HrGr3z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dd40fed-b592-4d08-8109-6521cb06f383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14986, 2) (3465, 2) (3683, 2)\n"
          ]
        }
      ],
      "source": [
        "def read_conll2003(file_path):\n",
        "    data = []\n",
        "    sample = []\n",
        "    for idx, line in enumerate(open(file_path)):\n",
        "        if idx == 0:\n",
        "            continue\n",
        "        line = line.strip()\n",
        "        if line == \"\":\n",
        "            if len(sample) != 0:\n",
        "                data.append(sample)\n",
        "            sample = []\n",
        "        else:\n",
        "            line = line.split()\n",
        "            assert len(line) == 4\n",
        "            sample.append([line[0], line[-1]])\n",
        "    if len(sample) != 0:\n",
        "        data.append(sample)\n",
        "    data = [{\"word\": [i[0] for i in sample], \"tag\": [i[1] for i in sample]} for sample in data]\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Setting the chosen dataset, conll2003 or ner_datasetreference.\n",
        "if Config.train_data_name == \"conll2003\":\n",
        "    train_path = os.path.join(Config.train_data_name, 'train.txt')\n",
        "    dev_path = os.path.join(Config.train_data_name, 'valid.txt')\n",
        "    test_path = os.path.join(Config.train_data_name, 'test.txt')\n",
        "    train_df = read_conll2003(train_path)\n",
        "    valid_df = read_conll2003(dev_path)\n",
        "    test_df = read_conll2003(test_path)\n",
        "    print(train_df.shape, valid_df.shape, test_df.shape)\n",
        "elif Config.train_data_name == \"ner_datasetreference\":\n",
        "    df = pd.read_csv(\"ner_datasetreference.csv\", encoding='iso-8859-1')\n",
        "    data = []\n",
        "    word, tag = [], []\n",
        "    for i,j,k in zip(df['Sentence #'], df['Word'], df['Tag']):\n",
        "        if not pd.isnull(i):\n",
        "            assert i.startswith('Sentence')\n",
        "            if len(word) > 0:\n",
        "                data.append({\"word\":word, \"tag\":tag})\n",
        "            word, tag = [], []\n",
        "        if isinstance(j, str) and isinstance(k, str):\n",
        "            # remove 'art', 'eve', 'nat' label for better macro results\n",
        "            if any( t in k for t in ['art', 'eve', 'nat']):\n",
        "                continue\n",
        "            word.append(j)\n",
        "            tag.append(k)\n",
        "    if len(word) > 0:\n",
        "        data.append({\"word\":word, \"tag\":tag})\n",
        "        word, tag = [], []\n",
        "    print(data[0], data[-1])\n",
        "    df = pd.DataFrame(data)\n",
        "    train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
        "    valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "    print(df.shape, train_df.shape, valid_df.shape, test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYJxdEf9Gr30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "705fb374-cb2a-47c5-f567-0f5832c1cd6d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    word  \\\n",
              "0      [EU, rejects, German, call, to, boycott, Briti...   \n",
              "1                                     [Peter, Blackburn]   \n",
              "2                                 [BRUSSELS, 1996-08-22]   \n",
              "3      [The, European, Commission, said, on, Thursday...   \n",
              "4      [Germany, 's, representative, to, the, Europea...   \n",
              "...                                                  ...   \n",
              "14981                                    [on, Friday, :]   \n",
              "14982                                    [Division, two]   \n",
              "14983                          [Plymouth, 2, Preston, 1]   \n",
              "14984                                  [Division, three]   \n",
              "14985                           [Swansea, 1, Lincoln, 2]   \n",
              "\n",
              "                                                     tag  \n",
              "0              [B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]  \n",
              "1                                         [B-PER, I-PER]  \n",
              "2                                             [B-LOC, O]  \n",
              "3      [O, B-ORG, I-ORG, O, O, O, O, O, O, B-MISC, O,...  \n",
              "4      [B-LOC, O, O, O, O, B-ORG, I-ORG, O, O, O, B-P...  \n",
              "...                                                  ...  \n",
              "14981                                          [O, O, O]  \n",
              "14982                                             [O, O]  \n",
              "14983                               [B-ORG, O, B-ORG, O]  \n",
              "14984                                             [O, O]  \n",
              "14985                               [B-ORG, O, B-ORG, O]  \n",
              "\n",
              "[14986 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-22a2247b-93ad-45cf-977d-cfdff6fd0353\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[EU, rejects, German, call, to, boycott, Briti...</td>\n",
              "      <td>[B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Peter, Blackburn]</td>\n",
              "      <td>[B-PER, I-PER]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[BRUSSELS, 1996-08-22]</td>\n",
              "      <td>[B-LOC, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[The, European, Commission, said, on, Thursday...</td>\n",
              "      <td>[O, B-ORG, I-ORG, O, O, O, O, O, O, B-MISC, O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Germany, 's, representative, to, the, Europea...</td>\n",
              "      <td>[B-LOC, O, O, O, O, B-ORG, I-ORG, O, O, O, B-P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14981</th>\n",
              "      <td>[on, Friday, :]</td>\n",
              "      <td>[O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14982</th>\n",
              "      <td>[Division, two]</td>\n",
              "      <td>[O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14983</th>\n",
              "      <td>[Plymouth, 2, Preston, 1]</td>\n",
              "      <td>[B-ORG, O, B-ORG, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14984</th>\n",
              "      <td>[Division, three]</td>\n",
              "      <td>[O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14985</th>\n",
              "      <td>[Swansea, 1, Lincoln, 2]</td>\n",
              "      <td>[B-ORG, O, B-ORG, O]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14986 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22a2247b-93ad-45cf-977d-cfdff6fd0353')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-22a2247b-93ad-45cf-977d-cfdff6fd0353 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-22a2247b-93ad-45cf-977d-cfdff6fd0353');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a8bcc572-5758-4fe5-914c-848f546b164a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a8bcc572-5758-4fe5-914c-848f546b164a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a8bcc572-5758-4fe5-914c-848f546b164a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_08972b4e-fd4e-4757-a518-b5b2daf9dbd4\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('train_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_08972b4e-fd4e-4757-a518-b5b2daf9dbd4 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('train_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df",
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 14986,\n  \"fields\": [\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tag\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNmakWvLGr30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "d36e4b17-6f20-4e68-b436-1ff5febb9a9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   word  \\\n",
              "0     [SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...   \n",
              "1                                        [Nadim, Ladki]   \n",
              "2       [AL-AIN, ,, United, Arab, Emirates, 1996-12-06]   \n",
              "3     [Japan, began, the, defence, of, their, Asian,...   \n",
              "4     [But, China, saw, their, luck, desert, them, i...   \n",
              "...                                                 ...   \n",
              "3678  [That, is, why, this, is, so, emotional, a, ni...   \n",
              "3679  [\", It, was, the, joy, that, we, all, had, ove...   \n",
              "3680  [Charlton, managed, Ireland, for, 93, matches,...   \n",
              "3681  [He, guided, Ireland, to, two, successive, Wor...   \n",
              "3682  [The, lanky, former, Leeds, United, defender, ...   \n",
              "\n",
              "                                                    tag  \n",
              "0          [O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O]  \n",
              "1                                        [B-PER, I-PER]  \n",
              "2                    [B-LOC, O, B-LOC, I-LOC, I-LOC, O]  \n",
              "3     [B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...  \n",
              "4     [O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...  \n",
              "...                                                 ...  \n",
              "3678  [O, O, O, O, O, O, O, O, O, O, O, O, O, B-PER,...  \n",
              "3679  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "3680  [B-PER, O, B-LOC, O, O, O, O, O, O, O, O, O, O...  \n",
              "3681  [O, O, B-LOC, O, O, O, B-MISC, I-MISC, O, O, O...  \n",
              "3682  [O, O, O, B-ORG, I-ORG, O, O, O, O, O, B-LOC, ...  \n",
              "\n",
              "[3683 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eed524d4-e7a0-47fe-b48a-302c2717f1af\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...</td>\n",
              "      <td>[O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Nadim, Ladki]</td>\n",
              "      <td>[B-PER, I-PER]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[AL-AIN, ,, United, Arab, Emirates, 1996-12-06]</td>\n",
              "      <td>[B-LOC, O, B-LOC, I-LOC, I-LOC, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Japan, began, the, defence, of, their, Asian,...</td>\n",
              "      <td>[B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[But, China, saw, their, luck, desert, them, i...</td>\n",
              "      <td>[O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3678</th>\n",
              "      <td>[That, is, why, this, is, so, emotional, a, ni...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, B-PER,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3679</th>\n",
              "      <td>[\", It, was, the, joy, that, we, all, had, ove...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3680</th>\n",
              "      <td>[Charlton, managed, Ireland, for, 93, matches,...</td>\n",
              "      <td>[B-PER, O, B-LOC, O, O, O, O, O, O, O, O, O, O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3681</th>\n",
              "      <td>[He, guided, Ireland, to, two, successive, Wor...</td>\n",
              "      <td>[O, O, B-LOC, O, O, O, B-MISC, I-MISC, O, O, O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3682</th>\n",
              "      <td>[The, lanky, former, Leeds, United, defender, ...</td>\n",
              "      <td>[O, O, O, B-ORG, I-ORG, O, O, O, O, O, B-LOC, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3683 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eed524d4-e7a0-47fe-b48a-302c2717f1af')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eed524d4-e7a0-47fe-b48a-302c2717f1af button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eed524d4-e7a0-47fe-b48a-302c2717f1af');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cbdf1551-78e2-4380-b871-aea1f17ef997\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cbdf1551-78e2-4380-b871-aea1f17ef997')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cbdf1551-78e2-4380-b871-aea1f17ef997 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_d9d30a2d-1dd5-4805-aa98-0e29a9e6ea81\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('test_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d9d30a2d-1dd5-4805-aa98-0e29a9e6ea81 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('test_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_df",
              "summary": "{\n  \"name\": \"test_df\",\n  \"rows\": 3683,\n  \"fields\": [\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tag\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdv02qIvGr30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "2cafe36b-cf95-4daf-d0f1-447ac1251341"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   word  \\\n",
              "0     [CRICKET, -, LEICESTERSHIRE, TAKE, OVER, AT, T...   \n",
              "1                                  [LONDON, 1996-08-30]   \n",
              "2     [West, Indian, all-rounder, Phil, Simmons, too...   \n",
              "3     [Their, stay, on, top, ,, though, ,, may, be, ...   \n",
              "4     [After, bowling, Somerset, out, for, 83, on, t...   \n",
              "...                                                 ...   \n",
              "3460  [But, the, prices, may, move, in, a, close, ra...   \n",
              "3461  [Brokers, said, blue, chips, like, IDLC, ,, Ba...   \n",
              "3462  [They, said, there, was, still, demand, for, b...   \n",
              "3463  [The, DSE, all, share, price, index, closed, 2...   \n",
              "3464                [--, Dhaka, Newsroom, 880-2-506363]   \n",
              "\n",
              "                                                    tag  \n",
              "0                 [O, O, B-ORG, O, O, O, O, O, O, O, O]  \n",
              "1                                            [B-LOC, O]  \n",
              "2     [B-MISC, I-MISC, O, B-PER, I-PER, O, O, O, O, ...  \n",
              "3     [O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG,...  \n",
              "4     [O, O, B-ORG, O, O, O, O, O, O, O, O, B-LOC, I...  \n",
              "...                                                 ...  \n",
              "3460   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
              "3461  [O, O, O, O, O, B-ORG, O, B-ORG, I-ORG, O, B-O...  \n",
              "3462  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
              "3463  [O, B-ORG, O, O, O, O, O, O, O, O, O, O, O, O,...  \n",
              "3464                               [O, B-ORG, I-ORG, O]  \n",
              "\n",
              "[3465 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0c592062-1f54-4f8a-848a-5f34eb5907d2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CRICKET, -, LEICESTERSHIRE, TAKE, OVER, AT, T...</td>\n",
              "      <td>[O, O, B-ORG, O, O, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[LONDON, 1996-08-30]</td>\n",
              "      <td>[B-LOC, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[West, Indian, all-rounder, Phil, Simmons, too...</td>\n",
              "      <td>[B-MISC, I-MISC, O, B-PER, I-PER, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Their, stay, on, top, ,, though, ,, may, be, ...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[After, bowling, Somerset, out, for, 83, on, t...</td>\n",
              "      <td>[O, O, B-ORG, O, O, O, O, O, O, O, O, B-LOC, I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3460</th>\n",
              "      <td>[But, the, prices, may, move, in, a, close, ra...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3461</th>\n",
              "      <td>[Brokers, said, blue, chips, like, IDLC, ,, Ba...</td>\n",
              "      <td>[O, O, O, O, O, B-ORG, O, B-ORG, I-ORG, O, B-O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3462</th>\n",
              "      <td>[They, said, there, was, still, demand, for, b...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3463</th>\n",
              "      <td>[The, DSE, all, share, price, index, closed, 2...</td>\n",
              "      <td>[O, B-ORG, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3464</th>\n",
              "      <td>[--, Dhaka, Newsroom, 880-2-506363]</td>\n",
              "      <td>[O, B-ORG, I-ORG, O]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3465 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0c592062-1f54-4f8a-848a-5f34eb5907d2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0c592062-1f54-4f8a-848a-5f34eb5907d2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0c592062-1f54-4f8a-848a-5f34eb5907d2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-184708d5-088d-4b2c-a0f3-2e28a42b37e1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-184708d5-088d-4b2c-a0f3-2e28a42b37e1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-184708d5-088d-4b2c-a0f3-2e28a42b37e1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_6776714b-2557-4df1-8438-7c5d2b889731\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('valid_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_6776714b-2557-4df1-8438-7c5d2b889731 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('valid_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "valid_df",
              "summary": "{\n  \"name\": \"valid_df\",\n  \"rows\": 3465,\n  \"fields\": [\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tag\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "valid_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5vlJtg-Gr31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d3290ff-8e74-4c1e-9a0c-df65655dda97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ner category ['LOC', 'MISC', 'ORG', 'PER'] .\n",
            "\n",
            "label list ['O', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC', 'B-ORG', 'I-ORG', 'B-PER', 'I-PER'] .\n",
            "\n",
            "label2id {'O': 0, 'B-LOC': 1, 'I-LOC': 2, 'B-MISC': 3, 'I-MISC': 4, 'B-ORG': 5, 'I-ORG': 6, 'B-PER': 7, 'I-PER': 8} .\n",
            "\n",
            "id2label {0: 'O', 1: 'B-LOC', 2: 'I-LOC', 3: 'B-MISC', 4: 'I-MISC', 5: 'B-ORG', 6: 'I-ORG', 7: 'B-PER', 8: 'I-PER'}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def collect_label(df_list):\n",
        "    ret = set()\n",
        "    for df in df_list:\n",
        "        for labels in df['tag']:\n",
        "            for l in labels:\n",
        "                if l == \"O\":\n",
        "                    continue\n",
        "                assert l.startswith(\"B-\") or l.startswith(\"I-\")\n",
        "                ret.add(l[2:])\n",
        "    return sorted(list(ret))\n",
        "\n",
        "ner_category = collect_label([train_df, valid_df, test_df])\n",
        "label_list = []\n",
        "for l in ner_category:\n",
        "    label_list.append(\"B-\" + l)\n",
        "    label_list.append(\"I-\" + l)\n",
        "label_list = ['O'] + label_list\n",
        "label2id = dict([(v, idx) for idx, v in enumerate(label_list)])\n",
        "id2label = dict([(idx, v) for idx, v in enumerate(label_list)])\n",
        "print(f\"ner category {ner_category} .\\n\\nlabel list {label_list} .\\n\\nlabel2id {label2id} .\\n\\nid2label {id2label}\\n\\n\")\n",
        "label_list = label_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLLN7xMQGr31"
      },
      "source": [
        "## Import Reberta Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sy1aItuvGr31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290,
          "referenced_widgets": [
            "272b8cea9b9747a6918ed33c6df1bb56",
            "ade5608fe44944e3baf8ad73fe48faf7",
            "3b84581afd8c46bca40d9ffdf37336c2",
            "855e13ebf4204c3ab8b438077f1c7095",
            "01b6148973b6499fb528cca427e689c8",
            "0e3d61e53f9743e2b2989e351bf5d132",
            "8906fe81c2df4b83bc77f40c2c00ada1",
            "78af2c6b23294f41b4797a29b5085c64",
            "25542486a28c43a6ab7d5f7ce0b1bf3f",
            "f2e0372e82cb4a4f892d03e2d7867234",
            "317380c790964b45965b54837db63c12",
            "d01944511332439c8b29ed812f166c71",
            "4657ce4c94584433bc40e71e5670ee77",
            "1f4ad53eb8444c06a3614892e846cc9e",
            "92676f7af5094c3daee75d5cbf9f9537",
            "e7c9393192ec480baf23213969620373",
            "246ed6c4a6c141689e149eb95a899526",
            "f1229105f6714eb8b36aa15370a6e2eb",
            "4724cb108e7c4b3da88e712c18cc639a",
            "112a61000ad9410fa247a58c721acfa8",
            "595cdda5849942f4ac2334a9fd21a4d4",
            "5e248067f5524070905e4fe4d427ea13",
            "5a838cab58534b9891540a84b6a5f23d",
            "dc069ccbc971407287afd200fb6f939e",
            "a77451688b4c40e08a5606eda0ee4343",
            "595de7cb6d2040ac862a9d9f49a7f029",
            "14047109931845359efdea8ee84a9875",
            "9ff571357e1843d58553db31196cc52a",
            "118d8d54a3dd4f6ea9de39df11a74714",
            "f60127ab49244fc4a33eeb2ef2deecde",
            "0a806b483fc64bb184df259e59a35c0a",
            "6a6a44bed7fe4a0bbc3663e9e0bf5ba5",
            "d88dca07e92d4899a531152ec5f86204",
            "f29d7379cf47411d9a08685f74dae3fc",
            "6612caed7977492fb88726b604e5507e",
            "930506914b604da89fcc5e2cd5aa00da",
            "1e8ab9ff84d84418bf4c031eea443a84",
            "c344087223a647fdb2e9c8675e4c1fb1",
            "4b90de9f080b4b26a0a6063220679ff8",
            "4163bb0a8b664b8fb128ee982a6ba29d",
            "5be3ecb081154514b04348626de50599",
            "07200cacadaf4feb8b1cc17e9d42e08b",
            "d27dbbf76afb4092984c393991603843",
            "6dc1ecbc85da48dcb342b0ca9bc98146"
          ]
        },
        "outputId": "65b6dbfa-cabd-4c07-964b-0edee444c49f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "272b8cea9b9747a6918ed33c6df1bb56"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/474 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d01944511332439c8b29ed812f166c71"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a838cab58534b9891540a84b6a5f23d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f29d7379cf47411d9a08685f74dae3fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(Config.model_name, add_prefix_space=True)\n",
        "print(tokenizer.is_fast)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvXSJptjGr31"
      },
      "source": [
        "## tokenize and build Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G49tIcsZGr31"
      },
      "outputs": [],
      "source": [
        "def align(tag, word_ids):\n",
        "    aligned_tag = []\n",
        "    i = 0\n",
        "    while i < len(word_ids):\n",
        "        if word_ids[i] is None:\n",
        "            aligned_tag.append(None)\n",
        "            i += 1\n",
        "        elif tag[word_ids[i]] == \"O\":\n",
        "            aligned_tag.append(tag[word_ids[i]])\n",
        "            i += 1\n",
        "        elif tag[word_ids[i]].startswith(\"B-\"):\n",
        "            n = 0\n",
        "            while (i+n) < len(word_ids) and word_ids[i]  == word_ids[i+n]:\n",
        "                n += 1\n",
        "            aligned_tag.append(tag[word_ids[i]])\n",
        "            if n > 1:\n",
        "                aligned_tag.extend([\"I-\" + tag[word_ids[i]][2:] ] * (n-1))\n",
        "            i = i + n\n",
        "        else:\n",
        "            aligned_tag.append(tag[word_ids[i]])\n",
        "            i += 1\n",
        "    return aligned_tag\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LldB9JC9Gr31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d3913f9-44d8-40b3-ad05-e47033c2cdcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', '1996-08-22', '1996-08-22', 'I'] ['O', 'B-LOC', 'B-ORG', 'O']\n",
            "   tokens   tags  word-index\n",
            "0   [CLS]   None         NaN\n",
            "1      ĠI      O         0.0\n",
            "2   Ġ1996  B-LOC         1.0\n",
            "3       -  I-LOC         1.0\n",
            "4      08  I-LOC         1.0\n",
            "5       -  I-LOC         1.0\n",
            "6      22  I-LOC         1.0\n",
            "7   Ġ1996  B-ORG         2.0\n",
            "8       -  I-ORG         2.0\n",
            "9      08  I-ORG         2.0\n",
            "10      -  I-ORG         2.0\n",
            "11     22  I-ORG         2.0\n",
            "12     ĠI      O         3.0\n",
            "13  [SEP]   None         NaN\n"
          ]
        }
      ],
      "source": [
        "#words = train_df.iloc[2][\"word\"]\n",
        "#tag = train_df.iloc[2][\"label\"]\n",
        "words = ['I', '1996-08-22', '1996-08-22', 'I']\n",
        "tag = [\"O\", \"B-LOC\", \"B-ORG\", \"O\"]\n",
        "print(words, tag)\n",
        "s = tokenizer(words, truncation=True, is_split_into_words=True)\n",
        "word_ids = s.word_ids()\n",
        "# align tokens and words\n",
        "tokens = tokenizer.convert_ids_to_tokens(s['input_ids'])\n",
        "tags = align(tag, s.word_ids())\n",
        "print(pd.DataFrame(list(zip(tokens, tags, word_ids)), columns=['tokens', 'tags', 'word-index']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3i02DNiGr31"
      },
      "outputs": [],
      "source": [
        "def preprocess(x):\n",
        "    word = x['word']\n",
        "    r = tokenizer(word, truncation=True, is_split_into_words=True)\n",
        "    word_ids = r.word_ids()\n",
        "    tokens = tokenizer.convert_ids_to_tokens(r['input_ids'])\n",
        "    align_label = align(x['tag'], word_ids)\n",
        "    return tokens, align_label, r['input_ids'], [label2id[i] if i is not None else -100  for i in align_label], word_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmnu0LUoGr31"
      },
      "outputs": [],
      "source": [
        "train_df[['token', 'label', 'id', 'label_id', 'word_ids']] = train_df.apply(lambda x: pd.Series(preprocess(x)), axis=1)\n",
        "valid_df[['token', 'label', 'id', 'label_id', 'word_ids']] = valid_df.apply(lambda x: pd.Series(preprocess(x)), axis=1)\n",
        "test_df[['token', 'label', 'id', 'label_id', 'word_ids']] = test_df.apply(lambda x: pd.Series(preprocess(x)), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHAT0xb_Gr32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "outputId": "49419ffe-6b0e-491f-9b4e-2af4ab511452"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   word  \\\n",
              "0     [SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...   \n",
              "1                                        [Nadim, Ladki]   \n",
              "2       [AL-AIN, ,, United, Arab, Emirates, 1996-12-06]   \n",
              "3     [Japan, began, the, defence, of, their, Asian,...   \n",
              "4     [But, China, saw, their, luck, desert, them, i...   \n",
              "...                                                 ...   \n",
              "3678  [That, is, why, this, is, so, emotional, a, ni...   \n",
              "3679  [\", It, was, the, joy, that, we, all, had, ove...   \n",
              "3680  [Charlton, managed, Ireland, for, 93, matches,...   \n",
              "3681  [He, guided, Ireland, to, two, successive, Wor...   \n",
              "3682  [The, lanky, former, Leeds, United, defender, ...   \n",
              "\n",
              "                                                    tag  \\\n",
              "0          [O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O]   \n",
              "1                                        [B-PER, I-PER]   \n",
              "2                    [B-LOC, O, B-LOC, I-LOC, I-LOC, O]   \n",
              "3     [B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...   \n",
              "4     [O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...   \n",
              "...                                                 ...   \n",
              "3678  [O, O, O, O, O, O, O, O, O, O, O, O, O, B-PER,...   \n",
              "3679  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "3680  [B-PER, O, B-LOC, O, O, O, O, O, O, O, O, O, O...   \n",
              "3681  [O, O, B-LOC, O, O, O, B-MISC, I-MISC, O, O, O...   \n",
              "3682  [O, O, O, B-ORG, I-ORG, O, O, O, O, O, B-LOC, ...   \n",
              "\n",
              "                                                  token  \\\n",
              "0     [[CLS], ĠSO, CC, ER, Ġ-, ĠJ, AP, AN, ĠGET, ĠL,...   \n",
              "1                    [[CLS], ĠNad, im, ĠLad, ki, [SEP]]   \n",
              "2     [[CLS], ĠAL, -, AIN, Ġ,, ĠUnited, ĠArab, ĠEmir...   \n",
              "3     [[CLS], ĠJapan, Ġbegan, Ġthe, Ġdefence, Ġof, Ġ...   \n",
              "4     [[CLS], ĠBut, ĠChina, Ġsaw, Ġtheir, Ġluck, Ġde...   \n",
              "...                                                 ...   \n",
              "3678  [[CLS], ĠThat, Ġis, Ġwhy, Ġthis, Ġis, Ġso, Ġem...   \n",
              "3679  [[CLS], Ġ\", ĠIt, Ġwas, Ġthe, Ġjoy, Ġthat, Ġwe,...   \n",
              "3680  [[CLS], ĠCharl, ton, Ġmanaged, ĠIreland, Ġfor,...   \n",
              "3681  [[CLS], ĠHe, Ġguided, ĠIreland, Ġto, Ġtwo, Ġsu...   \n",
              "3682  [[CLS], ĠThe, Ġl, anky, Ġformer, ĠLeeds, ĠUnit...   \n",
              "\n",
              "                                                  label  \\\n",
              "0     [None, O, O, O, O, B-LOC, I-LOC, I-LOC, O, O, ...   \n",
              "1              [None, B-PER, I-PER, I-PER, I-PER, None]   \n",
              "2     [None, B-LOC, I-LOC, I-LOC, O, B-LOC, I-LOC, I...   \n",
              "3     [None, B-LOC, O, O, O, O, O, B-MISC, I-MISC, O...   \n",
              "4     [None, O, B-LOC, O, O, O, O, O, O, O, O, O, O,...   \n",
              "...                                                 ...   \n",
              "3678  [None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "3679  [None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
              "3680  [None, B-PER, I-PER, O, B-LOC, O, O, O, O, O, ...   \n",
              "3681  [None, O, O, B-LOC, O, O, O, B-MISC, I-MISC, O...   \n",
              "3682  [None, O, O, O, O, B-ORG, I-ORG, O, O, O, O, O...   \n",
              "\n",
              "                                                     id  \\\n",
              "0     [1, 13910, 3376, 2076, 111, 344, 591, 1889, 77...   \n",
              "1                        [1, 7157, 757, 17804, 3144, 2]   \n",
              "2     [1, 6019, 12, 33178, 2156, 315, 4681, 8313, 80...   \n",
              "3     [1, 1429, 880, 5, 2994, 9, 49, 3102, 968, 1270...   \n",
              "4     [1, 125, 436, 794, 49, 6620, 10348, 106, 11, 5...   \n",
              "...                                                 ...   \n",
              "3678  [1, 280, 16, 596, 42, 16, 98, 3722, 10, 363, 1...   \n",
              "3679  [1, 22, 85, 21, 5, 5823, 14, 52, 70, 56, 81, 5...   \n",
              "3680  [1, 15959, 1054, 2312, 2487, 13, 8060, 2856, 2...   \n",
              "3681  [1, 91, 10346, 2487, 7, 80, 12565, 623, 968, 6...   \n",
              "3682  [1, 20, 784, 30451, 320, 9245, 315, 5142, 222,...   \n",
              "\n",
              "                                               label_id  \\\n",
              "0     [-100, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, ...   \n",
              "1                              [-100, 7, 8, 8, 8, -100]   \n",
              "2      [-100, 1, 2, 2, 0, 1, 2, 2, 0, 0, 0, 0, 0, -100]   \n",
              "3     [-100, 1, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, ...   \n",
              "4     [-100, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "...                                                 ...   \n",
              "3678  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "3679  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "3680  [-100, 7, 8, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "3681  [-100, 0, 0, 1, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, ...   \n",
              "3682  [-100, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 0, 1, 0, ...   \n",
              "\n",
              "                                               word_ids  \n",
              "0     [None, 0, 0, 0, 1, 2, 2, 2, 3, 4, 4, 4, 5, 6, ...  \n",
              "1                              [None, 0, 0, 1, 1, None]  \n",
              "2      [None, 0, 0, 0, 1, 2, 3, 4, 5, 5, 5, 5, 5, None]  \n",
              "3     [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...  \n",
              "4     [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...  \n",
              "...                                                 ...  \n",
              "3678  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...  \n",
              "3679  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...  \n",
              "3680  [None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...  \n",
              "3681  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...  \n",
              "3682  [None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...  \n",
              "\n",
              "[3683 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8ccc60cb-2975-41d6-a6a8-e3164a5d9997\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "      <th>token</th>\n",
              "      <th>label</th>\n",
              "      <th>id</th>\n",
              "      <th>label_id</th>\n",
              "      <th>word_ids</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...</td>\n",
              "      <td>[O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O]</td>\n",
              "      <td>[[CLS], ĠSO, CC, ER, Ġ-, ĠJ, AP, AN, ĠGET, ĠL,...</td>\n",
              "      <td>[None, O, O, O, O, B-LOC, I-LOC, I-LOC, O, O, ...</td>\n",
              "      <td>[1, 13910, 3376, 2076, 111, 344, 591, 1889, 77...</td>\n",
              "      <td>[-100, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[None, 0, 0, 0, 1, 2, 2, 2, 3, 4, 4, 4, 5, 6, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Nadim, Ladki]</td>\n",
              "      <td>[B-PER, I-PER]</td>\n",
              "      <td>[[CLS], ĠNad, im, ĠLad, ki, [SEP]]</td>\n",
              "      <td>[None, B-PER, I-PER, I-PER, I-PER, None]</td>\n",
              "      <td>[1, 7157, 757, 17804, 3144, 2]</td>\n",
              "      <td>[-100, 7, 8, 8, 8, -100]</td>\n",
              "      <td>[None, 0, 0, 1, 1, None]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[AL-AIN, ,, United, Arab, Emirates, 1996-12-06]</td>\n",
              "      <td>[B-LOC, O, B-LOC, I-LOC, I-LOC, O]</td>\n",
              "      <td>[[CLS], ĠAL, -, AIN, Ġ,, ĠUnited, ĠArab, ĠEmir...</td>\n",
              "      <td>[None, B-LOC, I-LOC, I-LOC, O, B-LOC, I-LOC, I...</td>\n",
              "      <td>[1, 6019, 12, 33178, 2156, 315, 4681, 8313, 80...</td>\n",
              "      <td>[-100, 1, 2, 2, 0, 1, 2, 2, 0, 0, 0, 0, 0, -100]</td>\n",
              "      <td>[None, 0, 0, 0, 1, 2, 3, 4, 5, 5, 5, 5, 5, None]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Japan, began, the, defence, of, their, Asian,...</td>\n",
              "      <td>[B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...</td>\n",
              "      <td>[[CLS], ĠJapan, Ġbegan, Ġthe, Ġdefence, Ġof, Ġ...</td>\n",
              "      <td>[None, B-LOC, O, O, O, O, O, B-MISC, I-MISC, O...</td>\n",
              "      <td>[1, 1429, 880, 5, 2994, 9, 49, 3102, 968, 1270...</td>\n",
              "      <td>[-100, 1, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[But, China, saw, their, luck, desert, them, i...</td>\n",
              "      <td>[O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
              "      <td>[[CLS], ĠBut, ĠChina, Ġsaw, Ġtheir, Ġluck, Ġde...</td>\n",
              "      <td>[None, O, B-LOC, O, O, O, O, O, O, O, O, O, O,...</td>\n",
              "      <td>[1, 125, 436, 794, 49, 6620, 10348, 106, 11, 5...</td>\n",
              "      <td>[-100, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3678</th>\n",
              "      <td>[That, is, why, this, is, so, emotional, a, ni...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, B-PER,...</td>\n",
              "      <td>[[CLS], ĠThat, Ġis, Ġwhy, Ġthis, Ġis, Ġso, Ġem...</td>\n",
              "      <td>[None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>[1, 280, 16, 596, 42, 16, 98, 3722, 10, 363, 1...</td>\n",
              "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3679</th>\n",
              "      <td>[\", It, was, the, joy, that, we, all, had, ove...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>[[CLS], Ġ\", ĠIt, Ġwas, Ġthe, Ġjoy, Ġthat, Ġwe,...</td>\n",
              "      <td>[None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>[1, 22, 85, 21, 5, 5823, 14, 52, 70, 56, 81, 5...</td>\n",
              "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3680</th>\n",
              "      <td>[Charlton, managed, Ireland, for, 93, matches,...</td>\n",
              "      <td>[B-PER, O, B-LOC, O, O, O, O, O, O, O, O, O, O...</td>\n",
              "      <td>[[CLS], ĠCharl, ton, Ġmanaged, ĠIreland, Ġfor,...</td>\n",
              "      <td>[None, B-PER, I-PER, O, B-LOC, O, O, O, O, O, ...</td>\n",
              "      <td>[1, 15959, 1054, 2312, 2487, 13, 8060, 2856, 2...</td>\n",
              "      <td>[-100, 7, 8, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[None, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3681</th>\n",
              "      <td>[He, guided, Ireland, to, two, successive, Wor...</td>\n",
              "      <td>[O, O, B-LOC, O, O, O, B-MISC, I-MISC, O, O, O...</td>\n",
              "      <td>[[CLS], ĠHe, Ġguided, ĠIreland, Ġto, Ġtwo, Ġsu...</td>\n",
              "      <td>[None, O, O, B-LOC, O, O, O, B-MISC, I-MISC, O...</td>\n",
              "      <td>[1, 91, 10346, 2487, 7, 80, 12565, 623, 968, 6...</td>\n",
              "      <td>[-100, 0, 0, 1, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3682</th>\n",
              "      <td>[The, lanky, former, Leeds, United, defender, ...</td>\n",
              "      <td>[O, O, O, B-ORG, I-ORG, O, O, O, O, O, B-LOC, ...</td>\n",
              "      <td>[[CLS], ĠThe, Ġl, anky, Ġformer, ĠLeeds, ĠUnit...</td>\n",
              "      <td>[None, O, O, O, O, B-ORG, I-ORG, O, O, O, O, O...</td>\n",
              "      <td>[1, 20, 784, 30451, 320, 9245, 315, 5142, 222,...</td>\n",
              "      <td>[-100, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
              "      <td>[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3683 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ccc60cb-2975-41d6-a6a8-e3164a5d9997')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8ccc60cb-2975-41d6-a6a8-e3164a5d9997 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8ccc60cb-2975-41d6-a6a8-e3164a5d9997');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a835bd52-eefd-4983-9574-8ccf074f55c4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a835bd52-eefd-4983-9574-8ccf074f55c4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a835bd52-eefd-4983-9574-8ccf074f55c4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_c978cbdb-20a2-4415-b013-cc68e57565b1\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('test_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c978cbdb-20a2-4415-b013-cc68e57565b1 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('test_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_df",
              "summary": "{\n  \"name\": \"test_df\",\n  \"rows\": 3683,\n  \"fields\": [\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tag\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"token\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_id\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word_ids\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWRSGLs4Gr32"
      },
      "source": [
        "## Building Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNHgo_2cGr32"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NerDataset(Dataset):\n",
        "    def __init__(self, df, device):\n",
        "        self.data = df.to_dict(orient='records')\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.data[item]\n",
        "\n",
        "    def collate_to_max_length(self, batch):\n",
        "        max_seq_length = max([len(s['id']) for s in batch])\n",
        "        batch = sorted(batch, key=lambda x: -len(x['id']))\n",
        "        seq_length = torch.tensor([len(x['id']) for x in batch])\n",
        "        input_ids = torch.tensor([x[\"id\"] + [0] * (max_seq_length - len(x['id'])) for x in batch]).to(self.device)\n",
        "        labels = torch.tensor([x[\"label_id\"] + [-100] * (max_seq_length - len(x['label_id'])) for x in batch]).to(self.device)\n",
        "        return {\"id\": input_ids, \"label_id\": labels, 'seq_length':seq_length, \"sample\":batch}\n",
        "\n",
        "\n",
        "dataset_train = NerDataset(train_df, Config.device)\n",
        "\n",
        "train_dataloader = DataLoader(dataset_train,\n",
        "                              sampler=RandomSampler(dataset_train),\n",
        "                              batch_size=Config.batch_size,\n",
        "                              drop_last=False,\n",
        "                              collate_fn=dataset_train.collate_to_max_length)\n",
        "\n",
        "\n",
        "\n",
        "dataset_valid = NerDataset(valid_df, Config.device)\n",
        "\n",
        "valid_dataloader = DataLoader(dataset_valid,\n",
        "                              sampler=RandomSampler(dataset_valid),\n",
        "                              batch_size=Config.batch_size,\n",
        "                              drop_last=False,\n",
        "                              collate_fn=dataset_valid.collate_to_max_length)\n",
        "\n",
        "\n",
        "dataset_test = NerDataset(test_df, Config.device)\n",
        "\n",
        "test_dataloader = DataLoader(dataset_test,\n",
        "                              sampler=RandomSampler(dataset_test),\n",
        "                              batch_size=Config.batch_size,\n",
        "                              drop_last=False,\n",
        "                              collate_fn=dataset_test.collate_to_max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5B33GkMGr32"
      },
      "source": [
        "## Building Custom loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp_D66RpGr32"
      },
      "outputs": [],
      "source": [
        "\n",
        "class L1_Loss:\n",
        "    def __init__(self):\n",
        "        self.l1_loss = nn.L1Loss()\n",
        "    def loss(self, target, logit, label_num):\n",
        "\n",
        "        target = target.view(-1)\n",
        "        logit = logit.view(-1, label_num)\n",
        "        mask = target.ne(-100).to(logit.device)\n",
        "        logit = torch.masked_select(logit, mask.unsqueeze(-1).expand_as(logit)).reshape(-1, label_num)\n",
        "        target = torch.masked_select(target, mask)\n",
        "\n",
        "        target = F.one_hot(target, num_classes=label_num)\n",
        "        return self.l1_loss(logit, target.float())\n",
        "\n",
        "\n",
        "class L2_Loss:\n",
        "    def __init__(self):\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "    def loss(self, target, logit,label_num):\n",
        "        target = target.view(-1)\n",
        "        logit = logit.view(-1, label_num)\n",
        "        mask = target.ne(-100).to(logit.device)\n",
        "        logit = torch.masked_select(logit, mask.unsqueeze(-1).expand_as(logit)).reshape(-1, label_num)\n",
        "        target = torch.masked_select(target, mask)\n",
        "\n",
        "        target = F.one_hot(target, num_classes=label_num)\n",
        "        loss = self.mse_loss(logit, target.float())\n",
        "        return loss\n",
        "\n",
        "class CE_Loss:\n",
        "    def __init__(self):\n",
        "        self.ce_loss = nn.CrossEntropyLoss(ignore_index=-100, reduce='mean')\n",
        "    def loss(self, target, logit, label_num):\n",
        "        return self.ce_loss(logit.reshape(-1, label_num), target.reshape(-1) )\n",
        "\n",
        "class KLDivergenceLoss:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def loss(self, target, logit, label_num):\n",
        "        target = target.view(-1)\n",
        "        logit = logit.view(-1, label_num)\n",
        "\n",
        "        mask = target.ne(-100).to(logit.device)\n",
        "        logit = torch.masked_select(logit, mask.unsqueeze(-1).expand_as(logit)).reshape(-1, label_num)\n",
        "        target = torch.masked_select(target, mask)\n",
        "\n",
        "        probs = F.softmax(logit, dim=-1)\n",
        "\n",
        "        # One-hot encode the targets to get true probabilities\n",
        "        true_probs = F.one_hot(target, num_classes=label_num).float()\n",
        "\n",
        "        mask_true_probs = true_probs > 0\n",
        "\n",
        "        # Calculate g function for non-zero elements using the mask\n",
        "        kl_values = torch.zeros_like(probs)\n",
        "        kl_values[mask_true_probs] = true_probs[mask_true_probs] * torch.log(true_probs[mask_true_probs]/probs[mask_true_probs])\n",
        "\n",
        "        # Sum over all classes and average over the batch size\n",
        "        loss = kl_values.sum(dim=-1).mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "# DLITE Loss function\n",
        "class DLITELoss:\n",
        "    def __init__(self):\n",
        "        super(DLITELoss, self).__init__()\n",
        "\n",
        "    def loss(self, targets, logits, label_num, epsilon=1e-10):\n",
        "        targets = targets.view(-1)\n",
        "        logits = logits.view(-1, label_num)\n",
        "\n",
        "        mask = targets.ne(-100).to(logits.device)\n",
        "        logits = torch.masked_select(logits, mask.unsqueeze(-1).expand_as(logits)).reshape(-1, label_num)\n",
        "        targets = torch.masked_select(targets, mask)\n",
        "\n",
        "        # Convert logits to probabilities using softmax\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # One-hot encode the targets to get true probabilities\n",
        "        true_probs = F.one_hot(targets, num_classes=probs.size(-1)).float()\n",
        "\n",
        "        # Define the g function\n",
        "        g_values = torch.abs(probs * (1 - torch.log(probs + epsilon)) - true_probs * (1 - torch.log(true_probs + epsilon)))\n",
        "\n",
        "        # Define the delta_h function\n",
        "        delta_h_values = torch.abs(probs**2 * (1 - 2 * torch.log(probs + epsilon)) - true_probs**2 * (1 - 2 * torch.log(true_probs + epsilon))) / (2 * (probs + true_probs))\n",
        "\n",
        "        # Compute DLITE loss for each class\n",
        "        dl_values = g_values - delta_h_values\n",
        "\n",
        "        # Sum over all classes and average over batch size\n",
        "        loss = dl_values.sum(dim=-1).mean()\n",
        "\n",
        "        return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0x9wjATtD7Y"
      },
      "source": [
        "## Adding Custom Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bf_rR5pGr32"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LSTMEncoder(nn.Module):\n",
        "    \"\"\"lstm encoder\n",
        "    \"\"\"\n",
        "    def __init__(self, config, hidden_size):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "        self.lstm = torch.nn.LSTM(hidden_size, hidden_size,\n",
        "                                  num_layers=config.lstm_layer_num, bidirectional=config.bi_lstm,\n",
        "                                  batch_first=True)\n",
        "\n",
        "    def forward(self, hidden_state, seq_length):\n",
        "        sequence_output = pack_padded_sequence(hidden_state, seq_length, batch_first=True)\n",
        "        sequence_output, (h_n, c_n) = self.lstm(sequence_output)\n",
        "        sequence_output, _ = pad_packed_sequence(sequence_output, batch_first=True)\n",
        "        return sequence_output\n",
        "\n",
        "\n",
        "\n",
        "class Ner_Model(nn.Module):\n",
        "    def __init__(self,config, label_num, loss_name):\n",
        "        super(Ner_Model, self).__init__()\n",
        "        self.config = config\n",
        "        # deberat model\n",
        "        self.model = transformers.AutoModel.from_pretrained(config.model_name)\n",
        "\n",
        "        hidden_size = config.hidden_size\n",
        "\n",
        "        # using linear layer\n",
        "        linear_layer = []\n",
        "        if self.config.linear_layer_num  > 0:\n",
        "            for out_dim in config.linear_layer[0:config.linear_layer_num]:\n",
        "                linear_layer.append( nn.Linear(in_features=hidden_size, out_features=out_dim) )\n",
        "                linear_layer.append( nn.ReLU() )\n",
        "                hidden_size = out_dim\n",
        "\n",
        "            self.linear_model = nn.Sequential(*linear_layer)\n",
        "\n",
        "        # whether to use lstm layer\n",
        "        if config.lstm_layer_num > 0:\n",
        "            self.lstm = LSTMEncoder(config,hidden_size)\n",
        "\n",
        "        # identify label number\n",
        "        self.label_num = label_num\n",
        "\n",
        "        # whether to use bi-lstm layer\n",
        "        if config.bi_lstm and config.lstm_layer_num > 0:\n",
        "            hidden_size = hidden_size * 2\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_size, label_num)\n",
        "\n",
        "        if loss_name == 'ce':\n",
        "            self.loss_func = CE_Loss()\n",
        "        elif loss_name == 'l1':\n",
        "            self.loss_func = L1_Loss()\n",
        "        elif loss_name == 'l2':\n",
        "            self.loss_func = L2_Loss()\n",
        "        elif loss_name == 'kl':\n",
        "            self.loss_func = KLDivergenceLoss()\n",
        "        elif loss_name == 'dlite':\n",
        "            self.loss_func = DLITELoss()\n",
        "        else:\n",
        "            assert 1==0\n",
        "\n",
        "        print(\"model configuration\")\n",
        "        print(\"%\" * 20)\n",
        "        print(self)\n",
        "        print(\"%\" * 20)\n",
        "\n",
        "    def forward(self, input_ids, seq_length, attention_mask, labels):\n",
        "        output = self.model(input_ids, attention_mask)\n",
        "        sequence_output = output[0]\n",
        "        if self.config.linear_layer_num > 0:\n",
        "            sequence_output = self.linear_model(sequence_output)\n",
        "\n",
        "        if self.config.lstm_layer_num > 0:\n",
        "            sequence_output = self.lstm(sequence_output, seq_length)\n",
        "\n",
        "        logit = self.classifier(sequence_output)\n",
        "        loss = self.loss_func.loss(labels, logit, len(label2id))\n",
        "        return loss, logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8F51tFmwGr32"
      },
      "outputs": [],
      "source": [
        "# Building optimizer\n",
        "def get_optimizer(model, config):\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.1},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "\n",
        "    optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                      betas=(0.9, 0.98),\n",
        "                      lr=config.lr)\n",
        "    return optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uXq2R_yGr32"
      },
      "source": [
        "## Defining the training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDIJoYpWGr33"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate(model, data_loader, mode=\"Validation\"):\n",
        "    ground_truth, predict = [], []\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples = 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(data_loader):\n",
        "            attention_mask = batch[\"id\"].ne(0)\n",
        "            targets = batch['label_id']\n",
        "            loss, logit = model(batch[\"id\"], batch['seq_length'], attention_mask=attention_mask,\n",
        "                                             labels=targets)\n",
        "            eval_loss += loss.cpu().item()\n",
        "            if (step+1) % 100==0:\n",
        "                loss_step = eval_loss / (step+1)\n",
        "                print(f\"{mode} loss per 100 evaluation steps: {loss_step}\")\n",
        "\n",
        "            # compute training accuracy\n",
        "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = logit.view(-1, len(label2id)) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            active_accuracy = flattened_targets.ne(-100) # active accuracy is also of shape (batch_size * seq_len,)\n",
        "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "            eval_preds.extend(predictions.tolist())\n",
        "            eval_labels.extend(targets.tolist())\n",
        "\n",
        "    eval_loss = eval_loss / (step+1)\n",
        "    eval_accuracy = eval_accuracy / (step+1)\n",
        "    eval_labels,eval_preds = [id2label[i] for i in eval_labels], [id2label[i] for i in eval_preds]\n",
        "\n",
        "\n",
        "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(eval_labels, eval_preds, average='micro')\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(eval_labels, eval_preds, average='macro')\n",
        "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(eval_labels, eval_preds,average='weighted')\n",
        "\n",
        "    p_r_f1 = [[round(precision_micro,4), round(recall_micro,4), round(f1_micro,4)],\n",
        "              [round(precision_macro,4), round(recall_macro,4), round(f1_macro,4)],\n",
        "              [round(precision_weighted,4), round(recall_weighted,4), round(f1_weighted,4)]]\n",
        "\n",
        "    p_r_f1 = pd.DataFrame(p_r_f1, columns=['precision', 'recall', 'f1'], index=['micro', 'macro', 'weighted'])\n",
        "\n",
        "    print(f\"{mode} Loss: {eval_loss}\")\n",
        "    print(f\"{mode} Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    p_r_f1_each_label = classification_report(eval_labels, eval_preds)\n",
        "    print(f\"{mode} P-R-F1 for each label: \\n{p_r_f1_each_label}\")\n",
        "    print(f\"{mode} P-R-F1 tor all label: \\n{p_r_f1}\")\n",
        "    print(f\"{mode} steps: {(step+1)}\")\n",
        "    return eval_loss, p_r_f1, p_r_f1_each_label\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMapwwmhGr33"
      },
      "source": [
        "## Running under 5 custom loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzSeh-WWGr33"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "loss_list = ['l1', 'l2', 'ce', 'kl', 'dlite']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6U0QqPKGr33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cbf243125d394b87bcf403ce152cf944",
            "6f59fe3d162f46af814b140099fc7ffc",
            "5199def6b37c4e0cbc1cceb4e4cc7594",
            "e266ae10304843f0ad670313707874a0",
            "ab22b162e5bb4837b9398bbcd570a664",
            "52665719116b431bb656cbafc8c185cf",
            "7b89e2907ec445b78fa3dca6899b313d",
            "bc0c5ed68cb748f8bda7baff5020dabd",
            "97b5d9bd18054c099981ae1f23d07eb0",
            "25e5d911947a4f1a80155c33b91f7439",
            "e7c07384960842d5ba37da12620bd1b7"
          ]
        },
        "outputId": "e2d702fd-0b1f-4eb1-a437-3e8801ab01b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "loss_name: l1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/559M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cbf243125d394b87bcf403ce152cf944"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model configuration\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Ner_Model(\n",
            "  (model): DebertaModel(\n",
            "    (embeddings): DebertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "      (LayerNorm): DebertaLayerNorm()\n",
            "      (dropout): StableDropout()\n",
            "    )\n",
            "    (encoder): DebertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x DebertaLayer(\n",
            "          (attention): DebertaAttention(\n",
            "            (self): DisentangledSelfAttention(\n",
            "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
            "              (pos_dropout): StableDropout()\n",
            "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "            (output): DebertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): DebertaLayerNorm()\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "          )\n",
            "          (intermediate): DebertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): DebertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): DebertaLayerNorm()\n",
            "            (dropout): StableDropout()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (rel_embeddings): Embedding(1024, 768)\n",
            "    )\n",
            "  )\n",
            "  (linear_model): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=384, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=384, out_features=256, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (9): ReLU()\n",
            "    (10): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (11): ReLU()\n",
            "    (12): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (13): ReLU()\n",
            "    (14): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (15): ReLU()\n",
            "  )\n",
            "  (classifier): Linear(in_features=128, out_features=9, bias=True)\n",
            ")\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.1440770986676216\n",
            "Training loss per 100 training steps: 0.13779715865850448\n",
            "Training loss per 100 training steps: 0.13121794735391934\n",
            "Training loss per 100 training steps: 0.1251802028529346\n",
            "Training loss per 100 training steps: 0.12025129507482052\n",
            "Training loss per 100 training steps: 0.11613730764637391\n",
            "Training loss per 100 training steps: 0.1099772243361388\n",
            "Training loss per 100 training steps: 0.10124548768217209\n",
            "Training loss per 100 training steps: 0.09385725469173242\n",
            "Training loss per 100 training steps: 0.0878457225850143\n",
            "Training loss per 100 training steps: 0.08299576375212796\n",
            "Training loss per 100 training steps: 0.07867371058481998\n",
            "Training loss per 100 training steps: 0.07520798188618545\n",
            "Training loss per 100 training steps: 0.07199725881954822\n",
            "Training loss per 100 training steps: 0.06917942071865157\n",
            "Training loss per 100 training steps: 0.06654635003800649\n",
            "Training loss per 100 training steps: 0.06420856862606289\n",
            "Training loss per 100 training steps: 0.06224632127381887\n",
            "Training loss per 100 training steps: 0.060493054795722284\n",
            "Training loss per 100 training steps: 0.05889621084721875\n",
            "Training loss per 100 training steps: 0.05741893397136924\n",
            "Training loss per 100 training steps: 0.0560202831684199\n",
            "Training loss per 100 training steps: 0.054804169237234066\n",
            "Training loss per 100 training steps: 0.05374646515277467\n",
            "Training loss per 100 training steps: 0.0527940387720475\n",
            "Training loss per 100 training steps: 0.0518095858603192\n",
            "Training loss per 100 training steps: 0.050982934617409395\n",
            "Training loss per 100 training steps: 0.050159276173955214\n",
            "Training loss per 100 training steps: 0.049437804280816386\n",
            "Training loss per 100 training steps: 0.04877791869681096\n",
            "Training loss per 100 training steps: 0.0480826690338736\n",
            "Training loss per 100 training steps: 0.04738689723818425\n",
            "Training loss per 100 training steps: 0.04682010110115781\n",
            "Training loss per 100 training steps: 0.04631778759221882\n",
            "Training loss per 100 training steps: 0.045810797029054294\n",
            "Training loss per 100 training steps: 0.04531157595042007\n",
            "Training loss per 100 training steps: 0.044868055268932554\n",
            "Training loss epoch: 0.04467220244667392\n",
            "Training accuracy epoch: 0.7836306728278062\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.030121805276721716\n",
            "Validation loss per 100 evaluation steps: 0.028698607973055915\n",
            "Validation loss per 100 evaluation steps: 0.029399699763550113\n",
            "Validation loss per 100 evaluation steps: 0.029649271086673253\n",
            "Validation loss per 100 evaluation steps: 0.02930253636511043\n",
            "Validation loss per 100 evaluation steps: 0.029061514782176043\n",
            "Validation loss per 100 evaluation steps: 0.02895184695820457\n",
            "Validation loss per 100 evaluation steps: 0.02871306803455809\n",
            "Validation Loss: 0.02869793973624655\n",
            "Validation Accuracy: 0.8167728009272965\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.00      0.00      0.00      1837\n",
            "      B-MISC       0.00      0.00      0.00       922\n",
            "       B-ORG       0.00      0.00      0.00      1341\n",
            "       B-PER       0.00      0.00      0.00      1842\n",
            "       I-LOC       0.00      0.00      0.00      1801\n",
            "      I-MISC       0.00      0.00      0.00       935\n",
            "       I-ORG       0.00      0.00      0.00      2319\n",
            "       I-PER       0.27      1.00      0.43      4219\n",
            "           O       0.99      0.99      0.99     51041\n",
            "\n",
            "    accuracy                           0.82     66257\n",
            "   macro avg       0.14      0.22      0.16     66257\n",
            "weighted avg       0.78      0.82      0.79     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.8244  0.8244  0.8244\n",
            "macro        0.1405  0.2206  0.1575\n",
            "weighted     0.7824  0.8244  0.7902\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.029723954796791078\n",
            "Test loss per 100 evaluation steps: 0.029033893467858434\n",
            "Test loss per 100 evaluation steps: 0.02973221759001414\n",
            "Test loss per 100 evaluation steps: 0.029854958606883884\n",
            "Test loss per 100 evaluation steps: 0.029606207909062505\n",
            "Test loss per 100 evaluation steps: 0.02972971463420739\n",
            "Test loss per 100 evaluation steps: 0.02944434956630825\n",
            "Test loss per 100 evaluation steps: 0.02979855116675026\n",
            "Test loss per 100 evaluation steps: 0.030083455325285387\n",
            "Test Loss: 0.030079036827571286\n",
            "Test Accuracy: 0.8011041737428757\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.00      0.00      0.00      1668\n",
            "      B-MISC       0.00      0.00      0.00       702\n",
            "       B-ORG       0.00      0.00      0.00      1661\n",
            "       B-PER       0.00      0.00      0.00      1617\n",
            "       I-LOC       0.00      0.00      0.00      1394\n",
            "      I-MISC       0.00      0.00      0.00       736\n",
            "       I-ORG       0.00      0.00      0.00      2804\n",
            "       I-PER       0.25      1.00      0.40      3810\n",
            "           O       0.99      0.98      0.99     47094\n",
            "\n",
            "    accuracy                           0.81     61486\n",
            "   macro avg       0.14      0.22      0.15     61486\n",
            "weighted avg       0.78      0.81      0.78     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.8096  0.8096  0.8096\n",
            "macro        0.1382  0.2194  0.1538\n",
            "weighted     0.7770  0.8096  0.7794\n",
            "Test steps: 921\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.028041606261394916\n",
            "Training loss per 100 training steps: 0.02882449798838934\n",
            "Training loss per 100 training steps: 0.02920367476113218\n",
            "Training loss per 100 training steps: 0.029033969918164076\n",
            "Training loss per 100 training steps: 0.029142949787259568\n",
            "Training loss per 100 training steps: 0.029309618470579155\n",
            "Training loss per 100 training steps: 0.029116010625813423\n",
            "Training loss per 100 training steps: 0.028788551806792385\n",
            "Training loss per 100 training steps: 0.0284931658548885\n",
            "Training loss per 100 training steps: 0.028449677172960947\n",
            "Training loss per 100 training steps: 0.028349853439642837\n",
            "Training loss per 100 training steps: 0.028321361580092342\n",
            "Training loss per 100 training steps: 0.028242117861541023\n",
            "Training loss per 100 training steps: 0.02826926770297827\n",
            "Training loss per 100 training steps: 0.028246510612875378\n",
            "Training loss per 100 training steps: 0.028305146392558527\n",
            "Training loss per 100 training steps: 0.028351864551283034\n",
            "Training loss per 100 training steps: 0.028271859041859797\n",
            "Training loss per 100 training steps: 0.028183085840298047\n",
            "Training loss per 100 training steps: 0.028245157937104522\n",
            "Training loss per 100 training steps: 0.02825588319566339\n",
            "Training loss per 100 training steps: 0.028279251218555145\n",
            "Training loss per 100 training steps: 0.02837742487216019\n",
            "Training loss per 100 training steps: 0.02835376911451628\n",
            "Training loss per 100 training steps: 0.028406100980617337\n",
            "Training loss per 100 training steps: 0.028380795156682813\n",
            "Training loss per 100 training steps: 0.02838287516009317\n",
            "Training loss per 100 training steps: 0.0283701545342618\n",
            "Training loss per 100 training steps: 0.028366513215803243\n",
            "Training loss per 100 training steps: 0.028377870689383904\n",
            "Training loss per 100 training steps: 0.028396802188930922\n",
            "Training loss per 100 training steps: 0.02840067077525873\n",
            "Training loss per 100 training steps: 0.028402688078768376\n",
            "Training loss per 100 training steps: 0.028467539432693978\n",
            "Training loss per 100 training steps: 0.028549889444735268\n",
            "Training loss per 100 training steps: 0.028518736485821136\n",
            "Training loss per 100 training steps: 0.028546612888690228\n",
            "Training loss epoch: 0.028525399763926346\n",
            "Training accuracy epoch: 0.8180188927821863\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.027391489224974065\n",
            "Validation loss per 100 evaluation steps: 0.027225185075658374\n",
            "Validation loss per 100 evaluation steps: 0.027576200407541666\n",
            "Validation loss per 100 evaluation steps: 0.027679982316913085\n",
            "Validation loss per 100 evaluation steps: 0.027770181480795145\n",
            "Validation loss per 100 evaluation steps: 0.0278241798421368\n",
            "Validation loss per 100 evaluation steps: 0.02805195586872287\n",
            "Validation loss per 100 evaluation steps: 0.028136432326282376\n",
            "Validation Loss: 0.028269501346713438\n",
            "Validation Accuracy: 0.8222544387624452\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.00      0.00      0.00      1837\n",
            "      B-MISC       0.00      0.00      0.00       922\n",
            "       B-ORG       0.00      0.00      0.00      1341\n",
            "       B-PER       0.01      0.00      0.00      1842\n",
            "       I-LOC       0.00      0.00      0.00      1801\n",
            "      I-MISC       0.00      0.00      0.00       935\n",
            "       I-ORG       0.00      0.00      0.00      2319\n",
            "       I-PER       0.29      0.99      0.45      4219\n",
            "           O       0.99      1.00      0.99     51041\n",
            "\n",
            "    accuracy                           0.83     66257\n",
            "   macro avg       0.14      0.22      0.16     66257\n",
            "weighted avg       0.78      0.83      0.79     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.8301  0.8301  0.8301\n",
            "macro        0.1434  0.2212  0.1612\n",
            "weighted     0.7814  0.8301  0.7937\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.03075079005677253\n",
            "Test loss per 100 evaluation steps: 0.030914201005361974\n",
            "Test loss per 100 evaluation steps: 0.030782046872191132\n",
            "Test loss per 100 evaluation steps: 0.03022267230320722\n",
            "Test loss per 100 evaluation steps: 0.029463820633478462\n",
            "Test loss per 100 evaluation steps: 0.029580904129155292\n",
            "Test loss per 100 evaluation steps: 0.029707142248516904\n",
            "Test loss per 100 evaluation steps: 0.029499723637272837\n",
            "Test loss per 100 evaluation steps: 0.029649349941173568\n",
            "Test Loss: 0.02959302039506029\n",
            "Test Accuracy: 0.8092330688603755\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.00      0.00      0.00      1668\n",
            "      B-MISC       0.00      0.00      0.00       702\n",
            "       B-ORG       0.00      0.00      0.00      1661\n",
            "       B-PER       0.01      0.00      0.00      1617\n",
            "       I-LOC       0.00      0.00      0.00      1394\n",
            "      I-MISC       0.00      0.00      0.00       736\n",
            "       I-ORG       0.00      0.00      0.00      2804\n",
            "       I-PER       0.28      0.99      0.43      3810\n",
            "           O       0.99      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.82     61486\n",
            "   macro avg       0.14      0.22      0.16     61486\n",
            "weighted avg       0.78      0.82      0.78     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.8181  0.8181  0.8181\n",
            "macro        0.1412  0.2204  0.1581\n",
            "weighted     0.7754  0.8181  0.7841\n",
            "Test steps: 921\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.02941938992589712\n",
            "Training loss per 100 training steps: 0.029003343486692756\n",
            "Training loss per 100 training steps: 0.029177003400885346\n",
            "Training loss per 100 training steps: 0.029449663316991063\n",
            "Training loss per 100 training steps: 0.029250961812154856\n",
            "Training loss per 100 training steps: 0.028688755694117087\n",
            "Training loss per 100 training steps: 0.028575945006118025\n",
            "Training loss per 100 training steps: 0.0285284503827279\n",
            "Training loss per 100 training steps: 0.02861480881860997\n",
            "Training loss per 100 training steps: 0.028573083628783934\n",
            "Training loss per 100 training steps: 0.02858839815725911\n",
            "Training loss per 100 training steps: 0.02848343983515709\n",
            "Training loss per 100 training steps: 0.028561496620084375\n",
            "Training loss per 100 training steps: 0.028587711330952257\n",
            "Training loss per 100 training steps: 0.02843177207520542\n",
            "Training loss per 100 training steps: 0.028374921748982162\n",
            "Training loss per 100 training steps: 0.028435465172035893\n",
            "Training loss per 100 training steps: 0.028447361181897578\n",
            "Training loss per 100 training steps: 0.028446790491580033\n",
            "Training loss per 100 training steps: 0.028349272988474696\n",
            "Training loss per 100 training steps: 0.028351431173685427\n",
            "Training loss per 100 training steps: 0.028233670260855104\n",
            "Training loss per 100 training steps: 0.028222509119368622\n",
            "Training loss per 100 training steps: 0.028187305806701868\n",
            "Training loss per 100 training steps: 0.028135943672864233\n",
            "Training loss per 100 training steps: 0.02811639246282329\n",
            "Training loss per 100 training steps: 0.02807635299459906\n",
            "Training loss per 100 training steps: 0.028048711510112168\n",
            "Training loss per 100 training steps: 0.0280573253575582\n",
            "Training loss per 100 training steps: 0.028090635045387898\n",
            "Training loss per 100 training steps: 0.028111672728475475\n",
            "Training loss per 100 training steps: 0.02814999872763565\n",
            "Training loss per 100 training steps: 0.028188734038042392\n",
            "Training loss per 100 training steps: 0.02826332249596068\n",
            "Training loss per 100 training steps: 0.028254346296045698\n",
            "Training loss per 100 training steps: 0.0281790825465133\n",
            "Training loss per 100 training steps: 0.028174633424681128\n",
            "Training loss epoch: 0.028179717706722528\n",
            "Training accuracy epoch: 0.8292425484728634\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.026468954581068828\n",
            "Validation loss per 100 evaluation steps: 0.0273134015587857\n",
            "Validation loss per 100 evaluation steps: 0.02754759111558087\n",
            "Validation loss per 100 evaluation steps: 0.027808276897703762\n",
            "Validation loss per 100 evaluation steps: 0.027992809469578786\n",
            "Validation loss per 100 evaluation steps: 0.027912065990579625\n",
            "Validation loss per 100 evaluation steps: 0.02781035063921341\n",
            "Validation loss per 100 evaluation steps: 0.027835187439486618\n",
            "Validation Loss: 0.028023482108329698\n",
            "Validation Accuracy: 0.8225170410239419\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.00      0.00      0.00      1837\n",
            "      B-MISC       0.00      0.00      0.00       922\n",
            "       B-ORG       0.00      0.00      0.00      1341\n",
            "       B-PER       0.00      0.00      0.00      1842\n",
            "       I-LOC       0.00      0.00      0.00      1801\n",
            "      I-MISC       0.00      0.00      0.00       935\n",
            "       I-ORG       0.00      0.00      0.00      2319\n",
            "       I-PER       0.28      0.99      0.43      4219\n",
            "           O       0.99      0.99      0.99     51041\n",
            "\n",
            "    accuracy                           0.83     66257\n",
            "   macro avg       0.14      0.22      0.16     66257\n",
            "weighted avg       0.78      0.83      0.79     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.8289  0.8289  0.8289\n",
            "macro        0.1410  0.2208  0.1586\n",
            "weighted     0.7811  0.8289  0.7922\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.02833911273162812\n",
            "Test loss per 100 evaluation steps: 0.029599073252175003\n",
            "Test loss per 100 evaluation steps: 0.029878828722673157\n",
            "Test loss per 100 evaluation steps: 0.02906641196983401\n",
            "Test loss per 100 evaluation steps: 0.02882791384542361\n",
            "Test loss per 100 evaluation steps: 0.029270403971507524\n",
            "Test loss per 100 evaluation steps: 0.02958003705068092\n",
            "Test loss per 100 evaluation steps: 0.029658957645006014\n",
            "Test loss per 100 evaluation steps: 0.0294550284363019\n",
            "Test Loss: 0.029473225472102613\n",
            "Test Accuracy: 0.80898053367792\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.00      0.00      0.00      1668\n",
            "      B-MISC       0.00      0.00      0.00       702\n",
            "       B-ORG       0.00      0.00      0.00      1661\n",
            "       B-PER       0.00      0.00      0.00      1617\n",
            "       I-LOC       0.00      0.00      0.00      1394\n",
            "      I-MISC       0.00      0.00      0.00       736\n",
            "       I-ORG       0.00      0.00      0.00      2804\n",
            "       I-PER       0.26      0.99      0.41      3810\n",
            "           O       0.99      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.82     61486\n",
            "   macro avg       0.14      0.22      0.16     61486\n",
            "weighted avg       0.78      0.82      0.78     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.8187  0.8187  0.8187\n",
            "macro        0.1391  0.2204  0.1559\n",
            "weighted     0.7752  0.8187  0.7837\n",
            "Test steps: 921\n",
            "Training epoch: 4\n",
            "Training loss per 100 training steps: 0.027823120682296577\n",
            "Training loss per 100 training steps: 0.02733872423450521\n",
            "Training loss per 100 training steps: 0.028105439509818097\n",
            "Training loss per 100 training steps: 0.02866463541478879\n",
            "Training loss per 100 training steps: 0.028465580019081244\n",
            "Training loss per 100 training steps: 0.028250681520827735\n",
            "Training loss per 100 training steps: 0.028365262009361427\n",
            "Training loss per 100 training steps: 0.02846888547846902\n",
            "Training loss per 100 training steps: 0.028513401253375276\n",
            "Training loss per 100 training steps: 0.028410823003039695\n",
            "Training loss per 100 training steps: 0.028088867170418697\n",
            "Training loss per 100 training steps: 0.027862143817692413\n",
            "Training loss per 100 training steps: 0.027744787774214415\n",
            "Training loss per 100 training steps: 0.027569361071873574\n",
            "Training loss per 100 training steps: 0.027338074549324423\n",
            "Training loss per 100 training steps: 0.027158463597215812\n",
            "Training loss per 100 training steps: 0.02690406911779599\n",
            "Training loss per 100 training steps: 0.026597156689555656\n",
            "Training loss per 100 training steps: 0.026325237901925758\n",
            "Training loss per 100 training steps: 0.02612133891144913\n",
            "Training loss per 100 training steps: 0.02602152948964171\n",
            "Training loss per 100 training steps: 0.025839646141427114\n",
            "Training loss per 100 training steps: 0.02569456174815929\n",
            "Training loss per 100 training steps: 0.025572655618846814\n",
            "Training loss per 100 training steps: 0.025398773920180976\n",
            "Training loss per 100 training steps: 0.025326500296678905\n",
            "Training loss per 100 training steps: 0.0252374545250711\n",
            "Training loss per 100 training steps: 0.025159010512169126\n",
            "Training loss per 100 training steps: 0.024979424953971516\n",
            "Training loss per 100 training steps: 0.02498369243070192\n",
            "Training loss per 100 training steps: 0.024912488940289253\n",
            "Training loss per 100 training steps: 0.02487753897669336\n",
            "Training loss per 100 training steps: 0.024803669834337516\n",
            "Training loss per 100 training steps: 0.024741052325908265\n",
            "Training loss per 100 training steps: 0.024707893609306277\n",
            "Training loss per 100 training steps: 0.02465908870951858\n",
            "Training loss per 100 training steps: 0.02458601248891461\n",
            "Training loss epoch: 0.02454096756797361\n",
            "Training accuracy epoch: 0.8317146858279943\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.020170995234511793\n",
            "Validation loss per 100 evaluation steps: 0.02054437116952613\n",
            "Validation loss per 100 evaluation steps: 0.020755626967487235\n",
            "Validation loss per 100 evaluation steps: 0.02063889753771946\n",
            "Validation loss per 100 evaluation steps: 0.021100256682839245\n",
            "Validation loss per 100 evaluation steps: 0.02152468995540403\n",
            "Validation loss per 100 evaluation steps: 0.02159854834716368\n",
            "Validation loss per 100 evaluation steps: 0.02156289670863771\n",
            "Validation Loss: 0.021628752690922542\n",
            "Validation Accuracy: 0.8507956858901884\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.00      0.00      0.00      1837\n",
            "      B-MISC       0.00      0.00      0.00       922\n",
            "       B-ORG       0.00      0.00      0.00      1341\n",
            "       B-PER       0.80      0.95      0.87      1842\n",
            "       I-LOC       0.14      0.05      0.08      1801\n",
            "      I-MISC       0.00      0.00      0.00       935\n",
            "       I-ORG       0.00      0.00      0.00      2319\n",
            "       I-PER       0.34      0.99      0.51      4219\n",
            "           O       0.99      0.99      0.99     51041\n",
            "\n",
            "    accuracy                           0.86     66257\n",
            "   macro avg       0.25      0.33      0.27     66257\n",
            "weighted avg       0.81      0.86      0.82     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.8566  0.8566  0.8566\n",
            "macro        0.2529  0.3320  0.2716\n",
            "weighted     0.8132  0.8566  0.8239\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.02325770603958517\n",
            "Test loss per 100 evaluation steps: 0.024135887671727686\n",
            "Test loss per 100 evaluation steps: 0.024053618105826898\n",
            "Test loss per 100 evaluation steps: 0.023834443643281703\n",
            "Test loss per 100 evaluation steps: 0.023733463162789122\n",
            "Test loss per 100 evaluation steps: 0.023720270462993843\n",
            "Test loss per 100 evaluation steps: 0.023872552856503585\n",
            "Test loss per 100 evaluation steps: 0.023967178135499125\n",
            "Test loss per 100 evaluation steps: 0.023823980002457067\n",
            "Test Loss: 0.02388637547657282\n",
            "Test Accuracy: 0.8286680667446286\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.00      0.00      0.00      1668\n",
            "      B-MISC       0.00      0.00      0.00       702\n",
            "       B-ORG       0.00      0.00      0.00      1661\n",
            "       B-PER       0.78      0.92      0.84      1617\n",
            "       I-LOC       0.12      0.05      0.07      1394\n",
            "      I-MISC       0.00      0.00      0.00       736\n",
            "       I-ORG       0.00      0.00      0.00      2804\n",
            "       I-PER       0.31      0.98      0.47      3810\n",
            "           O       0.99      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.84     61486\n",
            "   macro avg       0.24      0.33      0.26     61486\n",
            "weighted avg       0.80      0.84      0.81     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.8420  0.8420  0.8420\n",
            "macro        0.2437  0.3273  0.2637\n",
            "weighted     0.8027  0.8420  0.8109\n",
            "Test steps: 921\n",
            "====================================================================================================\n",
            "loss_name: l2\n",
            "model configuration\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Ner_Model(\n",
            "  (model): DebertaModel(\n",
            "    (embeddings): DebertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "      (LayerNorm): DebertaLayerNorm()\n",
            "      (dropout): StableDropout()\n",
            "    )\n",
            "    (encoder): DebertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x DebertaLayer(\n",
            "          (attention): DebertaAttention(\n",
            "            (self): DisentangledSelfAttention(\n",
            "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
            "              (pos_dropout): StableDropout()\n",
            "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "            (output): DebertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): DebertaLayerNorm()\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "          )\n",
            "          (intermediate): DebertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): DebertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): DebertaLayerNorm()\n",
            "            (dropout): StableDropout()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (rel_embeddings): Embedding(1024, 768)\n",
            "    )\n",
            "  )\n",
            "  (linear_model): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=384, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=384, out_features=256, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (9): ReLU()\n",
            "    (10): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (11): ReLU()\n",
            "    (12): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (13): ReLU()\n",
            "    (14): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (15): ReLU()\n",
            "  )\n",
            "  (classifier): Linear(in_features=128, out_features=9, bias=True)\n",
            ")\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.12225458458065987\n",
            "Training loss per 100 training steps: 0.11386066522449255\n",
            "Training loss per 100 training steps: 0.09996366093556086\n",
            "Training loss per 100 training steps: 0.08569086171453819\n",
            "Training loss per 100 training steps: 0.0744089124724269\n",
            "Training loss per 100 training steps: 0.06677069010601068\n",
            "Training loss per 100 training steps: 0.06097200046077238\n",
            "Training loss per 100 training steps: 0.056880318148614606\n",
            "Training loss per 100 training steps: 0.05339253072885589\n",
            "Training loss per 100 training steps: 0.05050440396880731\n",
            "Training loss per 100 training steps: 0.048247790418251744\n",
            "Training loss per 100 training steps: 0.04621213173333066\n",
            "Training loss per 100 training steps: 0.04447130181785794\n",
            "Training loss per 100 training steps: 0.04295589319871007\n",
            "Training loss per 100 training steps: 0.04156158153922297\n",
            "Training loss per 100 training steps: 0.040279447719153726\n",
            "Training loss per 100 training steps: 0.03910159074256679\n",
            "Training loss per 100 training steps: 0.03801254353117757\n",
            "Training loss per 100 training steps: 0.037069564187645725\n",
            "Training loss per 100 training steps: 0.03605493198206841\n",
            "Training loss per 100 training steps: 0.03506118869317236\n",
            "Training loss per 100 training steps: 0.03422303271827993\n",
            "Training loss per 100 training steps: 0.03333191720995471\n",
            "Training loss per 100 training steps: 0.032544844488394724\n",
            "Training loss per 100 training steps: 0.03185092581743811\n",
            "Training loss per 100 training steps: 0.031196411244772418\n",
            "Training loss per 100 training steps: 0.030547326370208423\n",
            "Training loss per 100 training steps: 0.02993295395637233\n",
            "Training loss per 100 training steps: 0.029323758048809895\n",
            "Training loss per 100 training steps: 0.028809807089376287\n",
            "Training loss per 100 training steps: 0.028239803774862182\n",
            "Training loss per 100 training steps: 0.02769645804969116\n",
            "Training loss per 100 training steps: 0.02718924035306345\n",
            "Training loss per 100 training steps: 0.02669556078516935\n",
            "Training loss per 100 training steps: 0.026229860681192804\n",
            "Training loss per 100 training steps: 0.025736824460368554\n",
            "Training loss per 100 training steps: 0.02524771824012434\n",
            "Training loss epoch: 0.02501142202976524\n",
            "Training accuracy epoch: 0.8340836506607786\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.006647652666433714\n",
            "Validation loss per 100 evaluation steps: 0.00747994695619127\n",
            "Validation loss per 100 evaluation steps: 0.007073013638995083\n",
            "Validation loss per 100 evaluation steps: 0.007160921679587773\n",
            "Validation loss per 100 evaluation steps: 0.0072224055731931\n",
            "Validation loss per 100 evaluation steps: 0.00733650893803618\n",
            "Validation loss per 100 evaluation steps: 0.007285625453171503\n",
            "Validation loss per 100 evaluation steps: 0.007273213835151182\n",
            "Validation Loss: 0.007238712600367751\n",
            "Validation Accuracy: 0.9589015267414452\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.93      0.95      0.94      1837\n",
            "      B-MISC       0.00      0.00      0.00       922\n",
            "       B-ORG       0.90      0.90      0.90      1341\n",
            "       B-PER       0.68      0.98      0.80      1842\n",
            "       I-LOC       0.80      0.91      0.85      1801\n",
            "      I-MISC       0.00      0.00      0.00       935\n",
            "       I-ORG       0.80      0.90      0.84      2319\n",
            "       I-PER       0.97      0.98      0.98      4219\n",
            "           O       0.99      1.00      0.99     51041\n",
            "\n",
            "    accuracy                           0.96     66257\n",
            "   macro avg       0.67      0.73      0.70     66257\n",
            "weighted avg       0.94      0.96      0.95     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9591  0.9591  0.9591\n",
            "macro        0.6746  0.7349  0.7010\n",
            "weighted     0.9367  0.9591  0.9469\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.00705525770157692\n",
            "Test loss per 100 evaluation steps: 0.006693370645443793\n",
            "Test loss per 100 evaluation steps: 0.007063760484161321\n",
            "Test loss per 100 evaluation steps: 0.007311767003011482\n",
            "Test loss per 100 evaluation steps: 0.007363687813543947\n",
            "Test loss per 100 evaluation steps: 0.0074346133889169625\n",
            "Test loss per 100 evaluation steps: 0.00761808459248901\n",
            "Test loss per 100 evaluation steps: 0.007753046841644391\n",
            "Test loss per 100 evaluation steps: 0.0077914616905521445\n",
            "Test Loss: 0.007829841855136918\n",
            "Test Accuracy: 0.9568674483306953\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.91      0.92      0.92      1668\n",
            "      B-MISC       0.00      0.00      0.00       702\n",
            "       B-ORG       0.89      0.87      0.88      1661\n",
            "       B-PER       0.68      0.97      0.80      1617\n",
            "       I-LOC       0.80      0.90      0.85      1394\n",
            "      I-MISC       0.00      0.00      0.00       736\n",
            "       I-ORG       0.80      0.91      0.85      2804\n",
            "       I-PER       0.96      0.98      0.97      3810\n",
            "           O       0.99      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.96     61486\n",
            "   macro avg       0.67      0.73      0.70     61486\n",
            "weighted avg       0.94      0.96      0.95     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9569  0.9569  0.9569\n",
            "macro        0.6698  0.7274  0.6950\n",
            "weighted     0.9390  0.9569  0.9471\n",
            "Test steps: 921\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.006479278718488786\n",
            "Training loss per 100 training steps: 0.006943262383365436\n",
            "Training loss per 100 training steps: 0.00688402050820514\n",
            "Training loss per 100 training steps: 0.006761784117666139\n",
            "Training loss per 100 training steps: 0.006702999373988859\n",
            "Training loss per 100 training steps: 0.006542925766946913\n",
            "Training loss per 100 training steps: 0.006621283423150349\n",
            "Training loss per 100 training steps: 0.006573980755952107\n",
            "Training loss per 100 training steps: 0.006454198318820595\n",
            "Training loss per 100 training steps: 0.006285997180020786\n",
            "Training loss per 100 training steps: 0.00616171229057727\n",
            "Training loss per 100 training steps: 0.0061399354542406095\n",
            "Training loss per 100 training steps: 0.00610837834922327\n",
            "Training loss per 100 training steps: 0.005918900479694327\n",
            "Training loss per 100 training steps: 0.005898924920583037\n",
            "Training loss per 100 training steps: 0.005827964758769895\n",
            "Training loss per 100 training steps: 0.005720840844250607\n",
            "Training loss per 100 training steps: 0.005627365401153131\n",
            "Training loss per 100 training steps: 0.005545675527819664\n",
            "Training loss per 100 training steps: 0.005526172884381594\n",
            "Training loss per 100 training steps: 0.005424675097617312\n",
            "Training loss per 100 training steps: 0.005355119116809213\n",
            "Training loss per 100 training steps: 0.005287497559781079\n",
            "Training loss per 100 training steps: 0.005210684345868231\n",
            "Training loss per 100 training steps: 0.005226844803746644\n",
            "Training loss per 100 training steps: 0.005165876547606296\n",
            "Training loss per 100 training steps: 0.005094780353983542\n",
            "Training loss per 100 training steps: 0.005058270875442109\n",
            "Training loss per 100 training steps: 0.005009343119148478\n",
            "Training loss per 100 training steps: 0.004946388718675735\n",
            "Training loss per 100 training steps: 0.004908401845709558\n",
            "Training loss per 100 training steps: 0.004871534101245061\n",
            "Training loss per 100 training steps: 0.004821876359551838\n",
            "Training loss per 100 training steps: 0.004793717113476733\n",
            "Training loss per 100 training steps: 0.004758013778585466\n",
            "Training loss per 100 training steps: 0.004712091093505883\n",
            "Training loss per 100 training steps: 0.004688077425720178\n",
            "Training loss epoch: 0.0046607278737324925\n",
            "Training accuracy epoch: 0.9742903214888694\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.0022050978261177077\n",
            "Validation loss per 100 evaluation steps: 0.0034591928472491418\n",
            "Validation loss per 100 evaluation steps: 0.0033638661883257253\n",
            "Validation loss per 100 evaluation steps: 0.0031290882181360755\n",
            "Validation loss per 100 evaluation steps: 0.0033714782140577882\n",
            "Validation loss per 100 evaluation steps: 0.0031130328244141007\n",
            "Validation loss per 100 evaluation steps: 0.0030540158220264857\n",
            "Validation loss per 100 evaluation steps: 0.003001660763130758\n",
            "Validation Loss: 0.0029606045032556246\n",
            "Validation Accuracy: 0.984791598047037\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.95      0.97      0.96      1837\n",
            "      B-MISC       0.93      0.90      0.91       922\n",
            "       B-ORG       0.93      0.94      0.93      1341\n",
            "       B-PER       0.97      0.97      0.97      1842\n",
            "       I-LOC       0.94      0.97      0.95      1801\n",
            "      I-MISC       0.86      0.84      0.85       935\n",
            "       I-ORG       0.92      0.92      0.92      2319\n",
            "       I-PER       0.98      0.97      0.98      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.94      0.94      0.94     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9851  0.9851  0.9851\n",
            "macro        0.9421  0.9418  0.9419\n",
            "weighted     0.9851  0.9851  0.9851\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.005512013996885798\n",
            "Test loss per 100 evaluation steps: 0.005372464751790176\n",
            "Test loss per 100 evaluation steps: 0.005475385291241158\n",
            "Test loss per 100 evaluation steps: 0.005338717176357477\n",
            "Test loss per 100 evaluation steps: 0.005748241514995243\n",
            "Test loss per 100 evaluation steps: 0.005835121579152656\n",
            "Test loss per 100 evaluation steps: 0.005609053880901099\n",
            "Test loss per 100 evaluation steps: 0.005874750837759848\n",
            "Test loss per 100 evaluation steps: 0.0057818365592973145\n",
            "Test Loss: 0.005774990033260051\n",
            "Test Accuracy: 0.9705000287078309\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.90      0.94      0.92      1668\n",
            "      B-MISC       0.83      0.82      0.82       702\n",
            "       B-ORG       0.89      0.91      0.90      1661\n",
            "       B-PER       0.97      0.96      0.96      1617\n",
            "       I-LOC       0.84      0.93      0.88      1394\n",
            "      I-MISC       0.64      0.69      0.67       736\n",
            "       I-ORG       0.88      0.93      0.90      2804\n",
            "       I-PER       0.97      0.96      0.97      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.88      0.90      0.89     61486\n",
            "weighted avg       0.97      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9724  0.9724  0.9724\n",
            "macro        0.8804  0.9035  0.8914\n",
            "weighted     0.9735  0.9724  0.9728\n",
            "Test steps: 921\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.002798154266547499\n",
            "Training loss per 100 training steps: 0.0024074991693396443\n",
            "Training loss per 100 training steps: 0.0023946480440736194\n",
            "Training loss per 100 training steps: 0.0024881701409435663\n",
            "Training loss per 100 training steps: 0.00240422964170466\n",
            "Training loss per 100 training steps: 0.0025227263700450444\n",
            "Training loss per 100 training steps: 0.0025548102857711847\n",
            "Training loss per 100 training steps: 0.0025999035181746423\n",
            "Training loss per 100 training steps: 0.002626828908107402\n",
            "Training loss per 100 training steps: 0.002616445465624565\n",
            "Training loss per 100 training steps: 0.002599713023911053\n",
            "Training loss per 100 training steps: 0.002554138431036487\n",
            "Training loss per 100 training steps: 0.0025916800779619874\n",
            "Training loss per 100 training steps: 0.002587584382726423\n",
            "Training loss per 100 training steps: 0.002597027540629521\n",
            "Training loss per 100 training steps: 0.0026261103785333263\n",
            "Training loss per 100 training steps: 0.002594420613231705\n",
            "Training loss per 100 training steps: 0.0025937381562713803\n",
            "Training loss per 100 training steps: 0.0025945066340624947\n",
            "Training loss per 100 training steps: 0.0025600760650540907\n",
            "Training loss per 100 training steps: 0.0025530693489487196\n",
            "Training loss per 100 training steps: 0.002538466143331458\n",
            "Training loss per 100 training steps: 0.0025316587492951563\n",
            "Training loss per 100 training steps: 0.002531411168415237\n",
            "Training loss per 100 training steps: 0.002535691301638508\n",
            "Training loss per 100 training steps: 0.002591478499099279\n",
            "Training loss per 100 training steps: 0.002598886924383974\n",
            "Training loss per 100 training steps: 0.0026128279868229192\n",
            "Training loss per 100 training steps: 0.002606486503980642\n",
            "Training loss per 100 training steps: 0.0026028766238138513\n",
            "Training loss per 100 training steps: 0.002604746449251252\n",
            "Training loss per 100 training steps: 0.002590659178615624\n",
            "Training loss per 100 training steps: 0.002578641766060283\n",
            "Training loss per 100 training steps: 0.0025682557939741375\n",
            "Training loss per 100 training steps: 0.002560098433798495\n",
            "Training loss per 100 training steps: 0.002561954007017246\n",
            "Training loss per 100 training steps: 0.0025709687064831874\n",
            "Training loss epoch: 0.002591928752251358\n",
            "Training accuracy epoch: 0.9863919541622066\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.002825867853607633\n",
            "Validation loss per 100 evaluation steps: 0.0028749512693320867\n",
            "Validation loss per 100 evaluation steps: 0.0027527776462860252\n",
            "Validation loss per 100 evaluation steps: 0.0027807927162575652\n",
            "Validation loss per 100 evaluation steps: 0.002798545153622399\n",
            "Validation loss per 100 evaluation steps: 0.0029648265852786912\n",
            "Validation loss per 100 evaluation steps: 0.0029939583076130866\n",
            "Validation loss per 100 evaluation steps: 0.003145686574162028\n",
            "Validation Loss: 0.0031136620108226437\n",
            "Validation Accuracy: 0.9841929863225726\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.95      0.96      1837\n",
            "      B-MISC       0.92      0.93      0.92       922\n",
            "       B-ORG       0.89      0.96      0.92      1341\n",
            "       B-PER       0.98      0.98      0.98      1842\n",
            "       I-LOC       0.97      0.92      0.94      1801\n",
            "      I-MISC       0.84      0.89      0.87       935\n",
            "       I-ORG       0.88      0.96      0.92      2319\n",
            "       I-PER       0.99      0.98      0.98      4219\n",
            "           O       1.00      0.99      1.00     51041\n",
            "\n",
            "    accuracy                           0.98     66257\n",
            "   macro avg       0.94      0.95      0.94     66257\n",
            "weighted avg       0.99      0.98      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9849  0.9849  0.9849\n",
            "macro        0.9362  0.9511  0.9431\n",
            "weighted     0.9856  0.9849  0.9851\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.006508626914510387\n",
            "Test loss per 100 evaluation steps: 0.005855607932244311\n",
            "Test loss per 100 evaluation steps: 0.005992294351429639\n",
            "Test loss per 100 evaluation steps: 0.006168155478462722\n",
            "Test loss per 100 evaluation steps: 0.006192787710970152\n",
            "Test loss per 100 evaluation steps: 0.0063500672574688605\n",
            "Test loss per 100 evaluation steps: 0.006289468797913287\n",
            "Test loss per 100 evaluation steps: 0.0061691072914527465\n",
            "Test loss per 100 evaluation steps: 0.006159356217855626\n",
            "Test Loss: 0.0060969517537997255\n",
            "Test Accuracy: 0.9680096738645461\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.93      0.92      0.93      1668\n",
            "      B-MISC       0.78      0.85      0.81       702\n",
            "       B-ORG       0.86      0.92      0.89      1661\n",
            "       B-PER       0.95      0.95      0.95      1617\n",
            "       I-LOC       0.90      0.89      0.90      1394\n",
            "      I-MISC       0.59      0.70      0.64       736\n",
            "       I-ORG       0.84      0.95      0.89      2804\n",
            "       I-PER       0.97      0.97      0.97      3810\n",
            "           O       1.00      0.98      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.87      0.90      0.89     61486\n",
            "weighted avg       0.97      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9695  0.9695  0.9695\n",
            "macro        0.8684  0.9041  0.8851\n",
            "weighted     0.9718  0.9695  0.9703\n",
            "Test steps: 921\n",
            "Training epoch: 4\n",
            "Training loss per 100 training steps: 0.0017402305117502693\n",
            "Training loss per 100 training steps: 0.0017542823293752007\n",
            "Training loss per 100 training steps: 0.0019570368111969097\n",
            "Training loss per 100 training steps: 0.0018931542997199812\n",
            "Training loss per 100 training steps: 0.0019467865433880434\n",
            "Training loss per 100 training steps: 0.0019364583442234107\n",
            "Training loss per 100 training steps: 0.0019356935610871265\n",
            "Training loss per 100 training steps: 0.0020559208071477997\n",
            "Training loss per 100 training steps: 0.0020713388100618126\n",
            "Training loss per 100 training steps: 0.0020257062891178065\n",
            "Training loss per 100 training steps: 0.002010768593051001\n",
            "Training loss per 100 training steps: 0.0019931640906384017\n",
            "Training loss per 100 training steps: 0.0019599679100652935\n",
            "Training loss per 100 training steps: 0.0019723307950932525\n",
            "Training loss per 100 training steps: 0.0019782553428794925\n",
            "Training loss per 100 training steps: 0.0019417594754878565\n",
            "Training loss per 100 training steps: 0.0019710254172674896\n",
            "Training loss per 100 training steps: 0.0019502228585836039\n",
            "Training loss per 100 training steps: 0.0019525791761694372\n",
            "Training loss per 100 training steps: 0.0019565664951142025\n",
            "Training loss per 100 training steps: 0.0019483448006749745\n",
            "Training loss per 100 training steps: 0.0019466353176854185\n",
            "Training loss per 100 training steps: 0.0019472036931675457\n",
            "Training loss per 100 training steps: 0.0019339334453419117\n",
            "Training loss per 100 training steps: 0.0018902590493235039\n",
            "Training loss per 100 training steps: 0.0019189285815145222\n",
            "Training loss per 100 training steps: 0.0019229806961394151\n",
            "Training loss per 100 training steps: 0.0019134324928771612\n",
            "Training loss per 100 training steps: 0.0019130518209390331\n",
            "Training loss per 100 training steps: 0.0019104338919231243\n",
            "Training loss per 100 training steps: 0.0018926212928420593\n",
            "Training loss per 100 training steps: 0.001941939106426247\n",
            "Training loss per 100 training steps: 0.0019650696521184177\n",
            "Training loss per 100 training steps: 0.001973600450441261\n",
            "Training loss per 100 training steps: 0.0019536445432892964\n",
            "Training loss per 100 training steps: 0.001955065052358249\n",
            "Training loss per 100 training steps: 0.00197122561147783\n",
            "Training loss epoch: 0.0019661676244499098\n",
            "Training accuracy epoch: 0.9897598815850621\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.0026233584260626232\n",
            "Validation loss per 100 evaluation steps: 0.00290147001749574\n",
            "Validation loss per 100 evaluation steps: 0.0029202280337146174\n",
            "Validation loss per 100 evaluation steps: 0.0028547750118013936\n",
            "Validation loss per 100 evaluation steps: 0.002816485924857261\n",
            "Validation loss per 100 evaluation steps: 0.002920993903735507\n",
            "Validation loss per 100 evaluation steps: 0.0028974559736369494\n",
            "Validation loss per 100 evaluation steps: 0.0028472556368024015\n",
            "Validation Loss: 0.002895786586940176\n",
            "Validation Accuracy: 0.9855552085978817\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.96      0.97      1837\n",
            "      B-MISC       0.92      0.93      0.93       922\n",
            "       B-ORG       0.91      0.95      0.93      1341\n",
            "       B-PER       0.98      0.98      0.98      1842\n",
            "       I-LOC       0.99      0.92      0.95      1801\n",
            "      I-MISC       0.86      0.89      0.87       935\n",
            "       I-ORG       0.86      0.96      0.91      2319\n",
            "       I-PER       0.99      0.98      0.98      4219\n",
            "           O       1.00      0.99      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.94      0.95      0.95     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9858  0.9858  0.9858\n",
            "macro        0.9417  0.9514  0.9460\n",
            "weighted     0.9865  0.9858  0.9860\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.00689002595438069\n",
            "Test loss per 100 evaluation steps: 0.006309043509754701\n",
            "Test loss per 100 evaluation steps: 0.006387457745707555\n",
            "Test loss per 100 evaluation steps: 0.00623078170188819\n",
            "Test loss per 100 evaluation steps: 0.0064360585620743226\n",
            "Test loss per 100 evaluation steps: 0.006245865245309687\n",
            "Test loss per 100 evaluation steps: 0.006177544615789624\n",
            "Test loss per 100 evaluation steps: 0.006066926191097082\n",
            "Test loss per 100 evaluation steps: 0.006144787394385074\n",
            "Test Loss: 0.006102672819640102\n",
            "Test Accuracy: 0.9693002633618995\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.95      0.92      0.93      1668\n",
            "      B-MISC       0.82      0.85      0.84       702\n",
            "       B-ORG       0.86      0.95      0.90      1661\n",
            "       B-PER       0.97      0.95      0.96      1617\n",
            "       I-LOC       0.94      0.88      0.91      1394\n",
            "      I-MISC       0.63      0.70      0.66       736\n",
            "       I-ORG       0.80      0.97      0.88      2804\n",
            "       I-PER       0.98      0.96      0.97      3810\n",
            "           O       1.00      0.98      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.88      0.91      0.89     61486\n",
            "weighted avg       0.97      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9711  0.9711  0.9711\n",
            "macro        0.8826  0.9055  0.8924\n",
            "weighted     0.9738  0.9711  0.9719\n",
            "Test steps: 921\n",
            "====================================================================================================\n",
            "loss_name: ce\n",
            "model configuration\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Ner_Model(\n",
            "  (model): DebertaModel(\n",
            "    (embeddings): DebertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "      (LayerNorm): DebertaLayerNorm()\n",
            "      (dropout): StableDropout()\n",
            "    )\n",
            "    (encoder): DebertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x DebertaLayer(\n",
            "          (attention): DebertaAttention(\n",
            "            (self): DisentangledSelfAttention(\n",
            "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
            "              (pos_dropout): StableDropout()\n",
            "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "            (output): DebertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): DebertaLayerNorm()\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "          )\n",
            "          (intermediate): DebertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): DebertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): DebertaLayerNorm()\n",
            "            (dropout): StableDropout()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (rel_embeddings): Embedding(1024, 768)\n",
            "    )\n",
            "  )\n",
            "  (linear_model): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=384, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=384, out_features=256, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (9): ReLU()\n",
            "    (10): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (11): ReLU()\n",
            "    (12): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (13): ReLU()\n",
            "    (14): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (15): ReLU()\n",
            "  )\n",
            "  (classifier): Linear(in_features=128, out_features=9, bias=True)\n",
            ")\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 2.1682180404663085\n",
            "Training loss per 100 training steps: 2.135759643912315\n",
            "Training loss per 100 training steps: 2.0380766324202217\n",
            "Training loss per 100 training steps: 1.8491662101447581\n",
            "Training loss per 100 training steps: 1.658661277115345\n",
            "Training loss per 100 training steps: 1.496878319780032\n",
            "Training loss per 100 training steps: 1.3704012700689159\n",
            "Training loss per 100 training steps: 1.2698429353337268\n",
            "Training loss per 100 training steps: 1.1909215661624653\n",
            "Training loss per 100 training steps: 1.1238556473632344\n",
            "Training loss per 100 training steps: 1.069473881911571\n",
            "Training loss per 100 training steps: 1.029072933389883\n",
            "Training loss per 100 training steps: 0.9943628254249836\n",
            "Training loss per 100 training steps: 0.9628576644513357\n",
            "Training loss per 100 training steps: 0.9324486455821122\n",
            "Training loss per 100 training steps: 0.90841753573186\n",
            "Training loss per 100 training steps: 0.8871521759912481\n",
            "Training loss per 100 training steps: 0.8693997119866415\n",
            "Training loss per 100 training steps: 0.8520766298436119\n",
            "Training loss per 100 training steps: 0.8375033077301923\n",
            "Training loss per 100 training steps: 0.8252857691051794\n",
            "Training loss per 100 training steps: 0.8125131555493201\n",
            "Training loss per 100 training steps: 0.8005276907331553\n",
            "Training loss per 100 training steps: 0.7878955064712985\n",
            "Training loss per 100 training steps: 0.7755069581060671\n",
            "Training loss per 100 training steps: 0.7625473374458782\n",
            "Training loss per 100 training steps: 0.7498084910090633\n",
            "Training loss per 100 training steps: 0.7371159033092304\n",
            "Training loss per 100 training steps: 0.7244200751867049\n",
            "Training loss per 100 training steps: 0.7122179393495899\n",
            "Training loss per 100 training steps: 0.6986890457236328\n",
            "Training loss per 100 training steps: 0.6863626156214013\n",
            "Training loss per 100 training steps: 0.6746100089987599\n",
            "Training loss per 100 training steps: 0.6633426613063955\n",
            "Training loss per 100 training steps: 0.6520695969717012\n",
            "Training loss per 100 training steps: 0.640315005068276\n",
            "Training loss per 100 training steps: 0.6288637810004907\n",
            "Training loss epoch: 0.624239660160555\n",
            "Training accuracy epoch: 0.7840960176550497\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.21533309947699308\n",
            "Validation loss per 100 evaluation steps: 0.21804584008175878\n",
            "Validation loss per 100 evaluation steps: 0.21751955467276274\n",
            "Validation loss per 100 evaluation steps: 0.22190287434728817\n",
            "Validation loss per 100 evaluation steps: 0.22068626884371043\n",
            "Validation loss per 100 evaluation steps: 0.22089341699591994\n",
            "Validation loss per 100 evaluation steps: 0.22285502422185216\n",
            "Validation loss per 100 evaluation steps: 0.22347541174953223\n",
            "Validation Loss: 0.22674139679182373\n",
            "Validation Accuracy: 0.9119657268153457\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.62      0.89      0.73      1837\n",
            "      B-MISC       0.00      0.00      0.00       922\n",
            "       B-ORG       0.71      0.16      0.26      1341\n",
            "       B-PER       0.62      0.98      0.76      1842\n",
            "       I-LOC       0.00      0.00      0.00      1801\n",
            "      I-MISC       0.00      0.00      0.00       935\n",
            "       I-ORG       0.51      0.85      0.64      2319\n",
            "       I-PER       0.90      0.99      0.94      4219\n",
            "           O       0.98      1.00      0.99     51041\n",
            "\n",
            "    accuracy                           0.92     66257\n",
            "   macro avg       0.48      0.54      0.48     66257\n",
            "weighted avg       0.88      0.92      0.89     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9168  0.9168  0.9168\n",
            "macro        0.4824  0.5406  0.4798\n",
            "weighted     0.8798  0.9168  0.8913\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.22808253031224013\n",
            "Test loss per 100 evaluation steps: 0.23540139923803508\n",
            "Test loss per 100 evaluation steps: 0.24581950356562932\n",
            "Test loss per 100 evaluation steps: 0.24684948842041193\n",
            "Test loss per 100 evaluation steps: 0.24930185518111103\n",
            "Test loss per 100 evaluation steps: 0.2446171719726408\n",
            "Test loss per 100 evaluation steps: 0.24151652507400806\n",
            "Test loss per 100 evaluation steps: 0.23999050470461952\n",
            "Test loss per 100 evaluation steps: 0.2404883323044568\n",
            "Test Loss: 0.24078473798365885\n",
            "Test Accuracy: 0.9144866232646511\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.55      0.91      0.68      1668\n",
            "      B-MISC       0.00      0.00      0.00       702\n",
            "       B-ORG       0.78      0.15      0.25      1661\n",
            "       B-PER       0.64      0.97      0.77      1617\n",
            "       I-LOC       0.00      0.00      0.00      1394\n",
            "      I-MISC       0.00      0.00      0.00       736\n",
            "       I-ORG       0.62      0.88      0.73      2804\n",
            "       I-PER       0.88      0.99      0.93      3810\n",
            "           O       0.98      1.00      0.99     47094\n",
            "\n",
            "    accuracy                           0.92     61486\n",
            "   macro avg       0.49      0.54      0.48     61486\n",
            "weighted avg       0.89      0.92      0.89     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9182  0.9182  0.9182\n",
            "macro        0.4947  0.5435  0.4837\n",
            "weighted     0.8887  0.9182  0.8943\n",
            "Test steps: 921\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.23782444514334203\n",
            "Training loss per 100 training steps: 0.2338865937294031\n",
            "Training loss per 100 training steps: 0.22980959012493257\n",
            "Training loss per 100 training steps: 0.22080181731034826\n",
            "Training loss per 100 training steps: 0.2188793999044574\n",
            "Training loss per 100 training steps: 0.21149022390711858\n",
            "Training loss per 100 training steps: 0.20784537971310782\n",
            "Training loss per 100 training steps: 0.20317342079964873\n",
            "Training loss per 100 training steps: 0.1983110817596187\n",
            "Training loss per 100 training steps: 0.19678828986844746\n",
            "Training loss per 100 training steps: 0.19250068170711696\n",
            "Training loss per 100 training steps: 0.18910387182448177\n",
            "Training loss per 100 training steps: 0.18533696687740023\n",
            "Training loss per 100 training steps: 0.1837301056868247\n",
            "Training loss per 100 training steps: 0.17902452762110624\n",
            "Training loss per 100 training steps: 0.17492800707575953\n",
            "Training loss per 100 training steps: 0.1713217120814507\n",
            "Training loss per 100 training steps: 0.1685290823710966\n",
            "Training loss per 100 training steps: 0.165849024329647\n",
            "Training loss per 100 training steps: 0.16311335957396658\n",
            "Training loss per 100 training steps: 0.160536158379317\n",
            "Training loss per 100 training steps: 0.15827619791605024\n",
            "Training loss per 100 training steps: 0.15678461132362312\n",
            "Training loss per 100 training steps: 0.15401616317181227\n",
            "Training loss per 100 training steps: 0.15268514192756266\n",
            "Training loss per 100 training steps: 0.1507045369070525\n",
            "Training loss per 100 training steps: 0.1488520122800643\n",
            "Training loss per 100 training steps: 0.14780932494904847\n",
            "Training loss per 100 training steps: 0.14594927381989065\n",
            "Training loss per 100 training steps: 0.1443386792372912\n",
            "Training loss per 100 training steps: 0.14274634646501527\n",
            "Training loss per 100 training steps: 0.14158796114968936\n",
            "Training loss per 100 training steps: 0.13983592577936538\n",
            "Training loss per 100 training steps: 0.1386308034290628\n",
            "Training loss per 100 training steps: 0.1373231114502331\n",
            "Training loss per 100 training steps: 0.13627642433553572\n",
            "Training loss per 100 training steps: 0.13416051718343847\n",
            "Training loss epoch: 0.1337890200158369\n",
            "Training accuracy epoch: 0.9565910087090147\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.07057131363661029\n",
            "Validation loss per 100 evaluation steps: 0.06807091289141681\n",
            "Validation loss per 100 evaluation steps: 0.07474920413224026\n",
            "Validation loss per 100 evaluation steps: 0.07786595913232304\n",
            "Validation loss per 100 evaluation steps: 0.07569167053885759\n",
            "Validation loss per 100 evaluation steps: 0.07539801809781542\n",
            "Validation loss per 100 evaluation steps: 0.07626772116100515\n",
            "Validation loss per 100 evaluation steps: 0.07528031711361109\n",
            "Validation Loss: 0.0745938284570153\n",
            "Validation Accuracy: 0.9825224929237708\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.96      0.96      0.96      1837\n",
            "      B-MISC       0.81      0.93      0.87       922\n",
            "       B-ORG       0.93      0.91      0.92      1341\n",
            "       B-PER       0.98      0.98      0.98      1842\n",
            "       I-LOC       0.93      0.92      0.93      1801\n",
            "      I-MISC       0.77      0.86      0.81       935\n",
            "       I-ORG       0.90      0.88      0.89      2319\n",
            "       I-PER       0.99      0.98      0.98      4219\n",
            "           O       1.00      0.99      1.00     51041\n",
            "\n",
            "    accuracy                           0.98     66257\n",
            "   macro avg       0.92      0.94      0.93     66257\n",
            "weighted avg       0.98      0.98      0.98     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9820  0.9820  0.9820\n",
            "macro        0.9189  0.9351  0.9263\n",
            "weighted     0.9826  0.9820  0.9822\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.10597751446533948\n",
            "Test loss per 100 evaluation steps: 0.11006059309176636\n",
            "Test loss per 100 evaluation steps: 0.12038695010938681\n",
            "Test loss per 100 evaluation steps: 0.13135950160823995\n",
            "Test loss per 100 evaluation steps: 0.1358925953328144\n",
            "Test loss per 100 evaluation steps: 0.12913000607698147\n",
            "Test loss per 100 evaluation steps: 0.1329597273714275\n",
            "Test loss per 100 evaluation steps: 0.13500048081514252\n",
            "Test loss per 100 evaluation steps: 0.13330677016418324\n",
            "Test Loss: 0.13522205794767056\n",
            "Test Accuracy: 0.969373254879776\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.94      0.92      0.93      1668\n",
            "      B-MISC       0.69      0.88      0.77       702\n",
            "       B-ORG       0.89      0.90      0.89      1661\n",
            "       B-PER       0.96      0.96      0.96      1617\n",
            "       I-LOC       0.88      0.89      0.88      1394\n",
            "      I-MISC       0.61      0.70      0.65       736\n",
            "       I-ORG       0.87      0.91      0.89      2804\n",
            "       I-PER       0.97      0.98      0.98      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.87      0.90      0.88     61486\n",
            "weighted avg       0.97      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9711  0.9711  0.9711\n",
            "macro        0.8680  0.9031  0.8838\n",
            "weighted     0.9731  0.9711  0.9719\n",
            "Test steps: 921\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.053454291322268546\n",
            "Training loss per 100 training steps: 0.056883449922315775\n",
            "Training loss per 100 training steps: 0.06034402331376138\n",
            "Training loss per 100 training steps: 0.06017362350394251\n",
            "Training loss per 100 training steps: 0.06162966167059494\n",
            "Training loss per 100 training steps: 0.05959732461725556\n",
            "Training loss per 100 training steps: 0.05914607919773386\n",
            "Training loss per 100 training steps: 0.05879614687557478\n",
            "Training loss per 100 training steps: 0.060180782500651224\n",
            "Training loss per 100 training steps: 0.05988835796659987\n",
            "Training loss per 100 training steps: 0.05929764791766171\n",
            "Training loss per 100 training steps: 0.06065148118212771\n",
            "Training loss per 100 training steps: 0.06050563666395297\n",
            "Training loss per 100 training steps: 0.06119237700773478\n",
            "Training loss per 100 training steps: 0.060577410131081706\n",
            "Training loss per 100 training steps: 0.05973176844143154\n",
            "Training loss per 100 training steps: 0.059207183109433306\n",
            "Training loss per 100 training steps: 0.05853307859029277\n",
            "Training loss per 100 training steps: 0.05914066290453583\n",
            "Training loss per 100 training steps: 0.05814790411091053\n",
            "Training loss per 100 training steps: 0.05857544184369804\n",
            "Training loss per 100 training steps: 0.05871010915387987\n",
            "Training loss per 100 training steps: 0.058768877922568366\n",
            "Training loss per 100 training steps: 0.058339761247927224\n",
            "Training loss per 100 training steps: 0.05838077140381647\n",
            "Training loss per 100 training steps: 0.058932286804739796\n",
            "Training loss per 100 training steps: 0.05869534088562634\n",
            "Training loss per 100 training steps: 0.059614439260142586\n",
            "Training loss per 100 training steps: 0.05972627396177343\n",
            "Training loss per 100 training steps: 0.059326031056968834\n",
            "Training loss per 100 training steps: 0.05936843398190362\n",
            "Training loss per 100 training steps: 0.05972859091709211\n",
            "Training loss per 100 training steps: 0.058869681810301146\n",
            "Training loss per 100 training steps: 0.05912121451458862\n",
            "Training loss per 100 training steps: 0.05910422713355157\n",
            "Training loss per 100 training steps: 0.05865572201720978\n",
            "Training loss per 100 training steps: 0.058708575297754875\n",
            "Training loss epoch: 0.05855567726809185\n",
            "Training accuracy epoch: 0.9870126038727355\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.06739573940867558\n",
            "Validation loss per 100 evaluation steps: 0.07754222908086376\n",
            "Validation loss per 100 evaluation steps: 0.08039834743870111\n",
            "Validation loss per 100 evaluation steps: 0.07477682113880292\n",
            "Validation loss per 100 evaluation steps: 0.07294168118340895\n",
            "Validation loss per 100 evaluation steps: 0.06880843734620914\n",
            "Validation loss per 100 evaluation steps: 0.07085774047466527\n",
            "Validation loss per 100 evaluation steps: 0.07129572425234074\n",
            "Validation Loss: 0.06940491068008546\n",
            "Validation Accuracy: 0.984503264300901\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.98      0.96      0.97      1837\n",
            "      B-MISC       0.84      0.95      0.89       922\n",
            "       B-ORG       0.94      0.92      0.93      1341\n",
            "       B-PER       0.97      0.99      0.98      1842\n",
            "       I-LOC       0.97      0.93      0.95      1801\n",
            "      I-MISC       0.72      0.94      0.82       935\n",
            "       I-ORG       0.93      0.91      0.92      2319\n",
            "       I-PER       0.97      0.99      0.98      4219\n",
            "           O       1.00      0.99      1.00     51041\n",
            "\n",
            "    accuracy                           0.98     66257\n",
            "   macro avg       0.93      0.95      0.94     66257\n",
            "weighted avg       0.99      0.98      0.98     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9845  0.9845  0.9845\n",
            "macro        0.9263  0.9526  0.9375\n",
            "weighted     0.9858  0.9845  0.9849\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.14245993526477832\n",
            "Test loss per 100 evaluation steps: 0.13581159791530809\n",
            "Test loss per 100 evaluation steps: 0.1362296703797377\n",
            "Test loss per 100 evaluation steps: 0.14044839869689896\n",
            "Test loss per 100 evaluation steps: 0.14627218967070804\n",
            "Test loss per 100 evaluation steps: 0.15084660233890948\n",
            "Test loss per 100 evaluation steps: 0.15339583254568945\n",
            "Test loss per 100 evaluation steps: 0.16214644385494467\n",
            "Test loss per 100 evaluation steps: 0.16537433447587924\n",
            "Test Loss: 0.164337060050915\n",
            "Test Accuracy: 0.9696931380125475\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.95      0.92      0.94      1668\n",
            "      B-MISC       0.72      0.88      0.79       702\n",
            "       B-ORG       0.90      0.90      0.90      1661\n",
            "       B-PER       0.95      0.97      0.96      1617\n",
            "       I-LOC       0.91      0.89      0.90      1394\n",
            "      I-MISC       0.52      0.82      0.63       736\n",
            "       I-ORG       0.90      0.92      0.91      2804\n",
            "       I-PER       0.96      0.99      0.98      3810\n",
            "           O       1.00      0.98      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.87      0.92      0.89     61486\n",
            "weighted avg       0.98      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9710  0.9710  0.9710\n",
            "macro        0.8678  0.9191  0.8886\n",
            "weighted     0.9752  0.9710  0.9725\n",
            "Test steps: 921\n",
            "Training epoch: 4\n",
            "Training loss per 100 training steps: 0.03999869238410611\n",
            "Training loss per 100 training steps: 0.0471708267244685\n",
            "Training loss per 100 training steps: 0.044624814611840216\n",
            "Training loss per 100 training steps: 0.044290149260123146\n",
            "Training loss per 100 training steps: 0.0419255924107274\n",
            "Training loss per 100 training steps: 0.042636239542989646\n",
            "Training loss per 100 training steps: 0.041932966597357464\n",
            "Training loss per 100 training steps: 0.04007327546979468\n",
            "Training loss per 100 training steps: 0.03935274158467817\n",
            "Training loss per 100 training steps: 0.038657680350610464\n",
            "Training loss per 100 training steps: 0.03832803603577883\n",
            "Training loss per 100 training steps: 0.03805051439292583\n",
            "Training loss per 100 training steps: 0.038352429041036744\n",
            "Training loss per 100 training steps: 0.03821675580779681\n",
            "Training loss per 100 training steps: 0.04032165551708022\n",
            "Training loss per 100 training steps: 0.04044771048151688\n",
            "Training loss per 100 training steps: 0.04056691418244822\n",
            "Training loss per 100 training steps: 0.040291774834529656\n",
            "Training loss per 100 training steps: 0.04024642089536907\n",
            "Training loss per 100 training steps: 0.040998440624829526\n",
            "Training loss per 100 training steps: 0.04043646840026971\n",
            "Training loss per 100 training steps: 0.040919201908759945\n",
            "Training loss per 100 training steps: 0.04088830364364027\n",
            "Training loss per 100 training steps: 0.040718691000337606\n",
            "Training loss per 100 training steps: 0.04098681941779432\n",
            "Training loss per 100 training steps: 0.0411601285731873\n",
            "Training loss per 100 training steps: 0.04060208211522169\n",
            "Training loss per 100 training steps: 0.040867158376544074\n",
            "Training loss per 100 training steps: 0.04075993280707954\n",
            "Training loss per 100 training steps: 0.040671563616975014\n",
            "Training loss per 100 training steps: 0.04011647640661448\n",
            "Training loss per 100 training steps: 0.0400001408522138\n",
            "Training loss per 100 training steps: 0.04074827255380362\n",
            "Training loss per 100 training steps: 0.040626054732154124\n",
            "Training loss per 100 training steps: 0.04036604880504924\n",
            "Training loss per 100 training steps: 0.04044357285780633\n",
            "Training loss per 100 training steps: 0.04004906672303912\n",
            "Training loss epoch: 0.040318945314338474\n",
            "Training accuracy epoch: 0.990925383583062\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.05480001665026066\n",
            "Validation loss per 100 evaluation steps: 0.06333709335194726\n",
            "Validation loss per 100 evaluation steps: 0.06047056084042803\n",
            "Validation loss per 100 evaluation steps: 0.05941027904103976\n",
            "Validation loss per 100 evaluation steps: 0.05826072390167974\n",
            "Validation loss per 100 evaluation steps: 0.06069245439119792\n",
            "Validation loss per 100 evaluation steps: 0.05899240546071918\n",
            "Validation loss per 100 evaluation steps: 0.061715991372757344\n",
            "Validation Loss: 0.06191313603370833\n",
            "Validation Accuracy: 0.9858052041542502\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.95      0.98      0.96      1837\n",
            "      B-MISC       0.89      0.93      0.91       922\n",
            "       B-ORG       0.95      0.91      0.93      1341\n",
            "       B-PER       0.98      0.98      0.98      1842\n",
            "       I-LOC       0.92      0.97      0.95      1801\n",
            "      I-MISC       0.83      0.89      0.86       935\n",
            "       I-ORG       0.95      0.89      0.92      2319\n",
            "       I-PER       0.98      0.99      0.98      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.94      0.95      0.94     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9861  0.9861  0.9861\n",
            "macro        0.9379  0.9498  0.9434\n",
            "weighted     0.9864  0.9861  0.9862\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.127107597241411\n",
            "Test loss per 100 evaluation steps: 0.1440001398315326\n",
            "Test loss per 100 evaluation steps: 0.14920496218463086\n",
            "Test loss per 100 evaluation steps: 0.14432703884062903\n",
            "Test loss per 100 evaluation steps: 0.14538670385002478\n",
            "Test loss per 100 evaluation steps: 0.1420907681206639\n",
            "Test loss per 100 evaluation steps: 0.14375936422800026\n",
            "Test loss per 100 evaluation steps: 0.14436857464433614\n",
            "Test loss per 100 evaluation steps: 0.1456948839287563\n",
            "Test Loss: 0.14534930249361416\n",
            "Test Accuracy: 0.9712596202532591\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.91      0.95      0.93      1668\n",
            "      B-MISC       0.77      0.86      0.81       702\n",
            "       B-ORG       0.92      0.90      0.91      1661\n",
            "       B-PER       0.97      0.96      0.97      1617\n",
            "       I-LOC       0.82      0.93      0.88      1394\n",
            "      I-MISC       0.59      0.74      0.66       736\n",
            "       I-ORG       0.90      0.90      0.90      2804\n",
            "       I-PER       0.97      0.98      0.98      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.87      0.91      0.89     61486\n",
            "weighted avg       0.98      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9731  0.9731  0.9731\n",
            "macro        0.8742  0.9127  0.8918\n",
            "weighted     0.9752  0.9731  0.9739\n",
            "Test steps: 921\n",
            "====================================================================================================\n",
            "loss_name: kl\n",
            "model configuration\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Ner_Model(\n",
            "  (model): DebertaModel(\n",
            "    (embeddings): DebertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "      (LayerNorm): DebertaLayerNorm()\n",
            "      (dropout): StableDropout()\n",
            "    )\n",
            "    (encoder): DebertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x DebertaLayer(\n",
            "          (attention): DebertaAttention(\n",
            "            (self): DisentangledSelfAttention(\n",
            "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
            "              (pos_dropout): StableDropout()\n",
            "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "            (output): DebertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): DebertaLayerNorm()\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "          )\n",
            "          (intermediate): DebertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): DebertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): DebertaLayerNorm()\n",
            "            (dropout): StableDropout()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (rel_embeddings): Embedding(1024, 768)\n",
            "    )\n",
            "  )\n",
            "  (linear_model): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=384, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=384, out_features=256, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (9): ReLU()\n",
            "    (10): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (11): ReLU()\n",
            "    (12): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (13): ReLU()\n",
            "    (14): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (15): ReLU()\n",
            "  )\n",
            "  (classifier): Linear(in_features=128, out_features=9, bias=True)\n",
            ")\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 2.2406769013404846\n",
            "Training loss per 100 training steps: 2.220354495048523\n",
            "Training loss per 100 training steps: 2.17050172885259\n",
            "Training loss per 100 training steps: 2.0441393756866457\n",
            "Training loss per 100 training steps: 1.8562423247098923\n",
            "Training loss per 100 training steps: 1.6713038392737507\n",
            "Training loss per 100 training steps: 1.5187529021900679\n",
            "Training loss per 100 training steps: 1.3974951381911525\n",
            "Training loss per 100 training steps: 1.3019367242707975\n",
            "Training loss per 100 training steps: 1.228151983069256\n",
            "Training loss per 100 training steps: 1.1680700595693831\n",
            "Training loss per 100 training steps: 1.1173612205513443\n",
            "Training loss per 100 training steps: 1.0734941330150916\n",
            "Training loss per 100 training steps: 1.0345152891427278\n",
            "Training loss per 100 training steps: 1.003202282398939\n",
            "Training loss per 100 training steps: 0.9734693639306351\n",
            "Training loss per 100 training steps: 0.9467831372376532\n",
            "Training loss per 100 training steps: 0.9202290940900437\n",
            "Training loss per 100 training steps: 0.8934583637387933\n",
            "Training loss per 100 training steps: 0.869584036606364\n",
            "Training loss per 100 training steps: 0.8487175818240004\n",
            "Training loss per 100 training steps: 0.8292510702607846\n",
            "Training loss per 100 training steps: 0.810368884964922\n",
            "Training loss per 100 training steps: 0.7929427480645973\n",
            "Training loss per 100 training steps: 0.7778348786595045\n",
            "Training loss per 100 training steps: 0.7637292688628972\n",
            "Training loss per 100 training steps: 0.74917427384674\n",
            "Training loss per 100 training steps: 0.7361641906088751\n",
            "Training loss per 100 training steps: 0.7245562690861748\n",
            "Training loss per 100 training steps: 0.7125020857235261\n",
            "Training loss per 100 training steps: 0.7017659871477724\n",
            "Training loss per 100 training steps: 0.6911601899092057\n",
            "Training loss per 100 training steps: 0.6813016761607265\n",
            "Training loss per 100 training steps: 0.6714148431155584\n",
            "Training loss per 100 training steps: 0.6617203005376006\n",
            "Training loss per 100 training steps: 0.652255000296605\n",
            "Training loss per 100 training steps: 0.6429189937279207\n",
            "Training loss epoch: 0.6385850930331817\n",
            "Training accuracy epoch: 0.7819845321579587\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.30154817502945663\n",
            "Validation loss per 100 evaluation steps: 0.2885722001828253\n",
            "Validation loss per 100 evaluation steps: 0.2903055096231401\n",
            "Validation loss per 100 evaluation steps: 0.29167832418330364\n",
            "Validation loss per 100 evaluation steps: 0.2907021035625367\n",
            "Validation loss per 100 evaluation steps: 0.29047142583246266\n",
            "Validation loss per 100 evaluation steps: 0.2906854739784363\n",
            "Validation loss per 100 evaluation steps: 0.2892796324054507\n",
            "Validation Loss: 0.2888043706264103\n",
            "Validation Accuracy: 0.8865859105366117\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.00      0.00      0.00      1837\n",
            "      B-MISC       0.00      0.00      0.00       922\n",
            "       B-ORG       0.14      0.23      0.17      1341\n",
            "       B-PER       0.48      0.95      0.64      1842\n",
            "       I-LOC       0.00      0.00      0.00      1801\n",
            "      I-MISC       0.00      0.00      0.00       935\n",
            "       I-ORG       0.52      0.88      0.66      2319\n",
            "       I-PER       0.97      0.96      0.97      4219\n",
            "           O       0.97      1.00      0.99     51041\n",
            "\n",
            "    accuracy                           0.89     66257\n",
            "   macro avg       0.34      0.45      0.38     66257\n",
            "weighted avg       0.85      0.89      0.87     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.8925  0.8925  0.8925\n",
            "macro        0.3435  0.4473  0.3805\n",
            "weighted     0.8472  0.8925  0.8658\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.32002732457593086\n",
            "Test loss per 100 evaluation steps: 0.3087996241170913\n",
            "Test loss per 100 evaluation steps: 0.3141285777899126\n",
            "Test loss per 100 evaluation steps: 0.3160007501533255\n",
            "Test loss per 100 evaluation steps: 0.31021917759627105\n",
            "Test loss per 100 evaluation steps: 0.3086653023647765\n",
            "Test loss per 100 evaluation steps: 0.30684976635542366\n",
            "Test loss per 100 evaluation steps: 0.30170050813620036\n",
            "Test loss per 100 evaluation steps: 0.3012979231508992\n",
            "Test Loss: 0.3016466757227125\n",
            "Test Accuracy: 0.8899466585887211\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.00      0.00      0.00      1668\n",
            "      B-MISC       0.00      0.00      0.00       702\n",
            "       B-ORG       0.19      0.24      0.21      1661\n",
            "       B-PER       0.42      0.94      0.59      1617\n",
            "       I-LOC       0.00      0.00      0.00      1394\n",
            "      I-MISC       0.00      0.00      0.00       736\n",
            "       I-ORG       0.63      0.92      0.75      2804\n",
            "       I-PER       0.97      0.96      0.96      3810\n",
            "           O       0.98      1.00      0.99     47094\n",
            "\n",
            "    accuracy                           0.90     61486\n",
            "   macro avg       0.35      0.45      0.39     61486\n",
            "weighted avg       0.85      0.90      0.87     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.8958  0.8958  0.8958\n",
            "macro        0.3546  0.4510  0.3885\n",
            "weighted     0.8540  0.8958  0.8709\n",
            "Test steps: 921\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.27778504183748737\n",
            "Training loss per 100 training steps: 0.2639117495075334\n",
            "Training loss per 100 training steps: 0.2616490557735475\n",
            "Training loss per 100 training steps: 0.2468265844427515\n",
            "Training loss per 100 training steps: 0.2411546457055956\n",
            "Training loss per 100 training steps: 0.23296172464247017\n",
            "Training loss per 100 training steps: 0.22340870064533583\n",
            "Training loss per 100 training steps: 0.2153118811499735\n",
            "Training loss per 100 training steps: 0.20532581803065517\n",
            "Training loss per 100 training steps: 0.1962009794708574\n",
            "Training loss per 100 training steps: 0.18974644918809644\n",
            "Training loss per 100 training steps: 0.18391949585318798\n",
            "Training loss per 100 training steps: 0.17950412706839136\n",
            "Training loss per 100 training steps: 0.17367962108001977\n",
            "Training loss per 100 training steps: 0.16842533227219247\n",
            "Training loss per 100 training steps: 0.1644345119319769\n",
            "Training loss per 100 training steps: 0.15903520501581678\n",
            "Training loss per 100 training steps: 0.15558001562118687\n",
            "Training loss per 100 training steps: 0.15259925638000774\n",
            "Training loss per 100 training steps: 0.1498513620069134\n",
            "Training loss per 100 training steps: 0.14607834176875517\n",
            "Training loss per 100 training steps: 0.14456897949233694\n",
            "Training loss per 100 training steps: 0.1421071449953192\n",
            "Training loss per 100 training steps: 0.13886783440699219\n",
            "Training loss per 100 training steps: 0.13506371438343776\n",
            "Training loss per 100 training steps: 0.13210740575109062\n",
            "Training loss per 100 training steps: 0.1301708067683999\n",
            "Training loss per 100 training steps: 0.1290338981186817\n",
            "Training loss per 100 training steps: 0.12705476594220744\n",
            "Training loss per 100 training steps: 0.126114709066489\n",
            "Training loss per 100 training steps: 0.12446044209100598\n",
            "Training loss per 100 training steps: 0.12409719500074971\n",
            "Training loss per 100 training steps: 0.12210193374955976\n",
            "Training loss per 100 training steps: 0.12129467780489316\n",
            "Training loss per 100 training steps: 0.1198209213386269\n",
            "Training loss per 100 training steps: 0.1184139759934927\n",
            "Training loss per 100 training steps: 0.11678438455121068\n",
            "Training loss epoch: 0.1163119921577361\n",
            "Training accuracy epoch: 0.966707135594667\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.04789926158613525\n",
            "Validation loss per 100 evaluation steps: 0.06285014472203329\n",
            "Validation loss per 100 evaluation steps: 0.05878203195752576\n",
            "Validation loss per 100 evaluation steps: 0.05890295781107852\n",
            "Validation loss per 100 evaluation steps: 0.06100937387405429\n",
            "Validation loss per 100 evaluation steps: 0.06100890522182453\n",
            "Validation loss per 100 evaluation steps: 0.060555151697979974\n",
            "Validation loss per 100 evaluation steps: 0.06112883982103085\n",
            "Validation Loss: 0.061912636446594414\n",
            "Validation Accuracy: 0.9849048014446793\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.96      0.97      1837\n",
            "      B-MISC       0.89      0.92      0.90       922\n",
            "       B-ORG       0.91      0.93      0.92      1341\n",
            "       B-PER       0.99      0.97      0.98      1842\n",
            "       I-LOC       0.93      0.95      0.94      1801\n",
            "      I-MISC       0.85      0.80      0.82       935\n",
            "       I-ORG       0.92      0.92      0.92      2319\n",
            "       I-PER       0.99      0.98      0.98      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.98     66257\n",
            "   macro avg       0.94      0.94      0.94     66257\n",
            "weighted avg       0.98      0.98      0.98     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9850  0.9850  0.9850\n",
            "macro        0.9388  0.9355  0.9369\n",
            "weighted     0.9849  0.9850  0.9849\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.08607336638378911\n",
            "Test loss per 100 evaluation steps: 0.09867484089354547\n",
            "Test loss per 100 evaluation steps: 0.105130645267321\n",
            "Test loss per 100 evaluation steps: 0.1142886256775273\n",
            "Test loss per 100 evaluation steps: 0.12294333033678413\n",
            "Test loss per 100 evaluation steps: 0.12048032132987525\n",
            "Test loss per 100 evaluation steps: 0.11957105620474197\n",
            "Test loss per 100 evaluation steps: 0.11810024651059393\n",
            "Test loss per 100 evaluation steps: 0.1208806384909177\n",
            "Test Loss: 0.12210697298671831\n",
            "Test Accuracy: 0.9729260841524153\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.95      0.92      0.93      1668\n",
            "      B-MISC       0.77      0.86      0.81       702\n",
            "       B-ORG       0.88      0.90      0.89      1661\n",
            "       B-PER       0.98      0.95      0.96      1617\n",
            "       I-LOC       0.85      0.92      0.88      1394\n",
            "      I-MISC       0.70      0.67      0.68       736\n",
            "       I-ORG       0.89      0.92      0.91      2804\n",
            "       I-PER       0.98      0.97      0.98      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.89      0.90      0.89     61486\n",
            "weighted avg       0.98      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9746  0.9746  0.9746\n",
            "macro        0.8878  0.9006  0.8937\n",
            "weighted     0.9752  0.9746  0.9748\n",
            "Test steps: 921\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.05148418620985467\n",
            "Training loss per 100 training steps: 0.051126474214615886\n",
            "Training loss per 100 training steps: 0.05477133673732169\n",
            "Training loss per 100 training steps: 0.05177732302407094\n",
            "Training loss per 100 training steps: 0.051678839633153985\n",
            "Training loss per 100 training steps: 0.05108326745774927\n",
            "Training loss per 100 training steps: 0.050002623834204445\n",
            "Training loss per 100 training steps: 0.05282593570168501\n",
            "Training loss per 100 training steps: 0.05266863557873067\n",
            "Training loss per 100 training steps: 0.05174848889135319\n",
            "Training loss per 100 training steps: 0.05099925885791229\n",
            "Training loss per 100 training steps: 0.049157925466661256\n",
            "Training loss per 100 training steps: 0.04988317666024019\n",
            "Training loss per 100 training steps: 0.05058387572427559\n",
            "Training loss per 100 training steps: 0.0504536227662223\n",
            "Training loss per 100 training steps: 0.05130945628806785\n",
            "Training loss per 100 training steps: 0.05150474671568458\n",
            "Training loss per 100 training steps: 0.04999055155816879\n",
            "Training loss per 100 training steps: 0.050880951658599724\n",
            "Training loss per 100 training steps: 0.0511449105687243\n",
            "Training loss per 100 training steps: 0.05049079322195515\n",
            "Training loss per 100 training steps: 0.049886342492169534\n",
            "Training loss per 100 training steps: 0.04969978450524492\n",
            "Training loss per 100 training steps: 0.0492514092899152\n",
            "Training loss per 100 training steps: 0.04949226120128587\n",
            "Training loss per 100 training steps: 0.04943905946494431\n",
            "Training loss per 100 training steps: 0.0491722765711327\n",
            "Training loss per 100 training steps: 0.048521835758232816\n",
            "Training loss per 100 training steps: 0.048268868457648904\n",
            "Training loss per 100 training steps: 0.04859952456331545\n",
            "Training loss per 100 training steps: 0.04853710373559513\n",
            "Training loss per 100 training steps: 0.049044988627684916\n",
            "Training loss per 100 training steps: 0.049043159764155325\n",
            "Training loss per 100 training steps: 0.04907386816876169\n",
            "Training loss per 100 training steps: 0.04899590078530829\n",
            "Training loss per 100 training steps: 0.048740792366449265\n",
            "Training loss per 100 training steps: 0.04891734652906126\n",
            "Training loss epoch: 0.049123128794928264\n",
            "Training accuracy epoch: 0.9888609794300904\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.05860512053826824\n",
            "Validation loss per 100 evaluation steps: 0.062003035069283216\n",
            "Validation loss per 100 evaluation steps: 0.06292036878313714\n",
            "Validation loss per 100 evaluation steps: 0.0627107785071712\n",
            "Validation loss per 100 evaluation steps: 0.06401383397547761\n",
            "Validation loss per 100 evaluation steps: 0.06252784941903276\n",
            "Validation loss per 100 evaluation steps: 0.06251491169669732\n",
            "Validation loss per 100 evaluation steps: 0.06299026669448722\n",
            "Validation Loss: 0.06281888399873965\n",
            "Validation Accuracy: 0.9856493392484491\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.95      0.98      0.96      1837\n",
            "      B-MISC       0.88      0.93      0.90       922\n",
            "       B-ORG       0.96      0.92      0.94      1341\n",
            "       B-PER       0.99      0.97      0.98      1842\n",
            "       I-LOC       0.90      0.98      0.94      1801\n",
            "      I-MISC       0.81      0.88      0.84       935\n",
            "       I-ORG       0.96      0.90      0.93      2319\n",
            "       I-PER       0.99      0.98      0.99      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.94      0.95      0.94     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9862  0.9862  0.9862\n",
            "macro        0.9364  0.9503  0.9427\n",
            "weighted     0.9867  0.9862  0.9863\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.1363981287216302\n",
            "Test loss per 100 evaluation steps: 0.13090931488055502\n",
            "Test loss per 100 evaluation steps: 0.1375550346923895\n",
            "Test loss per 100 evaluation steps: 0.1353783460235354\n",
            "Test loss per 100 evaluation steps: 0.1339863646896847\n",
            "Test loss per 100 evaluation steps: 0.1374545727220296\n",
            "Test loss per 100 evaluation steps: 0.13717754425823972\n",
            "Test loss per 100 evaluation steps: 0.13449869453348584\n",
            "Test loss per 100 evaluation steps: 0.1352319643585684\n",
            "Test Loss: 0.13425906249622832\n",
            "Test Accuracy: 0.9716579183285137\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.90      0.94      0.92      1668\n",
            "      B-MISC       0.77      0.88      0.82       702\n",
            "       B-ORG       0.91      0.89      0.90      1661\n",
            "       B-PER       0.98      0.95      0.97      1617\n",
            "       I-LOC       0.80      0.94      0.86      1394\n",
            "      I-MISC       0.61      0.76      0.68       736\n",
            "       I-ORG       0.92      0.90      0.91      2804\n",
            "       I-PER       0.98      0.98      0.98      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.97     61486\n",
            "   macro avg       0.87      0.91      0.89     61486\n",
            "weighted avg       0.98      0.97      0.97     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9730  0.9730  0.9730\n",
            "macro        0.8739  0.9143  0.8921\n",
            "weighted     0.9753  0.9730  0.9738\n",
            "Test steps: 921\n",
            "Training epoch: 4\n",
            "Training loss per 100 training steps: 0.022957473447022494\n",
            "Training loss per 100 training steps: 0.025265665924962377\n",
            "Training loss per 100 training steps: 0.031968344399162255\n",
            "Training loss per 100 training steps: 0.03414417133302777\n",
            "Training loss per 100 training steps: 0.03642285474011442\n",
            "Training loss per 100 training steps: 0.03711959240137124\n",
            "Training loss per 100 training steps: 0.03755587138579618\n",
            "Training loss per 100 training steps: 0.03672815771965361\n",
            "Training loss per 100 training steps: 0.037999193101285425\n",
            "Training loss per 100 training steps: 0.0374427285208003\n",
            "Training loss per 100 training steps: 0.03639346417138189\n",
            "Training loss per 100 training steps: 0.036177592777245686\n",
            "Training loss per 100 training steps: 0.036394880556601075\n",
            "Training loss per 100 training steps: 0.03603062572039302\n",
            "Training loss per 100 training steps: 0.035705659062897514\n",
            "Training loss per 100 training steps: 0.03562990881337555\n",
            "Training loss per 100 training steps: 0.03623756262527262\n",
            "Training loss per 100 training steps: 0.035549004713514784\n",
            "Training loss per 100 training steps: 0.03659501350623415\n",
            "Training loss per 100 training steps: 0.03633335175734101\n",
            "Training loss per 100 training steps: 0.0365902148055431\n",
            "Training loss per 100 training steps: 0.03628564078794708\n",
            "Training loss per 100 training steps: 0.03570321055429778\n",
            "Training loss per 100 training steps: 0.036339342735119164\n",
            "Training loss per 100 training steps: 0.03605871189563841\n",
            "Training loss per 100 training steps: 0.036339884158565976\n",
            "Training loss per 100 training steps: 0.035915195317911305\n",
            "Training loss per 100 training steps: 0.03641724180079304\n",
            "Training loss per 100 training steps: 0.03605010579202105\n",
            "Training loss per 100 training steps: 0.03609545866764226\n",
            "Training loss per 100 training steps: 0.03599246842731689\n",
            "Training loss per 100 training steps: 0.03578167017114026\n",
            "Training loss per 100 training steps: 0.03567165373209333\n",
            "Training loss per 100 training steps: 0.035833075924656975\n",
            "Training loss per 100 training steps: 0.035610007311162914\n",
            "Training loss per 100 training steps: 0.0351398448369274\n",
            "Training loss per 100 training steps: 0.03477655840215792\n",
            "Training loss epoch: 0.03476242397652474\n",
            "Training accuracy epoch: 0.9923588849443411\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.06695505659765331\n",
            "Validation loss per 100 evaluation steps: 0.05840730985463779\n",
            "Validation loss per 100 evaluation steps: 0.04981107662527089\n",
            "Validation loss per 100 evaluation steps: 0.05660188470043522\n",
            "Validation loss per 100 evaluation steps: 0.05693254998488192\n",
            "Validation loss per 100 evaluation steps: 0.060049770748873924\n",
            "Validation loss per 100 evaluation steps: 0.061171218960654576\n",
            "Validation loss per 100 evaluation steps: 0.06198771006537072\n",
            "Validation Loss: 0.06320301899691365\n",
            "Validation Accuracy: 0.9876093980432181\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.97      0.97      1837\n",
            "      B-MISC       0.91      0.93      0.92       922\n",
            "       B-ORG       0.94      0.95      0.94      1341\n",
            "       B-PER       0.98      0.98      0.98      1842\n",
            "       I-LOC       0.95      0.96      0.96      1801\n",
            "      I-MISC       0.84      0.89      0.87       935\n",
            "       I-ORG       0.94      0.93      0.94      2319\n",
            "       I-PER       0.99      0.99      0.99      4219\n",
            "           O       1.00      1.00      1.00     51041\n",
            "\n",
            "    accuracy                           0.99     66257\n",
            "   macro avg       0.95      0.96      0.95     66257\n",
            "weighted avg       0.99      0.99      0.99     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9882  0.9882  0.9882\n",
            "macro        0.9468  0.9559  0.9513\n",
            "weighted     0.9884  0.9882  0.9883\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.134654852469248\n",
            "Test loss per 100 evaluation steps: 0.11350334063667106\n",
            "Test loss per 100 evaluation steps: 0.12876450366602513\n",
            "Test loss per 100 evaluation steps: 0.15164738857449264\n",
            "Test loss per 100 evaluation steps: 0.15209541637306392\n",
            "Test loss per 100 evaluation steps: 0.15304443099779746\n",
            "Test loss per 100 evaluation steps: 0.15711667780686445\n",
            "Test loss per 100 evaluation steps: 0.1520214959236273\n",
            "Test loss per 100 evaluation steps: 0.15391877847924965\n",
            "Test Loss: 0.15361379552903345\n",
            "Test Accuracy: 0.9730380453459324\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.95      0.93      0.94      1668\n",
            "      B-MISC       0.80      0.87      0.83       702\n",
            "       B-ORG       0.90      0.92      0.91      1661\n",
            "       B-PER       0.97      0.97      0.97      1617\n",
            "       I-LOC       0.88      0.92      0.90      1394\n",
            "      I-MISC       0.61      0.73      0.67       736\n",
            "       I-ORG       0.89      0.94      0.91      2804\n",
            "       I-PER       0.98      0.98      0.98      3810\n",
            "           O       1.00      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.98     61486\n",
            "   macro avg       0.89      0.92      0.90     61486\n",
            "weighted avg       0.98      0.98      0.98     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9755  0.9755  0.9755\n",
            "macro        0.8860  0.9166  0.9005\n",
            "weighted     0.9770  0.9755  0.9761\n",
            "Test steps: 921\n",
            "====================================================================================================\n",
            "loss_name: dlite\n",
            "model configuration\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Ner_Model(\n",
            "  (model): DebertaModel(\n",
            "    (embeddings): DebertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "      (LayerNorm): DebertaLayerNorm()\n",
            "      (dropout): StableDropout()\n",
            "    )\n",
            "    (encoder): DebertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x DebertaLayer(\n",
            "          (attention): DebertaAttention(\n",
            "            (self): DisentangledSelfAttention(\n",
            "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
            "              (pos_dropout): StableDropout()\n",
            "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "            (output): DebertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): DebertaLayerNorm()\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "          )\n",
            "          (intermediate): DebertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): DebertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): DebertaLayerNorm()\n",
            "            (dropout): StableDropout()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (rel_embeddings): Embedding(1024, 768)\n",
            "    )\n",
            "  )\n",
            "  (linear_model): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=384, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=384, out_features=256, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (9): ReLU()\n",
            "    (10): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (11): ReLU()\n",
            "    (12): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (13): ReLU()\n",
            "    (14): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (15): ReLU()\n",
            "  )\n",
            "  (classifier): Linear(in_features=128, out_features=9, bias=True)\n",
            ")\n",
            "%%%%%%%%%%%%%%%%%%%%\n",
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.6772360867261886\n",
            "Training loss per 100 training steps: 0.6713514873385429\n",
            "Training loss per 100 training steps: 0.6557173085212707\n",
            "Training loss per 100 training steps: 0.6092043434083462\n",
            "Training loss per 100 training steps: 0.5434796139001846\n",
            "Training loss per 100 training steps: 0.490825785441945\n",
            "Training loss per 100 training steps: 0.4495546985550651\n",
            "Training loss per 100 training steps: 0.4166105077537941\n",
            "Training loss per 100 training steps: 0.3886667538832666\n",
            "Training loss per 100 training steps: 0.36756523758592086\n",
            "Training loss per 100 training steps: 0.34900093143899785\n",
            "Training loss per 100 training steps: 0.33309845111108494\n",
            "Training loss per 100 training steps: 0.31997611690392097\n",
            "Training loss per 100 training steps: 0.3076501264430512\n",
            "Training loss per 100 training steps: 0.29654342105285225\n",
            "Training loss per 100 training steps: 0.28776435089223923\n",
            "Training loss per 100 training steps: 0.2796907008541936\n",
            "Training loss per 100 training steps: 0.27248882788889106\n",
            "Training loss per 100 training steps: 0.26491996887203545\n",
            "Training loss per 100 training steps: 0.2578434709484791\n",
            "Training loss per 100 training steps: 0.2518728598275629\n",
            "Training loss per 100 training steps: 0.24631975894377817\n",
            "Training loss per 100 training steps: 0.2405884985683129\n",
            "Training loss per 100 training steps: 0.23511617889634484\n",
            "Training loss per 100 training steps: 0.22973253882906283\n",
            "Training loss per 100 training steps: 0.2250358366547768\n",
            "Training loss per 100 training steps: 0.21998865152715114\n",
            "Training loss per 100 training steps: 0.2151130607801263\n",
            "Training loss per 100 training steps: 0.21092958896554875\n",
            "Training loss per 100 training steps: 0.20682885940704243\n",
            "Training loss per 100 training steps: 0.20285131023375735\n",
            "Training loss per 100 training steps: 0.19929705239270334\n",
            "Training loss per 100 training steps: 0.1957817547538875\n",
            "Training loss per 100 training steps: 0.19209502130457515\n",
            "Training loss per 100 training steps: 0.1886742961994348\n",
            "Training loss per 100 training steps: 0.18543570798170878\n",
            "Training loss per 100 training steps: 0.18206975174365284\n",
            "Training loss epoch: 0.18045023177055672\n",
            "Training accuracy epoch: 0.7944326438578048\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.049608731362968686\n",
            "Validation loss per 100 evaluation steps: 0.051118995905999325\n",
            "Validation loss per 100 evaluation steps: 0.05092500559454493\n",
            "Validation loss per 100 evaluation steps: 0.051112017704381285\n",
            "Validation loss per 100 evaluation steps: 0.05141367472342972\n",
            "Validation loss per 100 evaluation steps: 0.051840566429079145\n",
            "Validation loss per 100 evaluation steps: 0.05125921432108693\n",
            "Validation loss per 100 evaluation steps: 0.05076625967075188\n",
            "Validation Loss: 0.05060912640847175\n",
            "Validation Accuracy: 0.9506568486786556\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.68      0.96      0.80      1837\n",
            "      B-MISC       0.00      0.00      0.00       922\n",
            "       B-ORG       0.56      0.88      0.69      1341\n",
            "       B-PER       0.83      0.97      0.90      1842\n",
            "       I-LOC       0.91      0.67      0.77      1801\n",
            "      I-MISC       0.00      0.00      0.00       935\n",
            "       I-ORG       0.87      0.87      0.87      2319\n",
            "       I-PER       0.96      0.98      0.97      4219\n",
            "           O       0.99      1.00      0.99     51041\n",
            "\n",
            "    accuracy                           0.95     66257\n",
            "   macro avg       0.65      0.70      0.67     66257\n",
            "weighted avg       0.93      0.95      0.94     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9506  0.9506  0.9506\n",
            "macro        0.6454  0.7040  0.6656\n",
            "weighted     0.9315  0.9506  0.9390\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.050612932345829906\n",
            "Test loss per 100 evaluation steps: 0.05249233073322102\n",
            "Test loss per 100 evaluation steps: 0.05309871277422644\n",
            "Test loss per 100 evaluation steps: 0.05243886447529349\n",
            "Test loss per 100 evaluation steps: 0.05503139544152873\n",
            "Test loss per 100 evaluation steps: 0.055188363848061875\n",
            "Test loss per 100 evaluation steps: 0.05508211987111541\n",
            "Test loss per 100 evaluation steps: 0.05377797987764779\n",
            "Test loss per 100 evaluation steps: 0.053354195158347766\n",
            "Test Loss: 0.05340674460852277\n",
            "Test Accuracy: 0.9470759588918148\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.68      0.93      0.79      1668\n",
            "      B-MISC       0.00      0.00      0.00       702\n",
            "       B-ORG       0.60      0.86      0.71      1661\n",
            "       B-PER       0.82      0.96      0.88      1617\n",
            "       I-LOC       0.89      0.64      0.75      1394\n",
            "      I-MISC       0.00      0.00      0.00       736\n",
            "       I-ORG       0.87      0.90      0.89      2804\n",
            "       I-PER       0.96      0.98      0.97      3810\n",
            "           O       0.99      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.95     61486\n",
            "   macro avg       0.65      0.70      0.66     61486\n",
            "weighted avg       0.93      0.95      0.94     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9493  0.9493  0.9493\n",
            "macro        0.6466  0.6957  0.6636\n",
            "weighted     0.9340  0.9493  0.9399\n",
            "Test steps: 921\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.05792491515981965\n",
            "Training loss per 100 training steps: 0.054537752775358966\n",
            "Training loss per 100 training steps: 0.0531084932400942\n",
            "Training loss per 100 training steps: 0.05234158252893849\n",
            "Training loss per 100 training steps: 0.049584828522438326\n",
            "Training loss per 100 training steps: 0.048802663606341716\n",
            "Training loss per 100 training steps: 0.04827502556188197\n",
            "Training loss per 100 training steps: 0.04800975368575905\n",
            "Training loss per 100 training steps: 0.04738465187430342\n",
            "Training loss per 100 training steps: 0.04628829519006013\n",
            "Training loss per 100 training steps: 0.045887221371607334\n",
            "Training loss per 100 training steps: 0.04539232320787808\n",
            "Training loss per 100 training steps: 0.04534811856636085\n",
            "Training loss per 100 training steps: 0.04495234999483728\n",
            "Training loss per 100 training steps: 0.045183836693763926\n",
            "Training loss per 100 training steps: 0.04483583413424185\n",
            "Training loss per 100 training steps: 0.044814770995260864\n",
            "Training loss per 100 training steps: 0.044619920825968065\n",
            "Training loss per 100 training steps: 0.04455473003653831\n",
            "Training loss per 100 training steps: 0.043811552127139976\n",
            "Training loss per 100 training steps: 0.04360820604398582\n",
            "Training loss per 100 training steps: 0.04318671806199992\n",
            "Training loss per 100 training steps: 0.04345114496262359\n",
            "Training loss per 100 training steps: 0.04330259972152418\n",
            "Training loss per 100 training steps: 0.04328348830897502\n",
            "Training loss per 100 training steps: 0.04305361170733038\n",
            "Training loss per 100 training steps: 0.042700843159906667\n",
            "Training loss per 100 training steps: 0.04213365644579912\n",
            "Training loss per 100 training steps: 0.04193740069417359\n",
            "Training loss per 100 training steps: 0.04175423927997062\n",
            "Training loss per 100 training steps: 0.04147934636426998\n",
            "Training loss per 100 training steps: 0.04130064504525174\n",
            "Training loss per 100 training steps: 0.041164444926716975\n",
            "Training loss per 100 training steps: 0.04110816444389223\n",
            "Training loss per 100 training steps: 0.040880120665201185\n",
            "Training loss per 100 training steps: 0.04068602688105716\n",
            "Training loss per 100 training steps: 0.040293742509315694\n",
            "Training loss epoch: 0.04014172330217517\n",
            "Training accuracy epoch: 0.954414808539265\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: 0.02794264990014199\n",
            "Validation loss per 100 evaluation steps: 0.029587781170621382\n",
            "Validation loss per 100 evaluation steps: 0.029966372822036645\n",
            "Validation loss per 100 evaluation steps: 0.0296406896805172\n",
            "Validation loss per 100 evaluation steps: 0.02879259836300389\n",
            "Validation loss per 100 evaluation steps: 0.02831181602466965\n",
            "Validation loss per 100 evaluation steps: 0.028266880949080198\n",
            "Validation loss per 100 evaluation steps: 0.02810177606323066\n",
            "Validation Loss: 0.028043393498258084\n",
            "Validation Accuracy: 0.9751208298604269\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.86      0.98      0.91      1837\n",
            "      B-MISC       0.87      0.85      0.86       922\n",
            "       B-ORG       0.89      0.88      0.88      1341\n",
            "       B-PER       0.96      0.96      0.96      1842\n",
            "       I-LOC       0.85      0.98      0.91      1801\n",
            "      I-MISC       0.85      0.44      0.58       935\n",
            "       I-ORG       0.92      0.87      0.90      2319\n",
            "       I-PER       0.97      0.98      0.97      4219\n",
            "           O       0.99      0.99      0.99     51041\n",
            "\n",
            "    accuracy                           0.98     66257\n",
            "   macro avg       0.91      0.88      0.89     66257\n",
            "weighted avg       0.98      0.98      0.97     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9752  0.9752  0.9752\n",
            "macro        0.9070  0.8807  0.8858\n",
            "weighted     0.9751  0.9752  0.9740\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: 0.0398944856615708\n",
            "Test loss per 100 evaluation steps: 0.03911570053647665\n",
            "Test loss per 100 evaluation steps: 0.03763951031486006\n",
            "Test loss per 100 evaluation steps: 0.038702886234784725\n",
            "Test loss per 100 evaluation steps: 0.037617641200629805\n",
            "Test loss per 100 evaluation steps: 0.0364507889002751\n",
            "Test loss per 100 evaluation steps: 0.03718401897336207\n",
            "Test loss per 100 evaluation steps: 0.03797701371767118\n",
            "Test loss per 100 evaluation steps: 0.0389887229631702\n",
            "Test Loss: 0.03890573303991041\n",
            "Test Accuracy: 0.9613982954647401\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.83      0.94      0.88      1668\n",
            "      B-MISC       0.70      0.80      0.75       702\n",
            "       B-ORG       0.87      0.86      0.86      1661\n",
            "       B-PER       0.93      0.94      0.94      1617\n",
            "       I-LOC       0.76      0.93      0.84      1394\n",
            "      I-MISC       0.51      0.33      0.40       736\n",
            "       I-ORG       0.90      0.88      0.89      2804\n",
            "       I-PER       0.96      0.97      0.97      3810\n",
            "           O       0.99      0.99      0.99     47094\n",
            "\n",
            "    accuracy                           0.96     61486\n",
            "   macro avg       0.83      0.85      0.84     61486\n",
            "weighted avg       0.96      0.96      0.96     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.9631  0.9631  0.9631\n",
            "macro        0.8284  0.8498  0.8353\n",
            "weighted     0.9634  0.9631  0.9627\n",
            "Test steps: 921\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.04498427586753678\n",
            "Training loss per 100 training steps: 0.03569997063153096\n",
            "Training loss per 100 training steps: 0.032703882628797205\n",
            "Training loss per 100 training steps: 0.03138097691586886\n",
            "Training loss per 100 training steps: 0.03227927902690721\n",
            "Training loss per 100 training steps: 0.03228828802158389\n",
            "Training loss per 100 training steps: 0.03199118801251802\n",
            "Training loss per 100 training steps: 0.0330656500782359\n",
            "Training loss per 100 training steps: 0.033814278714944745\n",
            "Training loss per 100 training steps: 0.03357828270464435\n",
            "Training loss per 100 training steps: 0.033819417739007734\n",
            "Training loss per 100 training steps: 0.03344921888979549\n",
            "Training loss per 100 training steps: 0.033023978699661866\n",
            "Training loss per 100 training steps: 0.03272628562558873\n",
            "Training loss per 100 training steps: 0.03248553231049131\n",
            "Training loss per 100 training steps: nan\n",
            "Training loss per 100 training steps: nan\n",
            "Training loss per 100 training steps: nan\n",
            "Training loss per 100 training steps: nan\n",
            "Training loss per 100 training steps: nan\n",
            "Training loss per 100 training steps: nan\n",
            "Training loss per 100 training steps: nan\n",
            "Training loss per 100 training steps: nan\n",
            "Training loss per 100 training steps: nan\n",
            "Training loss per 100 training steps: nan\n",
            "Training loss per 100 training steps: nan\n",
            "Training loss per 100 training steps: nan\n",
            "Training loss per 100 training steps: nan\n",
            "Training loss per 100 training steps: nan\n",
            "Training loss per 100 training steps: nan\n",
            "Training loss per 100 training steps: nan\n",
            "Training loss per 100 training steps: nan\n",
            "Training loss per 100 training steps: nan\n",
            "Training loss per 100 training steps: nan\n",
            "Training loss per 100 training steps: nan\n",
            "Training loss per 100 training steps: nan\n",
            "Training loss per 100 training steps: nan\n",
            "Training loss epoch: nan\n",
            "Training accuracy epoch: 0.8497081969171904\n",
            "Training steps: 3747\n",
            "\n",
            "\n",
            "\n",
            "Validation loss per 100 evaluation steps: nan\n",
            "Validation loss per 100 evaluation steps: nan\n",
            "Validation loss per 100 evaluation steps: nan\n",
            "Validation loss per 100 evaluation steps: nan\n",
            "Validation loss per 100 evaluation steps: nan\n",
            "Validation loss per 100 evaluation steps: nan\n",
            "Validation loss per 100 evaluation steps: nan\n",
            "Validation loss per 100 evaluation steps: nan\n",
            "Validation Loss: nan\n",
            "Validation Accuracy: 0.7675269555664592\n",
            "Validation P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.00      0.00      0.00      1837\n",
            "      B-MISC       0.00      0.00      0.00       922\n",
            "       B-ORG       0.00      0.00      0.00      1341\n",
            "       B-PER       0.00      0.00      0.00      1842\n",
            "       I-LOC       0.00      0.00      0.00      1801\n",
            "      I-MISC       0.00      0.00      0.00       935\n",
            "       I-ORG       0.00      0.00      0.00      2319\n",
            "       I-PER       0.00      0.00      0.00      4219\n",
            "           O       0.77      1.00      0.87     51041\n",
            "\n",
            "    accuracy                           0.77     66257\n",
            "   macro avg       0.09      0.11      0.10     66257\n",
            "weighted avg       0.59      0.77      0.67     66257\n",
            "\n",
            "Validation P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.7703  0.7703  0.7703\n",
            "macro        0.0856  0.1111  0.0967\n",
            "weighted     0.5934  0.7703  0.6704\n",
            "Validation steps: 867\n",
            "\n",
            "\n",
            "\n",
            "Test loss per 100 evaluation steps: nan\n",
            "Test loss per 100 evaluation steps: nan\n",
            "Test loss per 100 evaluation steps: nan\n",
            "Test loss per 100 evaluation steps: nan\n",
            "Test loss per 100 evaluation steps: nan\n",
            "Test loss per 100 evaluation steps: nan\n",
            "Test loss per 100 evaluation steps: nan\n",
            "Test loss per 100 evaluation steps: nan\n",
            "Test loss per 100 evaluation steps: nan\n",
            "Test Loss: nan\n",
            "Test Accuracy: 0.7621713224424927\n",
            "Test P-R-F1 for each label: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.00      0.00      0.00      1668\n",
            "      B-MISC       0.00      0.00      0.00       702\n",
            "       B-ORG       0.00      0.00      0.00      1661\n",
            "       B-PER       0.00      0.00      0.00      1617\n",
            "       I-LOC       0.00      0.00      0.00      1394\n",
            "      I-MISC       0.00      0.00      0.00       736\n",
            "       I-ORG       0.00      0.00      0.00      2804\n",
            "       I-PER       0.00      0.00      0.00      3810\n",
            "           O       0.77      1.00      0.87     47094\n",
            "\n",
            "    accuracy                           0.77     61486\n",
            "   macro avg       0.09      0.11      0.10     61486\n",
            "weighted avg       0.59      0.77      0.66     61486\n",
            "\n",
            "Test P-R-F1 tor all label: \n",
            "          precision  recall      f1\n",
            "micro        0.7659  0.7659  0.7659\n",
            "macro        0.0851  0.1111  0.0964\n",
            "weighted     0.5866  0.7659  0.6644\n",
            "Test steps: 921\n",
            "Training epoch: 4\n",
            "Training loss per 100 training steps: nan\n",
            "Training loss per 100 training steps: nan\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-f81c5a3d6f16>\u001b[0m in \u001b[0;36m<cell line: 89>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'l1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ce'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'kl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dlite'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-f81c5a3d6f16>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config, loss_name)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             loss, logit= model(batch[\"id\"], batch['seq_length'], attention_mask=attention_mask,\n\u001b[0m\u001b[1;32m     31\u001b[0m                                              labels=targets)\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-69b733d1df54>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, seq_length, attention_mask, labels)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_layer_num\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    962\u001b[0m         )\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    965\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    458\u001b[0m                 )\n\u001b[1;32m    459\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m                 hidden_states = layer_module(\n\u001b[0m\u001b[1;32m    461\u001b[0m                     \u001b[0mnext_kv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     ):\n\u001b[0;32m--> 373\u001b[0;31m         attention_output = self.attention(\n\u001b[0m\u001b[1;32m    374\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     ):\n\u001b[0;32m--> 306\u001b[0;31m         self_output = self.self(\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mquery_states\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0mqp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# .split(self.all_head_size, dim=-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "def train(config,loss_name):\n",
        "    print(\"=\" * 100)\n",
        "    print(f\"loss_name: {loss_name}\")\n",
        "    model = Ner_Model(config, len(label2id), loss_name).to(config.device)\n",
        "    optimizer = get_optimizer(model, config)\n",
        "\n",
        "    valid_each_label_p_r_f1_list = []\n",
        "    valid_p_r_f1_list = []\n",
        "    test_each_label_p_r_f1_list = []\n",
        "    test_p_r_f1_list = []\n",
        "\n",
        "    valid_loss_list = []\n",
        "    test_loss_list = []\n",
        "\n",
        "    model.train()\n",
        "    interval = 100\n",
        "    for epoch in range(config.epochs):\n",
        "        print(f\"Training epoch: {epoch + 1}\")\n",
        "        tr_preds,tr_labels = [], []\n",
        "        total_loss = 0.0\n",
        "        tr_accuracy = 0.0\n",
        "        # print(\"\\n\" + \"-\" * 30 + \"\\n\")\n",
        "        # print(f\"epoch: {epoch},  train dataloader size: {len(train_dataloader)}\")\n",
        "        # print(f\"epoch: {epoch},  valid dataloader size: {len(valid_dataloader)}\")\n",
        "        # print(f\"epoch: {epoch},  test dataloader size: {len(test_dataloader)}\")\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            attention_mask = batch[\"id\"].ne(0)\n",
        "            targets = batch['label_id']\n",
        "            loss, logit= model(batch[\"id\"], batch['seq_length'], attention_mask=attention_mask,\n",
        "                                             labels=targets)\n",
        "\n",
        "            # compute training accuracy\n",
        "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = logit.view(-1, len(label2id)) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            active_accuracy = flattened_targets.ne(-100) # active accuracy is also of shape (batch_size * seq_len,)\n",
        "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "            tr_accuracy += tmp_tr_accuracy\n",
        "            tr_preds.extend(predictions)\n",
        "            tr_labels.extend(targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            if (step + 1) % interval == 0:\n",
        "                print(f\"Training loss per 100 training steps: {total_loss / (step+1)}\")\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        print(f\"Training loss epoch: {total_loss / (step+1)}\")\n",
        "        print(f\"Training accuracy epoch: {tr_accuracy / (step+1)}\")\n",
        "        print(f\"Training steps: {step+1}\")\n",
        "        print(\"\\n\\n\")\n",
        "        model.eval()\n",
        "\n",
        "\n",
        "        valid_loss, valid_p_r_f1,  valid_each_label_p_r_f1 = evaluate(model,valid_dataloader, \"Validation\")\n",
        "        valid_loss_list.append(valid_loss)\n",
        "        valid_p_r_f1_list.append(valid_p_r_f1)\n",
        "        valid_each_label_p_r_f1_list.append(valid_each_label_p_r_f1)\n",
        "\n",
        "        print(\"\\n\\n\")\n",
        "        test_loss, test_p_r_f1,test_each_label_p_r_f1  = evaluate(model,test_dataloader, \"Test\")\n",
        "        test_loss_list.append(test_loss)\n",
        "        test_p_r_f1_list.append(test_p_r_f1)\n",
        "        test_each_label_p_r_f1_list.append(test_each_label_p_r_f1)\n",
        "\n",
        "\n",
        "        #print(f\"epoch: {epoch}, train_loss: {train_loss}, \\n{train_p_r_f1}\")\n",
        "        #print(f\"epoch: {epoch}, valid_loss: {valid_loss}, \\n{valid_p_r_f1}\")\n",
        "        #print(f\"epoch: {epoch}, test_loss: {test_loss},  \\n {test_p_r_f1}\")\n",
        "        model.train()\n",
        "    return   {\n",
        "              \"valid_loss_list\":valid_loss_list,\n",
        "              \"test_loss_list\":test_loss_list,\n",
        "\n",
        "              \"valid_p_r_f1_list\":valid_p_r_f1_list,\n",
        "              \"valid_each_label_p_r_f1_list\":valid_each_label_p_r_f1_list,\n",
        "\n",
        "              \"test_p_r_f1_list\":test_p_r_f1_list,\n",
        "              \"test_each_label_p_r_f1_list\": test_each_label_p_r_f1_list}\n",
        "\n",
        "\n",
        "result = {}\n",
        "for loss_name in ['l1', 'l2', 'ce', 'kl', 'dlite']:\n",
        "    r = train(Config, loss_name)\n",
        "    result[loss_name] = r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS1O-MxfGr33"
      },
      "source": [
        "## Result Comparison after cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciR0WOCJGr33"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(\"result.pkl\", \"wb\") as f:\n",
        "    pickle.dump(result, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYodcFxGGr33"
      },
      "source": [
        "#### Overall Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgKkKD11Gr33"
      },
      "outputs": [],
      "source": [
        "columns = ['loss', 'precision', 'recall', 'f1']\n",
        "for t in ['micro', 'macro', 'weighted']:\n",
        "    df = []\n",
        "    for loss_name in loss_list:\n",
        "        row = {'loss': loss_name}\n",
        "        row['precision'] = result[loss_name]['test_p_r_f1_list'][-1].loc[t, 'precision']\n",
        "        row['recall'] = result[loss_name]['test_p_r_f1_list'][-1].loc[t, 'recall']\n",
        "        row['f1'] = result[loss_name]['test_p_r_f1_list'][-1].loc[t, 'f1']\n",
        "        df.append(row)\n",
        "    print(\"=\"*100)\n",
        "    print(t)\n",
        "    print(pd.DataFrame(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKeZYzqvGr33"
      },
      "source": [
        "#### Each label Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejx5I88-Gr34"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"test dataset\")\n",
        "for loss_name in loss_list:\n",
        "    print(\"-\"*50)\n",
        "    print(loss_name)\n",
        "    print(result[loss_name]['test_each_label_p_r_f1_list'][-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqfxwsPKGr38"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "272b8cea9b9747a6918ed33c6df1bb56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ade5608fe44944e3baf8ad73fe48faf7",
              "IPY_MODEL_3b84581afd8c46bca40d9ffdf37336c2",
              "IPY_MODEL_855e13ebf4204c3ab8b438077f1c7095"
            ],
            "layout": "IPY_MODEL_01b6148973b6499fb528cca427e689c8"
          }
        },
        "ade5608fe44944e3baf8ad73fe48faf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e3d61e53f9743e2b2989e351bf5d132",
            "placeholder": "​",
            "style": "IPY_MODEL_8906fe81c2df4b83bc77f40c2c00ada1",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "3b84581afd8c46bca40d9ffdf37336c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78af2c6b23294f41b4797a29b5085c64",
            "max": 52,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_25542486a28c43a6ab7d5f7ce0b1bf3f",
            "value": 52
          }
        },
        "855e13ebf4204c3ab8b438077f1c7095": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2e0372e82cb4a4f892d03e2d7867234",
            "placeholder": "​",
            "style": "IPY_MODEL_317380c790964b45965b54837db63c12",
            "value": " 52.0/52.0 [00:00&lt;00:00, 4.21kB/s]"
          }
        },
        "01b6148973b6499fb528cca427e689c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e3d61e53f9743e2b2989e351bf5d132": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8906fe81c2df4b83bc77f40c2c00ada1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78af2c6b23294f41b4797a29b5085c64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25542486a28c43a6ab7d5f7ce0b1bf3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f2e0372e82cb4a4f892d03e2d7867234": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "317380c790964b45965b54837db63c12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d01944511332439c8b29ed812f166c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4657ce4c94584433bc40e71e5670ee77",
              "IPY_MODEL_1f4ad53eb8444c06a3614892e846cc9e",
              "IPY_MODEL_92676f7af5094c3daee75d5cbf9f9537"
            ],
            "layout": "IPY_MODEL_e7c9393192ec480baf23213969620373"
          }
        },
        "4657ce4c94584433bc40e71e5670ee77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_246ed6c4a6c141689e149eb95a899526",
            "placeholder": "​",
            "style": "IPY_MODEL_f1229105f6714eb8b36aa15370a6e2eb",
            "value": "config.json: 100%"
          }
        },
        "1f4ad53eb8444c06a3614892e846cc9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4724cb108e7c4b3da88e712c18cc639a",
            "max": 474,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_112a61000ad9410fa247a58c721acfa8",
            "value": 474
          }
        },
        "92676f7af5094c3daee75d5cbf9f9537": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_595cdda5849942f4ac2334a9fd21a4d4",
            "placeholder": "​",
            "style": "IPY_MODEL_5e248067f5524070905e4fe4d427ea13",
            "value": " 474/474 [00:00&lt;00:00, 38.6kB/s]"
          }
        },
        "e7c9393192ec480baf23213969620373": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "246ed6c4a6c141689e149eb95a899526": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1229105f6714eb8b36aa15370a6e2eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4724cb108e7c4b3da88e712c18cc639a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "112a61000ad9410fa247a58c721acfa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "595cdda5849942f4ac2334a9fd21a4d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e248067f5524070905e4fe4d427ea13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a838cab58534b9891540a84b6a5f23d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc069ccbc971407287afd200fb6f939e",
              "IPY_MODEL_a77451688b4c40e08a5606eda0ee4343",
              "IPY_MODEL_595de7cb6d2040ac862a9d9f49a7f029"
            ],
            "layout": "IPY_MODEL_14047109931845359efdea8ee84a9875"
          }
        },
        "dc069ccbc971407287afd200fb6f939e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ff571357e1843d58553db31196cc52a",
            "placeholder": "​",
            "style": "IPY_MODEL_118d8d54a3dd4f6ea9de39df11a74714",
            "value": "vocab.json: 100%"
          }
        },
        "a77451688b4c40e08a5606eda0ee4343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f60127ab49244fc4a33eeb2ef2deecde",
            "max": 898825,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0a806b483fc64bb184df259e59a35c0a",
            "value": 898825
          }
        },
        "595de7cb6d2040ac862a9d9f49a7f029": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a6a44bed7fe4a0bbc3663e9e0bf5ba5",
            "placeholder": "​",
            "style": "IPY_MODEL_d88dca07e92d4899a531152ec5f86204",
            "value": " 899k/899k [00:00&lt;00:00, 972kB/s]"
          }
        },
        "14047109931845359efdea8ee84a9875": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ff571357e1843d58553db31196cc52a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "118d8d54a3dd4f6ea9de39df11a74714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f60127ab49244fc4a33eeb2ef2deecde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a806b483fc64bb184df259e59a35c0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a6a44bed7fe4a0bbc3663e9e0bf5ba5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d88dca07e92d4899a531152ec5f86204": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f29d7379cf47411d9a08685f74dae3fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6612caed7977492fb88726b604e5507e",
              "IPY_MODEL_930506914b604da89fcc5e2cd5aa00da",
              "IPY_MODEL_1e8ab9ff84d84418bf4c031eea443a84"
            ],
            "layout": "IPY_MODEL_c344087223a647fdb2e9c8675e4c1fb1"
          }
        },
        "6612caed7977492fb88726b604e5507e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b90de9f080b4b26a0a6063220679ff8",
            "placeholder": "​",
            "style": "IPY_MODEL_4163bb0a8b664b8fb128ee982a6ba29d",
            "value": "merges.txt: 100%"
          }
        },
        "930506914b604da89fcc5e2cd5aa00da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5be3ecb081154514b04348626de50599",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_07200cacadaf4feb8b1cc17e9d42e08b",
            "value": 456318
          }
        },
        "1e8ab9ff84d84418bf4c031eea443a84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d27dbbf76afb4092984c393991603843",
            "placeholder": "​",
            "style": "IPY_MODEL_6dc1ecbc85da48dcb342b0ca9bc98146",
            "value": " 456k/456k [00:00&lt;00:00, 1.06MB/s]"
          }
        },
        "c344087223a647fdb2e9c8675e4c1fb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b90de9f080b4b26a0a6063220679ff8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4163bb0a8b664b8fb128ee982a6ba29d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5be3ecb081154514b04348626de50599": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07200cacadaf4feb8b1cc17e9d42e08b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d27dbbf76afb4092984c393991603843": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dc1ecbc85da48dcb342b0ca9bc98146": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cbf243125d394b87bcf403ce152cf944": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f59fe3d162f46af814b140099fc7ffc",
              "IPY_MODEL_5199def6b37c4e0cbc1cceb4e4cc7594",
              "IPY_MODEL_e266ae10304843f0ad670313707874a0"
            ],
            "layout": "IPY_MODEL_ab22b162e5bb4837b9398bbcd570a664"
          }
        },
        "6f59fe3d162f46af814b140099fc7ffc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52665719116b431bb656cbafc8c185cf",
            "placeholder": "​",
            "style": "IPY_MODEL_7b89e2907ec445b78fa3dca6899b313d",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "5199def6b37c4e0cbc1cceb4e4cc7594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc0c5ed68cb748f8bda7baff5020dabd",
            "max": 558614189,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_97b5d9bd18054c099981ae1f23d07eb0",
            "value": 558614189
          }
        },
        "e266ae10304843f0ad670313707874a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25e5d911947a4f1a80155c33b91f7439",
            "placeholder": "​",
            "style": "IPY_MODEL_e7c07384960842d5ba37da12620bd1b7",
            "value": " 559M/559M [00:01&lt;00:00, 509MB/s]"
          }
        },
        "ab22b162e5bb4837b9398bbcd570a664": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52665719116b431bb656cbafc8c185cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b89e2907ec445b78fa3dca6899b313d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc0c5ed68cb748f8bda7baff5020dabd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97b5d9bd18054c099981ae1f23d07eb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "25e5d911947a4f1a80155c33b91f7439": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7c07384960842d5ba37da12620bd1b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}