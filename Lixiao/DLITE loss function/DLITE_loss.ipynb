{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLITE Loss Function Modification\n",
    "## Based on original loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    # Define the Cross Entropy loss function here\n",
    "    ce_loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    out = {}\n",
    "    ce_out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        dlite_losses = torch.zeros(eval_iters)\n",
    "        ce_losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, _ = model(X, Y)\n",
    "            dlite_losses[k] = dlite_loss(logits, Y.view(-1)).item()\n",
    "            ce_losses[k] = ce_loss_func(logits.view(-1, vocab_size), Y.view(-1)).item()\n",
    "        out[split] = dlite_losses.mean()\n",
    "        ce_out[split] = ce_losses.mean()\n",
    "    model.train()\n",
    "    return out, ce_out\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# Original version\n",
    "\n",
    "class DLITELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DLITELoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Convert logits to probabilities\n",
    "        P = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Convert targets to one-hot encoded format and ensure it's float type for operations\n",
    "        Q = F.one_hot(targets, num_classes=vocab_size).float()\n",
    "\n",
    "        # Masks for non-zero elements of P and Q\n",
    "        mask_P = P > 0\n",
    "        mask_Q = Q > 0\n",
    "\n",
    "        # LIT(P, Q) part\n",
    "        LIT_P = torch.zeros_like(P)\n",
    "        LIT_Q = torch.zeros_like(Q)\n",
    "        \n",
    "        # Compute only for non-zero elements\n",
    "        LIT_P[mask_P] = P[mask_P] * (1 - torch.log(P[mask_P]))\n",
    "        LIT_Q[mask_Q] = Q[mask_Q] * (1 - torch.log(Q[mask_Q]))\n",
    "        \n",
    "        LIT_term = torch.sum(torch.abs(LIT_P - LIT_Q))\n",
    "\n",
    "        # dH(P, Q) part\n",
    "        dH_P = torch.zeros_like(P)\n",
    "        dH_Q = torch.zeros_like(Q)\n",
    "        \n",
    "        # Compute only for non-zero elements\n",
    "        dH_P[mask_P] = P[mask_P]**2 * (1 - 2*torch.log(P[mask_P]))\n",
    "        dH_Q[mask_Q] = Q[mask_Q]**2 * (1 - 2*torch.log(Q[mask_Q]))\n",
    "        \n",
    "        dH_term = torch.sum(torch.abs(dH_P - dH_Q) / (2 * (P + Q)))\n",
    "\n",
    "        # DLITE(P, Q)\n",
    "        DLITE = LIT_term - dH_term\n",
    "        \n",
    "        return DLITE\n",
    "'''\n",
    "# Mask-operation improved DLITE loss\n",
    "# Modified DLITE loss to accept logits and targets directly\n",
    "class DLITELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DLITELoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Convert logits to probabilities\n",
    "        P = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Convert targets to one-hot encoded format and ensure it's float type for operations\n",
    "        Q = F.one_hot(targets, num_classes=vocab_size).float()\n",
    "\n",
    "        # Masks for non-zero elements of P and Q\n",
    "        mask_P = (P > 0).float()\n",
    "        mask_Q = (Q > 0).float()\n",
    "\n",
    "        # LIT(P, Q) part\n",
    "        LIT_P = P * (1 - torch.log(P + 1e-9)) * mask_P\n",
    "        LIT_Q = Q * (1 - torch.log(Q + 1e-9)) * mask_Q\n",
    "        LIT_term = torch.sum(torch.abs(LIT_P - LIT_Q))\n",
    "\n",
    "        # dH(P, Q) part\n",
    "        dH_P = P**2 * (1 - 2 * torch.log(P + 1e-9)) * mask_P\n",
    "        dH_Q = Q**2 * (1 - 2 * torch.log(Q + 1e-9)) * mask_Q\n",
    "        dH_term = torch.sum(torch.abs(dH_P - dH_Q) / (2 * (P + Q + 1e-9)))\n",
    "\n",
    "        # DLITE(P, Q)\n",
    "        DLITE = LIT_term - dH_term\n",
    "        \n",
    "        return DLITE\n",
    "'''\n",
    "    \n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.loss_func = DLITELoss()  # Initialize the DLITE loss function\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = self.loss_func(logits, targets)  # Use DLITE loss here\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline result shows a very large loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "step 0: train loss 475.1874, val loss 474.8864, CE train loss: 4.4116, CE val loss 4.4022\n",
      "\n",
      "step 100: train loss 344.2476, val loss 344.8734, CE train loss: 3.3289, CE val loss 3.4023\n",
      "\n",
      "step 200: train loss 331.4117, val loss 332.6119, CE train loss: 3.3358, CE val loss 3.4170\n",
      "\n",
      "step 300: train loss 323.1970, val loss 326.0095, CE train loss: 3.3310, CE val loss 3.4452\n",
      "\n",
      "step 400: train loss 316.9828, val loss 319.1192, CE train loss: 3.2579, CE val loss 3.3511\n",
      "\n",
      "step 500: train loss 310.5850, val loss 313.5168, CE train loss: 3.2304, CE val loss 3.3182\n",
      "\n",
      "step 600: train loss 302.1467, val loss 302.7675, CE train loss: 3.0796, CE val loss 3.1085\n",
      "\n",
      "step 700: train loss 297.8466, val loss 300.0448, CE train loss: 3.0905, CE val loss 3.1318\n",
      "\n",
      "step 800: train loss 290.9451, val loss 294.6096, CE train loss: 3.0216, CE val loss 3.0721\n",
      "\n",
      "step 900: train loss 287.2826, val loss 290.7303, CE train loss: 2.9895, CE val loss 3.0326\n",
      "\n",
      "step 1000: train loss 284.9845, val loss 288.5309, CE train loss: 3.1018, CE val loss 3.1368\n",
      "\n",
      "step 1100: train loss 282.7030, val loss 287.7215, CE train loss: 3.0983, CE val loss 3.1601\n",
      "\n",
      "step 1200: train loss 278.8946, val loss 282.7245, CE train loss: 3.0205, CE val loss 3.0483\n",
      "\n",
      "step 1300: train loss 278.8029, val loss 282.5893, CE train loss: 2.9926, CE val loss 3.0275\n",
      "\n",
      "step 1400: train loss 273.7228, val loss 279.0357, CE train loss: 2.9711, CE val loss 3.0184\n",
      "\n",
      "step 1500: train loss 272.3697, val loss 278.3900, CE train loss: 2.9669, CE val loss 3.0317\n",
      "\n",
      "step 1600: train loss 269.8928, val loss 278.9879, CE train loss: 2.9649, CE val loss 3.0952\n",
      "\n",
      "step 1700: train loss 269.4583, val loss 276.5941, CE train loss: 3.0176, CE val loss 3.0996\n",
      "\n",
      "step 1800: train loss 265.2060, val loss 275.3260, CE train loss: 2.9616, CE val loss 3.0843\n",
      "\n",
      "step 1900: train loss 266.3964, val loss 273.6994, CE train loss: 3.0104, CE val loss 3.0895\n",
      "\n",
      "step 2000: train loss 263.6903, val loss 274.7971, CE train loss: 2.9802, CE val loss 3.1143\n",
      "\n",
      "step 2100: train loss 261.9739, val loss 272.4744, CE train loss: 2.9661, CE val loss 3.0961\n",
      "\n",
      "step 2200: train loss 260.2519, val loss 269.3907, CE train loss: 2.9269, CE val loss 3.0343\n",
      "\n",
      "step 2300: train loss 259.2748, val loss 269.4750, CE train loss: 2.8939, CE val loss 3.0336\n",
      "\n",
      "step 2400: train loss 258.0059, val loss 268.1421, CE train loss: 2.9325, CE val loss 3.0563\n",
      "\n",
      "step 2500: train loss 255.3240, val loss 267.4344, CE train loss: 2.9324, CE val loss 3.0815\n",
      "\n",
      "step 2600: train loss 256.3707, val loss 267.4830, CE train loss: 2.9419, CE val loss 3.0580\n",
      "\n",
      "step 2700: train loss 254.2679, val loss 265.6867, CE train loss: 2.9587, CE val loss 3.0810\n",
      "\n",
      "step 2800: train loss 255.0851, val loss 266.1180, CE train loss: 2.8625, CE val loss 3.0005\n",
      "\n",
      "step 2900: train loss 252.9018, val loss 264.9135, CE train loss: 2.8825, CE val loss 3.0346\n",
      "\n",
      "step 3000: train loss 252.0926, val loss 263.5901, CE train loss: 2.9304, CE val loss 3.0680\n",
      "\n",
      "step 3100: train loss 249.3324, val loss 263.0304, CE train loss: 2.8781, CE val loss 3.0649\n",
      "\n",
      "step 3200: train loss 247.7562, val loss 261.1835, CE train loss: 2.8640, CE val loss 3.0349\n",
      "\n",
      "step 3300: train loss 248.6860, val loss 262.1852, CE train loss: 2.9628, CE val loss 3.0984\n",
      "\n",
      "step 3400: train loss 247.8068, val loss 259.4981, CE train loss: 2.8682, CE val loss 3.0311\n",
      "\n",
      "step 3500: train loss 246.2411, val loss 260.3211, CE train loss: 2.9184, CE val loss 3.0938\n",
      "\n",
      "step 3600: train loss 245.1718, val loss 260.2577, CE train loss: 2.9020, CE val loss 3.0948\n",
      "\n",
      "step 3700: train loss 246.0463, val loss 259.8903, CE train loss: 2.9312, CE val loss 3.0992\n",
      "\n",
      "step 3800: train loss 244.3197, val loss 259.8417, CE train loss: 2.8774, CE val loss 3.0787\n",
      "\n",
      "step 3900: train loss 242.9335, val loss 256.3607, CE train loss: 2.8409, CE val loss 3.0053\n",
      "\n",
      "step 4000: train loss 241.8283, val loss 254.7794, CE train loss: 2.8547, CE val loss 3.0153\n",
      "\n",
      "step 4100: train loss 243.4092, val loss 256.5540, CE train loss: 2.8703, CE val loss 3.0373\n",
      "\n",
      "step 4200: train loss 241.6929, val loss 255.9670, CE train loss: 2.8457, CE val loss 3.0186\n",
      "\n",
      "step 4300: train loss 241.8848, val loss 255.2335, CE train loss: 2.9165, CE val loss 3.0660\n",
      "\n",
      "step 4400: train loss 241.3501, val loss 253.8500, CE train loss: 2.8602, CE val loss 3.0152\n",
      "\n",
      "step 4500: train loss 240.1463, val loss 253.3555, CE train loss: 2.8046, CE val loss 2.9676\n",
      "\n",
      "step 4600: train loss 239.4789, val loss 253.4066, CE train loss: 2.8175, CE val loss 2.9753\n",
      "\n",
      "step 4700: train loss 238.5912, val loss 253.1111, CE train loss: 2.8356, CE val loss 3.0092\n",
      "\n",
      "step 4800: train loss 237.2397, val loss 253.6010, CE train loss: 2.8458, CE val loss 3.0805\n",
      "\n",
      "step 4900: train loss 238.6655, val loss 253.3802, CE train loss: 2.8311, CE val loss 3.0371\n",
      "\n",
      "step 4999: train loss 237.7134, val loss 252.1266, CE train loss: 2.8365, CE val loss 3.0098\n",
      "\n",
      "\n",
      "That be with so see the consel to more that to his to me the like,\n",
      "To to see this grantiOnt\n",
      "That mistry and so the portion of the my theart the brother have were\n",
      "That have men he beart the deaven\n",
      "The a sing to to so me bither it all be so singeres\n",
      "With the sing a so the are the proverte still the to that the contrenter your his to death sing\n",
      "That the for my theart of this griest send to the can a so that you was the worlad this the the do seee would of sortre him and sire and was my hing to the sering my more to lord.\n",
      "\n",
      "LUCIO:\n",
      "That madest with the sire to the seen the conttrenger the worse to so the farest the contrant to the can this worth of the will to the contrent thou this the call a the shall and my bonten oner the shall my with madeate to me prrived for the in in all the send sire\n",
      "Thou was my sontright my sonter hour were would\n",
      "To contrenters one and the constrong a wornter his do here mine to mines mine torther,\n",
      "To the son of my not strall of the go to the him of your to the contranter to the shall to stare of the the mear of that farther that that ward your banisher so I conder,\n",
      "And his the did to the to that have to the world the sand to the worling and so my lord.\n",
      "\n",
      "First and that be mestress and so sto my songer of my light to to antring to to to the to the haver that to the strong this that would the the said of worder of the with with the contrate of the string to shoul this are to to the your a more so the the pright a the were so my sorte\n",
      "Which with a thou my for a the to the present so thou the father bear to my latenners in a that see made thou the say of men my for him maday and fair the contrenter,\n",
      "That bride of sout the the light the haster and here to the come.\n",
      "\n",
      "LUCIO:\n",
      "Where we shall is worther morth, and the so the first of the parters\n",
      "And heart to the will thou to be this father to my lord.\n",
      "\n",
      "Set mULIO:\n",
      "The love that his so for to the more and bring mronger,\n",
      "And my may the sount in to there him and for the send this with the like to the distratt\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16\n",
    "block_size = 32\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# Create an instance of the custom DLITE loss class\n",
    "dlite_loss = DLITELoss()\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        dlite_losses, ce_losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {dlite_losses['train']:.4f}, val loss {dlite_losses['val']:.4f}, CE train loss: {ce_losses['train']:.4f}, CE val loss {ce_losses['val']:.4f}\\n\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the logits\n",
    "    logits, _ = model(xb, yb)\n",
    "\n",
    "    # Flatten the logits and target tensor\n",
    "    logits_flat = logits.view(-1, logits.shape[-1])\n",
    "    targets_flat = yb.view(-1)\n",
    "\n",
    "    # Compute the DLITE loss using the flattened logits and targets\n",
    "    loss = dlite_loss(logits_flat, targets_flat)\n",
    "\n",
    "    # Backpropagation and optimization step\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on Lixiao's previous version\n",
    "The following code is modified based on Prof. Ke's version of DLITE loss function, I used multification instead of indexing for the mask to improve the calculation speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "ce_loss_func = nn.CrossEntropyLoss()\n",
    "dlite_loss_func = DLITELoss()\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    dlite_out = {}\n",
    "    ce_out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        dlite_losses = torch.zeros(eval_iters)\n",
    "        ce_losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, _ = model(X, Y)\n",
    "            \n",
    "            # Compute DLITE loss\n",
    "            dlite_losses[k] = dlite_loss_func(logits.view(-1, vocab_size), Y.view(-1)).item()\n",
    "            \n",
    "            # Compute cross-entropy loss\n",
    "            ce_losses[k] = ce_loss_func(logits.view(-1, vocab_size), Y.view(-1)).item()\n",
    "\n",
    "        dlite_out[split] = dlite_losses.mean()\n",
    "        ce_out[split] = ce_losses.mean()\n",
    "    model.train()\n",
    "    return dlite_out, ce_out\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "'''\n",
    "def one_hot_encode(indices, num_classes):\n",
    "    \"\"\"Converts indices (LongTensor) into a one-hot encoded tensor.\"\"\"\n",
    "    one_hot = torch.zeros(indices.shape[0], num_classes, device=indices.device)\n",
    "    one_hot.scatter_(1, indices.unsqueeze(-1), 1)\n",
    "    return one_hot\n",
    "'''    \n",
    "class DLITELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DLITELoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits, targets, epsilon=1e-10):\n",
    "        # Convert logits to probabilities using softmax\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Convert targets to one-hot encoded format and ensure it's float type for operations\n",
    "        Q = F.one_hot(targets, num_classes=vocab_size).float()\n",
    "        P = probs\n",
    "        \n",
    "        # Masks for non-zero elements of P and Q\n",
    "        mask_P = (P > 0).float()\n",
    "        mask_Q = (Q > 0).float()\n",
    "\n",
    "        # LIT(P, Q) part\n",
    "        LIT_P = P * (1 - torch.log(P + epsilon)) * mask_P\n",
    "        LIT_Q = Q * (1 - torch.log(Q + epsilon)) * mask_Q\n",
    "        LIT_term = torch.sum(torch.abs(LIT_P - LIT_Q))\n",
    "\n",
    "        # dH(P, Q) part\n",
    "        dH_P = P**2 * (1 - 2 * torch.log(P + epsilon)) * mask_P\n",
    "        dH_Q = Q**2 * (1 - 2 * torch.log(Q + epsilon)) * mask_Q\n",
    "        dH_term = torch.sum(torch.abs(dH_P - dH_Q) / (2 * (P + Q + epsilon)))\n",
    "\n",
    "        # DLITE(P, Q)\n",
    "        DLITE = LIT_term - dH_term\n",
    "        \n",
    "        return DLITE\n",
    "\n",
    "    \n",
    "# super simple bigram model\n",
    "class DLITEBigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.loss_func = DLITELoss()  # Initialize the DLITE loss function\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = self.loss_func(logits, targets)  # Use DLITE loss here\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n",
      "step 0: train loss 475.1874, val loss 474.8864, CE train loss: 4.4116, CE val loss 4.4022\n",
      "\n",
      "step 100: train loss 344.2480, val loss 344.8739, CE train loss: 3.3289, CE val loss 3.4023\n",
      "\n",
      "step 200: train loss 331.4091, val loss 332.6132, CE train loss: 3.3353, CE val loss 3.4166\n",
      "\n",
      "step 300: train loss 323.1790, val loss 326.0051, CE train loss: 3.3310, CE val loss 3.4455\n",
      "\n",
      "step 400: train loss 316.9781, val loss 319.1335, CE train loss: 3.2563, CE val loss 3.3502\n",
      "\n",
      "step 500: train loss 310.6371, val loss 313.5950, CE train loss: 3.2320, CE val loss 3.3204\n",
      "\n",
      "step 600: train loss 302.1804, val loss 302.8387, CE train loss: 3.0792, CE val loss 3.1113\n",
      "\n",
      "step 700: train loss 297.7800, val loss 299.8920, CE train loss: 3.0864, CE val loss 3.1272\n",
      "\n",
      "step 800: train loss 290.9508, val loss 294.5722, CE train loss: 3.0211, CE val loss 3.0722\n",
      "\n",
      "step 900: train loss 287.5330, val loss 290.9762, CE train loss: 2.9829, CE val loss 3.0255\n",
      "\n",
      "step 1000: train loss 284.7902, val loss 288.3075, CE train loss: 3.0901, CE val loss 3.1229\n",
      "\n",
      "step 1100: train loss 283.1449, val loss 288.1043, CE train loss: 3.1005, CE val loss 3.1605\n",
      "\n",
      "step 1200: train loss 278.6960, val loss 282.5609, CE train loss: 3.0218, CE val loss 3.0486\n",
      "\n",
      "step 1300: train loss 278.6591, val loss 282.4593, CE train loss: 3.0003, CE val loss 3.0331\n",
      "\n",
      "step 1400: train loss 273.6905, val loss 278.8415, CE train loss: 2.9647, CE val loss 3.0124\n",
      "\n",
      "step 1500: train loss 273.3733, val loss 279.0300, CE train loss: 2.9573, CE val loss 3.0194\n",
      "\n",
      "step 1600: train loss 269.9174, val loss 278.7915, CE train loss: 2.9662, CE val loss 3.0900\n",
      "\n",
      "step 1700: train loss 269.2507, val loss 276.2798, CE train loss: 3.0191, CE val loss 3.0979\n",
      "\n",
      "step 1800: train loss 265.5168, val loss 275.2299, CE train loss: 2.9816, CE val loss 3.0953\n",
      "\n",
      "step 1900: train loss 266.4735, val loss 273.3926, CE train loss: 3.0060, CE val loss 3.0796\n",
      "\n",
      "step 2000: train loss 263.9416, val loss 274.8801, CE train loss: 3.0010, CE val loss 3.1296\n",
      "\n",
      "step 2100: train loss 261.8416, val loss 271.9418, CE train loss: 2.9799, CE val loss 3.1002\n",
      "\n",
      "step 2200: train loss 260.3987, val loss 269.5648, CE train loss: 2.9358, CE val loss 3.0425\n",
      "\n",
      "step 2300: train loss 259.4266, val loss 269.8196, CE train loss: 2.9017, CE val loss 3.0414\n",
      "\n",
      "step 2400: train loss 258.0197, val loss 267.7094, CE train loss: 2.9296, CE val loss 3.0368\n",
      "\n",
      "step 2500: train loss 255.3897, val loss 267.6045, CE train loss: 2.9174, CE val loss 3.0513\n",
      "\n",
      "step 2600: train loss 257.0327, val loss 267.5389, CE train loss: 2.9537, CE val loss 3.0561\n",
      "\n",
      "step 2700: train loss 254.7542, val loss 265.8997, CE train loss: 2.9828, CE val loss 3.0976\n",
      "\n",
      "step 2800: train loss 254.5187, val loss 265.2877, CE train loss: 2.8535, CE val loss 2.9825\n",
      "\n",
      "step 2900: train loss 252.6637, val loss 264.7257, CE train loss: 2.8780, CE val loss 3.0324\n",
      "\n",
      "step 3000: train loss 252.8810, val loss 264.4488, CE train loss: 2.9590, CE val loss 3.0969\n",
      "\n",
      "step 3100: train loss 249.5279, val loss 262.4596, CE train loss: 2.8745, CE val loss 3.0437\n",
      "\n",
      "step 3200: train loss 247.3718, val loss 261.4159, CE train loss: 2.8753, CE val loss 3.0490\n",
      "\n",
      "step 3300: train loss 248.7236, val loss 261.7048, CE train loss: 2.9484, CE val loss 3.0808\n",
      "\n",
      "step 3400: train loss 248.0115, val loss 259.6789, CE train loss: 2.8791, CE val loss 3.0442\n",
      "\n",
      "step 3500: train loss 246.7780, val loss 260.6485, CE train loss: 2.8927, CE val loss 3.0659\n",
      "\n",
      "step 3600: train loss 244.6283, val loss 259.2743, CE train loss: 2.8545, CE val loss 3.0399\n",
      "\n",
      "step 3700: train loss 245.0074, val loss 259.2637, CE train loss: 2.8450, CE val loss 3.0076\n",
      "\n",
      "step 3800: train loss 244.2057, val loss 259.2376, CE train loss: 2.8401, CE val loss 3.0375\n",
      "\n",
      "step 3900: train loss 242.5775, val loss 256.8740, CE train loss: 2.8362, CE val loss 3.0141\n",
      "\n",
      "step 4000: train loss 241.7619, val loss 255.1703, CE train loss: 2.8565, CE val loss 3.0164\n",
      "\n",
      "step 4100: train loss 242.4923, val loss 256.3925, CE train loss: 2.8824, CE val loss 3.0543\n",
      "\n",
      "step 4200: train loss 241.3385, val loss 256.2123, CE train loss: 2.8460, CE val loss 3.0228\n",
      "\n",
      "step 4300: train loss 241.6956, val loss 255.4057, CE train loss: 2.9053, CE val loss 3.0709\n",
      "\n",
      "step 4400: train loss 241.1305, val loss 253.0476, CE train loss: 2.8730, CE val loss 3.0221\n",
      "\n",
      "step 4500: train loss 239.6396, val loss 253.4698, CE train loss: 2.8067, CE val loss 2.9774\n",
      "\n",
      "step 4600: train loss 239.0259, val loss 252.6148, CE train loss: 2.8386, CE val loss 2.9898\n",
      "\n",
      "step 4700: train loss 238.6972, val loss 253.1564, CE train loss: 2.8660, CE val loss 3.0337\n",
      "\n",
      "step 4800: train loss 236.7867, val loss 253.0915, CE train loss: 2.8534, CE val loss 3.0982\n",
      "\n",
      "step 4900: train loss 237.3997, val loss 252.3414, CE train loss: 2.8368, CE val loss 3.0540\n",
      "\n",
      "step 4999: train loss 237.0786, val loss 251.0127, CE train loss: 2.8204, CE val loss 2.9817\n",
      "\n",
      "\n",
      "That be with so see the content I made that to have I may light a thou shall be which or that mastion to the countrous it would him streast that is I will the men man of the moun that may be to be that sing to the, the bread your haste of singert to me the storth on the are the provert is in the contrant,\n",
      "The will the bear of the contruther,\n",
      "That that say that sto the come the contring of your was me to to me to the worse have all so me of his will comes of with the worth a so be made all\n",
      "This for in my bing and lord.\n",
      "\n",
      "LUCIO:\n",
      "That maste have so what strong and the stonge to mare to leave to his live\n",
      "And in and hear will so hear him in to the poor that to provest of the you your in the stear light\n",
      "Lord to be that is so destread the cannot a we posert right his soul do by hinger her to the would be should for him sing the call to I past a hour with stand,\n",
      "That man the day to me the hour brother in this may and herer the come and with which mistrant,\n",
      "The cannot be send with a with a be world of the worn,\n",
      "Whou destire of and the men of your of this in the pan ward your banishe.\n",
      "\n",
      "LERCOLIO:\n",
      "Thou what the did the face of and will so prover to him with with a were to send me the come.\n",
      "\n",
      "BUCIO:\n",
      "And in the do that booth, and with the would and as to the storth the so breath,\n",
      "And not that his sing to have to the prour streath,\n",
      "Thou have face of the with with the contrite of it your and that I string his I so be to thou and bread the would And letter the lord,\n",
      "That me proud and thou may on this done\n",
      "And the mare a more mere to not me to my land,\n",
      "The should leave to me thou have a hour singer to him mare to me a let to the sentrent the brring our him with all fither haste have here to stire\n",
      "The pring to me the so soul the botreave to with the sell we die the to the peart the breath the of the do the bunt,\n",
      "That bre to my lord that was be to streng that his so for to the more to the morere of it the you in connour with a seen his all be to the sell brother be to string with a so to\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16\n",
    "block_size = 32\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "model = DLITEBigramLanguageModel().to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        dlite_losses, ce_losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {dlite_losses['train']:.4f}, val loss {dlite_losses['val']:.4f}, CE train loss: {ce_losses['train']:.4f}, CE val loss {ce_losses['val']:.4f}\\n\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the logits\n",
    "    logits, _ = model(xb, yb)\n",
    "\n",
    "    # Flatten the logits and target tensor\n",
    "    logits_flat = logits.view(-1, logits.shape[-1])\n",
    "    targets_flat = yb.view(-1)\n",
    "\n",
    "    # Compute the DLITE loss using the flattened logits and targets\n",
    "    loss = dlite_loss_func(logits_flat, targets_flat)\n",
    "\n",
    "    # Backpropagation and optimization step\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Generate some sample text\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both versions shows large loss using DLITE, the following part needs to be examined to address potential issue:\n",
    "- One-hot transformation causing dimentionality increase (line 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
