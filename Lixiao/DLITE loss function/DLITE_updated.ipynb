{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "def one_hot_encode(indices, num_classes):\n",
    "    \"\"\"Converts indices (LongTensor) into a one-hot encoded tensor.\"\"\"\n",
    "    one_hot = torch.zeros(indices.shape[0], num_classes, device=indices.device)\n",
    "    one_hot.scatter_(1, indices.unsqueeze(-1), 1)\n",
    "    return one_hot\n",
    "    \n",
    "class DLITELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        assert logits.dtype == torch.float32, \"Logits must be of type Float\"\n",
    "        \n",
    "        # logits are the predicted probability distributions\n",
    "        P = F.softmax(logits, dim=-1)  # Apply softmax to get probabilities\n",
    "        \n",
    "        # Convert targets to one-hot encoded representation\n",
    "        Q = one_hot_encode(targets, num_classes=logits.shape[-1])\n",
    "\n",
    "        # Calculate g(p, q) and delta_h(p, q)\n",
    "        g = torch.sqrt((P * (1 - torch.log(P)) - Q * (1 - torch.log(Q)))**2)\n",
    "        delta_h = torch.sqrt(((P**2 * (1 - 2 * torch.log(P)) - Q**2 * (1 - 2 * torch.log(Q))) / (2 * (P + Q)))**2)\n",
    "\n",
    "        # Calculate DLITE\n",
    "        DLITE = torch.sum(g - delta_h)\n",
    "\n",
    "        return DLITE\n",
    "    \n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = DLITELoss()(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "'''\n",
    "def one_hot_encode(indices, num_classes):\n",
    "    \"\"\"Converts indices (LongTensor) into a one-hot encoded tensor.\"\"\"\n",
    "    one_hot = torch.zeros(indices.shape[0], num_classes, device=indices.device)\n",
    "    one_hot.scatter_(1, indices.unsqueeze(-1), 1)\n",
    "    return one_hot\n",
    "'''    \n",
    "class DLITELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DLITELoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits, targets, epsilon=1e-10):\n",
    "        # Convert logits to probabilities using softmax\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # One-hot encode the targets to get true probabilities\n",
    "        true_probs = F.one_hot(targets, num_classes=probs.size(-1)).float()\n",
    "        \n",
    "        # Define the g function\n",
    "        g_values = torch.abs(probs * (1 - torch.log(probs + epsilon)) - true_probs * (1 - torch.log(true_probs + epsilon)))\n",
    "\n",
    "        # Define the delta_h function\n",
    "        delta_h_values = torch.abs(probs**2 * (1 - 2 * torch.log(probs + epsilon)) - true_probs**2 * (1 - 2 * torch.log(true_probs + epsilon))) / (2 * (probs + true_probs))\n",
    "\n",
    "        # Compute DLITE loss for each class\n",
    "        dl_values = g_values - delta_h_values\n",
    "        \n",
    "        # Sum over all classes and average over batch size\n",
    "        loss = dl_values.sum(dim=-1).mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "# super simple bigram model\n",
    "class DLITEBigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.loss_func = DLITELoss()  # Initialize the DLITE loss function\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = self.loss_func(logits, targets)  # Use DLITE loss here\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.259521 M parameters\n",
      "step 0: train loss 0.9233, val loss 0.9233\n",
      "step 100: train loss 0.6688, val loss 0.6699\n",
      "step 200: train loss 0.6442, val loss 0.6483\n",
      "step 300: train loss 0.6310, val loss 0.6343\n",
      "step 400: train loss 0.6204, val loss 0.6219\n",
      "step 500: train loss 0.6061, val loss 0.6129\n",
      "step 600: train loss 0.5962, val loss 0.5998\n",
      "step 700: train loss 0.5798, val loss 0.5818\n",
      "step 800: train loss 0.5660, val loss 0.5712\n",
      "step 900: train loss 0.5595, val loss 0.5672\n",
      "step 1000: train loss 0.5553, val loss 0.5622\n",
      "step 1100: train loss 0.5483, val loss 0.5566\n",
      "step 1200: train loss 0.5416, val loss 0.5543\n",
      "step 1300: train loss 0.5380, val loss 0.5501\n",
      "step 1400: train loss 0.5340, val loss 0.5495\n",
      "step 1500: train loss 0.5319, val loss 0.5419\n",
      "step 1600: train loss 0.5252, val loss 0.5387\n",
      "step 1700: train loss 0.5217, val loss 0.5342\n",
      "step 1800: train loss 0.5175, val loss 0.5338\n",
      "step 1900: train loss 0.5155, val loss 0.5304\n",
      "step 2000: train loss 0.5114, val loss 0.5297\n",
      "step 2100: train loss 0.5094, val loss 0.5271\n",
      "step 2200: train loss 0.5068, val loss 0.5262\n",
      "step 2300: train loss 0.5025, val loss 0.5239\n",
      "step 2400: train loss 0.5003, val loss 0.5244\n",
      "step 2500: train loss 0.4976, val loss 0.5221\n",
      "step 2600: train loss 0.4957, val loss 0.5183\n",
      "step 2700: train loss 0.4981, val loss 0.5189\n",
      "step 2800: train loss 0.4942, val loss 0.5195\n",
      "step 2900: train loss 0.4900, val loss 0.5167\n",
      "step 3000: train loss 0.4883, val loss 0.5171\n",
      "step 3100: train loss 0.4848, val loss 0.5078\n",
      "step 3200: train loss 0.4824, val loss 0.5102\n",
      "step 3300: train loss 0.4822, val loss 0.5092\n",
      "step 3400: train loss 0.4808, val loss 0.5086\n",
      "step 3500: train loss 0.4795, val loss 0.5054\n",
      "step 3600: train loss 0.4762, val loss 0.5023\n",
      "step 3700: train loss 0.4783, val loss 0.5048\n",
      "step 3800: train loss 0.4746, val loss 0.5002\n",
      "step 3900: train loss 0.4731, val loss 0.5017\n",
      "step 4000: train loss 0.4738, val loss 0.4998\n",
      "step 4100: train loss 0.4673, val loss 0.4961\n",
      "step 4200: train loss 0.4680, val loss 0.5004\n",
      "step 4300: train loss 0.4658, val loss 0.4976\n",
      "step 4400: train loss 0.4671, val loss 0.4941\n",
      "step 4500: train loss 0.4659, val loss 0.4973\n",
      "step 4600: train loss 0.4647, val loss 0.4918\n",
      "step 4700: train loss 0.4640, val loss 0.4944\n",
      "step 4800: train loss 0.4653, val loss 0.4969\n",
      "step 4900: train loss 0.4614, val loss 0.4934\n",
      "step 4999: train loss 0.4613, val loss 0.4894\n",
      "\n",
      "To be the need with of it you sent the bear stand,\n",
      "Which and the word the hand his the seath the pers the will the dear a was my ber and in the will the house for the stand he was the will a part a so and of say his near,\n",
      "The prince of well the shall now son,\n",
      "And the hath the house with who the soul not the to be made word,\n",
      "As that the heart sing and the so mone of his it be a a plamion.\n",
      "\n",
      "First all all hear his field heart a mand boor\n",
      "What so stand her some not for the house for the that shall honh a day more the to the are the band the seecr of the cans with the shall for the day of part hast,\n",
      "The was the are to be dear mest the heaven his bear,\n",
      "In that honour of the son, and a comes the God fall how say be a that the would so dear him plant for you sell the love a perest her so can me thou the pent the will not be you will be at the stall not the heart to the honour,\n",
      "That his man but come it the country with whom sin,\n",
      "And stand hear his both the pronder,\n",
      "And which his not not my lord, my shall shall for the word, that death his love,\n",
      "And the the are that be sint of the partion.\n",
      "\n",
      "DUKE I man, sir, and he so see this some to be his of the would his for the day he stand the was and we a was which do the has come a man stand her honour to dear you heart the a present thou present,\n",
      "That what the part it the hofor what fayer stand in the beling of honours hand be and bear so ball from hing son,\n",
      "As me that the well be his his the souls a shall the proy in to this neer of a notanger to well my was of the that do you that me a conting and the stay me to the would be and the will home,\n",
      "Which a the the some thou hath here the place in the dear,\n",
      "And the hath a should be him lady,\n",
      "As that his man a a this bear him feeld of it the shall stand of your brond and the bear of the his son the sir,\n",
      "To still the honou hand be my so bear,\n",
      "My stare the han chard that was the part in the can a partion,\n",
      "Angain the por the can that by the shall for the was in this fear me that the parding a\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16\n",
    "block_size = 32\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# Create an instance of the custom DLITE loss class\n",
    "dlite_loss = DLITELoss()\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the logits\n",
    "    logits, _ = model(xb, yb)\n",
    "\n",
    "    # Compute the DLITE loss\n",
    "    loss = dlite_loss(logits, yb.view(-1))\n",
    "\n",
    "    # Backpropagation and optimization step\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.508481 M parameters\n",
      "step 0: train loss 0.9228, val loss 0.9226\n",
      "step 100: train loss 0.6717, val loss 0.6724\n",
      "step 200: train loss 0.6439, val loss 0.6507\n",
      "step 300: train loss 0.6294, val loss 0.6330\n",
      "step 400: train loss 0.6160, val loss 0.6193\n",
      "step 500: train loss 0.6054, val loss 0.6107\n",
      "step 600: train loss 0.5920, val loss 0.5977\n",
      "step 700: train loss 0.5839, val loss 0.5862\n",
      "step 800: train loss 0.5682, val loss 0.5721\n",
      "step 900: train loss 0.5618, val loss 0.5648\n",
      "step 1000: train loss 0.5481, val loss 0.5565\n",
      "step 1100: train loss 0.5458, val loss 0.5542\n",
      "step 1200: train loss 0.5360, val loss 0.5469\n",
      "step 1300: train loss 0.5299, val loss 0.5401\n",
      "step 1400: train loss 0.5258, val loss 0.5389\n",
      "step 1500: train loss 0.5233, val loss 0.5344\n",
      "step 1600: train loss 0.5192, val loss 0.5298\n",
      "step 1700: train loss 0.5130, val loss 0.5303\n",
      "step 1800: train loss 0.5119, val loss 0.5296\n",
      "step 1900: train loss 0.5068, val loss 0.5226\n",
      "step 2000: train loss 0.5043, val loss 0.5185\n",
      "step 2100: train loss 0.4995, val loss 0.5173\n",
      "step 2200: train loss 0.4966, val loss 0.5180\n",
      "step 2300: train loss 0.4940, val loss 0.5161\n",
      "step 2400: train loss 0.4935, val loss 0.5135\n",
      "step 2500: train loss 0.4897, val loss 0.5133\n",
      "step 2600: train loss 0.4891, val loss 0.5130\n",
      "step 2700: train loss 0.4833, val loss 0.5111\n",
      "step 2800: train loss 0.4844, val loss 0.5062\n",
      "step 2900: train loss 0.4829, val loss 0.5082\n",
      "step 3000: train loss 0.4787, val loss 0.5061\n",
      "step 3100: train loss 0.4796, val loss 0.5084\n",
      "step 3200: train loss 0.4783, val loss 0.5059\n",
      "step 3300: train loss 0.4736, val loss 0.5000\n",
      "step 3400: train loss 0.4736, val loss 0.5008\n",
      "step 3500: train loss 0.4707, val loss 0.5014\n",
      "step 3600: train loss 0.4694, val loss 0.4956\n",
      "step 3700: train loss 0.4684, val loss 0.5005\n",
      "step 3800: train loss 0.4652, val loss 0.4972\n",
      "step 3900: train loss 0.4669, val loss 0.4961\n",
      "step 4000: train loss 0.4601, val loss 0.4910\n",
      "step 4100: train loss 0.4632, val loss 0.4903\n",
      "step 4200: train loss 0.4604, val loss 0.4885\n",
      "step 4300: train loss 0.4585, val loss 0.4866\n",
      "step 4400: train loss 0.4575, val loss 0.4883\n",
      "step 4500: train loss 0.4621, val loss 0.4917\n",
      "step 4600: train loss 0.4554, val loss 0.4880\n",
      "step 4700: train loss 0.4556, val loss 0.4863\n",
      "step 4800: train loss 0.4554, val loss 0.4882\n",
      "step 4900: train loss 0.4543, val loss 0.4844\n",
      "step 4999: train loss 0.4537, val loss 0.4862\n",
      "\n",
      "Lord you may are his come to that seen thou consere\n",
      "To in the will and to the so my poring\n",
      "To the mean of this please the that the weart or the controun of that, say be that be he lord is the to make our thand your thou been some hasten to the pray the to that is the bloods.\n",
      "\n",
      "Post seen the seen will as the call thou paint\n",
      "And in the the are thou art that well seen thou dead him with all the poor hath.\n",
      "\n",
      "CORIOLANUS:\n",
      "And the some it on theur seen some to our land died and to do this death thou hast to the do contal to detter the sound it thou dow,\n",
      "Hadise to thou hath are look of all the pray\n",
      "That is in the see him his words and that in it the contry\n",
      "Where will the that been to love the weart that we that his death and our the being\n",
      "And from so more to seen the are that in hand a death and as the love in son,\n",
      "And that you do the dain, the hast your been with.\n",
      "\n",
      "Second the howere\n",
      "And some to and the doth soul be sont the daded of the sease your for the hase of this winding of the death.\n",
      "\n",
      "POLINGAUnd the some the dear and one thank brother,\n",
      "Which he castil and to the seen been his leaven to the the done\n",
      "And lies and me my sound some as the will and some and in the for the bold of comanse,\n",
      "And whose than the seen and thou do beast to\n",
      "Make the that is a done of all heaven to dand my lord,\n",
      "I love to death of the your wearse,\n",
      "The deather him and wear and my words your all heart.\n",
      "\n",
      "CLARENTIO:\n",
      "The more word some that were he could haved\n",
      "And the band to the will a prove the look ose that seen more to the been the poor that sing the are one that heart on you to this side thank your to do the leave the thou dorth.\n",
      "\n",
      "COLIO:\n",
      "I the pettir to do be do him to are our your wellessed and not his brother sow to be,\n",
      "The which thou behal it be to my seen the heaven heaven some to to the heart on of with streed to that that is hold as the the did could\n",
      "Whom more to head that coundent on my with are thou will all was to to my make to me the boded,\n",
      "Thou art it see to the dows is and bried in to se\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16\n",
    "block_size = 32\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 10\n",
    "dropout = 0.0\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# Create an instance of the custom DLITE loss class\n",
    "dlite_loss = DLITELoss()\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the logits\n",
    "    logits, _ = model(xb, yb)\n",
    "\n",
    "    # Compute the DLITE loss\n",
    "    loss = dlite_loss(logits, yb.view(-1))\n",
    "\n",
    "    # Backpropagation and optimization step\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16\n",
    "block_size = 32\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 128\n",
    "n_head = 4\n",
    "n_layer = 5\n",
    "dropout = 0.0\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# Create an instance of the custom DLITE loss class\n",
    "dlite_loss = DLITELoss()\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the logits\n",
    "    logits, _ = model(xb, yb)\n",
    "\n",
    "    # Compute the DLITE loss\n",
    "    loss = dlite_loss(logits, yb.view(-1))\n",
    "\n",
    "    # Backpropagation and optimization step\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.259521 M parameters\n",
      "step 0: train loss 0.9233, val loss 0.9233\n",
      "step 100: train loss 0.6763, val loss 0.6790\n",
      "step 200: train loss 0.6583, val loss 0.6604\n",
      "step 300: train loss 0.6520, val loss 0.6523\n",
      "step 400: train loss 0.6439, val loss 0.6474\n",
      "step 500: train loss 0.6397, val loss 0.6406\n",
      "step 600: train loss 0.6290, val loss 0.6329\n",
      "step 700: train loss 0.6237, val loss 0.6305\n",
      "step 800: train loss 0.6150, val loss 0.6197\n",
      "step 900: train loss 0.6114, val loss 0.6144\n",
      "step 1000: train loss 0.6040, val loss 0.6080\n",
      "step 1100: train loss 0.5956, val loss 0.5981\n",
      "step 1200: train loss 0.5888, val loss 0.5907\n",
      "step 1300: train loss 0.5858, val loss 0.5903\n",
      "step 1400: train loss 0.5826, val loss 0.5843\n",
      "step 1500: train loss 0.5784, val loss 0.5815\n",
      "step 1600: train loss 0.5761, val loss 0.5776\n",
      "step 1700: train loss 0.5741, val loss 0.5782\n",
      "step 1800: train loss 0.5668, val loss 0.5749\n",
      "step 1900: train loss 0.5654, val loss 0.5725\n",
      "step 2000: train loss 0.5640, val loss 0.5682\n",
      "step 2100: train loss 0.5617, val loss 0.5678\n",
      "step 2200: train loss 0.5578, val loss 0.5656\n",
      "step 2300: train loss 0.5563, val loss 0.5664\n",
      "step 2400: train loss 0.5543, val loss 0.5658\n",
      "step 2500: train loss 0.5494, val loss 0.5601\n",
      "step 2600: train loss 0.5478, val loss 0.5557\n",
      "step 2700: train loss 0.5482, val loss 0.5592\n",
      "step 2800: train loss 0.5468, val loss 0.5566\n",
      "step 2900: train loss 0.5462, val loss 0.5556\n",
      "step 3000: train loss 0.5435, val loss 0.5537\n",
      "step 3100: train loss 0.5436, val loss 0.5538\n",
      "step 3200: train loss 0.5414, val loss 0.5506\n",
      "step 3300: train loss 0.5390, val loss 0.5514\n",
      "step 3400: train loss 0.5370, val loss 0.5484\n",
      "step 3500: train loss 0.5362, val loss 0.5481\n",
      "step 3600: train loss 0.5320, val loss 0.5455\n",
      "step 3700: train loss 0.5320, val loss 0.5438\n",
      "step 3800: train loss 0.5322, val loss 0.5463\n",
      "step 3900: train loss 0.5299, val loss 0.5440\n",
      "step 4000: train loss 0.5314, val loss 0.5417\n",
      "step 4100: train loss 0.5275, val loss 0.5424\n",
      "step 4200: train loss 0.5275, val loss 0.5415\n",
      "step 4300: train loss 0.5261, val loss 0.5370\n",
      "step 4400: train loss 0.5233, val loss 0.5398\n",
      "step 4500: train loss 0.5231, val loss 0.5383\n",
      "step 4600: train loss 0.5257, val loss 0.5351\n",
      "step 4700: train loss 0.5225, val loss 0.5361\n",
      "step 4800: train loss 0.5199, val loss 0.5324\n",
      "step 4900: train loss 0.5213, val loss 0.5349\n",
      "step 4999: train loss 0.5164, val loss 0.5321\n",
      "\n",
      "And me here with withe ciend the shou in the the the mues were soold thoull with sell word that shat that thour of to whath me the be all in man houst my word not shim tor for wen we to she fake cheurst the fore as there to so the hear card do that for with thath his the sir thae the that she the ward that the are mance,\n",
      "And li, the should shall wellan, the the the the the and seent, for and me the that your wout the mo for the the and be arlant hat ant there till den the wenlourd wher with that cour his hour wall woun to the mante win,\n",
      "And my she hast his wore this thou hour the in stane sall bre that my be wost sorde hall hould now to dake the sing with woull be doestere to Fear hath the the the whath the whe be shour thall the he to seell mans to ard this shall ent the that wore have will thould my now the ben to the this so I hat do my we cand to a thot thise wis ther the the for stienthe thou me herear and stat thour,\n",
      "And ward be ston comesend wo full strale, my we my soule his mame the is herese be shall so nould cand brown she mor,\n",
      "That shall the well which me a do bit the weard not my his with we to mis there that we tho for sird wall lore thou now that so whour for herer and that nour be seee thall sill the teare,\n",
      "To cateater wore heres make on the so to my he shis so thou me hath should sto liks and hath and hour the shat lies the to stand tike and the so where the wich and me be thou ward the to mest the mest for chis and sent in hime be sirs the shing with he seard shis to he so leardee hath the the that the sire theo fuld the fand of fase come with of neard the sort and sing not alove st she me thou make the me the so that hat have sye werome stan in wilin should be the can he my hour the is he fathis would shis his thare an the the thou my he in son wirs the sh leath thathere the be wither the the to man sould the the thourse ell love that shon the the me my nowre thet and sire this will have and the to stas seneard ther mall so cant with the ward ther\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16\n",
    "block_size = 32\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 5\n",
    "dropout = 0.5\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# Create an instance of the custom DLITE loss class\n",
    "dlite_loss = DLITELoss()\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the logits\n",
    "    logits, _ = model(xb, yb)\n",
    "\n",
    "    # Compute the DLITE loss\n",
    "    loss = dlite_loss(logits, yb.view(-1))\n",
    "\n",
    "    # Backpropagation and optimization step\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
