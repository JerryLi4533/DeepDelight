{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Exploration for Failed Sentences\n",
    "12/15/2023\n",
    "\n",
    "Lixiao Yang\n",
    "\n",
    "This notebook is updated based on `9. NewsQA Prompt Engineering.ipynb`, for loop running for large dataset, refer to the `NewsQA Loop - v2.ipunb`. The output is only demonstrating the first story's results.\n",
    "\n",
    "\n",
    "#### Updates\n",
    "1. Make the MiniLM embedding returns the top 3 sentences retrieved for anwering.\n",
    "2. Modify the data into the incorrect answered questions from previous results.\n",
    "3. Unable to run WizardLM model locally due to memory limitations, consider adding more GPU to the JupyterHub. (DELAYED)\n",
    "\n",
    "#### Findings\n",
    "1. The MiniLM model still can not get the correct sentence from top 3 scored retrieved sentences.\n",
    "2. No obvious improvement for the two failed questions for GPT4ALL model with combination of different chunk sizes and overlap percentages. Same sentences are retrieved for all prompt-modified results.\n",
    "3. WizardLm is a Llama-based model, force the model to bypass the GPU configuration may cause new issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama-cpp-python langchain sentence_transformers InstructorEmbedding pyllama transformers pyqt5 pyqtwebengine pyllamacpp --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wk77/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import torch\n",
    "from langchain.llms import GPT4All\n",
    "from pathlib import Path\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceInstructEmbeddings\n",
    "from transformers import set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='data/combined-newsqa-data-story1-2questions.json'\n",
    "data = json.loads(Path(file_path).read_text())\n",
    "\n",
    "file_path='data/combined-newsqa-data-story1.json'\n",
    "data_2 = json.loads(Path(file_path).read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate Exact Match (EM) score\n",
    "def calculate_em(predicted, actual):\n",
    "    return int(predicted == actual)\n",
    "\n",
    "# Function to calculate the token-wise F1 score for text answers\n",
    "def calculate_token_f1(predicted, actual):\n",
    "    predicted_tokens = predicted.split()\n",
    "    actual_tokens = actual.split()\n",
    "    common_tokens = Counter(predicted_tokens) & Counter(actual_tokens)\n",
    "    num_same = sum(common_tokens.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(predicted_tokens)\n",
    "    recall = 1.0 * num_same / len(actual_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "# Helper function to extract answer ranges from the consensus field\n",
    "def extract_ranges(consensus):\n",
    "    if 's' in consensus and 'e' in consensus:\n",
    "        return [(consensus['s'], consensus['e'])]\n",
    "    return []\n",
    "\n",
    "def extract_answer_from_gpt4all_output(output, prompt_end=\"Answer:\"):\n",
    "    \"\"\"\n",
    "    Extracts the answer from the output of the GPT4ALL model.\n",
    "\n",
    "    Parameters:\n",
    "        output (str): The output string from the GPT4ALL model.\n",
    "        prompt_end (str): The delimiter indicating the end of the prompt and the start of the answer.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted answer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Find the start of the answer\n",
    "    start_idx = output.find(prompt_end)\n",
    "    if start_idx == -1:\n",
    "        return \"Answer not found in output\"\n",
    "\n",
    "    # Extract the answer\n",
    "    start_idx += len(prompt_end)\n",
    "    answer = output[start_idx:]\n",
    "\n",
    "    # Optional: Clean up the answer, remove any additional text after the answer\n",
    "    # This can be tailored based on how your model generates responses\n",
    "    end_characters = [\".\", \"?\", \"!\"]\n",
    "    for end_char in end_characters:\n",
    "        end_idx = answer.find(end_char)\n",
    "        if end_idx != -1:\n",
    "            answer = answer[:end_idx + 1]\n",
    "            break\n",
    "\n",
    "    return answer.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import collections\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = word_tokenize(a_gold)\n",
    "    pred_toks = word_tokenize(a_pred)\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    \n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1704738504.857436 Started.\n"
     ]
    }
   ],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Parameters\n",
    "max_stories = 100\n",
    "chunk_sizes = [200]\n",
    "overlap_percentages = [0.1]  # Expressed as percentages\n",
    "\n",
    "# Results storage\n",
    "f1_results = defaultdict(list)\n",
    "em_results = defaultdict(list)\n",
    "\n",
    "# Initialize the language model (GPT4ALL)\n",
    "# llm_path = \"C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "llm_path = \"/Users/wk77/Library/CloudStorage/OneDrive-DrexelUniversity/Documents/data/gpt4all/models/gpt4all-falcon-q4_0.gguf\"\n",
    "llm = GPT4All(model=llm_path, max_tokens=2048)\n",
    "\n",
    "# HuggingFace Instruct Embeddings parameters\n",
    "instruct_embedding_model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "# instruct_embedding_model_kwargs = {'device': 'cpu'}\n",
    "instruct_embedding_model_kwargs = {'device': 'mps'}\n",
    "instruct_embedding_encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# Start time calculation\n",
    "start_time = time.time()\n",
    "print(f\"{start_time} Started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Top 3 Retrieved Sentence from MiniLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from heapq import nlargest\n",
    "\n",
    "def process_story_questions(data, model_name, instruction):\n",
    "    # model_kwargs = {'device': 'cpu'}\n",
    "    model_kwargs = {'device': 'mps'}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "    story_data = data['data'][0]  # First story\n",
    "    story_text = story_data['text']\n",
    "\n",
    "    # Segment story text into sentences and embed each sentence\n",
    "    sentences = sent_tokenize(story_text)  # Splitting into sentences\n",
    "    hf_story_embs = HuggingFaceInstructEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "        embed_instruction=\"Use the following pieces of context to answer the question at the end:\"\n",
    "    )\n",
    "    sentence_embs = hf_story_embs.embed_documents(sentences)\n",
    "\n",
    "    # Process each question\n",
    "    for question_data in story_data['questions']:\n",
    "        question = question_data['q']\n",
    "\n",
    "        hf_query_embs = HuggingFaceInstructEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs=model_kwargs,\n",
    "            encode_kwargs=encode_kwargs,\n",
    "            query_instruction=instruction\n",
    "        )\n",
    "        question_emb = hf_query_embs.embed_documents([question])[0]\n",
    "\n",
    "        # Compute cosine similarity scores with each sentence\n",
    "        scores = [torch.cosine_similarity(torch.tensor(sentence_emb).unsqueeze(0), torch.tensor(question_emb).unsqueeze(0))[0].item() for sentence_emb in sentence_embs]\n",
    "\n",
    "        # Find the sentences with the top 3 highest scores\n",
    "        top_scores_indices = nlargest(3, range(len(scores)), key=lambda idx: scores[idx])\n",
    "        top_sentences = [sentences[idx] for idx in top_scores_indices]\n",
    "\n",
    "        # Extract actual answer using the consensus range\n",
    "        consensus = question_data['consensus']\n",
    "        actual_answer = story_text[consensus['s']:consensus['e']]\n",
    "        \n",
    "        # Calculate F1 score for the best sentence\n",
    "        best_sentence_idx = scores.index(max(scores))\n",
    "        best_sentence = sentences[best_sentence_idx]\n",
    "        f1_score = compute_f1(best_sentence, actual_answer)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"Question: {question}\")\n",
    "        for i, sentence in enumerate(top_sentences):\n",
    "            print(f\"Top {i+1} Predicted Answer: {sentence}\")\n",
    "        print(f\"Actual Answer: {actual_answer}\")\n",
    "        print(f\"Best Sentence F1 Score: {f1_score:.4f}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: What was the amount of children murdered?\n",
      "Top 1 Predicted Answer: The teen was one of 19 victims -- children and young women -- in one of the most gruesome serial killings in India in recent years.\n",
      "Top 2 Predicted Answer: Pandher and his domestic employee Surinder Koli were sentenced to death in February by a lower court for the rape and murder of the 14-year-old.\n",
      "Top 3 Predicted Answer: Pandher faces trial in the remaining 18 killings and could remain in custody, the attorney said.\n",
      "Actual Answer: 19 \n",
      "Best Sentence F1 Score: 0.0714\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: When was Pandher sentenced to death?\n",
      "Top 1 Predicted Answer: Moninder Singh Pandher was sentenced to death by a lower court in February.\n",
      "Top 2 Predicted Answer: Pandher and his domestic employee Surinder Koli were sentenced to death in February by a lower court for the rape and murder of the 14-year-old.\n",
      "Top 3 Predicted Answer: Pandher faces trial in the remaining 18 killings and could remain in custody, the attorney said.\n",
      "Actual Answer: February.\n",
      "\n",
      "Best Sentence F1 Score: 0.2500\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: The court aquitted Moninder Singh Pandher of what crime?\n",
      "Top 1 Predicted Answer: The Allahabad high court has acquitted Moninder Singh Pandher, his lawyer Sikandar B. Kochar told CNN.\n",
      "Top 2 Predicted Answer: Moninder Singh Pandher was sentenced to death by a lower court in February.\n",
      "Top 3 Predicted Answer: Pandher faces trial in the remaining 18 killings and could remain in custody, the attorney said.\n",
      "Actual Answer: rape and murder \n",
      "Best Sentence F1 Score: 0.0000\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: who was acquitted\n",
      "Top 1 Predicted Answer: The Allahabad high court has acquitted Moninder Singh Pandher, his lawyer Sikandar B. Kochar told CNN.\n",
      "Top 2 Predicted Answer: Pandher was not named a main suspect by investigators initially, but was summoned as co-accused during the trial, Kochar said.\n",
      "Top 3 Predicted Answer: Moninder Singh Pandher was sentenced to death by a lower court in February.\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "Best Sentence F1 Score: 0.2857\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: who was sentenced\n",
      "Top 1 Predicted Answer: Moninder Singh Pandher was sentenced to death by a lower court in February.\n",
      "Top 2 Predicted Answer: Pandher and his domestic employee Surinder Koli were sentenced to death in February by a lower court for the rape and murder of the 14-year-old.\n",
      "Top 3 Predicted Answer: The high court upheld Koli's death sentence, Kochar said.\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "Best Sentence F1 Score: 0.3529\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: What was Moninder Singh Pandher acquitted for?\n",
      "Top 1 Predicted Answer: The Allahabad high court has acquitted Moninder Singh Pandher, his lawyer Sikandar B. Kochar told CNN.\n",
      "Top 2 Predicted Answer: Moninder Singh Pandher was sentenced to death by a lower court in February.\n",
      "Top 3 Predicted Answer: Pandher was not named a main suspect by investigators initially, but was summoned as co-accused during the trial, Kochar said.\n",
      "Actual Answer: the killing of a teen \n",
      "Best Sentence F1 Score: 0.0000\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: Who was sentenced to death in February?\n",
      "Top 1 Predicted Answer: Moninder Singh Pandher was sentenced to death by a lower court in February.\n",
      "Top 2 Predicted Answer: Pandher and his domestic employee Surinder Koli were sentenced to death in February by a lower court for the rape and murder of the 14-year-old.\n",
      "Top 3 Predicted Answer: The high court upheld Koli's death sentence, Kochar said.\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "Best Sentence F1 Score: 0.3529\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: how many people died\n",
      "Top 1 Predicted Answer: The teen was one of 19 victims -- children and young women -- in one of the most gruesome serial killings in India in recent years.\n",
      "Top 2 Predicted Answer: Moninder Singh Pandher was sentenced to death by a lower court in February.\n",
      "Top 3 Predicted Answer: Pandher faces trial in the remaining 18 killings and could remain in custody, the attorney said.\n",
      "Actual Answer: 19 \n",
      "Best Sentence F1 Score: 0.0714\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: How many children and young women were murdered?\n",
      "Top 1 Predicted Answer: The teen was one of 19 victims -- children and young women -- in one of the most gruesome serial killings in India in recent years.\n",
      "Top 2 Predicted Answer: Pandher and his domestic employee Surinder Koli were sentenced to death in February by a lower court for the rape and murder of the 14-year-old.\n",
      "Top 3 Predicted Answer: Pandher faces trial in the remaining 18 killings and could remain in custody, the attorney said.\n",
      "Actual Answer: 19 \n",
      "Best Sentence F1 Score: 0.0714\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "instruction = \"Represent the story for retrieval:\" \n",
    "process_story_questions(data_2, model_name, instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPT4ALL with HuggingFace InstructEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Question Answering Loop Encapsulation\n",
    "Encapsulate the loop process to simplify the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newsqa_loop_test(data, llm, output_file_path, chunk_sizes, overlap_percentages, max_stories, instruct_embedding_model_name, instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, QA_CHAIN_PROMPT):\n",
    "    \"\"\"\n",
    "    Processes a collection of stories, answering questions using a QA chain based on GPT4ALL language model and\n",
    "    HuggingFaceInstructEmbeddings.\n",
    "\n",
    "    For each story, the text is split into chunks, and each chunk is processed to answer questions. The results,\n",
    "    including calculated F1 and EM scores, are written to an output file.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The dataset containing the stories and questions.\n",
    "        llm (LLM): The language model instance used for generating answers.\n",
    "        output_file_path (str): Path to the output file where results will be stored.\n",
    "        chunk_sizes (list): List of chunk sizes for text splitting.\n",
    "        overlap_percentages (list): List of overlap percentages for text splitting.\n",
    "        max_stories (int): Maximum number of stories to process.\n",
    "        instruct_embedding_model_name (str): Name of the HuggingFace model for embeddings.\n",
    "        instruct_embedding_model_kwargs (dict): Keyword arguments for the embedding model.\n",
    "        instruct_embedding_encode_kwargs (dict): Encoding arguments for the embedding model.\n",
    "        QA_CHAIN_PROMPT (str): The prompt template for the QA chain.\n",
    "\n",
    "    Returns:\n",
    "        The function writes the results to a file and print the results of question, answer, actual answer, \n",
    "        and F1 scores.\n",
    "    \"\"\"\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        word_embed = HuggingFaceInstructEmbeddings(\n",
    "            model_name=instruct_embedding_model_name,\n",
    "            model_kwargs=instruct_embedding_model_kwargs,\n",
    "            encode_kwargs=instruct_embedding_encode_kwargs\n",
    "        )\n",
    "        start_time = time.time()\n",
    "        for chunk_size in chunk_sizes:\n",
    "            print(f\"\\n{time.time()-start_time} Processing chunk size {chunk_size}:\")\n",
    "            last_time = time.time()\n",
    "            for overlap_percentage in overlap_percentages:\n",
    "                actual_overlap = int(chunk_size * overlap_percentage)\n",
    "                print(f\"\\n{time.time()-start_time}\\t{time.time()-last_time}\\tOverlap [{overlap_percentage}] {actual_overlap}\")\n",
    "                text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=actual_overlap)\n",
    "        \n",
    "                for i, story in enumerate(data['data']):\n",
    "                    if i >= max_stories:\n",
    "                        break\n",
    "                    now_time = time.time()\n",
    "                    print(f\"\\n{now_time-start_time}\\t{now_time-last_time}\\t\\tstory {i+1}: \", end='')\n",
    "                    last_time = now_time\n",
    "        \n",
    "                    all_splits = text_splitter.split_text(story['text'])\n",
    "                    vectorstore = Chroma.from_texts(texts=all_splits, embedding=word_embed)\n",
    "                    qa_chain = RetrievalQA.from_chain_type(\n",
    "                        llm, \n",
    "                        retriever=vectorstore.as_retriever(), \n",
    "                        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "                        return_source_documents=True)\n",
    "        \n",
    "                    for j, question_data in enumerate(story['questions']):\n",
    "                        j+=1\n",
    "                        # print(f\"{time.time()-start_time}\\t\\t\\tquestion {j}\")\n",
    "                        print('.', end='')\n",
    "        \n",
    "                        # TODO: embed query and perform similarity_search_by_vector() instead\n",
    "                        question = question_data['q']\n",
    "                        question_vector = word_embed.embed_query(question)\n",
    "                        # docs = vectorstore.similarity_search(question)\n",
    "                        docs = vectorstore.similarity_search_by_vector(question_vector)\n",
    "                        answer_ranges = extract_ranges(question_data['consensus'])\n",
    "                        \n",
    "                        # Get the prediction from the model\n",
    "                        result = qa_chain({\"query\": question})\n",
    "                        \n",
    "                        # Extract and print the retrieved sentence(s) if available in the result\n",
    "                        retrieved_sentences = result.get('source_documents', [])\n",
    "                        print(\"\\nRetrieved Sentence(s):\")\n",
    "                        for sentence in retrieved_sentences:\n",
    "                            print(sentence)\n",
    "                        \n",
    "                        # Check if the predicted answer is in the expected format (string)\n",
    "                        predicted_answer = result['result']\n",
    "                        if isinstance(predicted_answer, dict):\n",
    "                            # If it's a dictionary, you need to adapt this part of the code to extract the answer string\n",
    "                            predicted_answer = predicted_answer.get('answer', '')  # Assuming 'answer' is the key for the answer string\n",
    "                        elif not isinstance(predicted_answer, str):\n",
    "                            # If the answer is not a string and not a dictionary, log an error or handle it appropriately\n",
    "                            print(f\"Unexpected format for predicted answer: {predicted_answer}\")\n",
    "                            continue  # Skip to the next question\n",
    "                        actual_answer = story['text'][answer_ranges[0][0]:answer_ranges[0][1]] if answer_ranges else \"\"\n",
    "                        \n",
    "                        # If there is an actual answer, get it from the story text using the character ranges\n",
    "                        if answer_ranges:\n",
    "                            actual_answer = story['text'][answer_ranges[0][0]:answer_ranges[0][1]]\n",
    "                        else:\n",
    "                            actual_answer = \"\"\n",
    "                        \n",
    "                        # Calculate the scores\n",
    "                        em_score = calculate_em(predicted_answer, actual_answer)\n",
    "                        f1_score_value = calculate_token_f1(predicted_answer, actual_answer)\n",
    "                        # modified_f1 = calculate_modified_f1(predicted_answer, actual_answer)\n",
    "                        file.write(f\"{chunk_size}\\t{overlap_percentage}\\t{i}\\t{j}\\t{f1_score_value:.4f}\\t{em_score:.2f}\\n\")\n",
    "        \n",
    "                        # Store the scores\n",
    "                        em_results[(chunk_size, overlap_percentage)].append(em_score)\n",
    "                        f1_results[(chunk_size, overlap_percentage)].append(f1_score_value)\n",
    "                        \n",
    "                        # Print results\n",
    "                        print(f\"\\nQuestion: {question}\")\n",
    "                        print(f\"Predicted Answer: {predicted_answer}\")\n",
    "                        print(f\"Actual Answer: {actual_answer}\")\n",
    "                        print(f\"F1 Score: {f1_score_value:4f}\")\n",
    "                        \n",
    "                    # delete object for memory\n",
    "                    del qa_chain\n",
    "                    del vectorstore\n",
    "                    del all_splits\n",
    "                    \n",
    "                # delete splitter instance\n",
    "                del text_splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Try different chunk sizes and overlap percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_original = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use ten words maximum and keep the answer as concise as possible. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT_ORIGINAL = PromptTemplate.from_template(template_original)\n",
    "\n",
    "############## Important Parameters ##############\n",
    "max_stories = 100\n",
    "chunk_sizes = [100, 200, 400, 800]\n",
    "overlap_percentages = [0, 0.1, 0.2, 0.4]  # Expressed as percentages (0.1 = 10%)\n",
    "random_seed = 123\n",
    "output_file_path = \"results/scores_10.txt\"\n",
    "# model_location = \"C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "model_location = \"/Users/wk77/Library/CloudStorage/OneDrive-DrexelUniversity/Documents/data/gpt4all/models/gpt4all-falcon-q4_0.gguf\"\n",
    "\n",
    "##################################################\n",
    "\n",
    "# logging.basicConfig(level=logging.WARNING)  # This will show only warnings and errors\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "set_seed(random_seed)\n",
    "\n",
    "# Results storage\n",
    "f1_results = defaultdict(list)\n",
    "em_results = defaultdict(list)\n",
    "text_results = []\n",
    "\n",
    "# Initialize the language model and the QA chain\n",
    "llm = GPT4All(model=model_location, max_tokens=2048, seed=random_seed)\n",
    "\n",
    "# HuggingFace Instruct Embeddings parameters\n",
    "instruct_embedding_model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "# instruct_embedding_model_kwargs = {'device': 'cpu'}\n",
    "instruct_embedding_model_kwargs = {'device': 'mps'}\n",
    "instruct_embedding_encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# The following code would iterate over the stories and questions to calculate the scores\n",
    "start_time = time.time()\n",
    "print(f\"{start_time} Started.\")\n",
    "\n",
    "################ Main Function ################\n",
    "newsqa_loop_test(data, llm, output_file_path, chunk_sizes, overlap_percentages, max_stories, instruct_embedding_model_name,\n",
    "                 instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, QA_CHAIN_PROMPT_ORIGINAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prompt Focusing on Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_1 = \"\"\"When you embed each sentence, focus closely on the details and the overall context of the news story. Pay attention to who is involved, what exactly happened, the timing of the events, and where they took place. Also, consider why these events are significant. This detailed analysis will help you accurately answer questions that require a deep understanding of the news story. \\n\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT_1 = PromptTemplate.from_template(template_1)\n",
    "\n",
    "############## Important Parameters ##############\n",
    "max_stories = 100\n",
    "chunk_sizes = [100, 200, 400, 800]\n",
    "overlap_percentages = [0, 0.1, 0.2, 0.4]  # Expressed as percentages (0.1 = 10%)\n",
    "random_seed = 123\n",
    "output_file_path = \"results/scores_11.txt\"\n",
    "# model_location = \"C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "model_location = \"/Users/wk77/Library/CloudStorage/OneDrive-DrexelUniversity/Documents/data/gpt4all/models/gpt4all-falcon-q4_0.gguf\"\n",
    "\n",
    "##################################################\n",
    "\n",
    "# logging.basicConfig(level=logging.WARNING)  # This will show only warnings and errors\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "set_seed(random_seed)\n",
    "\n",
    "# Results storage\n",
    "f1_results = defaultdict(list)\n",
    "em_results = defaultdict(list)\n",
    "text_results = []\n",
    "\n",
    "# Initialize the language model and the QA chain\n",
    "llm = GPT4All(model=model_location, max_tokens=2048, seed=random_seed)\n",
    "\n",
    "# HuggingFace Instruct Embeddings parameters\n",
    "instruct_embedding_model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "# instruct_embedding_model_kwargs = {'device': 'cpu'}\n",
    "instruct_embedding_model_kwargs = {'device': 'mps'}\n",
    "instruct_embedding_encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# The following code would iterate over the stories and questions to calculate the scores\n",
    "start_time = time.time()\n",
    "print(f\"{start_time} Started.\")\n",
    "\n",
    "################ Main Function ################\n",
    "newsqa_loop_test(data, llm, output_file_path, chunk_sizes, overlap_percentages, max_stories, instruct_embedding_model_name,\n",
    "                 instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, QA_CHAIN_PROMPT_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Prompt Highlighting Key Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New template that only let the model answer questions from the original text.\n",
    "template_2 = \"\"\"As you process the sentences, concentrate on the main events and the participants in the story. Make sure to identify the key players (who), the actions or events described (what), the timeline of these events (when), the locations involved (where), and the reasons or motives behind them (why). This focus will enable you to provide precise answers to questions that center around these crucial elements of the news.\\n\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT_2 = PromptTemplate.from_template(template_2)\n",
    "\n",
    "############## Important Parameters ##############\n",
    "max_stories = 100\n",
    "chunk_sizes = [100, 200, 400, 800]\n",
    "overlap_percentages = [0, 0.1, 0.2, 0.4]  # Expressed as percentages (0.1 = 10%)\n",
    "random_seed = 123\n",
    "output_file_path = \"results/scores_12.txt\"\n",
    "# model_location = \"C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "model_location = \"/Users/wk77/Library/CloudStorage/OneDrive-DrexelUniversity/Documents/data/gpt4all/models/gpt4all-falcon-q4_0.gguf\"\n",
    "\n",
    "##################################################\n",
    "\n",
    "# logging.basicConfig(level=logging.WARNING)  # This will show only warnings and errors\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "set_seed(random_seed)\n",
    "\n",
    "# Results storage\n",
    "f1_results = defaultdict(list)\n",
    "em_results = defaultdict(list)\n",
    "text_results = []\n",
    "\n",
    "# HuggingFace Instruct Embeddings parameters\n",
    "instruct_embedding_model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "# instruct_embedding_model_kwargs = {'device': 'cpu'}\n",
    "instruct_embedding_model_kwargs = {'device': 'mps'}\n",
    "instruct_embedding_encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# The following code would iterate over the stories and questions to calculate the scores\n",
    "start_time = time.time()\n",
    "print(f\"{start_time} Started.\")\n",
    "\n",
    "newsqa_loop_test(data, llm, output_file_path, chunk_sizes, overlap_percentages, max_stories, instruct_embedding_model_name,\n",
    "                 instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, QA_CHAIN_PROMPT_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Prompt Emphasizing Narrative Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New template that only let the model answer questions from the original text.\n",
    "template_3 = \"\"\"Your task is to capture the narrative flow of the news story, with an emphasis on the sequence of events and causality. Examine the connections between the participants (who), the series of events (what), their chronological order (when), the settings (where), and the motivations behind them (why). This approach will assist you in answering questions that depend on understanding how the story unfolds and the cause-and-effect relationships within it.\\n\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT_3 = PromptTemplate.from_template(template_3)\n",
    "\n",
    "############## Important Parameters ##############\n",
    "max_stories = 100\n",
    "chunk_sizes = [100, 200, 400, 800]\n",
    "overlap_percentages = [0, 0.1, 0.2, 0.4]  # Expressed as percentages (0.1 = 10%)\n",
    "random_seed = 123\n",
    "output_file_path = \"results/scores_13.txt\"\n",
    "model_location = \"C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "# model_location = \"/Users/wk77/Library/CloudStorage/OneDrive-DrexelUniversity/Documents/data/gpt4all/models/gpt4all-falcon-q4_0.gguf\"\n",
    "\n",
    "##################################################\n",
    "\n",
    "# logging.basicConfig(level=logging.WARNING)  # This will show only warnings and errors\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "set_seed(random_seed)\n",
    "\n",
    "# Results storage\n",
    "f1_results = defaultdict(list)\n",
    "em_results = defaultdict(list)\n",
    "text_results = []\n",
    "\n",
    "# HuggingFace Instruct Embeddings parameters\n",
    "instruct_embedding_model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "# instruct_embedding_model_kwargs = {'device': 'cpu'}\n",
    "instruct_embedding_model_kwargs = {'device': 'mps'}\n",
    "instruct_embedding_encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# The following code would iterate over the stories and questions to calculate the scores\n",
    "start_time = time.time()\n",
    "print(f\"{start_time} Started.\")\n",
    "\n",
    "newsqa_loop_test(data, llm, output_file_path, chunk_sizes, overlap_percentages, max_stories, instruct_embedding_model_name,\n",
    "                 instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, QA_CHAIN_PROMPT_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset vectorstore directly\n",
    "Chroma().delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Two Models Combination (Delayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Check that MPS is available\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "mps_device = torch.device(\"mps\")\n",
    "\n",
    "def process_story_questions_new(data, model_name, instruction):\n",
    "    # model_kwargs = {'device': 'cpu'}\n",
    "    model_kwargs = {'device': 'mps'}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "    story_data = data['data'][0]  # First story\n",
    "    story_text = story_data['text']\n",
    "\n",
    "    # Segment story text into sentences and embed each sentence\n",
    "    sentences = sent_tokenize(story_text)  # Splitting into sentences\n",
    "    hf_story_embs = HuggingFaceInstructEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "        embed_instruction=\"Use the following pieces of context to answer the question at the end:\"\n",
    "    )\n",
    "    sentence_embs = hf_story_embs.embed_documents(sentences)\n",
    "\n",
    "    # Process each question\n",
    "    for question_data in story_data['questions']:\n",
    "        question = question_data['q']\n",
    "\n",
    "        hf_query_embs = HuggingFaceInstructEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs=model_kwargs,\n",
    "            encode_kwargs=encode_kwargs,\n",
    "            query_instruction=instruction\n",
    "        )\n",
    "        question_emb = hf_query_embs.embed_documents([question])[0]\n",
    "\n",
    "        # Compute cosine similarity scores with each sentence\n",
    "        scores = [torch.cosine_similarity(torch.tensor(sentence_emb).unsqueeze(0), torch.tensor(question_emb).unsqueeze(0))[0].item() for sentence_emb in sentence_embs]\n",
    "\n",
    "        # Find the sentence with the highest score\n",
    "        best_sentence_idx = scores.index(max(scores))\n",
    "        best_sentence = sentences[best_sentence_idx]\n",
    "\n",
    "        # Extract actual answer using the consensus range\n",
    "        consensus = question_data['consensus']\n",
    "        actual_answer = story_text[consensus['s']:consensus['e']]\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        f1_score = compute_f1(best_sentence, actual_answer)\n",
    "        \n",
    "        # Print results\n",
    "        # print(f\"Question: {question}\")\n",
    "        # print(f\"Predicted Answer: {best_sentence}\")\n",
    "        # print(f\"Actual Answer: {actual_answer}\")\n",
    "        # print(f\"F1 Score: {f1_score:.4f}\")\n",
    "        # print()\n",
    "        \n",
    "        generate_answer_from_best_sentence(best_sentence, question)\n",
    "\n",
    "def generate_answer_from_best_sentence(best_sentence, question):\n",
    "    # model_name_or_path = \"TheBloke/WizardLM-13B-V1.1-GPTQ\"\n",
    "    \n",
    "    # Load the model and tokenizer\n",
    "    # model = AutoModelForCausalLM.from_pretrained(model_name_or_path, \n",
    "    #                                              device_map=\"auto\", \n",
    "    #                                              trust_remote_code=True, \n",
    "    #                                              revision=\"main\")\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"WizardLM/WizardLM-13B-V1.2\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"WizardLM/WizardLM-13B-V1.2\")\n",
    "    model.to(mps_device)\n",
    "\n",
    "    # Create the prompt\n",
    "    prompt = \"Answer the question, give shortest answer.\"\n",
    "    prompt_template = f\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Read the following sentence: {best_sentence}, give me the answer for this question: {question}, use the shortest answer possible. ASSISTANT:\"\n",
    "\n",
    "    # Generate the response\n",
    "    # input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "    input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.to(\"mps\")\n",
    "    output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "    model_answer = tokenizer.decode(output[0])\n",
    "\n",
    "    # Extracting the answer from the generated text\n",
    "    start_phrase = \"ASSISTANT: \"\n",
    "    start_idx = model_answer.find(start_phrase)\n",
    "    if start_idx != -1:\n",
    "        start_idx += len(start_phrase)\n",
    "        answer = model_answer[start_idx:].split('\\n')[0]  # Assuming the answer ends with a newline\n",
    "    else:\n",
    "        answer = \"Answer not found in generated text\"\n",
    "\n",
    "    # Print the question, best sentence, and the answer\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Best Retrieved Sentence: {best_sentence}\")\n",
    "    print(f\"Generated Answer: {answer}\")\n",
    "    print()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "instruction = \"Represent the story for retrieval:\" \n",
    "process_story_questions_new(data_2, model_name, instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "1. GPT4All Langchain Demo: https://gist.github.com/segestic/4822f3765147418fc084a250598c1fe6\n",
    "2. Sematic - use GPT4ALL, FAISS & HuggingFace Embeddings for local context-enhanced question answering https://www.youtube.com/watch?v=y32vbJkabCw\n",
    "3. Model used for InstructEmbeddings: [Hugging Face](https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1)\n",
    "4. Reset the Chroma db collection: https://github.com/langchain-ai/langchain/issues/\n",
    "5. F1 score in NLP span-based Question Answering task: https://kierszbaumsamuel.medium.com/f1-score-in-nlp-span-based-qa-task-5b115a5e7d41\n",
    "6. The Stanford Question Answering Dataset: https://rajpurkar.github.io/SQuAD-explorer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
