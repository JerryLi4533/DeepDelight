{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Exploration for Failed Sentences\n",
    "12/15/2023\n",
    "\n",
    "Lixiao Yang\n",
    "\n",
    "This notebook is updated based on `9. NewsQA Prompt Engineering.ipynb`, for loop running for large dataset, refer to the `NewsQA Loop - v2.ipunb`. The output is only demonstrating the first story's results.\n",
    "\n",
    "\n",
    "#### Updates\n",
    "1. Make the MiniLM embedding returns the top 3 sentences retrieved for anwering.\n",
    "2. Modify the data into the incorrect answered questions from previous results.\n",
    "3. Unable to run WizardLM model locally due to memory limitations, consider adding more GPU to the JupyterHub. (DELAYED)\n",
    "\n",
    "#### Findings\n",
    "1. The MiniLM model still can not get the correct sentence from top 3 scored retrieved sentences.\n",
    "2. No obvious improvement for the two failed questions for GPT4ALL model with combination of different chunk sizes and overlap percentages. Same sentences are retrieved for all prompt-modified results.\n",
    "3. WizardLm is a Llama-based model, force the model to bypass the GPU configuration may cause new issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama-cpp-python langchain sentence_transformers InstructorEmbedding pyllama transformers pyqt5 pyqtwebengine pyllamacpp --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wk77/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import torch\n",
    "from langchain.llms import GPT4All\n",
    "from pathlib import Path\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceInstructEmbeddings\n",
    "from transformers import set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='data/combined-newsqa-data-story1-2questions.json'\n",
    "data = json.loads(Path(file_path).read_text())\n",
    "\n",
    "file_path='data/combined-newsqa-data-story1.json'\n",
    "data_2 = json.loads(Path(file_path).read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate Exact Match (EM) score\n",
    "def calculate_em(predicted, actual):\n",
    "    return int(predicted == actual)\n",
    "\n",
    "# Function to calculate the token-wise F1 score for text answers\n",
    "def calculate_token_f1(predicted, actual):\n",
    "    predicted_tokens = predicted.split()\n",
    "    actual_tokens = actual.split()\n",
    "    common_tokens = Counter(predicted_tokens) & Counter(actual_tokens)\n",
    "    num_same = sum(common_tokens.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(predicted_tokens)\n",
    "    recall = 1.0 * num_same / len(actual_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "# Helper function to extract answer ranges from the consensus field\n",
    "def extract_ranges(consensus):\n",
    "    if 's' in consensus and 'e' in consensus:\n",
    "        return [(consensus['s'], consensus['e'])]\n",
    "    return []\n",
    "\n",
    "def extract_answer_from_gpt4all_output(output, prompt_end=\"Answer:\"):\n",
    "    \"\"\"\n",
    "    Extracts the answer from the output of the GPT4ALL model.\n",
    "\n",
    "    Parameters:\n",
    "        output (str): The output string from the GPT4ALL model.\n",
    "        prompt_end (str): The delimiter indicating the end of the prompt and the start of the answer.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted answer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Find the start of the answer\n",
    "    start_idx = output.find(prompt_end)\n",
    "    if start_idx == -1:\n",
    "        return \"Answer not found in output\"\n",
    "\n",
    "    # Extract the answer\n",
    "    start_idx += len(prompt_end)\n",
    "    answer = output[start_idx:]\n",
    "\n",
    "    # Optional: Clean up the answer, remove any additional text after the answer\n",
    "    # This can be tailored based on how your model generates responses\n",
    "    end_characters = [\".\", \"?\", \"!\"]\n",
    "    for end_char in end_characters:\n",
    "        end_idx = answer.find(end_char)\n",
    "        if end_idx != -1:\n",
    "            answer = answer[:end_idx + 1]\n",
    "            break\n",
    "\n",
    "    return answer.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import collections\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = word_tokenize(a_gold)\n",
    "    pred_toks = word_tokenize(a_pred)\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    \n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1704738504.857436 Started.\n"
     ]
    }
   ],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Parameters\n",
    "max_stories = 100\n",
    "chunk_sizes = [200]\n",
    "overlap_percentages = [0.1]  # Expressed as percentages\n",
    "\n",
    "# Results storage\n",
    "f1_results = defaultdict(list)\n",
    "em_results = defaultdict(list)\n",
    "\n",
    "# Initialize the language model (GPT4ALL)\n",
    "# llm_path = \"C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "llm_path = \"/Users/wk77/Library/CloudStorage/OneDrive-DrexelUniversity/Documents/data/gpt4all/models/gpt4all-falcon-q4_0.gguf\"\n",
    "llm = GPT4All(model=llm_path, max_tokens=2048)\n",
    "\n",
    "# HuggingFace Instruct Embeddings parameters\n",
    "instruct_embedding_model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "# instruct_embedding_model_kwargs = {'device': 'cpu'}\n",
    "instruct_embedding_model_kwargs = {'device': 'mps'}\n",
    "instruct_embedding_encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# Start time calculation\n",
    "start_time = time.time()\n",
    "print(f\"{start_time} Started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Top 3 Retrieved Sentence from MiniLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from heapq import nlargest\n",
    "\n",
    "def process_story_questions(data, model_name, instruction):\n",
    "    # model_kwargs = {'device': 'cpu'}\n",
    "    model_kwargs = {'device': 'mps'}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "    story_data = data['data'][0]  # First story\n",
    "    story_text = story_data['text']\n",
    "\n",
    "    # Segment story text into sentences and embed each sentence\n",
    "    sentences = sent_tokenize(story_text)  # Splitting into sentences\n",
    "    hf_story_embs = HuggingFaceInstructEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "        embed_instruction=\"Use the following pieces of context to answer the question at the end:\"\n",
    "    )\n",
    "    sentence_embs = hf_story_embs.embed_documents(sentences)\n",
    "\n",
    "    # Process each question\n",
    "    for question_data in story_data['questions']:\n",
    "        question = question_data['q']\n",
    "\n",
    "        hf_query_embs = HuggingFaceInstructEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs=model_kwargs,\n",
    "            encode_kwargs=encode_kwargs,\n",
    "            query_instruction=instruction\n",
    "        )\n",
    "        question_emb = hf_query_embs.embed_documents([question])[0]\n",
    "\n",
    "        # Compute cosine similarity scores with each sentence\n",
    "        scores = [torch.cosine_similarity(torch.tensor(sentence_emb).unsqueeze(0), torch.tensor(question_emb).unsqueeze(0))[0].item() for sentence_emb in sentence_embs]\n",
    "\n",
    "        # Find the sentences with the top 3 highest scores\n",
    "        top_scores_indices = nlargest(3, range(len(scores)), key=lambda idx: scores[idx])\n",
    "        top_sentences = [sentences[idx] for idx in top_scores_indices]\n",
    "\n",
    "        # Extract actual answer using the consensus range\n",
    "        consensus = question_data['consensus']\n",
    "        actual_answer = story_text[consensus['s']:consensus['e']]\n",
    "        \n",
    "        # Calculate F1 score for the best sentence\n",
    "        best_sentence_idx = scores.index(max(scores))\n",
    "        best_sentence = sentences[best_sentence_idx]\n",
    "        f1_score = compute_f1(best_sentence, actual_answer)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"Question: {question}\")\n",
    "        for i, sentence in enumerate(top_sentences):\n",
    "            print(f\"Top {i+1} Predicted Answer: {sentence}\")\n",
    "        print(f\"Actual Answer: {actual_answer}\")\n",
    "        print(f\"Best Sentence F1 Score: {f1_score:.4f}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: What was the amount of children murdered?\n",
      "Top 1 Predicted Answer: The teen was one of 19 victims -- children and young women -- in one of the most gruesome serial killings in India in recent years.\n",
      "Top 2 Predicted Answer: Pandher and his domestic employee Surinder Koli were sentenced to death in February by a lower court for the rape and murder of the 14-year-old.\n",
      "Top 3 Predicted Answer: Pandher faces trial in the remaining 18 killings and could remain in custody, the attorney said.\n",
      "Actual Answer: 19 \n",
      "Best Sentence F1 Score: 0.0714\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: When was Pandher sentenced to death?\n",
      "Top 1 Predicted Answer: Moninder Singh Pandher was sentenced to death by a lower court in February.\n",
      "Top 2 Predicted Answer: Pandher and his domestic employee Surinder Koli were sentenced to death in February by a lower court for the rape and murder of the 14-year-old.\n",
      "Top 3 Predicted Answer: Pandher faces trial in the remaining 18 killings and could remain in custody, the attorney said.\n",
      "Actual Answer: February.\n",
      "\n",
      "Best Sentence F1 Score: 0.2500\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: The court aquitted Moninder Singh Pandher of what crime?\n",
      "Top 1 Predicted Answer: The Allahabad high court has acquitted Moninder Singh Pandher, his lawyer Sikandar B. Kochar told CNN.\n",
      "Top 2 Predicted Answer: Moninder Singh Pandher was sentenced to death by a lower court in February.\n",
      "Top 3 Predicted Answer: Pandher faces trial in the remaining 18 killings and could remain in custody, the attorney said.\n",
      "Actual Answer: rape and murder \n",
      "Best Sentence F1 Score: 0.0000\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: who was acquitted\n",
      "Top 1 Predicted Answer: The Allahabad high court has acquitted Moninder Singh Pandher, his lawyer Sikandar B. Kochar told CNN.\n",
      "Top 2 Predicted Answer: Pandher was not named a main suspect by investigators initially, but was summoned as co-accused during the trial, Kochar said.\n",
      "Top 3 Predicted Answer: Moninder Singh Pandher was sentenced to death by a lower court in February.\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "Best Sentence F1 Score: 0.2857\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: who was sentenced\n",
      "Top 1 Predicted Answer: Moninder Singh Pandher was sentenced to death by a lower court in February.\n",
      "Top 2 Predicted Answer: Pandher and his domestic employee Surinder Koli were sentenced to death in February by a lower court for the rape and murder of the 14-year-old.\n",
      "Top 3 Predicted Answer: The high court upheld Koli's death sentence, Kochar said.\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "Best Sentence F1 Score: 0.3529\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: What was Moninder Singh Pandher acquitted for?\n",
      "Top 1 Predicted Answer: The Allahabad high court has acquitted Moninder Singh Pandher, his lawyer Sikandar B. Kochar told CNN.\n",
      "Top 2 Predicted Answer: Moninder Singh Pandher was sentenced to death by a lower court in February.\n",
      "Top 3 Predicted Answer: Pandher was not named a main suspect by investigators initially, but was summoned as co-accused during the trial, Kochar said.\n",
      "Actual Answer: the killing of a teen \n",
      "Best Sentence F1 Score: 0.0000\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: Who was sentenced to death in February?\n",
      "Top 1 Predicted Answer: Moninder Singh Pandher was sentenced to death by a lower court in February.\n",
      "Top 2 Predicted Answer: Pandher and his domestic employee Surinder Koli were sentenced to death in February by a lower court for the rape and murder of the 14-year-old.\n",
      "Top 3 Predicted Answer: The high court upheld Koli's death sentence, Kochar said.\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "Best Sentence F1 Score: 0.3529\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: how many people died\n",
      "Top 1 Predicted Answer: The teen was one of 19 victims -- children and young women -- in one of the most gruesome serial killings in India in recent years.\n",
      "Top 2 Predicted Answer: Moninder Singh Pandher was sentenced to death by a lower court in February.\n",
      "Top 3 Predicted Answer: Pandher faces trial in the remaining 18 killings and could remain in custody, the attorney said.\n",
      "Actual Answer: 19 \n",
      "Best Sentence F1 Score: 0.0714\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: How many children and young women were murdered?\n",
      "Top 1 Predicted Answer: The teen was one of 19 victims -- children and young women -- in one of the most gruesome serial killings in India in recent years.\n",
      "Top 2 Predicted Answer: Pandher and his domestic employee Surinder Koli were sentenced to death in February by a lower court for the rape and murder of the 14-year-old.\n",
      "Top 3 Predicted Answer: Pandher faces trial in the remaining 18 killings and could remain in custody, the attorney said.\n",
      "Actual Answer: 19 \n",
      "Best Sentence F1 Score: 0.0714\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "instruction = \"Represent the story for retrieval:\" \n",
    "process_story_questions(data_2, model_name, instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPT4ALL with HuggingFace InstructEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Question Answering Loop Encapsulation\n",
    "Encapsulate the loop process to simplify the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newsqa_loop_test(data, llm, output_file_path, chunk_sizes, overlap_percentages, max_stories, instruct_embedding_model_name, instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, QA_CHAIN_PROMPT):\n",
    "    \"\"\"\n",
    "    Processes a collection of stories, answering questions using a QA chain based on GPT4ALL language model and\n",
    "    HuggingFaceInstructEmbeddings.\n",
    "\n",
    "    For each story, the text is split into chunks, and each chunk is processed to answer questions. The results,\n",
    "    including calculated F1 and EM scores, are written to an output file.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The dataset containing the stories and questions.\n",
    "        llm (LLM): The language model instance used for generating answers.\n",
    "        output_file_path (str): Path to the output file where results will be stored.\n",
    "        chunk_sizes (list): List of chunk sizes for text splitting.\n",
    "        overlap_percentages (list): List of overlap percentages for text splitting.\n",
    "        max_stories (int): Maximum number of stories to process.\n",
    "        instruct_embedding_model_name (str): Name of the HuggingFace model for embeddings.\n",
    "        instruct_embedding_model_kwargs (dict): Keyword arguments for the embedding model.\n",
    "        instruct_embedding_encode_kwargs (dict): Encoding arguments for the embedding model.\n",
    "        QA_CHAIN_PROMPT (str): The prompt template for the QA chain.\n",
    "\n",
    "    Returns:\n",
    "        The function writes the results to a file and print the results of question, answer, actual answer, \n",
    "        and F1 scores.\n",
    "    \"\"\"\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        word_embed = HuggingFaceInstructEmbeddings(\n",
    "            model_name=instruct_embedding_model_name,\n",
    "            model_kwargs=instruct_embedding_model_kwargs,\n",
    "            encode_kwargs=instruct_embedding_encode_kwargs\n",
    "        )\n",
    "        start_time = time.time()\n",
    "        for chunk_size in chunk_sizes:\n",
    "            print(f\"\\n{time.time()-start_time} Processing chunk size {chunk_size}:\")\n",
    "            last_time = time.time()\n",
    "            for overlap_percentage in overlap_percentages:\n",
    "                actual_overlap = int(chunk_size * overlap_percentage)\n",
    "                print(f\"\\n{time.time()-start_time}\\t{time.time()-last_time}\\tOverlap [{overlap_percentage}] {actual_overlap}\")\n",
    "                text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=actual_overlap)\n",
    "        \n",
    "                for i, story in enumerate(data['data']):\n",
    "                    if i >= max_stories:\n",
    "                        break\n",
    "                    now_time = time.time()\n",
    "                    print(f\"\\n{now_time-start_time}\\t{now_time-last_time}\\t\\tstory {i+1}: \", end='')\n",
    "                    last_time = now_time\n",
    "        \n",
    "                    all_splits = text_splitter.split_text(story['text'])\n",
    "                    vectorstore = Chroma.from_texts(texts=all_splits, embedding=word_embed)\n",
    "                    qa_chain = RetrievalQA.from_chain_type(\n",
    "                        llm, \n",
    "                        retriever=vectorstore.as_retriever(), \n",
    "                        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "                        return_source_documents=True)\n",
    "        \n",
    "                    for j, question_data in enumerate(story['questions']):\n",
    "                        j+=1\n",
    "                        # print(f\"{time.time()-start_time}\\t\\t\\tquestion {j}\")\n",
    "                        print('.', end='')\n",
    "        \n",
    "                        # TODO: embed query and perform similarity_search_by_vector() instead\n",
    "                        question = question_data['q']\n",
    "                        question_vector = word_embed.embed_query(question)\n",
    "                        # docs = vectorstore.similarity_search(question)\n",
    "                        docs = vectorstore.similarity_search_by_vector(question_vector)\n",
    "                        answer_ranges = extract_ranges(question_data['consensus'])\n",
    "                        \n",
    "                        # Get the prediction from the model\n",
    "                        result = qa_chain({\"query\": question})\n",
    "                        \n",
    "                        # Extract and print the retrieved sentence(s) if available in the result\n",
    "                        retrieved_sentences = result.get('source_documents', [])\n",
    "                        print(\"\\nRetrieved Sentence(s):\")\n",
    "                        for sentence in retrieved_sentences:\n",
    "                            print(sentence)\n",
    "                        \n",
    "                        # Check if the predicted answer is in the expected format (string)\n",
    "                        predicted_answer = result['result']\n",
    "                        if isinstance(predicted_answer, dict):\n",
    "                            # If it's a dictionary, you need to adapt this part of the code to extract the answer string\n",
    "                            predicted_answer = predicted_answer.get('answer', '')  # Assuming 'answer' is the key for the answer string\n",
    "                        elif not isinstance(predicted_answer, str):\n",
    "                            # If the answer is not a string and not a dictionary, log an error or handle it appropriately\n",
    "                            print(f\"Unexpected format for predicted answer: {predicted_answer}\")\n",
    "                            continue  # Skip to the next question\n",
    "                        actual_answer = story['text'][answer_ranges[0][0]:answer_ranges[0][1]] if answer_ranges else \"\"\n",
    "                        \n",
    "                        # If there is an actual answer, get it from the story text using the character ranges\n",
    "                        if answer_ranges:\n",
    "                            actual_answer = story['text'][answer_ranges[0][0]:answer_ranges[0][1]]\n",
    "                        else:\n",
    "                            actual_answer = \"\"\n",
    "                        \n",
    "                        # Calculate the scores\n",
    "                        em_score = calculate_em(predicted_answer, actual_answer)\n",
    "                        f1_score_value = calculate_token_f1(predicted_answer, actual_answer)\n",
    "                        # modified_f1 = calculate_modified_f1(predicted_answer, actual_answer)\n",
    "                        file.write(f\"{chunk_size}\\t{overlap_percentage}\\t{i}\\t{j}\\t{f1_score_value:.4f}\\t{em_score:.2f}\\n\")\n",
    "        \n",
    "                        # Store the scores\n",
    "                        em_results[(chunk_size, overlap_percentage)].append(em_score)\n",
    "                        f1_results[(chunk_size, overlap_percentage)].append(f1_score_value)\n",
    "                        \n",
    "                        # Print results\n",
    "                        print(f\"\\nQuestion: {question}\")\n",
    "                        print(f\"Predicted Answer: {predicted_answer}\")\n",
    "                        print(f\"Actual Answer: {actual_answer}\")\n",
    "                        print(f\"F1 Score: {f1_score_value:4f}\")\n",
    "                        \n",
    "                    # delete object for memory\n",
    "                    del qa_chain\n",
    "                    del vectorstore\n",
    "                    del all_splits\n",
    "                    \n",
    "                # delete splitter instance\n",
    "                del text_splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Try different chunk sizes and overlap percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_original = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use ten words maximum and keep the answer as concise as possible. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT_ORIGINAL = PromptTemplate.from_template(template_original)\n",
    "\n",
    "############## Important Parameters ##############\n",
    "max_stories = 100\n",
    "chunk_sizes = [100, 200, 400, 800]\n",
    "overlap_percentages = [0, 0.1, 0.2, 0.4]  # Expressed as percentages (0.1 = 10%)\n",
    "random_seed = 123\n",
    "output_file_path = \"results/scores_10.txt\"\n",
    "# model_location = \"C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "model_location = \"/Users/wk77/Library/CloudStorage/OneDrive-DrexelUniversity/Documents/data/gpt4all/models/gpt4all-falcon-q4_0.gguf\"\n",
    "\n",
    "##################################################\n",
    "\n",
    "# logging.basicConfig(level=logging.WARNING)  # This will show only warnings and errors\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "set_seed(random_seed)\n",
    "\n",
    "# Results storage\n",
    "f1_results = defaultdict(list)\n",
    "em_results = defaultdict(list)\n",
    "text_results = []\n",
    "\n",
    "# Initialize the language model and the QA chain\n",
    "llm = GPT4All(model=model_location, max_tokens=2048, seed=random_seed)\n",
    "\n",
    "# HuggingFace Instruct Embeddings parameters\n",
    "instruct_embedding_model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "# instruct_embedding_model_kwargs = {'device': 'cpu'}\n",
    "instruct_embedding_model_kwargs = {'device': 'mps'}\n",
    "instruct_embedding_encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# The following code would iterate over the stories and questions to calculate the scores\n",
    "start_time = time.time()\n",
    "print(f\"{start_time} Started.\")\n",
    "\n",
    "################ Main Function ################\n",
    "newsqa_loop_test(data, llm, output_file_path, chunk_sizes, overlap_percentages, max_stories, instruct_embedding_model_name,\n",
    "                 instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, QA_CHAIN_PROMPT_ORIGINAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prompt Focusing on Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_1 = \"\"\"When you embed each sentence, focus closely on the details and the overall context of the news story. Pay attention to who is involved, what exactly happened, the timing of the events, and where they took place. Also, consider why these events are significant. This detailed analysis will help you accurately answer questions that require a deep understanding of the news story. \\n\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT_1 = PromptTemplate.from_template(template_1)\n",
    "\n",
    "############## Important Parameters ##############\n",
    "max_stories = 100\n",
    "chunk_sizes = [100, 200, 400, 800]\n",
    "overlap_percentages = [0, 0.1, 0.2, 0.4]  # Expressed as percentages (0.1 = 10%)\n",
    "random_seed = 123\n",
    "output_file_path = \"results/scores_11.txt\"\n",
    "# model_location = \"C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "model_location = \"/Users/wk77/Library/CloudStorage/OneDrive-DrexelUniversity/Documents/data/gpt4all/models/gpt4all-falcon-q4_0.gguf\"\n",
    "\n",
    "##################################################\n",
    "\n",
    "# logging.basicConfig(level=logging.WARNING)  # This will show only warnings and errors\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "set_seed(random_seed)\n",
    "\n",
    "# Results storage\n",
    "f1_results = defaultdict(list)\n",
    "em_results = defaultdict(list)\n",
    "text_results = []\n",
    "\n",
    "# Initialize the language model and the QA chain\n",
    "llm = GPT4All(model=model_location, max_tokens=2048, seed=random_seed)\n",
    "\n",
    "# HuggingFace Instruct Embeddings parameters\n",
    "instruct_embedding_model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "# instruct_embedding_model_kwargs = {'device': 'cpu'}\n",
    "instruct_embedding_model_kwargs = {'device': 'mps'}\n",
    "instruct_embedding_encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# The following code would iterate over the stories and questions to calculate the scores\n",
    "start_time = time.time()\n",
    "print(f\"{start_time} Started.\")\n",
    "\n",
    "################ Main Function ################\n",
    "newsqa_loop_test(data, llm, output_file_path, chunk_sizes, overlap_percentages, max_stories, instruct_embedding_model_name,\n",
    "                 instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, QA_CHAIN_PROMPT_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Prompt Highlighting Key Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New template that only let the model answer questions from the original text.\n",
    "template_2 = \"\"\"As you process the sentences, concentrate on the main events and the participants in the story. Make sure to identify the key players (who), the actions or events described (what), the timeline of these events (when), the locations involved (where), and the reasons or motives behind them (why). This focus will enable you to provide precise answers to questions that center around these crucial elements of the news.\\n\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT_2 = PromptTemplate.from_template(template_2)\n",
    "\n",
    "############## Important Parameters ##############\n",
    "max_stories = 100\n",
    "chunk_sizes = [100, 200, 400, 800]\n",
    "overlap_percentages = [0, 0.1, 0.2, 0.4]  # Expressed as percentages (0.1 = 10%)\n",
    "random_seed = 123\n",
    "output_file_path = \"results/scores_12.txt\"\n",
    "# model_location = \"C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "model_location = \"/Users/wk77/Library/CloudStorage/OneDrive-DrexelUniversity/Documents/data/gpt4all/models/gpt4all-falcon-q4_0.gguf\"\n",
    "\n",
    "##################################################\n",
    "\n",
    "# logging.basicConfig(level=logging.WARNING)  # This will show only warnings and errors\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "set_seed(random_seed)\n",
    "\n",
    "# Results storage\n",
    "f1_results = defaultdict(list)\n",
    "em_results = defaultdict(list)\n",
    "text_results = []\n",
    "\n",
    "# HuggingFace Instruct Embeddings parameters\n",
    "instruct_embedding_model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "# instruct_embedding_model_kwargs = {'device': 'cpu'}\n",
    "instruct_embedding_model_kwargs = {'device': 'mps'}\n",
    "instruct_embedding_encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# The following code would iterate over the stories and questions to calculate the scores\n",
    "start_time = time.time()\n",
    "print(f\"{start_time} Started.\")\n",
    "\n",
    "newsqa_loop_test(data, llm, output_file_path, chunk_sizes, overlap_percentages, max_stories, instruct_embedding_model_name,\n",
    "                 instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, QA_CHAIN_PROMPT_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Prompt Emphasizing Narrative Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New template that only let the model answer questions from the original text.\n",
    "template_3 = \"\"\"Your task is to capture the narrative flow of the news story, with an emphasis on the sequence of events and causality. Examine the connections between the participants (who), the series of events (what), their chronological order (when), the settings (where), and the motivations behind them (why). This approach will assist you in answering questions that depend on understanding how the story unfolds and the cause-and-effect relationships within it.\\n\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT_3 = PromptTemplate.from_template(template_3)\n",
    "\n",
    "############## Important Parameters ##############\n",
    "max_stories = 100\n",
    "chunk_sizes = [100, 200, 400, 800]\n",
    "overlap_percentages = [0, 0.1, 0.2, 0.4]  # Expressed as percentages (0.1 = 10%)\n",
    "random_seed = 123\n",
    "output_file_path = \"results/scores_13.txt\"\n",
    "model_location = \"C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "# model_location = \"/Users/wk77/Library/CloudStorage/OneDrive-DrexelUniversity/Documents/data/gpt4all/models/gpt4all-falcon-q4_0.gguf\"\n",
    "\n",
    "##################################################\n",
    "\n",
    "# logging.basicConfig(level=logging.WARNING)  # This will show only warnings and errors\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "set_seed(random_seed)\n",
    "\n",
    "# Results storage\n",
    "f1_results = defaultdict(list)\n",
    "em_results = defaultdict(list)\n",
    "text_results = []\n",
    "\n",
    "# HuggingFace Instruct Embeddings parameters\n",
    "instruct_embedding_model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "# instruct_embedding_model_kwargs = {'device': 'cpu'}\n",
    "instruct_embedding_model_kwargs = {'device': 'mps'}\n",
    "instruct_embedding_encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# The following code would iterate over the stories and questions to calculate the scores\n",
    "start_time = time.time()\n",
    "print(f\"{start_time} Started.\")\n",
    "\n",
    "newsqa_loop_test(data, llm, output_file_path, chunk_sizes, overlap_percentages, max_stories, instruct_embedding_model_name,\n",
    "                 instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, QA_CHAIN_PROMPT_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset vectorstore directly\n",
    "Chroma().delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Two Models Combination (Delayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Check that MPS is available\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "mps_device = torch.device(\"mps\")\n",
    "\n",
    "def process_story_questions_new(data, model_name, instruction):\n",
    "    # model_kwargs = {'device': 'cpu'}\n",
    "    model_kwargs = {'device': 'mps'}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "    story_data = data['data'][0]  # First story\n",
    "    story_text = story_data['text']\n",
    "\n",
    "    # Segment story text into sentences and embed each sentence\n",
    "    sentences = sent_tokenize(story_text)  # Splitting into sentences\n",
    "    hf_story_embs = HuggingFaceInstructEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "        embed_instruction=\"Use the following pieces of context to answer the question at the end:\"\n",
    "    )\n",
    "    sentence_embs = hf_story_embs.embed_documents(sentences)\n",
    "\n",
    "    # Process each question\n",
    "    for question_data in story_data['questions']:\n",
    "        question = question_data['q']\n",
    "\n",
    "        hf_query_embs = HuggingFaceInstructEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs=model_kwargs,\n",
    "            encode_kwargs=encode_kwargs,\n",
    "            query_instruction=instruction\n",
    "        )\n",
    "        question_emb = hf_query_embs.embed_documents([question])[0]\n",
    "\n",
    "        # Compute cosine similarity scores with each sentence\n",
    "        scores = [torch.cosine_similarity(torch.tensor(sentence_emb).unsqueeze(0), torch.tensor(question_emb).unsqueeze(0))[0].item() for sentence_emb in sentence_embs]\n",
    "\n",
    "        # Find the sentence with the highest score\n",
    "        best_sentence_idx = scores.index(max(scores))\n",
    "        best_sentence = sentences[best_sentence_idx]\n",
    "\n",
    "        # Extract actual answer using the consensus range\n",
    "        consensus = question_data['consensus']\n",
    "        actual_answer = story_text[consensus['s']:consensus['e']]\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        f1_score = compute_f1(best_sentence, actual_answer)\n",
    "        \n",
    "        # Print results\n",
    "        # print(f\"Question: {question}\")\n",
    "        # print(f\"Predicted Answer: {best_sentence}\")\n",
    "        # print(f\"Actual Answer: {actual_answer}\")\n",
    "        # print(f\"F1 Score: {f1_score:.4f}\")\n",
    "        # print()\n",
    "        \n",
    "        generate_answer_from_best_sentence(best_sentence, question)\n",
    "\n",
    "def generate_answer_from_best_sentence(best_sentence, question):\n",
    "    # model_name_or_path = \"TheBloke/WizardLM-13B-V1.1-GPTQ\"\n",
    "    \n",
    "    # Load the model and tokenizer\n",
    "    # model = AutoModelForCausalLM.from_pretrained(model_name_or_path, \n",
    "    #                                              device_map=\"auto\", \n",
    "    #                                              trust_remote_code=True, \n",
    "    #                                              revision=\"main\")\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"WizardLM/WizardLM-13B-V1.2\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"WizardLM/WizardLM-13B-V1.2\")\n",
    "    model.to(mps_device)\n",
    "\n",
    "    # Create the prompt\n",
    "    prompt = \"Answer the question, give shortest answer.\"\n",
    "    prompt_template = f\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Read the following sentence: {best_sentence}, give me the answer for this question: {question}, use the shortest answer possible. ASSISTANT:\"\n",
    "\n",
    "    # Generate the response\n",
    "    # input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "    input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.to(\"mps\")\n",
    "    output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "    model_answer = tokenizer.decode(output[0])\n",
    "\n",
    "    # Extracting the answer from the generated text\n",
    "    start_phrase = \"ASSISTANT: \"\n",
    "    start_idx = model_answer.find(start_phrase)\n",
    "    if start_idx != -1:\n",
    "        start_idx += len(start_phrase)\n",
    "        answer = model_answer[start_idx:].split('\\n')[0]  # Assuming the answer ends with a newline\n",
    "    else:\n",
    "        answer = \"Answer not found in generated text\"\n",
    "\n",
    "    # Print the question, best sentence, and the answer\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Best Retrieved Sentence: {best_sentence}\")\n",
    "    print(f\"Generated Answer: {answer}\")\n",
    "    print()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wk77/Library/Python/3.9/lib/python/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/Users/wk77/Library/Python/3.9/lib/python/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/Users/wk77/Library/Python/3.9/lib/python/site-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/Users/wk77/Library/Python/3.9/lib/python/site-packages/transformers/generation/utils.py:1636: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on mps, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers/multi-qa-MiniLM-L6-cos-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m instruction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepresent the story for retrieval:\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[0;32m----> 3\u001b[0m \u001b[43mprocess_story_questions_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 58\u001b[0m, in \u001b[0;36mprocess_story_questions_new\u001b[0;34m(data, model_name, instruction)\u001b[0m\n\u001b[1;32m     49\u001b[0m f1_score \u001b[38;5;241m=\u001b[39m compute_f1(best_sentence, actual_answer)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# print(f\"Question: {question}\")\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# print(f\"Predicted Answer: {best_sentence}\")\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# print(f\"Actual Answer: {actual_answer}\")\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# print(f\"F1 Score: {f1_score:.4f}\")\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# print()\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[43mgenerate_answer_from_best_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_sentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 80\u001b[0m, in \u001b[0;36mgenerate_answer_from_best_sentence\u001b[0;34m(best_sentence, question)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Generate the response\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\u001b[39;00m\n\u001b[1;32m     79\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer(prompt_template, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m model_answer \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Extracting the answer from the generated text\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/generation/utils.py:1764\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1756\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1757\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1758\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1759\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1761\u001b[0m     )\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1772\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1773\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1774\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1779\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1781\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1782\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1787\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1788\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/generation/utils.py:2861\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2858\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2860\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2861\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2862\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2864\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2865\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2866\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2869\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/llama/modeling_llama.py:1181\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1178\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1181\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1193\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/llama/modeling_llama.py:1025\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1025\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_flash_attention_2:\n\u001b[1;32m   1028\u001b[0m     \u001b[38;5;66;03m# 2d mask is passed through the layers\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask \u001b[38;5;28;01mif\u001b[39;00m (attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attention_mask) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "instruction = \"Represent the story for retrieval:\" \n",
    "process_story_questions_new(data_2, model_name, instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "1. GPT4All Langchain Demo: https://gist.github.com/segestic/4822f3765147418fc084a250598c1fe6\n",
    "2. Sematic - use GPT4ALL, FAISS & HuggingFace Embeddings for local context-enhanced question answering https://www.youtube.com/watch?v=y32vbJkabCw\n",
    "3. Model used for InstructEmbeddings: [Hugging Face](https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1)\n",
    "4. Reset the Chroma db collection: https://github.com/langchain-ai/langchain/issues/\n",
    "5. F1 score in NLP span-based Question Answering task: https://kierszbaumsamuel.medium.com/f1-score-in-nlp-span-based-qa-task-5b115a5e7d41\n",
    "6. The Stanford Question Answering Dataset: https://rajpurkar.github.io/SQuAD-explorer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
