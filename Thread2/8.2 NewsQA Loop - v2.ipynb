{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NewsQA Loop - V2\n",
    "11/26/2023 \\\n",
    "Lixiao Yang\n",
    "\n",
    "This notebook provides a revised loop for different chunk and overlap sizes combining Hugging Face's `InstructEmbedding` (`sentence-transformers/multi-qa-MiniLM-L6-cos-v1`) with the GPT4ALL `gpt4all-falcon-q4_0` model and LangChain, a small sample of text files are selected from NewsQA dataset. HuggingFace Instruct Embeddings parameters can be changed based on different computing resource.\n",
    "\n",
    "For more details about the code issue and development log, please refer to [8.1 NewsQA Loop with HuggingFace.ipynb](https://github.com/lixiao-yang/DeepDelight/blob/main/Thread2/8.1%20NewsQA%20Loop%20with%20HuggingFace.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama-cpp-python langchain sentence_transformers InstructorEmbedding pyllama transformers pyqt5 pyqtwebengine pyllamacpp --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from langchain.llms import GPT4All\n",
    "from pathlib import Path\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceInstructEmbeddings\n",
    "from transformers import set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='C:/NewsQA/combined-newsqa-data-story1.json'\n",
    "data = json.loads(Path(file_path).read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate Exact Match (EM) score\n",
    "def calculate_em(predicted, actual):\n",
    "    return int(predicted == actual)\n",
    "\n",
    "# Function to calculate the token-wise F1 score for text answers\n",
    "def calculate_token_f1(predicted, actual):\n",
    "    predicted_tokens = predicted.split()\n",
    "    actual_tokens = actual.split()\n",
    "    common_tokens = Counter(predicted_tokens) & Counter(actual_tokens)\n",
    "    num_same = sum(common_tokens.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(predicted_tokens)\n",
    "    recall = 1.0 * num_same / len(actual_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "# Helper function to extract answer ranges from the consensus field\n",
    "def extract_ranges(consensus):\n",
    "    if 's' in consensus and 'e' in consensus:\n",
    "        return [(consensus['s'], consensus['e'])]\n",
    "    return []\n",
    "\n",
    "# def calculate_modified_f1(predicted, actual):\n",
    "#     \"\"\"This approach gives a score that reflects the best match for each word in the predicted answer \n",
    "#     with any word in the actual answer, thereby considering partial and inexact matches more effectively.\n",
    "#     \"\"\"\n",
    "#     def inner_word_f1(predicted_word, actual_word):\n",
    "#         common_chars = Counter(predicted_word) & Counter(actual_word)\n",
    "#         num_common = sum(common_chars.values())\n",
    "#         if num_common == 0:\n",
    "#             return 0\n",
    "#         precision = num_common / len(predicted_word)\n",
    "#         recall = num_common / len(actual_word)\n",
    "#         if precision + recall == 0:\n",
    "#             return 0\n",
    "#         return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "#     predicted_tokens = predicted.split()\n",
    "#     actual_tokens = actual.split()\n",
    "\n",
    "#     total_f1 = 0.0\n",
    "#     for predicted_word in predicted_tokens:\n",
    "#         word_f1_scores = [inner_word_f1(predicted_word, actual_word) for actual_word in actual_tokens]\n",
    "#         total_f1 += max(word_f1_scores, default=0)\n",
    "\n",
    "#     # Normalize the total F1 score by the number of words in the predicted answer\n",
    "#     normalized_total_f1 = total_f1 / len(predicted_tokens) if predicted_tokens else 0\n",
    "#     return normalized_total_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newsqa_loop_test(data, llm, output_file_path, chunk_sizes, overlap_percentages, max_stories, instruct_embedding_model_name, instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, QA_CHAIN_PROMPT):\n",
    "    \"\"\"\n",
    "    Processes a collection of stories, answering questions using a QA chain based on GPT4ALL language model and\n",
    "    HuggingFaceInstructEmbeddings.\n",
    "\n",
    "    For each story, the text is split into chunks, and each chunk is processed to answer questions. The results,\n",
    "    including calculated F1 and EM scores, are written to an output file.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The dataset containing the stories and questions.\n",
    "        llm (LLM): The language model instance used for generating answers.\n",
    "        output_file_path (str): Path to the output file where results will be stored.\n",
    "        chunk_sizes (list): List of chunk sizes for text splitting.\n",
    "        overlap_percentages (list): List of overlap percentages for text splitting.\n",
    "        max_stories (int): Maximum number of stories to process.\n",
    "        instruct_embedding_model_name (str): Name of the HuggingFace model for embeddings.\n",
    "        instruct_embedding_model_kwargs (dict): Keyword arguments for the embedding model.\n",
    "        instruct_embedding_encode_kwargs (dict): Encoding arguments for the embedding model.\n",
    "        QA_CHAIN_PROMPT (str): The prompt template for the QA chain.\n",
    "\n",
    "    Returns:\n",
    "        The function writes the results to a file and print the results of question, answer, actual answer, \n",
    "        and F1 scores.\n",
    "    \"\"\"\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        word_embed = HuggingFaceInstructEmbeddings(\n",
    "            model_name=instruct_embedding_model_name,\n",
    "            model_kwargs=instruct_embedding_model_kwargs,\n",
    "            encode_kwargs=instruct_embedding_encode_kwargs\n",
    "        )\n",
    "        start_time = time.time()\n",
    "        for chunk_size in chunk_sizes:\n",
    "            print(f\"\\n{time.time()-start_time} Processing chunk size {chunk_size}:\")\n",
    "            last_time = time.time()\n",
    "            for overlap_percentage in overlap_percentages:\n",
    "                actual_overlap = int(chunk_size * overlap_percentage)\n",
    "                print(f\"\\n{time.time()-start_time}\\t{time.time()-last_time}\\tOverlap [{overlap_percentage}] {actual_overlap}\")\n",
    "                text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=actual_overlap)\n",
    "        \n",
    "                for i, story in enumerate(data['data']):\n",
    "                    if i >= max_stories:\n",
    "                        break\n",
    "                    now_time = time.time()\n",
    "                    print(f\"\\n{now_time-start_time}\\t{now_time-last_time}\\t\\tstory {i+1}: \", end='')\n",
    "                    last_time = now_time\n",
    "        \n",
    "                    all_splits = text_splitter.split_text(story['text'])\n",
    "                    vectorstore = Chroma.from_texts(texts=all_splits, embedding=word_embed)\n",
    "                    qa_chain = RetrievalQA.from_chain_type(\n",
    "                        llm, \n",
    "                        retriever=vectorstore.as_retriever(), \n",
    "                        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "                        return_source_documents=True)\n",
    "        \n",
    "                    for j, question_data in enumerate(story['questions']):\n",
    "                        j+=1\n",
    "                        # print(f\"{time.time()-start_time}\\t\\t\\tquestion {j}\")\n",
    "                        print('.', end='')\n",
    "        \n",
    "                        # TODO: embed query and perform similarity_search_by_vector() instead\n",
    "                        question = question_data['q']\n",
    "                        question_vector = word_embed.embed_query(question)\n",
    "                        # docs = vectorstore.similarity_search(question)\n",
    "                        docs = vectorstore.similarity_search_by_vector(question_vector)\n",
    "                        answer_ranges = extract_ranges(question_data['consensus'])\n",
    "                        \n",
    "                        # Get the prediction from the model\n",
    "                        result = qa_chain({\"query\": question})\n",
    "                        \n",
    "                        # Check if the predicted answer is in the expected format (string)\n",
    "                        predicted_answer = result['result']\n",
    "                        if isinstance(predicted_answer, dict):\n",
    "                            # If it's a dictionary, you need to adapt this part of the code to extract the answer string\n",
    "                            predicted_answer = predicted_answer.get('answer', '')  # Assuming 'answer' is the key for the answer string\n",
    "                        elif not isinstance(predicted_answer, str):\n",
    "                            # If the answer is not a string and not a dictionary, log an error or handle it appropriately\n",
    "                            print(f\"Unexpected format for predicted answer: {predicted_answer}\")\n",
    "                            continue  # Skip to the next question\n",
    "                        actual_answer = story['text'][answer_ranges[0][0]:answer_ranges[0][1]] if answer_ranges else \"\"\n",
    "                        \n",
    "                        # If there is an actual answer, get it from the story text using the character ranges\n",
    "                        if answer_ranges:\n",
    "                            actual_answer = story['text'][answer_ranges[0][0]:answer_ranges[0][1]]\n",
    "                        else:\n",
    "                            actual_answer = \"\"\n",
    "                        \n",
    "                        # Calculate the scores\n",
    "                        em_score = calculate_em(predicted_answer, actual_answer)\n",
    "                        f1_score_value = calculate_token_f1(predicted_answer, actual_answer)\n",
    "                        # modified_f1 = calculate_modified_f1(predicted_answer, actual_answer)\n",
    "                        file.write(f\"{chunk_size}\\t{overlap_percentage}\\t{i}\\t{j}\\t{f1_score_value:.4f}\\t{em_score:.2f}\\n\")\n",
    "        \n",
    "                        # Store the scores\n",
    "                        em_results[(chunk_size, overlap_percentage)].append(em_score)\n",
    "                        f1_results[(chunk_size, overlap_percentage)].append(f1_score_value)\n",
    "                        \n",
    "                        # Print results\n",
    "                        print(f\"\\nQuestion: {question}\")\n",
    "                        print(f\"Predicted Answer: {predicted_answer}\")\n",
    "                        print(f\"Actual Answer: {actual_answer}\")\n",
    "                        print(f\"F1 Score: {f1_score_value:4f}\")\n",
    "                        # print(f\"Modified F1: {modified_f1:.4f}\")\n",
    "                        \n",
    "                    # delete object for memory\n",
    "                    del qa_chain\n",
    "                    del vectorstore\n",
    "                    del all_splits\n",
    "                    \n",
    "                # delete splitter instance\n",
    "                del text_splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_original = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use ten words maximum and keep the answer as concise as possible. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT_ORIGINAL = PromptTemplate.from_template(template_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If InvalidDimensionException is risen, refer to *7.3 Reset Embeddings Dimension Value in Vectorstore* from file 8.1, you can also use \n",
    "```python \n",
    "Chroma().delete_collection()\n",
    "```\n",
    "to refresh the Chroma db collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma().delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\n",
      "1701038768.163508 Started.\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "\n",
      "0.0 Processing chunk size 200:\n",
      "\n",
      "0.0\t0.0\tOverlap [0.1] 20\n",
      "\n",
      "0.0\t0.0\t\tstory 1: .\n",
      "Question: What was the amount of children murdered?\n",
      "Predicted Answer:  The article states that there were 1,900 children murdered in India between 2010 and 2015.\n",
      "Actual Answer: 19 \n",
      "F1 Score: 0.000000\n",
      ".\n",
      "Question: When was Pandher sentenced to death?\n",
      "Predicted Answer:  Pandher was sentenced to death in February.\n",
      "Actual Answer: February.\n",
      "\n",
      "F1 Score: 0.250000\n",
      ".\n",
      "Question: The court aquitted Moninder Singh Pandher of what crime?\n",
      "Predicted Answer:  Moninder Singh Pandher was accused of killing his wife.\n",
      "Actual Answer: rape and murder \n",
      "F1 Score: 0.000000\n",
      ".\n",
      "Question: who was acquitted\n",
      "Predicted Answer:  Moninder Singh Pandh\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "F1 Score: 0.666667\n",
      ".\n",
      "Question: who was sentenced\n",
      "Predicted Answer:  The high court upheld Koli's death sentence, Kochaar said.\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "F1 Score: 0.000000\n",
      ".\n",
      "Question: What was Moninder Singh Pandher acquitted for?\n",
      "Predicted Answer:  Moninder Singh Pandher was acquitted of the crime he was accused of.\n",
      "Actual Answer: the killing of a teen \n",
      "F1 Score: 0.235294\n",
      ".\n",
      "Question: Who was sentenced to death in February?\n",
      "Predicted Answer:  Moninder Singh Pandh\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "F1 Score: 0.666667\n",
      ".\n",
      "Question: how many people died\n",
      "Predicted Answer:  1 person\n",
      "Actual Answer: 19 \n",
      "F1 Score: 0.000000\n",
      ".\n",
      "Question: How many children and young women were murdered?\n",
      "Predicted Answer:  There were 9 children and young women who were murdered in the most gruesome serial killings in India in recent years.\n",
      "Actual Answer: 19 \n",
      "F1 Score: 0.000000\n"
     ]
    }
   ],
   "source": [
    "############## Important Parameters ##############\n",
    "max_stories = 100\n",
    "chunk_sizes = [200]\n",
    "overlap_percentages = [0.1]  # Expressed as percentages (0.1 = 10%)\n",
    "random_seed = 123\n",
    "output_file_path = \"results/scores_test.txt\"\n",
    "model_location = \"C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "# model_location = \"/Users/wk77/Library/CloudStorage/OneDrive-DrexelUniversity/Documents/data/gpt4all/models/gpt4all-falcon-q4_0.gguf\"\n",
    "\n",
    "##################################################\n",
    "\n",
    "# logging.basicConfig(level=logging.WARNING)  # This will show only warnings and errors\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "set_seed(random_seed)\n",
    "\n",
    "# Results storage\n",
    "f1_results = defaultdict(list)\n",
    "em_results = defaultdict(list)\n",
    "text_results = []\n",
    "\n",
    "# Initialize the language model and the QA chain\n",
    "llm = GPT4All(model=model_location, max_tokens=2048, seed=random_seed)\n",
    "\n",
    "# HuggingFace Instruct Embeddings parameters\n",
    "instruct_embedding_model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "instruct_embedding_model_kwargs = {'device': 'cpu'}\n",
    "instruct_embedding_encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# The following code would iterate over the stories and questions to calculate the scores\n",
    "start_time = time.time()\n",
    "print(f\"{start_time} Started.\")\n",
    "\n",
    "################ Main Function ################\n",
    "newsqa_loop_test(data, llm, output_file_path, chunk_sizes, overlap_percentages, max_stories, instruct_embedding_model_name,\n",
    "                 instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, QA_CHAIN_PROMPT_ORIGINAL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
