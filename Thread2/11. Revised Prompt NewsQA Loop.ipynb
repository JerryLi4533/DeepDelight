{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revised Prompt NewsQA Loop\n",
    "1/14/2024 \\\n",
    "Lixiao Yang\n",
    "\n",
    "This notebook provides a revised loop for different chunk and overlap sizes with the GPT4ALL `gpt4all-falcon-q4_0` model and LangChain, a small sample of text files are selected from NewsQA dataset. This notebook is derived from previous results and code from notebook `8.2 NewsQA Loop - V2`, fully compiled code is compiled with a separate file `11.2 NewsQA_Experiment.py`.\n",
    "\n",
    "Updates:\n",
    "1. Revised a more efficient prompt for a better question answering results\n",
    "2. Compile code into a separate file for full dataset runs\n",
    "3. Add precison and recall as evaluation metrics\n",
    "4. Adjusted the JSON reading function, considering handle with `isQuestionBad` and `isAnswerAbsent`  to improve the recall\n",
    "5. Modify the code to enable experiment results are saved as `.csv` format\n",
    "6. Initial try of building a two-step structure: use the retrieved chunk/sentence as the new embedding input before running the question answering chain\n",
    "\n",
    "\n",
    "For more details about the code issue and development log, please refer to `8.1 NewsQA Loop with HuggingFace.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  gpt4all > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama-cpp-python langchain sentence_transformers InstructorEmbedding pyllama transformers pyqt5 pyqtwebengine pyllamacpp --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from langchain.llms import GPT4All\n",
    "from pathlib import Path\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceInstructEmbeddings\n",
    "from transformers import set_seed\n",
    "from langchain.embeddings import GPT4AllEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='C:/NewsQA/combined-newsqa-data-story2.json'\n",
    "data = json.loads(Path(file_path).read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate Exact Match (EM) score\n",
    "def calculate_em(predicted, actual):\n",
    "    return int(predicted == actual)\n",
    "\n",
    "# Modified function to calculate the token-wise F1 score and return precision and recall\n",
    "def calculate_token_f1(predicted, actual):\n",
    "    predicted_tokens = predicted.split()\n",
    "    actual_tokens = actual.split()\n",
    "    common_tokens = Counter(predicted_tokens) & Counter(actual_tokens)\n",
    "    num_same = sum(common_tokens.values())\n",
    "\n",
    "    if num_same == 0:\n",
    "        return 0, 0, 0  # Return zero precision, recall, and F1 score\n",
    "\n",
    "    precision = 1.0 * num_same / len(predicted_tokens)\n",
    "    recall = 1.0 * num_same / len(actual_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    return f1, precision, recall\n",
    "\n",
    "# Helper function to extract answer ranges from the consensus field\n",
    "def extract_ranges(consensus):\n",
    "    if 's' in consensus and 'e' in consensus:\n",
    "        return [(consensus['s'], consensus['e'])]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newsqa_loop_oldest(data, llm, output_file_path, chunk_sizes, overlap_percentages, max_stories, instruct_embedding_model_name, instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, QA_CHAIN_PROMPT):\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        word_embed = HuggingFaceInstructEmbeddings(\n",
    "            model_name=instruct_embedding_model_name,\n",
    "            model_kwargs=instruct_embedding_model_kwargs,\n",
    "            encode_kwargs=instruct_embedding_encode_kwargs\n",
    "        )\n",
    "        start_time = time.time()\n",
    "        for chunk_size in chunk_sizes:\n",
    "            print(f\"\\n{time.time()-start_time} Processing chunk size {chunk_size}:\")\n",
    "            last_time = time.time()\n",
    "            for overlap_percentage in overlap_percentages:\n",
    "                actual_overlap = int(chunk_size * overlap_percentage)\n",
    "                print(f\"\\n{time.time()-start_time}\\t{time.time()-last_time}\\tOverlap [{overlap_percentage}] {actual_overlap}\")\n",
    "                text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=actual_overlap)\n",
    "        \n",
    "                for i, story in enumerate(data['data']):\n",
    "                    if i >= max_stories:\n",
    "                        break\n",
    "                    now_time = time.time()\n",
    "                    print(f\"\\n{now_time-start_time}\\t{now_time-last_time}\\t\\tstory {i+1}: \", end='')\n",
    "                    last_time = now_time\n",
    "        \n",
    "                    all_splits = text_splitter.split_text(story['text'])\n",
    "                    vectorstore = Chroma.from_texts(texts=all_splits, embedding=word_embed)\n",
    "                    qa_chain = RetrievalQA.from_chain_type(\n",
    "                        llm, \n",
    "                        retriever=vectorstore.as_retriever(), \n",
    "                        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "                        return_source_documents=True)\n",
    "        \n",
    "                    for j, question_data in enumerate(story['questions']):\n",
    "                        if question_data['isAnswerAbsent']:\n",
    "                        # Skip this question because an answer is absent\n",
    "                            continue\n",
    "\n",
    "                        j += 1\n",
    "                        # Extract and print the isQuestionBad number\n",
    "                        is_question_bad = question_data.get('isQuestionBad', 0)\n",
    "                        print(f\"\\nisQuestionBad: {is_question_bad}\")\n",
    "\n",
    "                        question = question_data['q']\n",
    "                        consensus = question_data['consensus']\n",
    "                        \n",
    "                        # Check if there is a consensus answer and extract it\n",
    "                        if 's' in consensus and 'e' in consensus:\n",
    "                            actual_answer = story['text'][consensus['s']:consensus['e']]\n",
    "                        else:\n",
    "                            actual_answer = \"No consensus answer\"\n",
    "                        \n",
    "                        # Embed the question and perform similarity search\n",
    "                        question_vector = word_embed.embed_query(question)\n",
    "                        docs = vectorstore.similarity_search_by_vector(question_vector)\n",
    "\n",
    "                        # Get the prediction from the model\n",
    "                        result = qa_chain({\"query\": question})\n",
    "                        \n",
    "                        # Extract and process the predicted answer\n",
    "                        predicted_answer = result['result'] if isinstance(result['result'], str) else \"\"\n",
    "\n",
    "                        # Calculate the F1 score, precision, and recall\n",
    "                        f1_score_value, precision, recall = calculate_token_f1(predicted_answer, actual_answer)\n",
    "                        em_score = calculate_em(predicted_answer, actual_answer)\n",
    "\n",
    "                        # Write the scores to the file\n",
    "                        file.write(f\"{chunk_size}\\t{overlap_percentage}\\t{i}\\t{j}\\t{f1_score_value:.4f}\\t{precision:.4f}\\t{recall:.4f}\\t{em_score:.2f}\\n\")\n",
    "\n",
    "                        # Store the scores\n",
    "                        em_results[(chunk_size, overlap_percentage)].append(em_score)\n",
    "                        f1_results[(chunk_size, overlap_percentage)].append(f1_score_value)\n",
    "                        # Consider storing precision and recall as well if needed\n",
    "\n",
    "                        # Print results\n",
    "                        print(f\"\\nQuestion: {question}\")\n",
    "                        print(f\"Retrieved Sentense: {retrieved_sentences}\")\n",
    "                        print(f\"Predicted Answer: {predicted_answer}\")\n",
    "                        print(f\"Actual Answer: {actual_answer}\")\n",
    "                        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score_value:.4f}\")\n",
    "\n",
    "                    # Cleanup\n",
    "                    del qa_chain\n",
    "                    del vectorstore\n",
    "                    del all_splits\n",
    "                    \n",
    "                del text_splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newsqa_loop(data, llm, output_file_path, chunk_sizes, overlap_percentages, max_stories, instruct_embedding_model_name, instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, QA_CHAIN_PROMPT):\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        word_embed = HuggingFaceInstructEmbeddings(\n",
    "            model_name=instruct_embedding_model_name,\n",
    "            model_kwargs=instruct_embedding_model_kwargs,\n",
    "            encode_kwargs=instruct_embedding_encode_kwargs\n",
    "        )\n",
    "        start_time = time.time()\n",
    "        for chunk_size in chunk_sizes:\n",
    "            print(f\"\\n{time.time()-start_time} Processing chunk size {chunk_size}:\")\n",
    "            last_time = time.time()\n",
    "            for overlap_percentage in overlap_percentages:\n",
    "                actual_overlap = int(chunk_size * overlap_percentage)\n",
    "                print(f\"\\n{time.time()-start_time}\\t{time.time()-last_time}\\tOverlap [{overlap_percentage}] {actual_overlap}\")\n",
    "                text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=actual_overlap)\n",
    "        \n",
    "                for i, story in enumerate(data['data']):\n",
    "                    if i >= max_stories:\n",
    "                        break\n",
    "                    now_time = time.time()\n",
    "                    print(f\"\\n{now_time-start_time}\\t{now_time-last_time}\\t\\tstory {i+1}: \", end='')\n",
    "                    last_time = now_time\n",
    "        \n",
    "                    all_splits = text_splitter.split_text(story['text'])\n",
    "                    vectorstore = Chroma.from_texts(texts=all_splits, embedding=word_embed)\n",
    "                    qa_chain = RetrievalQA.from_chain_type(\n",
    "                                llm, \n",
    "                                retriever=vectorstore.as_retriever(), \n",
    "                                chain_type=\"stuff\",\n",
    "                                verbose=True,\n",
    "                                chain_type_kwargs={\n",
    "                                    \"prompt\": QA_CHAIN_PROMPT,\n",
    "                                    \"verbose\": True},\n",
    "                                return_source_documents=True)\n",
    "                    \n",
    "                    chunk_boundaries = []\n",
    "                    start_index = 0\n",
    "                    for split in all_splits:\n",
    "                        end_index = start_index + len(split)\n",
    "                        chunk_boundaries.append((start_index, end_index))\n",
    "                        start_index = end_index\n",
    "\n",
    "\n",
    "                    for j, question_data in enumerate(story['questions']):\n",
    "                        if question_data['isAnswerAbsent']:\n",
    "                            continue  # Skip this question because an answer is absent\n",
    "                        \n",
    "                        # Extract and print the isQuestionBad number\n",
    "                        is_question_bad = question_data.get('isQuestionBad', 0)\n",
    "\n",
    "                        question = question_data['q']\n",
    "                        consensus = question_data['consensus']\n",
    "                        \n",
    "                        # Check if there is a consensus answer and extract it\n",
    "                        if 's' in consensus and 'e' in consensus:\n",
    "                            actual_answer = story['text'][consensus['s']:consensus['e']]\n",
    "                            answer_chunk_index = next((index for index, (start, end) in enumerate(chunk_boundaries) if consensus['s'] >= start and consensus['e'] <= end), None)\n",
    "                            if answer_chunk_index is not None:\n",
    "                                context_for_qa = all_splits[answer_chunk_index]\n",
    "                        else:\n",
    "                            continue  # No consensus answer, skip to the next question\n",
    "\n",
    "                        # Get the prediction from the model\n",
    "                        result = qa_chain({\"context\": context_for_qa, \"query\": question})\n",
    "                        \n",
    "                        # Extract and process the predicted answer\n",
    "                        predicted_answer = result['result'] if isinstance(result['result'], str) else \"\"\n",
    "\n",
    "                        # Calculate the F1 score, precision, and recall\n",
    "                        f1_score_value, precision, recall = calculate_token_f1(predicted_answer, actual_answer)\n",
    "                        em_score = calculate_em(predicted_answer, actual_answer)\n",
    "\n",
    "                        # Write the scores to the file\n",
    "                        file.write(f\"{chunk_size}\\t{overlap_percentage}\\t{i}\\t{j}\\t{f1_score_value:.4f}\\t{precision:.4f}\\t{recall:.4f}\\t{em_score:.2f}\\n\")\n",
    "\n",
    "                        # Print results\n",
    "                        print(f\"\\nQuestion: {question}\")\n",
    "                        print(f\"isQuestionBad: {is_question_bad}\")\n",
    "                        print(f\"Context Used: {context_for_qa}\")\n",
    "                        print(f\"Predicted Answer: {predicted_answer}\")\n",
    "                        print(f\"Actual Answer: {actual_answer}\")\n",
    "                        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score_value:.4f}\")\n",
    "                    \n",
    "\n",
    "                    # Cleanup\n",
    "                    del qa_chain\n",
    "                    del vectorstore\n",
    "                    del all_splits\n",
    "\n",
    "                # End of the story loop\n",
    "                del text_splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newsqa_loop_two_step(data, llm, output_file_path, chunk_sizes, overlap_percentages, max_stories, instruct_embedding_model_name, instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, QA_CHAIN_PROMPT):\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        word_embed = HuggingFaceInstructEmbeddings(\n",
    "            model_name=instruct_embedding_model_name,\n",
    "            model_kwargs=instruct_embedding_model_kwargs,\n",
    "            encode_kwargs=instruct_embedding_encode_kwargs\n",
    "        )\n",
    "        start_time = time.time()\n",
    "        for chunk_size in chunk_sizes:\n",
    "            print(f\"\\n{time.time()-start_time} Processing chunk size {chunk_size}:\")\n",
    "            last_time = time.time()\n",
    "            for overlap_percentage in overlap_percentages:\n",
    "                actual_overlap = int(chunk_size * overlap_percentage)\n",
    "                print(f\"\\n{time.time()-start_time}\\t{time.time()-last_time}\\tOverlap [{overlap_percentage}] {actual_overlap}\")\n",
    "                text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=actual_overlap)\n",
    "        \n",
    "                for i, story in enumerate(data['data']):\n",
    "                    if i >= max_stories:\n",
    "                        break\n",
    "                    now_time = time.time()\n",
    "                    print(f\"\\n{now_time-start_time}\\t{now_time-last_time}\\t\\tstory {i+1}: \", end='')\n",
    "                    last_time = now_time\n",
    "        \n",
    "                    all_splits = text_splitter.split_text(story['text'])\n",
    "                    vectorstore = Chroma.from_texts(texts=all_splits, embedding=word_embed)\n",
    "                    qa_chain = RetrievalQA.from_chain_type(\n",
    "                        llm, \n",
    "                        retriever=vectorstore.as_retriever(), \n",
    "                        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "                        return_source_documents=True)\n",
    "        \n",
    "                    for j, question_data in enumerate(story['questions']):\n",
    "                        if question_data['isAnswerAbsent']:\n",
    "                        # Skip this question because an answer is absent\n",
    "                            continue\n",
    "\n",
    "                        j += 1\n",
    "                        # Extract and print the isQuestionBad number\n",
    "                        is_question_bad = question_data.get('isQuestionBad', 0)\n",
    "                        print(f\"\\nisQuestionBad: {is_question_bad}\")\n",
    "\n",
    "                        question = question_data['q']\n",
    "                        consensus = question_data['consensus']\n",
    "\n",
    "                        # Proceed with processing if there is a consensus answer\n",
    "                        question = question_data['q']\n",
    "                        if 's' in consensus and 'e' in consensus:\n",
    "                            actual_answer = story['text'][consensus['s']:consensus['e']]\n",
    "                        else:\n",
    "                            actual_answer = \"No consensus answer\"  # Handle cases where there's no start and end index\n",
    "                                            \n",
    "                        # Embed the question and perform similarity search\n",
    "                        question_vector = word_embed.embed_query(question)\n",
    "                        docs = vectorstore.similarity_search_by_vector(question_vector)\n",
    "\n",
    "                        # Combine the top 3 retrieved sentences as new context\n",
    "                        new_context = \" \".join([doc.page_content for doc in docs[:3]])\n",
    "\n",
    "                        # Use the new context and question in the QA chain\n",
    "                        result = qa_chain({\"context\": new_context, \"query\": question})\n",
    "                                            \n",
    "                        # Extract and process the predicted answer\n",
    "                        predicted_answer = result['result'] if isinstance(result['result'], str) else \"\"\n",
    "\n",
    "                        # Calculate the F1 score, precision, and recall\n",
    "                        f1_score_value, precision, recall = calculate_token_f1(predicted_answer, actual_answer)\n",
    "                        em_score = calculate_em(predicted_answer, actual_answer)\n",
    "\n",
    "                        # Write the scores to the file\n",
    "                        file.write(f\"{chunk_size}\\t{overlap_percentage}\\t{i}\\t{j}\\t{f1_score_value:.4f}\\t{precision:.4f}\\t{recall:.4f}\\t{em_score:.2f}\\n\")\n",
    "\n",
    "                        # Store the scores\n",
    "                        em_results[(chunk_size, overlap_percentage)].append(em_score)\n",
    "                        f1_results[(chunk_size, overlap_percentage)].append(f1_score_value)\n",
    "                        # Consider storing precision and recall as well if needed\n",
    "\n",
    "                        # Print results\n",
    "                        print(f\"\\nQuestion: {question}\")\n",
    "                        print(f\"Retrieved Sentense: {new_context}\")\n",
    "                        print(f\"Predicted Answer: {predicted_answer}\")\n",
    "                        print(f\"Actual Answer: {actual_answer}\")\n",
    "                        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score_value:.4f}\")\n",
    "\n",
    "                    # Cleanup\n",
    "                    del qa_chain\n",
    "                    del vectorstore\n",
    "                    del all_splits\n",
    "                    \n",
    "                del text_splitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_original = \"\"\"\n",
    "                    Based on the following information only: \n",
    "                    \n",
    "                    {context}\n",
    "                    \n",
    "                    {question} Please provide the answer in as few words as possible and please do NOT repeat any word in the question, i.e. \"{question}\".\n",
    "\n",
    "                    Answer:\n",
    "                    \"\"\"\n",
    "QA_CHAIN_PROMPT_ORIGINAL = PromptTemplate.from_template(template_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "template_original = \"\"\"\n",
    "                    Based on the following information only: \n",
    "                    \n",
    "                    {context}\n",
    "                    \n",
    "                    {question} Please provide the answer in as few words as possible and please do NOT repeat any word in the question, i.e. \"{question}\".\n",
    "\n",
    "                    Answer:\n",
    "                    \"\"\"\n",
    "QA_CHAIN_PROMPT_ORIGINAL = PromptTemplate.from_template(template_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_two_step = \"\"\"\n",
    "                    Based on the following information only: \n",
    "                    \n",
    "                    {context}\n",
    "                    \n",
    "                    {question} Please provide the answer in as few words as possible and please do NOT repeat any word in the question, i.e. \"{question}\".\n",
    "\n",
    "                    Answer:\n",
    "                    \"\"\"\n",
    "QA_CHAIN_PROMPT_TWO_STEPS = PromptTemplate.from_template(template_two_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If InvalidDimensionException is risen, refer to *7.3 Reset Embeddings Dimension Value in Vectorstore* from file 8.1, you can also use \n",
    "```python \n",
    "Chroma().delete_collection()\n",
    "```\n",
    "to refresh the Chroma db collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma().delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1705378775.5835507 Started.\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "\n",
      "0.0 Processing chunk size 200:\n",
      "\n",
      "0.0\t0.0\tOverlap [0.1] 20\n",
      "\n",
      "0.0\t0.0\t\tstory 1: \n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "                    Based on the following information only: \n",
      "                    \n",
      "                    The Drug Enforcement Administration has joined the investigation into Jackson's death, a federal law enforcement official said Wednesday night.\n",
      "\n",
      "The Drug Enforcement Administration has joined the investigation into Jackson's death, a federal law enforcement official said Wednesday night.\n",
      "\n",
      "The Drug Enforcement Administration has joined the investigation into Jackson's death, a federal law enforcement official said Wednesday night.\n",
      "\n",
      "The Drug Enforcement Administration has joined the investigation into Jackson's death, a federal law enforcement official said Wednesday night.\n",
      "                    \n",
      "                    Who has  joined inquiry into Jackson's death? Please provide the answer in as few words as possible and please do NOT repeat any word in the question, i.e. \"Who has  joined inquiry into Jackson's death?\".\n",
      "\n",
      "                    Answer:\n",
      "                    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Question: Who has  joined inquiry into Jackson's death?\n",
      "isQuestionBad: 0\n",
      "Context Used: The Drug Enforcement Administration has joined the investigation into Jackson's death, a federal law enforcement official said Wednesday night.\n",
      "Predicted Answer: \n",
      "The Drug Enforcement Administration has joined the investigation into Jackson's death.\n",
      "Actual Answer: The Drug Enforcement Administration \n",
      "Precision: 0.3636, Recall: 1.0000, F1 Score: 0.5333\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "                    Based on the following information only: \n",
      "                    \n",
      "                    Michael Jackson is shown rehearsing at the Staples Center on June 23, two days before his death.\n",
      "\n",
      "Michael Jackson is shown rehearsing at the Staples Center on June 23, two days before his death.\n",
      "\n",
      "Michael Jackson is shown rehearsing at the Staples Center on June 23, two days before his death.\n",
      "\n",
      "Michael Jackson is shown rehearsing at the Staples Center on June 23, two days before his death.\n",
      "                    \n",
      "                    How many days before Jackson's death was the video taken? Please provide the answer in as few words as possible and please do NOT repeat any word in the question, i.e. \"How many days before Jackson's death was the video taken?\".\n",
      "\n",
      "                    Answer:\n",
      "                    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Question: How many days before Jackson's death was the video taken?\n",
      "isQuestionBad: 0.333333333333\n",
      "Context Used: Jackson rehearsed at Staples Center two nights before he died, and he appeared healthy in a video clip of the rehearsal obtained by CNN. Jackson died June 25 after collapsing at his rented home in\n",
      "Predicted Answer: \n",
      "The video was taken two days before Jackson's death.\n",
      "Actual Answer: two nights \n",
      "Precision: 0.1111, Recall: 0.5000, F1 Score: 0.1818\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "                    Based on the following information only: \n",
      "                    \n",
      "                    Speaking to CNN's Larry King, Jermaine Jackson said the ceremony will be held Tuesday morning, but he did not say where.\n",
      "\n",
      "Speaking to CNN's Larry King, Jermaine Jackson said the ceremony will be held Tuesday morning, but he did not say where.\n",
      "\n",
      "Speaking to CNN's Larry King, Jermaine Jackson said the ceremony will be held Tuesday morning, but he did not say where.\n",
      "\n",
      "Speaking to CNN's Larry King, Jermaine Jackson said the ceremony will be held Tuesday morning, but he did not say where.\n",
      "                    \n",
      "                    Where will Jackon's memorial be? Please provide the answer in as few words as possible and please do NOT repeat any word in the question, i.e. \"Where will Jackon's memorial be?\".\n",
      "\n",
      "                    Answer:\n",
      "                    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Question: Where will Jackon's memorial be?\n",
      "isQuestionBad: 0\n",
      "Context Used: Details on how to register for the 10 a.m. (1 p.m. ET) service at the 20,000-seat Staples Center in Los Angeles, California, Tuesday are to be announced Friday.\n",
      "Predicted Answer: \n",
      "The memorial will take place at the Staples Center in Los Angeles.\n",
      "Actual Answer: Staples Center in Los Angeles, \n",
      "Precision: 0.3333, Recall: 0.8000, F1 Score: 0.4706\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "                    Based on the following information only: \n",
      "                    \n",
      "                    about the death of Michael Jackson.\"\n",
      "\n",
      "about the death of Michael Jackson.\"\n",
      "\n",
      "about the death of Michael Jackson.\"\n",
      "\n",
      "about the death of Michael Jackson.\"\n",
      "                    \n",
      "                    Who joins the inquiry into Jackson's death? Please provide the answer in as few words as possible and please do NOT repeat any word in the question, i.e. \"Who joins the inquiry into Jackson's death?\".\n",
      "\n",
      "                    Answer:\n",
      "                    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Question: Who joins the inquiry into Jackson's death?\n",
      "isQuestionBad: 0\n",
      "Context Used: The Drug Enforcement Administration has joined the investigation into Jackson's death, a federal law enforcement official said Wednesday night.\n",
      "Predicted Answer: \n",
      "The answer is not provided in the given text.\n",
      "Actual Answer: The Drug Enforcement Administration \n",
      "Precision: 0.1111, Recall: 0.2500, F1 Score: 0.1538\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "                    Based on the following information only: \n",
      "                    \n",
      "                    Details on how to register for the 10 a.m. (1 p.m. ET) service at the 20,000-seat Staples Center in Los Angeles, California, Tuesday are to be announced Friday.\n",
      "\n",
      "Details on how to register for the 10 a.m. (1 p.m. ET) service at the 20,000-seat Staples Center in Los Angeles, California, Tuesday are to be announced Friday.\n",
      "\n",
      "Details on how to register for the 10 a.m. (1 p.m. ET) service at the 20,000-seat Staples Center in Los Angeles, California, Tuesday are to be announced Friday.\n",
      "\n",
      "Details on how to register for the 10 a.m. (1 p.m. ET) service at the 20,000-seat Staples Center in Los Angeles, California, Tuesday are to be announced Friday.\n",
      "                    \n",
      "                    How many service tickets will be made available? Please provide the answer in as few words as possible and please do NOT repeat any word in the question, i.e. \"How many service tickets will be made available?\".\n",
      "\n",
      "                    Answer:\n",
      "                    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Question: How many service tickets will be made available?\n",
      "isQuestionBad: 0\n",
      "Context Used: LOS ANGELES, California (CNN) -- Fans wishing to attend singer Michael Jackson's memorial service next week will have to register for the 11,000 free tickets, organizers said Thursday.\n",
      "Predicted Answer: \n",
      "The Staples Center in Los Angeles, California has a seating capacity of 20,000.\n",
      "Actual Answer: 11,000 \n",
      "Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "                    Based on the following information only: \n",
      "                    \n",
      "                    \"Plans are under way regarding a public memorial for Michael Jackson, and we will announce those plans shortly,\" said Ken Sunshine, whose public relations firm had been hired by the Jackson family.\n",
      "\n",
      "\"Plans are under way regarding a public memorial for Michael Jackson, and we will announce those plans shortly,\" said Ken Sunshine, whose public relations firm had been hired by the Jackson family.\n",
      "\n",
      "\"Plans are under way regarding a public memorial for Michael Jackson, and we will announce those plans shortly,\" said Ken Sunshine, whose public relations firm had been hired by the Jackson family.\n",
      "\n",
      "\"Plans are under way regarding a public memorial for Michael Jackson, and we will announce those plans shortly,\" said Ken Sunshine, whose public relations firm had been hired by the Jackson family.\n",
      "                    \n",
      "                    Whose memorial will be Tuesday? Please provide the answer in as few words as possible and please do NOT repeat any word in the question, i.e. \"Whose memorial will be Tuesday?\".\n",
      "\n",
      "                    Answer:\n",
      "                    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Question: Whose memorial will be Tuesday?\n",
      "isQuestionBad: 0\n",
      "Context Used: LOS ANGELES, California (CNN) -- Fans wishing to attend singer Michael Jackson's memorial service next week will have to register for the 11,000 free tickets, organizers said Thursday.\n",
      "Predicted Answer: \n",
      "The memorial for Michael Jackson will be Tuesday.\n",
      "Actual Answer: Michael Jackson's \n",
      "Precision: 0.1250, Recall: 0.5000, F1 Score: 0.2000\n"
     ]
    }
   ],
   "source": [
    "############## Important Parameters ##############\n",
    "max_stories = 100\n",
    "chunk_sizes = [200]\n",
    "overlap_percentages = [0.1]  # Expressed as percentages (0.1 = 10%)\n",
    "random_seed = 123\n",
    "output_file_path = \"results/scores_test.txt\"\n",
    "model_location = \"C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "# model_location = \"/Users/wk77/Library/CloudStorage/OneDrive-DrexelUniversity/Documents/data/gpt4all/models/gpt4all-falcon-q4_0.gguf\"\n",
    "\n",
    "##################################################\n",
    "\n",
    "# logging.basicConfig(level=logging.WARNING)  # This will show only warnings and errors\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "set_seed(random_seed)\n",
    "\n",
    "# Results storage\n",
    "f1_results = defaultdict(list)\n",
    "em_results = defaultdict(list)\n",
    "text_results = []\n",
    "\n",
    "# Initialize the language model and the QA chain\n",
    "llm = GPT4All(model=model_location, max_tokens=2048, seed=random_seed)\n",
    "\n",
    "# HuggingFace Instruct Embeddings parameters\n",
    "instruct_embedding_model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "instruct_embedding_model_kwargs = {'device': 'cpu'}\n",
    "instruct_embedding_encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# The following code would iterate over the stories and questions to calculate the scores\n",
    "start_time = time.time()\n",
    "print(f\"{start_time} Started.\")\n",
    "\n",
    "################ Main Function ################\n",
    "newsqa_loop(data, llm, output_file_path, chunk_sizes, overlap_percentages, max_stories, instruct_embedding_model_name,\n",
    "                 instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, QA_CHAIN_PROMPT_ORIGINAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of running `newsqa_processing_script_lyang.py`  should be similar to the following small sample results:\n",
    "```powershell\n",
    "Found model file at  C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\n",
    "falcon_model_load: loading model from 'C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin' - please wait ...\n",
    "falcon_model_load: n_vocab   = 65024\n",
    "falcon_model_load: n_embd    = 4544\n",
    "falcon_model_load: n_head    = 71\n",
    "falcon_model_load: n_head_kv = 1\n",
    "falcon_model_load: n_layer   = 32\n",
    "falcon_model_load: ftype     = 2\n",
    "falcon_model_load: qntvr     = 0\n",
    "falcon_model_load: ggml ctx size = 3872.64 MB\n",
    "falcon_model_load: memory_size =    32.00 MB, n_mem = 65536\n",
    "falcon_model_load: ........................ done\n",
    "falcon_model_load: model size =  3872.59 MB / num tensors = 196\n",
    "1705643318.023422 Started.\n",
    "load INSTRUCTOR_Transformer\n",
    "max_seq_length  512\n",
    "\n",
    "0.0 Processing chunk size 200:\n",
    "\n",
    "0.0     0.0     Overlap [0] 0\n",
    "\n",
    "0.0     0.0             story 1:\n",
    "233.9731228351593       233.9731228351593       Overlap [0.1] 20\n",
    "\n",
    "233.9731228351593       233.9731228351593               story 1:\n",
    "510.183185338974 Processing chunk size 400:\n",
    "\n",
    "510.183185338974        0.0     Overlap [0] 0\n",
    "\n",
    "510.183185338974        0.0             story 1:\n",
    "761.7193622589111       251.53617691993713      Overlap [0.1] 40\n",
    "\n",
    "761.7193622589111       251.53617691993713              story 1:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
