{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "373cddad-22bf-47fe-a5f1-de39e5d7d956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LanceDB config.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "---------Parameters to be changed between different devices----------\n",
    "1. Models and directory\n",
    "2. Input and output directory\n",
    "3. Device selection (CPU/GPU) - LINE 46 AND 300\n",
    "4. Install tantivy package for full index search\n",
    "\n",
    "Reference:\n",
    "1. similarity_search_by_vector(): https://python.langchain.com/docs/modules/data_connection/vectorstores/\n",
    "2. LanceDB Code documentation Q&A bot example with LangChain: https://lancedb.github.io/lancedb/notebooks/code_qa_bot/\n",
    "3. LanceDB embedding functions: https://lancedb.github.io/lancedb/embeddings/embedding_functions/\n",
    "4. LanceDB available models: https://lancedb.github.io/lancedb/embeddings/default_embedding_functions/#sentence-transformers\n",
    "5. https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/Code-Documentation-QA-Bot/main.ipynb\n",
    "6. Query via text: https://lancedb.github.io/lancedb/notebooks/DisappearingEmbeddingFunction/#querying-via-text\n",
    "7. Pre and post-filtering: https://lancedb.github.io/lancedb/sql/#pre-and-post-filtering\n",
    "8. Create FTS index on single column: https://lancedb.github.io/lancedb/fts/#create-fts-index-on-single-column\n",
    "9. LanceFtsQueryBuilder: https://lancedb.github.io/lancedb/python/python/#lancedb.query.LanceFtsQueryBuilder\n",
    "remove chunk size and overlap size for\n",
    "\n",
    "add similarity func, embedding_func into logging \n",
    "code structure adjustment, incorporate prompt parameter\n",
    "\"\"\"\n",
    "# Make S\n",
    "# pip install tantivy==0.20.1\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import json\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from heapq import nlargest\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import LanceDB\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceInstructEmbeddings\n",
    "from transformers import set_seed\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import LLMChain\n",
    "\n",
    "import lancedb\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from lancedb.embeddings import get_registry\n",
    "from lancedb.embeddings import EmbeddingFunctionRegistry\n",
    "from lancedb.pydantic import LanceModel, vector\n",
    "# from langchain_community.vectorstores import LanceDB\n",
    "\n",
    "random_seed = 123\n",
    "#The link of the files is https://github.com/salesforce/QAConv/tree/master/dataset, which in QAConv-V1.0.zip\n",
    "'''\n",
    "file_path='E:/QA/article_segment.json' #The file here should be article_segment.txt, but its actual structure is json file\n",
    "data = json.loads(Path(file_path).read_text())\n",
    "QA = pd.read_json(\"E:/QAConv-V1.0/QAConv-V1.0/sample.json\") #For the file here, either trn.json, tst.json or val.json can be used as Question_Answer data\n",
    "QA['text']=''\n",
    "for i in range(len(QA)):\n",
    "    text=''\n",
    "    for j in data[QA.iloc[i]['article_segment_id']]['prev_ctx']:\n",
    "        text+=j['text']\n",
    "    for k in data[QA.iloc[i]['article_segment_id']][\"seg_dialog\"]:\n",
    "        text+=k['text']\n",
    "    QA.loc[i,'text']=text\n",
    "QA.to_json('E:/QA/QA_data_sample.json', orient='records') #The file path here should be the same as the following line\n",
    "'''\n",
    "file_path='E:/QA/QA_data_sample.json'\n",
    "combined_data = json.loads(Path(file_path).read_text())\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import collections\n",
    "\n",
    "print(\"LanceDB config.\")\n",
    "registry = EmbeddingFunctionRegistry.get_instance()\n",
    "func = registry.get(\"sentence-transformers\").create(device=\"cpu\") \n",
    "\n",
    "template_original = \"\"\"\n",
    "                    Based on the following information only: \n",
    "                    \n",
    "                    {context}\n",
    "                    \n",
    "                    {question} Please provide the answer in as few words as possible and please do NOT repeat any word in the question, i.e. \"{question}\".\n",
    "\n",
    "                    Answer:\n",
    "                    \"\"\"\n",
    "QA_CHAIN_PROMPT_ORIGINAL = PromptTemplate.from_template(template_original)\n",
    "\n",
    "def process_and_store_chunks(data, db_dir, embedding_model, chunk_size, overlap_percentage, text_splitter, embedding_func):\n",
    "\n",
    "    # Ensure the database directory exists\n",
    "    if not os.path.exists(db_dir):\n",
    "        os.makedirs(db_dir, exist_ok=True)\n",
    "    \n",
    "    # Define the unique table name for this story's chunks\n",
    "    adjusted_story_id = data['id']\n",
    "    story_text = data['text']\n",
    "    table_name = f\"{adjusted_story_id}-chunks\"\n",
    "    story_db_dir = db_dir # os.path.join(db_dir, table_name)\n",
    "    print(db_dir)\n",
    "\n",
    "    # Connect to or create the database\n",
    "    db = lancedb.connect(story_db_dir)\n",
    "\n",
    "    try:\n",
    "        table = db.open_table(table_name)\n",
    "        print(\"\\n Found existing table in the location.\")\n",
    "    except Exception as e:\n",
    "        chunk_splits = text_splitter.split_text(story_text)\n",
    "        chunk_embs = embedding_model.embed_documents(chunk_splits)\n",
    "        \n",
    "        # Prepare the data for storage\n",
    "        chunks_embs_data = [\n",
    "            {\n",
    "                \"storyId\": adjusted_story_id,\n",
    "                \"chunk_size\": chunk_size,\n",
    "                \"overlap_size\": overlap_percentage,\n",
    "                \"embedding_function\": embedding_func,\n",
    "                \"vector\": c_emb,\n",
    "                \"chunk_text\": chunk_splits[i]\n",
    "            } \n",
    "            for i, c_emb in enumerate(chunk_embs)\n",
    "        ]\n",
    "        \n",
    "        # Create a new table for this story and configuration, and store the data\n",
    "        # print(f\"Creating table with name: {adjusted_story_id}-chunks at {story_db_dir}\")\n",
    "        db.create_table(\n",
    "            f\"{adjusted_story_id}-chunks\",\n",
    "            data=chunks_embs_data,\n",
    "            mode=\"overwrite\"  # Each story and configuration combination gets its own table\n",
    "        )\n",
    "        print(\"\\n Creating new table for the story.\")\n",
    "    \n",
    "def retrieve_best_matching_chunk(question, story_id, db_dir, top_n, embedding_model, similarity_func):\n",
    "    db = lancedb.connect(db_dir)\n",
    "    # print(f\"Retrieve table: {story_id}-chunks\")\n",
    "    table = db.open_table(f\"{story_id}-chunks\")\n",
    "\n",
    "    # Embed the question for the search\n",
    "    question_embedding = embedding_model.embed_query(question)#[0].tolist()\n",
    "\n",
    "    # Perform the search\n",
    "    search_results = table.search(question_embedding) \\\n",
    "                            .metric(similarity_func) \\\n",
    "                            .select([\"chunk_text\", \"vector\"]) \\\n",
    "                            .limit(top_n) \\\n",
    "                            .to_pandas()\n",
    "    \n",
    "    # Check if the DataFrame is not empty\n",
    "    if not search_results.empty:\n",
    "        best_chunk_text = search_results.iloc[0]['chunk_text']  # Accessing the first row's chunk_text\n",
    "        best_chunk_vector = search_results.iloc[0]['vector']  # Assuming 'vector' is stored and accessible\n",
    "        return best_chunk_text, best_chunk_vector\n",
    "    else:\n",
    "        raise Exception(\"Empty chunk retrieved. No matching chunk found.\")\n",
    "\n",
    "# Function to normalize and stem text\n",
    "def normalize_and_stem(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = nltk.word_tokenize(text.lower())  # Normalize to lowercase and tokenize\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens if token not in string.punctuation and token not in stopwords.words('english')]  # Stemming and removing punctuation\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "# Modified function to calculate the token-wise F1 score and return precision and recall\n",
    "def token_eval(predicted, actual):\n",
    "    predicted_tokens = predicted.split()\n",
    "    actual_tokens = actual.split()\n",
    "    common_tokens = Counter(predicted_tokens) & Counter(actual_tokens)\n",
    "    num_same = sum(common_tokens.values())\n",
    "\n",
    "    if num_same == 0 and len(predicted_tokens) == 0 and len(actual_tokens) == 0:\n",
    "        # Case where both predicted and actual answers are empty\n",
    "        return 1.0, 1.0, 1.0, 1  # Perfect score\n",
    "    elif num_same == 0:\n",
    "        return 0, 0, 0, 0  # Return zero precision, recall, F1 score, and exact match\n",
    "\n",
    "    precision = 1.0 * num_same / len(predicted_tokens)\n",
    "    recall = 1.0 * num_same / len(actual_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    em = int(predicted.strip() == actual.strip())  # Exact match score\n",
    "\n",
    "    return f1, precision, recall, em\n",
    "\n",
    "def QAConv_loop(data, llm, output_csv_path, output_log_path, max_stories, chunk_sizes, overlap_percentages,\n",
    "                instruct_embedding_model_name, instruct_embedding_model_kwargs, \n",
    "                instruct_embedding_encode_kwargs, top_n_chunk, db_dir, embedding_func, similarity_func):\n",
    "    \n",
    "    with open(output_csv_path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Chunk_size', 'Chunk_Overlap', 'Time', 'Story Number', 'Question Number', 'EM', 'Precision', 'Recall', 'F1', 'Error'])\n",
    "\n",
    "        if embedding_func == 'hf_emb':\n",
    "            # Embedding for story sentences\n",
    "            story_embs = HuggingFaceInstructEmbeddings(\n",
    "                model_name=instruct_embedding_model_name,\n",
    "                model_kwargs=instruct_embedding_model_kwargs,\n",
    "                encode_kwargs=instruct_embedding_encode_kwargs,\n",
    "                embed_instruction=\"Use the following pieces of context to answer the question at the end:\"\n",
    "            )\n",
    "\n",
    "            # Embedding for questions\n",
    "            query_embs = HuggingFaceInstructEmbeddings(\n",
    "                model_name=instruct_embedding_model_name,\n",
    "                model_kwargs=instruct_embedding_model_kwargs,\n",
    "                encode_kwargs=instruct_embedding_encode_kwargs,\n",
    "                query_instruction=\"How does this information relate to the question?\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"Unsupported embedding functions!\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        for chunk_size in chunk_sizes:\n",
    "            print(f\"\\n{time.time()-start_time} Processing chunk size {chunk_size}:\")\n",
    "            last_time = time.time()\n",
    "            \n",
    "            for overlap_percentage in overlap_percentages:\n",
    "                actual_overlap = int(chunk_size * overlap_percentage)\n",
    "                print(f\"\\n{time.time()-start_time}\\t{time.time()-last_time}\\tOverlap [{overlap_percentage}] {actual_overlap}\")\n",
    "                text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=actual_overlap)\n",
    "\n",
    "                for cov in data:\n",
    "                    now_time = time.time()\n",
    "                    print(f\"\\n{now_time - start_time}\\t{now_time - last_time}\\t\\tstory {cov['id']}: \", end='')\n",
    "                    last_time = now_time\n",
    "                    \n",
    "                    # Define a unique directory name for the story with the specified chunk size and overlap\n",
    "                    adjusted_story_id = cov['id']\n",
    "                    unique_dir_name = f\"{embedding_func}/chunk_{chunk_size}/overlap_{int(chunk_size * overlap_percentage)}/{adjusted_story_id}\"\n",
    "                    story_db_dir = f\"{db_dir}/{unique_dir_name}\"\n",
    "                    print(\"\\n In Loop:\", story_db_dir)\n",
    "                    process_and_store_chunks(\n",
    "                        data=cov, \n",
    "                        db_dir=story_db_dir,\n",
    "                        chunk_size=chunk_size,\n",
    "                        overlap_percentage=overlap_percentage,\n",
    "                        text_splitter=text_splitter,\n",
    "                        embedding_model=story_embs, \n",
    "                        embedding_func=embedding_func\n",
    "                    )\n",
    "                    \n",
    "\n",
    "                    question = cov['question']\n",
    "\n",
    "                    context, _ = retrieve_best_matching_chunk(question=question, \n",
    "                                                                                    story_id=adjusted_story_id, \n",
    "                                                                                    db_dir=story_db_dir, \n",
    "                                                                                    top_n=top_n_chunk,\n",
    "                                                                                    embedding_model=query_embs, \n",
    "                                                                                    similarity_func=similarity_func)\n",
    "                        # print(best_chunk_text)\n",
    "                    print(context)\n",
    "                    best_chunk_emb = Chroma.from_texts(context, query_embs)\n",
    "                    print(best_chunk_emb)\n",
    "\n",
    "                        # Initialize the QA chain with the vectorstore as the retriever\n",
    "                    '''\n",
    "                    qa_chain = RetrievalQA.from_chain_type(\n",
    "                        llm,\n",
    "                        chain_type=\"stuff\",\n",
    "                        retriever=best_chunk_emb.as_retriever(), \n",
    "                        chain_type_kwargs={\n",
    "                            #\"prompt\": QA_CHAIN_PROMPT_ORIGINAL,\n",
    "                            \"verbose\": True},\n",
    "                        return_source_documents=False\n",
    "                    )\n",
    "                    '''\n",
    "                    template = \"\"\"                    \n",
    "                    Based on the following information only: \n",
    "                        \n",
    "                    {context}\n",
    "                        \n",
    "                    {question} Please provide the answer in as few words as possible and please do NOT repeat any word in the question, i.e. \"{question}\".\n",
    "\n",
    "                    Answer:\n",
    "                    \"\"\"\n",
    "                    prompt = PromptTemplate(template=template, input_variables=[\"adjective\", \"subject\"])\n",
    "                    llm_chain = LLMChain(prompt=prompt, llm=llm,verbose=True)\n",
    "\n",
    "                    result=llm_chain.predict(context=context, question=question)\n",
    "                    print(\"这是大模型回答的结果：\")\n",
    "                    print(result)\n",
    "                    \n",
    "\n",
    "                            \n",
    "                        # Check if there is a consensus answer and extract it\n",
    "                    actual_answer = cov['answers'][0]\n",
    "                        \n",
    "                        # Get the prediction from the model\n",
    "                        # query = question # + \"Please provide the answer in as few words as possible and do NOT repeat any word in the question.\"\n",
    "                    query = f\"\"\"\n",
    "                    Based on the following information only: \n",
    "                        \n",
    "                    {context}\n",
    "                        \n",
    "                    {question} Please provide the answer in as few words as possible and please do NOT repeat any word in the question, i.e. \"{question}\".\n",
    "\n",
    "                    Answer:\n",
    "                    \"\"\"\n",
    "                    #result = qa_chain({\"query\":query})\n",
    "                    #result = qa_chain({\"context\":context,\"query\": question})\n",
    "                    #print('q:', query, 'a:', result)\n",
    "                        \n",
    "                    # Extract and process the predicted answer\n",
    "                    if result is None:\n",
    "                        result = 0, 0, 0, 0\n",
    "                    else:\n",
    "                        print(f\"Result from qa_chain.run(): {result}, type: {type(result)}\")\n",
    "                        predicted_answer = result if isinstance(result, str) else result['result']\n",
    "                        \n",
    "                    # Normalize and stem the predicted and actual answers\n",
    "                    normalized_predicted_answer = normalize_and_stem(predicted_answer)\n",
    "                    normalized_actual_answer = normalize_and_stem(actual_answer)\n",
    "\n",
    "                    # Calculate the F1 score, precision, and recall using normalized and stemmed answers\n",
    "                    # f1_score_value, precision, recall, em_score = token_eval(normalized_predicted_answer, normalized_actual_answer)\n",
    "                    # print(\"Calling token_eval with:\", normalized_predicted_answer, normalized_actual_answer)\n",
    "                    result = token_eval(normalized_predicted_answer, normalized_actual_answer)\n",
    "                    # print(\"token_eval returned:\", result)\n",
    "                    f1_score_value, precision, recall, em_score = result\n",
    "                        \n",
    "                    # Write the scores to the file\n",
    "                    error = 1 if 'error' in normalized_predicted_answer else 0\n",
    "                    if error==0: \n",
    "                        #writer.writerow([chunk_size, overlap_percentage, embedding_func, similarity_func, top_n_chunk, time.time() - start_time, i, j, em_score, precision, recall, f1_score_value, error])\n",
    "                        writer.writerow([chunk_size, overlap_percentage, embedding_func, similarity_func, top_n_chunk, time.time() - start_time, em_score, precision, recall, f1_score_value, error])\n",
    "                        \n",
    "                    with open(output_log_path, 'a') as details_file:\n",
    "                        details_file.write(f\"Chunk Size: {chunk_size}\\n\")\n",
    "                        details_file.write(f\"Overlap: {overlap_percentage}\\n\")\n",
    "                        details_file.write(f\"Embedding Func: {embedding_func}\\n\")\n",
    "                        details_file.write(f\"Similarity Func: {similarity_func}\\n\")\n",
    "                        details_file.write(f\"Top n Chunk Selected: {top_n_chunk}\\n\")\n",
    "                        #details_file.write(f\"Story: {i}\\n\")\n",
    "                        #details_file.write(f\"Question: {j}\\n\")\n",
    "                        details_file.write(f\"Correct Answer: {actual_answer}\\n\")\n",
    "                        details_file.write(f\"Normalized Actual Answer: {normalized_actual_answer}\\n\")\n",
    "                        details_file.write(f\"Predicted Answer: {predicted_answer}\\n\")\n",
    "                        details_file.write(f\"Normalized Predicted Answer: {normalized_predicted_answer}\\n\")\n",
    "                        details_file.write(f\"Time: {time.time() - start_time}\\n\")\n",
    "                        details_file.write(f\"EM Score: {em_score}\\n\")\n",
    "                        details_file.write(f\"Precision: {precision}\\n\")\n",
    "                        details_file.write(f\"Recall: {recall}\\n\")\n",
    "                        details_file.write(f\"F1: {f1_score_value}\\n\")\n",
    "                        details_file.write(\"----------------------------------------\\n\")\n",
    "\n",
    "                # Cleanup\n",
    "                #del qa_chain\n",
    "                #del chunk_embs\n",
    "\n",
    "            # End of the story loop\n",
    "            # del text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e27bf23-5975-4724-8d64-cf641146afce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data.\n",
      "Random seeding.\n",
      "Loading LLM.\n",
      "Preparing Parameters.\n",
      "1719885425.298961 Started.\n",
      "Processing.\n",
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "\n",
      "0.0 Processing chunk size 200:\n",
      "\n",
      "0.0\t0.0\tOverlap [0] 0\n",
      "\n",
      "0.0\t0.0\t\tstory val-12: \n",
      " In Loop: E:/QA/lancedb/hf_emb/chunk_200/overlap_0/val-12\n",
      "E:/QA/lancedb/hf_emb/chunk_200/overlap_0/val-12\n",
      "\n",
      " Found existing table in the location.\n",
      "California, States -- localities have the option of having the appeal go to the local legislature. That appeal is then reviewed -- is then determined. And finally, once you've gone through that\n",
      "<langchain_community.vectorstores.chroma.Chroma object at 0x0000018C341F84D0>\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m                    \n",
      "                    Based on the following information only: \n",
      "                        \n",
      "                    California, States -- localities have the option of having the appeal go to the local legislature. That appeal is then reviewed -- is then determined. And finally, once you've gone through that\n",
      "                        \n",
      "                    What is the name of the petitioner in the case? Please provide the answer in as few words as possible and please do NOT repeat any word in the question, i.e. \"What is the name of the petitioner in the case?\".\n",
      "\n",
      "                    Answer:\n",
      "                    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "这是大模型回答的结果：\n",
      "\n",
      "                    Petitioner\n",
      "Result from qa_chain.run(): \n",
      "                    Petitioner, type: <class 'str'>\n",
      "\n",
      "15.065507173538208\t15.065507173538208\t\tstory val-13: \n",
      " In Loop: E:/QA/lancedb/hf_emb/chunk_200/overlap_0/val-13\n",
      "E:/QA/lancedb/hf_emb/chunk_200/overlap_0/val-13\n",
      "\n",
      " Found existing table in the location.\n",
      "that statute has a gaping hole in it, as construed by the Federal Circuit, because the Federal Circuit has construed it to exclude protection for speech that is part of the employee's normal duties.\n",
      "<langchain_community.vectorstores.chroma.Chroma object at 0x0000018C354DA8D0>\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m                    \n",
      "                    Based on the following information only: \n",
      "                        \n",
      "                    that statute has a gaping hole in it, as construed by the Federal Circuit, because the Federal Circuit has construed it to exclude protection for speech that is part of the employee's normal duties.\n",
      "                        \n",
      "                    Which act has a piece missing from it, according to the Circuit court? Please provide the answer in as few words as possible and please do NOT repeat any word in the question, i.e. \"Which act has a piece missing from it, according to the Circuit court?\".\n",
      "\n",
      "                    Answer:\n",
      "                    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "这是大模型回答的结果：\n",
      "\n",
      "                    The Federal Circuit has construed the statute to exclude protection for speech that is part of an employee's normal duties.\n",
      "Result from qa_chain.run(): \n",
      "                    The Federal Circuit has construed the statute to exclude protection for speech that is part of an employee's normal duties., type: <class 'str'>\n",
      "\n",
      "34.2085075378418\t19.14300036430359\t\tstory val-14: \n",
      " In Loop: E:/QA/lancedb/hf_emb/chunk_200/overlap_0/val-14\n",
      "E:/QA/lancedb/hf_emb/chunk_200/overlap_0/val-14\n",
      "\n",
      " Found existing table in the location.\n",
      "reliance exists. And I have to correct the Solicitor General. The reliance does not merely come from the injured party. The restatement section, section three- -- or the restatement section 324 says\n",
      "<langchain_community.vectorstores.chroma.Chroma object at 0x0000018C3551F790>\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m                    \n",
      "                    Based on the following information only: \n",
      "                        \n",
      "                    reliance exists. And I have to correct the Solicitor General. The reliance does not merely come from the injured party. The restatement section, section three- -- or the restatement section 324 says\n",
      "                        \n",
      "                    Which restatement section was based on injured party? Please provide the answer in as few words as possible and please do NOT repeat any word in the question, i.e. \"Which restatement section was based on injured party?\".\n",
      "\n",
      "                    Answer:\n",
      "                    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "这是大模型回答的结果：\n",
      "\n",
      "                    None\n",
      "Result from qa_chain.run(): \n",
      "                    None, type: <class 'str'>\n",
      "\n",
      "50.63850712776184\t16.429999589920044\t\tstory val-15: \n",
      " In Loop: E:/QA/lancedb/hf_emb/chunk_200/overlap_0/val-15\n",
      "E:/QA/lancedb/hf_emb/chunk_200/overlap_0/val-15\n",
      "\n",
      " Found existing table in the location.\n",
      "beginning.Her story isn't unusual in Sweden, where anyone with a permanent position has a legal right to take unpaid leave for six months to launch a company, providing it doesn't compete with their\n",
      "<langchain_community.vectorstores.chroma.Chroma object at 0x0000018C34363350>\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m                    \n",
      "                    Based on the following information only: \n",
      "                        \n",
      "                    beginning.Her story isn't unusual in Sweden, where anyone with a permanent position has a legal right to take unpaid leave for six months to launch a company, providing it doesn't compete with their\n",
      "                        \n",
      "                    How many months of unpaid leave does anyone with a permanent position have in Sweden ? Please provide the answer in as few words as possible and please do NOT repeat any word in the question, i.e. \"How many months of unpaid leave does anyone with a permanent position have in Sweden ?\".\n",
      "\n",
      "                    Answer:\n",
      "                    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "这是大模型回答的结果：\n",
      "6\n",
      "Result from qa_chain.run(): 6, type: <class 'str'>\n",
      "\n",
      "67.44250798225403\t16.804000854492188\t\tstory val-16: \n",
      " In Loop: E:/QA/lancedb/hf_emb/chunk_200/overlap_0/val-16\n",
      "E:/QA/lancedb/hf_emb/chunk_200/overlap_0/val-16\n",
      "\n",
      " Found existing table in the location.\n",
      "determination that death is an appropriate sentence. And at the selection stage, the question is whether the jury has made a reliable, collective, responsible decision based upon the unique\n",
      "<langchain_community.vectorstores.chroma.Chroma object at 0x0000018C3A0C4910>\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m                    \n",
      "                    Based on the following information only: \n",
      "                        \n",
      "                    determination that death is an appropriate sentence. And at the selection stage, the question is whether the jury has made a reliable, collective, responsible decision based upon the unique\n",
      "                        \n",
      "                    What is the name of the amendment that requires reliability in the determination that death is an appropriate sentence ? Please provide the answer in as few words as possible and please do NOT repeat any word in the question, i.e. \"What is the name of the amendment that requires reliability in the determination that death is an appropriate sentence ?\".\n",
      "\n",
      "                    Answer:\n",
      "                    \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "这是大模型回答的结果：\n",
      "\n",
      "                    Due Process\n",
      "Result from qa_chain.run(): \n",
      "                    Due Process, type: <class 'str'>\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'qa_chain' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 76\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Main Function Execution\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 76\u001b[0m QAConv_loop(data, llm, output_csv_path, output_log_path, max_stories, chunk_sizes, overlap_percentages, \n\u001b[0;32m     77\u001b[0m             instruct_embedding_model_name, instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, \n\u001b[0;32m     78\u001b[0m             top_n_chunk, db_dir, embedding_func, similarity_func)\n",
      "Cell \u001b[1;32mIn[1], line 355\u001b[0m, in \u001b[0;36mQAConv_loop\u001b[1;34m(data, llm, output_csv_path, output_log_path, max_stories, chunk_sizes, overlap_percentages, instruct_embedding_model_name, instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, top_n_chunk, db_dir, embedding_func, similarity_func)\u001b[0m\n\u001b[0;32m    352\u001b[0m         details_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------------------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m# Cleanup\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m qa_chain\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'qa_chain' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "############## Running Parameters ##############\n",
    "max_stories = 50\n",
    "random_seed = 123\n",
    "db_dir = 'E:/QA/lancedb'\n",
    "embedding_func = 'hf_emb'\n",
    "similarity_func = \"cosine\" # \"cosine\", \"l2\", \"dot\"\n",
    "chunk_sizes = [200]\n",
    "# chunk_sizes = [50,25]\n",
    "# overlap_percentages = [0, 0.1, 0.2]  # Expressed as percentages (0.1 = 10%)\n",
    "overlap_percentages = [0]\n",
    "top_n_chunk = 1\n",
    "\n",
    "#model_location = 'E:/ggml-model-gpt4all-falcon-q4_0.bin'\n",
    "model_location = 'E:/mistral-7b-instruct-v0.1.Q4_0.gguf'\n",
    "#model_location = 'E:/ggml-all-MiniLM-L6-v2-f16.bin'\n",
    "# model_location = \"/Users/wk77/Library/CloudStorage/OneDrive-DrexelUniversity/Documents/data/gpt4all/models/gpt4all-falcon-q4_0.gguf\"\n",
    "# model_location = \"/Users/wk77/Documents/data/gpt4all-falcon-newbpe-q4_0.gguf\"\n",
    "# model_location = \"/Users/wk77/Documents/data/mistral-7b-instruct-v0.1.Q4_0.gguf\"\n",
    "input_file_path='E:/QA/QA_data_sample.json'\n",
    "# input_file_path = \"/Users/wk77/Documents/data/newsqa-data-v1/newsqa-data-v1.csv\"\n",
    "# input_file_path = \"/Users/wk77/Documents/data/newsqa-data-v1/combined-newsqa-data-v1.json\"\n",
    "# input_file_path = \"/Users/wk77/Documents/git/DeepDelight/Thread2/data/combined-newsqa-data-story1.json\"\n",
    "output_csv_path = 'E:/Results/combined_chunks3.csv'\n",
    "# output_file_path = \"/Users/wk77/Documents/data/newsqa-data-v1/story1_scores_test.csv\"\n",
    "# output_file_path = \"/Users/wk77/Documents/data/newsqa-data-v1/combined_scores_test.csv\"\n",
    "output_log_path = 'E:/Results/combined_chunks3.log'\n",
    "\n",
    "# # Initialize PairwiseDistance\n",
    "# pdist = nn.PairwiseDistance(p=2.0, eps=1e-06)\n",
    "\n",
    "##################################################\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# logging.basicConfig(level=logging.WARNING)  # This will show only warnings and errors\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "print(\"Loading data.\")\n",
    "data = json.loads(Path(input_file_path).read_text())\n",
    "\n",
    "# print(\"Setting template.\")\n",
    "# template_original = \"\"\"\n",
    "#                     Based on the following information only: \n",
    "                    \n",
    "#                     {context}\n",
    "                    \n",
    "#                     {question} Please provide the answer in as few words as possible and please do NOT repeat any word in the question, i.e. \"{question}\".\n",
    "\n",
    "#                     Answer:\n",
    "#                     \"\"\"\n",
    "# QA_CHAIN_PROMPT_ORIGINAL = PromptTemplate.from_template(template_original)\n",
    "\n",
    "print(\"Random seeding.\")\n",
    "set_seed(random_seed)\n",
    "\n",
    "# Results storage\n",
    "f1_results = defaultdict(list)\n",
    "em_results = defaultdict(list)\n",
    "text_results = []\n",
    "\n",
    "# Initialize the language model and the QA chain\n",
    "print(\"Loading LLM.\")\n",
    "llm = GPT4All(model=model_location, max_tokens=2048, seed=random_seed)\n",
    "\n",
    "print(\"Preparing Parameters.\")\n",
    "# HuggingFace Instruct Embeddings parameters\n",
    "instruct_embedding_model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "instruct_embedding_model_kwargs = {'device': 'cpu'}\n",
    "# instruct_embedding_model_kwargs = {'device': 'mps'}\n",
    "instruct_embedding_encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# The following code would iterate over the stories and questions to calculate the scores\n",
    "start_time = time.time()\n",
    "print(f\"{start_time} Started.\")                \n",
    "\n",
    "# Main Function Execution\n",
    "print(\"Processing.\")\n",
    "QAConv_loop(data, llm, output_csv_path, output_log_path, max_stories, chunk_sizes, overlap_percentages, \n",
    "            instruct_embedding_model_name, instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, \n",
    "            top_n_chunk, db_dir, embedding_func, similarity_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8ccb484-385c-4468-89f8-9b64e1face18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting langchain==0.0.343\n",
      "  Obtaining dependency information for langchain==0.0.343 from https://files.pythonhosted.org/packages/dc/0f/9ff3bee4b0bf67b56ef2c28fcfc05a3597c8de074ac51c4c67e45ec8d0c0/langchain-0.0.343-py3-none-any.whl.metadata\n",
      "  Downloading langchain-0.0.343-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\anaconda\\lib\\site-packages (from langchain==0.0.343) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\anaconda\\lib\\site-packages (from langchain==0.0.343) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\anaconda\\lib\\site-packages (from langchain==0.0.343) (3.9.3)\n",
      "Requirement already satisfied: anyio<4.0 in d:\\anaconda\\lib\\site-packages (from langchain==0.0.343) (3.7.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\anaconda\\lib\\site-packages (from langchain==0.0.343) (0.6.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\administrator\\appdata\\roaming\\python\\python311\\site-packages (from langchain==0.0.343) (1.33)\n",
      "Collecting langchain-core<0.1,>=0.0.7 (from langchain==0.0.343)\n",
      "  Obtaining dependency information for langchain-core<0.1,>=0.0.7 from https://files.pythonhosted.org/packages/96/30/1d6b35a757d698da75b24f8db74132b28de9fd964308e0bc8a2fe8d7be49/langchain_core-0.0.13-py3-none-any.whl.metadata\n",
      "  Downloading langchain_core-0.0.13-py3-none-any.whl.metadata (978 bytes)\n",
      "Collecting langsmith<0.1.0,>=0.0.63 (from langchain==0.0.343)\n",
      "  Obtaining dependency information for langsmith<0.1.0,>=0.0.63 from https://files.pythonhosted.org/packages/97/cd/1c618f89d3fcbb375c99a3ea950bffba8a01862cc0f0ab5032dfb95e8d1e/langsmith-0.0.92-py3-none-any.whl.metadata\n",
      "  Downloading langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in d:\\anaconda\\lib\\site-packages (from langchain==0.0.343) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\anaconda\\lib\\site-packages (from langchain==0.0.343) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\anaconda\\lib\\site-packages (from langchain==0.0.343) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in d:\\anaconda\\lib\\site-packages (from langchain==0.0.343) (8.2.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.343) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.343) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.343) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.343) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\anaconda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.343) (1.9.3)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\anaconda\\lib\\site-packages (from anyio<4.0->langchain==0.0.343) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\anaconda\\lib\\site-packages (from anyio<4.0->langchain==0.0.343) (1.3.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\anaconda\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.343) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\anaconda\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.343) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\anaconda\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.343) (2.1)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\administrator\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<0.1,>=0.0.7->langchain==0.0.343) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in d:\\anaconda\\lib\\site-packages (from pydantic<3,>=1->langchain==0.0.343) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2->langchain==0.0.343) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2->langchain==0.0.343) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2->langchain==0.0.343) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\anaconda\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.343) (3.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\anaconda\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.343) (1.0.0)\n",
      "Downloading langchain-0.0.343-py3-none-any.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.9 MB 217.9 kB/s eta 0:00:09\n",
      "   ---------------------------------------- 0.0/1.9 MB 217.9 kB/s eta 0:00:09\n",
      "   ---------------------------------------- 0.0/1.9 MB 217.9 kB/s eta 0:00:09\n",
      "    --------------------------------------- 0.0/1.9 MB 131.3 kB/s eta 0:00:15\n",
      "    --------------------------------------- 0.0/1.9 MB 131.3 kB/s eta 0:00:15\n",
      "   - -------------------------------------- 0.1/1.9 MB 137.9 kB/s eta 0:00:14\n",
      "   - -------------------------------------- 0.1/1.9 MB 178.6 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.1/1.9 MB 183.8 kB/s eta 0:00:10\n",
      "   -- ------------------------------------- 0.1/1.9 MB 210.9 kB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.1/1.9 MB 218.7 kB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.1/1.9 MB 224.6 kB/s eta 0:00:08\n",
      "   --- ------------------------------------ 0.2/1.9 MB 269.0 kB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 296.6 kB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 304.8 kB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 0.2/1.9 MB 313.3 kB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 0.3/1.9 MB 308.4 kB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 0.3/1.9 MB 315.3 kB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 0.3/1.9 MB 321.9 kB/s eta 0:00:06\n",
      "   ------- -------------------------------- 0.3/1.9 MB 343.8 kB/s eta 0:00:05\n",
      "   ------- -------------------------------- 0.3/1.9 MB 348.7 kB/s eta 0:00:05\n",
      "   ------- -------------------------------- 0.4/1.9 MB 358.4 kB/s eta 0:00:05\n",
      "   ------- -------------------------------- 0.4/1.9 MB 342.0 kB/s eta 0:00:05\n",
      "   ------- -------------------------------- 0.4/1.9 MB 342.0 kB/s eta 0:00:05\n",
      "   ------- -------------------------------- 0.4/1.9 MB 342.0 kB/s eta 0:00:05\n",
      "   ------- -------------------------------- 0.4/1.9 MB 342.0 kB/s eta 0:00:05\n",
      "   ------- -------------------------------- 0.4/1.9 MB 342.0 kB/s eta 0:00:05\n",
      "   ------- -------------------------------- 0.4/1.9 MB 342.0 kB/s eta 0:00:05\n",
      "   ------- -------------------------------- 0.4/1.9 MB 342.0 kB/s eta 0:00:05\n",
      "   ------- -------------------------------- 0.4/1.9 MB 342.0 kB/s eta 0:00:05\n",
      "   ------------- -------------------------- 0.6/1.9 MB 429.9 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 0.6/1.9 MB 429.9 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 0.6/1.9 MB 429.9 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 0.6/1.9 MB 429.9 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 0.7/1.9 MB 389.6 kB/s eta 0:00:04\n",
      "   ------------- -------------------------- 0.7/1.9 MB 389.6 kB/s eta 0:00:04\n",
      "   -------------- ------------------------- 0.7/1.9 MB 382.9 kB/s eta 0:00:04\n",
      "   -------------- ------------------------- 0.7/1.9 MB 393.2 kB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 0.8/1.9 MB 410.9 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 0.8/1.9 MB 424.4 kB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 0.8/1.9 MB 433.5 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 0.9/1.9 MB 440.4 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 0.9/1.9 MB 438.6 kB/s eta 0:00:03\n",
      "   ------------------- -------------------- 1.0/1.9 MB 449.9 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 1.0/1.9 MB 464.1 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.0/1.9 MB 472.9 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.1/1.9 MB 484.5 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 484.8 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 488.2 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.2/1.9 MB 500.1 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.2/1.9 MB 511.6 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.3/1.9 MB 518.4 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.3/1.9 MB 529.3 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.4/1.9 MB 530.7 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 1.4/1.9 MB 537.7 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.4/1.9 MB 539.8 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.5/1.9 MB 546.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.5/1.9 MB 558.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.6/1.9 MB 564.4 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.6/1.9 MB 568.9 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.7/1.9 MB 574.4 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.7/1.9 MB 578.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/1.9 MB 583.7 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.8/1.9 MB 591.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.9/1.9 MB 598.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 599.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 599.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 599.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 599.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 599.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 599.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 553.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 551.1 kB/s eta 0:00:00\n",
      "Downloading langchain_core-0.0.13-py3-none-any.whl (188 kB)\n",
      "   ---------------------------------------- 0.0/188.2 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 30.7/188.2 kB 1.3 MB/s eta 0:00:01\n",
      "   ------------ -------------------------- 61.4/188.2 kB 812.7 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 112.6/188.2 kB 819.2 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 143.4/188.2 kB 847.9 kB/s eta 0:00:01\n",
      "   -------------------------------------  184.3/188.2 kB 853.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- 188.2/188.2 kB 757.1 kB/s eta 0:00:00\n",
      "Downloading langsmith-0.0.92-py3-none-any.whl (56 kB)\n",
      "   ---------------------------------------- 0.0/56.5 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 30.7/56.5 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 56.5/56.5 kB 734.9 kB/s eta 0:00:00\n",
      "Installing collected packages: langsmith, langchain-core, langchain\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.1.40\n",
      "    Uninstalling langsmith-0.1.40:\n",
      "      Successfully uninstalled langsmith-0.1.40\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.1.40\n",
      "    Uninstalling langchain-core-0.1.40:\n",
      "      Successfully uninstalled langchain-core-0.1.40\n",
      "Successfully installed langchain-0.0.343 langchain-core-0.0.13 langsmith-0.0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "  WARNING: The script langsmith.exe is installed in 'C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script langchain-server.exe is installed in 'C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-community 0.0.31 requires langchain-core<0.2.0,>=0.1.37, but you have langchain-core 0.0.13 which is incompatible.\n",
      "langchain-community 0.0.31 requires langsmith<0.2.0,>=0.1.0, but you have langsmith 0.0.92 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain==0.0.343"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2afeaf-7c6e-4f35-8a92-87f654d45e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
