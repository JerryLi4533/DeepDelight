{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NewsQA Loop with HuggingFace\n",
    "11/25/2023\n",
    "\n",
    "Lixiao Yang\n",
    "\n",
    "Updated based on 6.2 NewsQA Loop_Ke.ipynb, for loop running for large dataset, refer to the [8.2 NewsQA Loop - v2.ipunb](https://github.com/lixiao-yang/DeepDelight/blob/main/Thread2/8.2%20NewsQA%20Loop%20-%20v2.ipynb).\n",
    "\n",
    "This notebook provides new details about the NewsQA loop with HuggingFaceInstructEmbeddings, and also addresses certain solutions to problems occured in previous version of codes. The sequence is organized based on the development log.\n",
    "\n",
    "1. Combining Hugging Face's `InstructEmbedding` [multi-qa-MiniLM-L6-cos-v1`](https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1) with GPT4ALL `gpt4all-falcon-q4_0` model for semantic search and local data training.\n",
    "2. Pure GPT4ALL  `gpt4all-falcon-q4_0` model results for question 1: Performs badly, may due to the local traning text (one story) size is too small. (see Section 5)\n",
    "3. Pure HuggingFace model results for question 1: By using cosine similarity, the model can locate the correct sentence for the actual answer. (see Section 6)\n",
    "4. GPT4ALL `gpt4all-falcon-q4_0` model with `multi-qa-MiniLM-L6-cos-v1` InstructEmbeddings: (see Section 7)\n",
    "5. Fixed inconsistent F1 score issue\n",
    "6. Fixed the previous prompt issue\n",
    "7. Observed that prompt change will impact the results, a better way of prompting can be important for improving F1 score.\n",
    "8. No obvious F1 score improvement for adding the InstructEmbedding methods for the test run.\n",
    "\n",
    "### Problems and Solutions\n",
    "- **Problem of incompatibility between Hugging Face and GPT4ALL**\n\n",
    "    InstructEmbedding requires a HuggingFace model to finish the embedding process, most of them are pretrained model that does not support local training.\n\n",
    "    **Solution**: Use `sentence-transformers/multi-qa-MiniLM-L6-cos-v1` from [Hugging Face](https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1) which is a pretrained sentence-transformer model. It maps sentences & paragraphs to a 384 dimensional dense vector space and was designed for **semantic search**. `GPT4ALL` is a `llama.cpp` based large language model under `langchain`. So we use llama to connect the two models together. Use this model only for embedding and combine GPT4ALL model to enable local data training following below procedures:\n",
    "   1. Download `GPT4ALL` model / Locally found the model\n",
    "   2. Download `llama.cpp` 7B model / Locally found the model\n",
    "   3. Transform `GPT4ALL` model\n",
    "   4. Store the converted model\n",
    "- **Problem of incompatibility of gpt4all-falcon-q4_0 model (Pending)**\n\n",
    "    Current `gpt4all-falcon-q4_0` model is not compatible with `llama.cpp`.\n\n",
    "    **Solution**: Switch to gpt4all-lora-quantized.bin, convert the model into llama.cpp compatible version, refer to [GPT4All Langchain Demo](https://gist.github.com/segestic/4822f3765147418fc084a250598c1fe6) (PENDING)\n",
    "- **Problem of dimensionality inconsistency**\n",
    "    See Section 7.3\n",
    "- **Previous problem of inconsistent results for different runs**\n\n",
    "    Previous code shows that for different runs, the F1 score is different causing the results replicate issue.\n\n",
    "    **Solution**: Set a fixed seed for GPT4ALL model, and also use `set_seed()` to set universal seed before the model initialization.\n",
    "- **Previous propmt issue**\n\n",
    "    In previous code, for different prompt template would genereate the same answers.\n\n",
    "    **Solution**: Revise the code, found that the QA_CHAIN_PROMPT parameter was neglected in previous version of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama-cpp-python langchain sentence_transformers InstructorEmbedding pyllama transformers pyqt5 pyqtwebengine pyllamacpp --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from langchain.llms import GPT4All\n",
    "from pathlib import Path\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceInstructEmbeddings\n",
    "from transformers import set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='C:/NewsQA/combined-newsqa-data-story1.json'\n",
    "data = json.loads(Path(file_path).read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'version': '1',\n",
       " 'data': [{'text': 'NEW DELHI, India (CNN) -- A high court in northern India on Friday acquitted a wealthy businessman facing the death sentence for the killing of a teen in a case dubbed \"the house of horrors.\"\\n\\n\\n\\nMoninder Singh Pandher was sentenced to death by a lower court in February.\\n\\n\\n\\nThe teen was one of 19 victims -- children and young women -- in one of the most gruesome serial killings in India in recent years.\\n\\n\\n\\nThe Allahabad high court has acquitted Moninder Singh Pandher, his lawyer Sikandar B. Kochar told CNN.\\n\\n\\n\\nPandher and his domestic employee Surinder Koli were sentenced to death in February by a lower court for the rape and murder of the 14-year-old.\\n\\n\\n\\nThe high court upheld Koli\\'s death sentence, Kochar said.\\n\\n\\n\\nThe two were arrested two years ago after body parts packed in plastic bags were found near their home in Noida, a New Delhi suburb. Their home was later dubbed a \"house of horrors\" by the Indian media.\\n\\n\\n\\nPandher was not named a main suspect by investigators initially, but was summoned as co-accused during the trial, Kochar said.\\n\\n\\n\\nKochar said his client was in Australia when the teen was raped and killed.\\n\\n\\n\\nPandher faces trial in the remaining 18 killings and could remain in custody, the attorney said.',\n",
       "   'type': 'train',\n",
       "   'questions': [{'isQuestionBad': 0.0,\n",
       "     'consensus': {'s': 294, 'e': 297},\n",
       "     'validatedAnswers': [{'count': 1, 'noAnswer': True},\n",
       "      {'count': 2, 's': 294, 'e': 297}],\n",
       "     'answers': [{'sourcerAnswers': [{'s': 294, 'e': 297}]},\n",
       "      {'sourcerAnswers': [{'noAnswer': True}]},\n",
       "      {'sourcerAnswers': [{'noAnswer': True}]}],\n",
       "     'q': 'What was the amount of children murdered?',\n",
       "     'isAnswerAbsent': 0.0},\n",
       "    {'q': 'When was Pandher sentenced to death?',\n",
       "     'isQuestionBad': 0.0,\n",
       "     'consensus': {'s': 261, 'e': 271},\n",
       "     'answers': [{'sourcerAnswers': [{'s': 261, 'e': 271}]},\n",
       "      {'sourcerAnswers': [{'s': 258, 'e': 271}]},\n",
       "      {'sourcerAnswers': [{'s': 261, 'e': 271}]}],\n",
       "     'isAnswerAbsent': 0.0},\n",
       "    {'isQuestionBad': 0.0,\n",
       "     'consensus': {'s': 624, 'e': 640},\n",
       "     'validatedAnswers': [{'count': 2, 's': 624, 'e': 640}],\n",
       "     'answers': [{'sourcerAnswers': [{'s': 26, 'e': 33}]},\n",
       "      {'sourcerAnswers': [{'noAnswer': True}]},\n",
       "      {'sourcerAnswers': [{'s': 624, 'e': 640}]}],\n",
       "     'q': 'The court aquitted Moninder Singh Pandher of what crime?',\n",
       "     'isAnswerAbsent': 0.0},\n",
       "    {'q': 'who was acquitted',\n",
       "     'isQuestionBad': 0.0,\n",
       "     'consensus': {'s': 195, 'e': 218},\n",
       "     'answers': [{'sourcerAnswers': [{'s': 195, 'e': 218}]},\n",
       "      {'sourcerAnswers': [{'s': 195, 'e': 218}]}],\n",
       "     'isAnswerAbsent': 0.0},\n",
       "    {'isQuestionBad': 0.0,\n",
       "     'consensus': {'s': 195, 'e': 218},\n",
       "     'validatedAnswers': [{'count': 2, 's': 195, 'e': 218}],\n",
       "     'answers': [{'sourcerAnswers': [{'noAnswer': True}]},\n",
       "      {'sourcerAnswers': [{'s': 195, 'e': 218}, {'s': 232, 'e': 271}]},\n",
       "      {'sourcerAnswers': [{'noAnswer': True}]}],\n",
       "     'q': 'who was sentenced',\n",
       "     'isAnswerAbsent': 0.33333333333299997},\n",
       "    {'isQuestionBad': 0.0,\n",
       "     'consensus': {'s': 129, 'e': 151},\n",
       "     'validatedAnswers': [{'count': 2, 's': 129, 'e': 151}],\n",
       "     'answers': [{'sourcerAnswers': [{'s': 129, 'e': 192}]},\n",
       "      {'sourcerAnswers': [{'s': 129, 'e': 151}]},\n",
       "      {'sourcerAnswers': [{'s': 133, 'e': 151}]}],\n",
       "     'q': 'What was Moninder Singh Pandher acquitted for?',\n",
       "     'isAnswerAbsent': 0.0},\n",
       "    {'q': 'Who was sentenced to death in February?',\n",
       "     'isQuestionBad': 0.0,\n",
       "     'consensus': {'s': 195, 'e': 218},\n",
       "     'answers': [{'sourcerAnswers': [{'s': 195, 'e': 218}]},\n",
       "      {'sourcerAnswers': [{'s': 195, 'e': 218}]}],\n",
       "     'isAnswerAbsent': 0.0},\n",
       "    {'q': 'how many people died',\n",
       "     'isQuestionBad': 0.0,\n",
       "     'consensus': {'s': 294, 'e': 297},\n",
       "     'answers': [{'sourcerAnswers': [{'s': 294, 'e': 297}]},\n",
       "      {'sourcerAnswers': [{'s': 294, 'e': 297}]}],\n",
       "     'isAnswerAbsent': 0.0},\n",
       "    {'q': 'How many children and young women were murdered?',\n",
       "     'isQuestionBad': 0.0,\n",
       "     'consensus': {'s': 294, 'e': 297},\n",
       "     'answers': [{'sourcerAnswers': [{'s': 294, 'e': 297}]},\n",
       "      {'sourcerAnswers': [{'s': 294, 'e': 297}]}],\n",
       "     'isAnswerAbsent': 0.0}],\n",
       "   'storyId': './cnn/stories/42d01e187213e86f5fe617fe32e716ff7fa3afc4.story'}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPT4ALL Model Transformation (PENDING)\n",
    "gpt4all-lora-quantized.bin + llama.cpp 7B -> gpt4all-lora-q-converted.bin\n",
    "\n",
    "Follow the instructions from https://colab.research.google.com/drive/1csJ9lzewAaBVNSO9icJC5iT7xVrUbcg0?usp=sharing#scrollTo=Of6lqaPY0lR5 in Part 1 to get the converted model. Remember to also save the converted model to the local machine for local training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 LLaMA 7b Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\24075\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe: Error while finding module specification for 'llama.download' (ModuleNotFoundError: No module named 'llama')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m llama.download --model_size 7B --folder llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate Exact Match (EM) score\n",
    "def calculate_em(predicted, actual):\n",
    "    return int(predicted == actual)\n",
    "\n",
    "# Function to calculate the token-wise F1 score for text answers\n",
    "def calculate_token_f1(predicted, actual):\n",
    "    predicted_tokens = predicted.split()\n",
    "    actual_tokens = actual.split()\n",
    "    common_tokens = Counter(predicted_tokens) & Counter(actual_tokens)\n",
    "    num_same = sum(common_tokens.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(predicted_tokens)\n",
    "    recall = 1.0 * num_same / len(actual_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "# Helper function to extract answer ranges from the consensus field\n",
    "def extract_ranges(consensus):\n",
    "    if 's' in consensus and 'e' in consensus:\n",
    "        return [(consensus['s'], consensus['e'])]\n",
    "    return []\n",
    "\n",
    "def extract_answer_from_gpt4all_output(output, prompt_end=\"Answer:\"):\n",
    "    \"\"\"\n",
    "    Extracts the answer from the output of the GPT4ALL model.\n",
    "\n",
    "    Parameters:\n",
    "        output (str): The output string from the GPT4ALL model.\n",
    "        prompt_end (str): The delimiter indicating the end of the prompt and the start of the answer.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted answer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Find the start of the answer\n",
    "    start_idx = output.find(prompt_end)\n",
    "    if start_idx == -1:\n",
    "        return \"Answer not found in output\"\n",
    "\n",
    "    # Extract the answer\n",
    "    start_idx += len(prompt_end)\n",
    "    answer = output[start_idx:]\n",
    "\n",
    "    # Optional: Clean up the answer, remove any additional text after the answer\n",
    "    # This can be tailored based on how your model generates responses\n",
    "    end_characters = [\".\", \"?\", \"!\"]\n",
    "    for end_char in end_characters:\n",
    "        end_idx = answer.find(end_char)\n",
    "        if end_idx != -1:\n",
    "            answer = answer[:end_idx + 1]\n",
    "            break\n",
    "\n",
    "    return answer.strip()\n",
    "\n",
    "def calculate_modified_f1(predicted, actual):\n",
    "    \"\"\"This approach gives a score that reflects the best match for each word in the predicted answer \n",
    "    with any word in the actual answer, thereby considering partial and inexact matches more effectively.\n",
    "    \"\"\"\n",
    "    def inner_word_f1(predicted_word, actual_word):\n",
    "        common_chars = Counter(predicted_word) & Counter(actual_word)\n",
    "        num_common = sum(common_chars.values())\n",
    "        if num_common == 0:\n",
    "            return 0\n",
    "        precision = num_common / len(predicted_word)\n",
    "        recall = num_common / len(actual_word)\n",
    "        if precision + recall == 0:\n",
    "            return 0\n",
    "        return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    predicted_tokens = predicted.split()\n",
    "    actual_tokens = actual.split()\n",
    "\n",
    "    total_f1 = 0.0\n",
    "    for predicted_word in predicted_tokens:\n",
    "        word_f1_scores = [inner_word_f1(predicted_word, actual_word) for actual_word in actual_tokens]\n",
    "        total_f1 += max(word_f1_scores, default=0)\n",
    "\n",
    "    # Normalize the total F1 score by the number of words in the predicted answer\n",
    "    normalized_total_f1 = total_f1 / len(predicted_tokens) if predicted_tokens else 0\n",
    "    return normalized_total_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logging Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\n",
      "1700878339.4739673 Started.\n"
     ]
    }
   ],
   "source": [
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Parameters\n",
    "max_stories = 100\n",
    "chunk_sizes = [200]\n",
    "overlap_percentages = [0.1]  # Expressed as percentages\n",
    "\n",
    "# Results storage\n",
    "f1_results = defaultdict(list)\n",
    "em_results = defaultdict(list)\n",
    "\n",
    "# Initialize the language model (GPT4ALL)\n",
    "llm_path = \"C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "llm = GPT4All(model=\"C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\", max_tokens=2048)\n",
    "\n",
    "# HuggingFace Instruct Embeddings parameters\n",
    "instruct_embedding_model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "instruct_embedding_model_kwargs = {'device': 'cpu'}\n",
    "instruct_embedding_encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# Start time calculation\n",
    "start_time = time.time()\n",
    "print(f\"{start_time} Started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GPT4ALL Model Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "\n",
      "165.83870601654053 Processing chunk size 200:\n",
      "\n",
      "165.83870601654053\t0.0\tOverlap [0.1]\n",
      "\n",
      "165.83870601654053\t0.0\t\tstory 1: .Question: What was the amount of children murdered?\n",
      "Predicted Answer:  The amount of children murdered is not provided in the given context.\n",
      "Actual Answer: 19 \n",
      "F1 Score: 0.0000\n",
      "\n",
      ".Question: When was Pandher sentenced to death?\n",
      "Predicted Answer:  Pandh\n",
      "Actual Answer: February.\n",
      "\n",
      "F1 Score: 0.0000\n",
      "\n",
      ".Question: The court aquitted Moninder Singh Pandher of what crime?\n",
      "Predicted Answer:  The context does not provide information on the specific crime for which Moninder Singh Pandher was acquitted.\n",
      "Actual Answer: rape and murder \n",
      "F1 Score: 0.0000\n",
      "\n",
      ".Question: who was acquitted\n",
      "Predicted Answer:  The person who was acquitted is Moninder Singh Pandh\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "F1 Score: 0.3333\n",
      "\n",
      ".Question: who was sentenced\n",
      "Predicted Answer:  The high court upheld the sentence of the person who was sentenced to death.\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "F1 Score: 0.0000\n",
      "\n",
      ".Question: What was Moninder Singh Pandher acquitted for?\n",
      "Predicted Answer:  The context does not provide any specific information about what Moninder Singh Pandh\n",
      "Actual Answer: the killing of a teen \n",
      "F1 Score: 0.0000\n",
      "\n",
      ".Question: Who was sentenced to death in February?\n",
      "Predicted Answer:  Moniinder Singh Pandh\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "F1 Score: 0.3333\n",
      "\n",
      ".Question: how many people died\n",
      "Predicted Answer:  The context does not provide information on how many people died.\n",
      "Actual Answer: 19 \n",
      "F1 Score: 0.0000\n",
      "\n",
      ".Question: How many children and young women were murdered?\n",
      "Predicted Answer:  The context states that there were 1,900 children and young women murdered in the serial killings in India in recent years.\n",
      "Actual Answer: 19 \n",
      "F1 Score: 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_file_path = \"results/scores_2.txt\"\n",
    "\n",
    "# Initialize HuggingFace Instruct Embeddings\n",
    "word_embed = HuggingFaceInstructEmbeddings(\n",
    "    model_name=instruct_embedding_model_name,\n",
    "    model_kwargs=instruct_embedding_model_kwargs,\n",
    "    encode_kwargs=instruct_embedding_encode_kwargs\n",
    ")\n",
    "\n",
    "with open(output_file_path, 'w') as file:\n",
    "    for chunk_size in chunk_sizes:\n",
    "        print(f\"\\n{time.time()-start_time} Processing chunk size {chunk_size}:\")\n",
    "        last_time = time.time()\n",
    "\n",
    "        for overlap_percentage in overlap_percentages:\n",
    "            print(f\"\\n{time.time()-start_time}\\t{time.time()-last_time}\\tOverlap [{overlap_percentage}]\")\n",
    "            last_time = time.time()\n",
    "\n",
    "            i = 0\n",
    "            for story in data['data']:\n",
    "                i += 1\n",
    "                if i > max_stories:\n",
    "                    break\n",
    "                now_time = time.time()\n",
    "                print(f\"\\n{now_time-start_time}\\t{now_time-last_time}\\t\\tstory {i}: \", end='')\n",
    "                last_time = now_time\n",
    "\n",
    "                qa_chain, vectorstore = create_qa_chain(\n",
    "                    story_text=story['text'],\n",
    "                    chunk_size=chunk_size,\n",
    "                    overlap_percentage=overlap_percentage,\n",
    "                    llm=llm,\n",
    "                    word_embed=word_embed\n",
    "                )\n",
    "\n",
    "                j = 0\n",
    "                for question_data in story['questions']:\n",
    "                    j += 1\n",
    "                    print('.', end='')\n",
    "\n",
    "                    question = question_data['q']\n",
    "                    question_vector = word_embed.embed_query(question)\n",
    "                    docs = vectorstore.similarity_search_by_vector(question_vector)\n",
    "                    answer_ranges = extract_ranges(question_data['consensus'])\n",
    "\n",
    "                    # Get the prediction from the model\n",
    "                    result = qa_chain({\"query\": question})\n",
    "                    predicted_answer = result['result']\n",
    "                    if isinstance(predicted_answer, dict):\n",
    "                        predicted_answer = predicted_answer.get('answer', '')\n",
    "                    elif not isinstance(predicted_answer, str):\n",
    "                        print(f\"Unexpected format for predicted answer: {predicted_answer}\")\n",
    "                        continue\n",
    "                    actual_answer = story['text'][answer_ranges[0][0]:answer_ranges[0][1]] if answer_ranges else \"\"\n",
    "\n",
    "                    # Calculate the scores\n",
    "                    em_score = calculate_em(predicted_answer, actual_answer)\n",
    "                    f1_score_value = calculate_token_f1(predicted_answer, actual_answer)\n",
    "                    file.write(f\"{chunk_size}\\t{overlap_percentage}\\t{i}\\t{j}\\t{f1_score_value:.4f}\\t{em_score:.2f}\\n\")\n",
    "\n",
    "                    em_results[(chunk_size, overlap_percentage)].append(em_score)\n",
    "                    f1_results[(chunk_size, overlap_percentage)].append(f1_score_value)\n",
    "\n",
    "                    # Extract actual answer\n",
    "                    consensus = question_data['consensus']\n",
    "\n",
    "                    # Calculate F1 score\n",
    "                    f1_score = calculate_token_f1(predicted_answer, actual_answer)\n",
    "\n",
    "                    # Print results\n",
    "                    print(f\"Question: {question}\")\n",
    "                    print(f\"Predicted Answer: {predicted_answer}\")\n",
    "                    print(f\"Actual Answer: {actual_answer}\")\n",
    "                    print(f\"F1 Score: {f1_score:.4f}\")\n",
    "                    print()\n",
    "\n",
    "                del qa_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hugging Face Pretrained Model Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: What was the amount of children murdered?\n",
      "Predicted Answer: of 19 victims -- children\n",
      "Actual Answer: 19 \n",
      "F1 Score: 0.3333\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: When was Pandher sentenced to death?\n",
      "Predicted Answer: Pandher was sentenced to death\n",
      "Actual Answer: February.\n",
      "\n",
      "F1 Score: 0.0000\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: The court aquitted Moninder Singh Pandher of what crime?\n",
      "Predicted Answer: acquitted Moninder Singh Pandher, his\n",
      "Actual Answer: rape and murder \n",
      "F1 Score: 0.0000\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: who was acquitted\n",
      "Predicted Answer: Allahabad high court has acquitted\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "F1 Score: 0.0000\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: who was sentenced\n",
      "Predicted Answer: was sentenced to death by\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "F1 Score: 0.0000\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: What was Moninder Singh Pandher acquitted for?\n",
      "Predicted Answer: acquitted Moninder Singh Pandher, his\n",
      "Actual Answer: the killing of a teen \n",
      "F1 Score: 0.0000\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: Who was sentenced to death in February?\n",
      "Predicted Answer: sentenced to death in February\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "F1 Score: 0.0000\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: how many people died\n",
      "Predicted Answer: death sentence for the killing\n",
      "Actual Answer: 19 \n",
      "F1 Score: 0.0000\n",
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Question: How many children and young women were murdered?\n",
      "Predicted Answer: victims -- children and young\n",
      "Actual Answer: 19 \n",
      "F1 Score: 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize the HuggingFaceInstructEmbeddings model\n",
    "model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# Assuming data is loaded into a variable named 'data'\n",
    "story_data = data['data'][0]  # First story\n",
    "story_text = story_data['text']\n",
    "\n",
    "# Split story text into words and embed each word\n",
    "words = story_text.split()  # Splitting into words\n",
    "hf_story_embs = HuggingFaceInstructEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    "    embed_instruction=\"Represent the story for retrieval:\"\n",
    ")\n",
    "word_embs = hf_story_embs.embed_documents(words)\n",
    "\n",
    "# Process each question\n",
    "for question_data in story_data['questions']:\n",
    "    question = question_data['q']\n",
    "\n",
    "    hf_query_embs = HuggingFaceInstructEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "        query_instruction=\"Use the following pieces of context to answer the question at the end. Use ten words maximum and keep the answer as concise as possible.Find the shortest possible answer:\"\n",
    "    )\n",
    "    question_emb = hf_query_embs.embed_documents([question])[0]\n",
    "\n",
    "    # Compute cosine similarity scores with each word\n",
    "    scores = [torch.cosine_similarity(torch.tensor(word_emb).unsqueeze(0), torch.tensor(question_emb).unsqueeze(0))[0].item() for word_emb in word_embs]\n",
    "\n",
    "    # Find the sequence of 5 consecutive words with the highest cumulative score\n",
    "    best_sequence_score = float('-inf')\n",
    "    best_sequence = []\n",
    "    for i in range(len(scores) - 4):\n",
    "        current_sequence_score = sum(scores[i:i+5])\n",
    "        if current_sequence_score > best_sequence_score:\n",
    "            best_sequence_score = current_sequence_score\n",
    "            best_sequence = words[i:i+5]\n",
    "\n",
    "    best_sequence_text = ' '.join(best_sequence)\n",
    "\n",
    "    # Extract actual answer using the consensus range\n",
    "    consensus = question_data['consensus']\n",
    "    actual_answer = story_text[consensus['s']:consensus['e']]\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1_score = calculate_token_f1(best_sequence_text, actual_answer)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Predicted Answer: {best_sequence_text}\")\n",
    "    print(f\"Actual Answer: {actual_answer}\")\n",
    "    print(f\"F1 Score: {f1_score:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.GPT4ALL with Hugging Face Instruct Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.0 Question Answering Loop Encapsulation\n",
    "Encapsulate the loop process to simplify the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newsqa_loop_test(data, llm, output_file_path, chunk_sizes, overlap_percentages, max_stories, instruct_embedding_model_name, instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, QA_CHAIN_PROMPT):\n",
    "    \"\"\"\n",
    "    Processes a collection of stories, answering questions using a QA chain based on GPT4ALL language model and\n",
    "    HuggingFaceInstructEmbeddings.\n",
    "\n",
    "    For each story, the text is split into chunks, and each chunk is processed to answer questions. The results,\n",
    "    including calculated F1 and EM scores, are written to an output file.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The dataset containing the stories and questions.\n",
    "        llm (LLM): The language model instance used for generating answers.\n",
    "        output_file_path (str): Path to the output file where results will be stored.\n",
    "        chunk_sizes (list): List of chunk sizes for text splitting.\n",
    "        overlap_percentages (list): List of overlap percentages for text splitting.\n",
    "        max_stories (int): Maximum number of stories to process.\n",
    "        instruct_embedding_model_name (str): Name of the HuggingFace model for embeddings.\n",
    "        instruct_embedding_model_kwargs (dict): Keyword arguments for the embedding model.\n",
    "        instruct_embedding_encode_kwargs (dict): Encoding arguments for the embedding model.\n",
    "        QA_CHAIN_PROMPT (str): The prompt template for the QA chain.\n",
    "\n",
    "    Returns:\n",
    "        The function writes the results to a file and print the results of question, answer, actual answer, \n",
    "        and F1 scores.\n",
    "    \"\"\"\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        word_embed = HuggingFaceInstructEmbeddings(\n",
    "            model_name=instruct_embedding_model_name,\n",
    "            model_kwargs=instruct_embedding_model_kwargs,\n",
    "            encode_kwargs=instruct_embedding_encode_kwargs\n",
    "        )\n",
    "        start_time = time.time()\n",
    "        for chunk_size in chunk_sizes:\n",
    "            print(f\"\\n{time.time()-start_time} Processing chunk size {chunk_size}:\")\n",
    "            last_time = time.time()\n",
    "            for overlap_percentage in overlap_percentages:\n",
    "                actual_overlap = int(chunk_size * overlap_percentage)\n",
    "                print(f\"\\n{time.time()-start_time}\\t{time.time()-last_time}\\tOverlap [{overlap_percentage}] {actual_overlap}\")\n",
    "                text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=actual_overlap)\n",
    "        \n",
    "                for i, story in enumerate(data['data']):\n",
    "                    if i >= max_stories:\n",
    "                        break\n",
    "                    now_time = time.time()\n",
    "                    print(f\"\\n{now_time-start_time}\\t{now_time-last_time}\\t\\tstory {i+1}: \", end='')\n",
    "                    last_time = now_time\n",
    "        \n",
    "                    all_splits = text_splitter.split_text(story['text'])\n",
    "                    vectorstore = Chroma.from_texts(texts=all_splits, embedding=word_embed)\n",
    "                    qa_chain = RetrievalQA.from_chain_type(\n",
    "                        llm, \n",
    "                        retriever=vectorstore.as_retriever(), \n",
    "                        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "                        return_source_documents=True)\n",
    "        \n",
    "                    for j, question_data in enumerate(story['questions']):\n",
    "                        j+=1\n",
    "                        # print(f\"{time.time()-start_time}\\t\\t\\tquestion {j}\")\n",
    "                        print('.', end='')\n",
    "        \n",
    "                        # TODO: embed query and perform similarity_search_by_vector() instead\n",
    "                        question = question_data['q']\n",
    "                        question_vector = word_embed.embed_query(question)\n",
    "                        # docs = vectorstore.similarity_search(question)\n",
    "                        docs = vectorstore.similarity_search_by_vector(question_vector)\n",
    "                        answer_ranges = extract_ranges(question_data['consensus'])\n",
    "                        \n",
    "                        # Get the prediction from the model\n",
    "                        result = qa_chain({\"query\": question})\n",
    "                        \n",
    "                        # Check if the predicted answer is in the expected format (string)\n",
    "                        predicted_answer = result['result']\n",
    "                        if isinstance(predicted_answer, dict):\n",
    "                            # If it's a dictionary, you need to adapt this part of the code to extract the answer string\n",
    "                            predicted_answer = predicted_answer.get('answer', '')  # Assuming 'answer' is the key for the answer string\n",
    "                        elif not isinstance(predicted_answer, str):\n",
    "                            # If the answer is not a string and not a dictionary, log an error or handle it appropriately\n",
    "                            print(f\"Unexpected format for predicted answer: {predicted_answer}\")\n",
    "                            continue  # Skip to the next question\n",
    "                        actual_answer = story['text'][answer_ranges[0][0]:answer_ranges[0][1]] if answer_ranges else \"\"\n",
    "                        \n",
    "                        # If there is an actual answer, get it from the story text using the character ranges\n",
    "                        if answer_ranges:\n",
    "                            actual_answer = story['text'][answer_ranges[0][0]:answer_ranges[0][1]]\n",
    "                        else:\n",
    "                            actual_answer = \"\"\n",
    "                        \n",
    "                        # Calculate the scores\n",
    "                        em_score = calculate_em(predicted_answer, actual_answer)\n",
    "                        f1_score_value = calculate_token_f1(predicted_answer, actual_answer)\n",
    "                        # modified_f1 = calculate_modified_f1(predicted_answer, actual_answer)\n",
    "                        file.write(f\"{chunk_size}\\t{overlap_percentage}\\t{i}\\t{j}\\t{f1_score_value:.4f}\\t{em_score:.2f}\\n\")\n",
    "        \n",
    "                        # Store the scores\n",
    "                        em_results[(chunk_size, overlap_percentage)].append(em_score)\n",
    "                        f1_results[(chunk_size, overlap_percentage)].append(f1_score_value)\n",
    "                        \n",
    "                        # Print results\n",
    "                        print(f\"\\nQuestion: {question}\")\n",
    "                        print(f\"Predicted Answer: {predicted_answer}\")\n",
    "                        print(f\"Actual Answer: {actual_answer}\")\n",
    "                        print(f\"F1 Score: {f1_score_value:4f}\")\n",
    "                        # print(f\"Modified F1: {modified_f1:.4f}\")\n",
    "                        \n",
    "                    # delete object for memory\n",
    "                    del qa_chain\n",
    "                    del vectorstore\n",
    "                    del all_splits\n",
    "                    \n",
    "                # delete splitter instance\n",
    "                del text_splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Original Propmpt\n",
    "Duplication for checking the consistency between different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\n",
      "1701040141.5350428 Started.\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "\n",
      "0.0 Processing chunk size 200:\n",
      "\n",
      "0.0\t0.0\tOverlap [0.1] 20\n",
      "\n",
      "0.003004312515258789\t0.003004312515258789\t\tstory 1: .\n",
      "Question: What was the amount of children murdered?\n",
      "Predicted Answer:  The article states that there were 1,900 children murdered in India between 2010 and 2015.\n",
      "Actual Answer: 19 \n",
      "F1 Score: 0.000000\n",
      ".\n",
      "Question: When was Pandher sentenced to death?\n",
      "Predicted Answer:  Pandher was sentenced to death in February.\n",
      "Actual Answer: February.\n",
      "\n",
      "F1 Score: 0.250000\n",
      ".\n",
      "Question: The court aquitted Moninder Singh Pandher of what crime?\n",
      "Predicted Answer:  Moninder Singh Pandher was accused of killing his wife.\n",
      "Actual Answer: rape and murder \n",
      "F1 Score: 0.000000\n",
      ".\n",
      "Question: who was acquitted\n",
      "Predicted Answer:  Moninder Singh Pandh\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "F1 Score: 0.666667\n",
      ".\n",
      "Question: who was sentenced\n",
      "Predicted Answer:  The high court upheld Koli's death sentence, Kochaar said.\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "F1 Score: 0.000000\n",
      ".\n",
      "Question: What was Moninder Singh Pandher acquitted for?\n",
      "Predicted Answer:  Moninder Singh Pandher was acquitted of the crime he was accused of.\n",
      "Actual Answer: the killing of a teen \n",
      "F1 Score: 0.235294\n",
      ".\n",
      "Question: Who was sentenced to death in February?\n",
      "Predicted Answer:  Moninder Singh Pandh\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "F1 Score: 0.666667\n",
      ".\n",
      "Question: how many people died\n",
      "Predicted Answer:  1 person\n",
      "Actual Answer: 19 \n",
      "F1 Score: 0.000000\n",
      ".\n",
      "Question: How many children and young women were murdered?\n",
      "Predicted Answer:  There were 9 children and young women who were murdered in the most gruesome serial killings in India in recent years.\n",
      "Actual Answer: 19 \n",
      "F1 Score: 0.000000\n"
     ]
    }
   ],
   "source": [
    "template_original = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use ten words maximum and keep the answer as concise as possible. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT_ORIGINAL = PromptTemplate.from_template(template_original)\n",
    "\n",
    "############## Important Parameters ##############\n",
    "max_stories = 100\n",
    "chunk_sizes = [200]\n",
    "overlap_percentages = [0.1]  # Expressed as percentages (0.1 = 10%)\n",
    "random_seed = 123\n",
    "output_file_path = \"results/scores_7.txt\"\n",
    "model_location = \"C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "# model_location = \"/Users/wk77/Library/CloudStorage/OneDrive-DrexelUniversity/Documents/data/gpt4all/models/gpt4all-falcon-q4_0.gguf\"\n",
    "\n",
    "##################################################\n",
    "\n",
    "# logging.basicConfig(level=logging.WARNING)  # This will show only warnings and errors\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "set_seed(random_seed)\n",
    "\n",
    "# Results storage\n",
    "f1_results = defaultdict(list)\n",
    "em_results = defaultdict(list)\n",
    "text_results = []\n",
    "\n",
    "# Initialize the language model and the QA chain\n",
    "llm = GPT4All(model=model_location, max_tokens=2048, seed=random_seed)\n",
    "\n",
    "# HuggingFace Instruct Embeddings parameters\n",
    "instruct_embedding_model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "instruct_embedding_model_kwargs = {'device': 'cpu'}\n",
    "instruct_embedding_encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# The following code would iterate over the stories and questions to calculate the scores\n",
    "start_time = time.time()\n",
    "print(f\"{start_time} Started.\")\n",
    "\n",
    "################ Main Function ################\n",
    "newsqa_loop_test(data, llm, output_file_path, chunk_sizes, overlap_percentages, max_stories, instruct_embedding_model_name,\n",
    "                 instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, QA_CHAIN_PROMPT_ORIGINAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\n",
      "1701045500.3580875 Started.\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "\n",
      "0.0 Processing chunk size 200:\n",
      "\n",
      "0.0\t0.0\tOverlap [0.1] 20\n",
      "\n",
      "0.002999544143676758\t0.002999544143676758\t\tstory 1: .\n",
      "Question: What was the amount of children murdered?\n",
      "Predicted Answer:  The amount of children murdered is not provided in the given context.\n",
      "Actual Answer: 19 \n",
      "F1 Score: 0.000000\n",
      ".\n",
      "Question: When was Pandher sentenced to death?\n",
      "Predicted Answer:  Pandher was sentenced to death in February.\n",
      "Actual Answer: February.\n",
      "\n",
      "F1 Score: 0.250000\n",
      ".\n",
      "Question: The court aquitted Moninder Singh Pandher of what crime?\n",
      "Predicted Answer:  The court aquitted Moninder Singh Pandher of a crime.\n",
      "Actual Answer: rape and murder \n",
      "F1 Score: 0.000000\n",
      ".\n",
      "Question: who was acquitted\n",
      "Predicted Answer:  Moninder Singh Pandh\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "F1 Score: 0.666667\n",
      ".\n",
      "Question: who was sentenced\n",
      "Predicted Answer:  The high court upheld the sentence of the person who committed the crime.\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "F1 Score: 0.000000\n",
      ".\n",
      "Question: What was Moninder Singh Pandher acquitted for?\n",
      "Predicted Answer:  Moninder Singh Pandh\n",
      "Actual Answer: the killing of a teen \n",
      "F1 Score: 0.000000\n",
      ".\n",
      "Question: Who was sentenced to death in February?\n",
      "Predicted Answer:  Moninder Singh Pandher was sentenced to death by a lower court in February.\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "F1 Score: 0.375000\n",
      ".\n",
      "Question: how many people died\n",
      "Predicted Answer:  The context does not provide information on how many people died.\n",
      "Actual Answer: 19 \n",
      "F1 Score: 0.000000\n",
      ".\n",
      "Question: How many children and young women were murdered?\n",
      "Predicted Answer:  The context states that there were 1,900 children and young women murdered in the serial killings in India in recent years.\n",
      "Actual Answer: 19 \n",
      "F1 Score: 0.000000\n"
     ]
    }
   ],
   "source": [
    "############## Important Parameters ##############\n",
    "max_stories = 100\n",
    "chunk_sizes = [200]\n",
    "overlap_percentages = [0.1]  # Expressed as percentages (0.1 = 10%)\n",
    "random_seed = 123\n",
    "output_file_path = \"results/scores_8.txt\"\n",
    "model_location = \"C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "# model_location = \"/Users/wk77/Library/CloudStorage/OneDrive-DrexelUniversity/Documents/data/gpt4all/models/gpt4all-falcon-q4_0.gguf\"\n",
    "\n",
    "##################################################\n",
    "\n",
    "# logging.basicConfig(level=logging.WARNING)  # This will show only warnings and errors\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "set_seed(random_seed)\n",
    "\n",
    "# Results storage\n",
    "f1_results = defaultdict(list)\n",
    "em_results = defaultdict(list)\n",
    "text_results = []\n",
    "\n",
    "# Initialize the language model and the QA chain\n",
    "llm = GPT4All(model=model_location, max_tokens=2048, seed=random_seed)\n",
    "\n",
    "# HuggingFace Instruct Embeddings parameters\n",
    "instruct_embedding_model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "instruct_embedding_model_kwargs = {'device': 'cpu'}\n",
    "instruct_embedding_encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# The following code would iterate over the stories and questions to calculate the scores\n",
    "start_time = time.time()\n",
    "print(f\"{start_time} Started.\")\n",
    "\n",
    "################ Main Function ################\n",
    "newsqa_loop_test(data, llm, output_file_path, chunk_sizes, overlap_percentages, max_stories, instruct_embedding_model_name,\n",
    "                 instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, QA_CHAIN_PROMPT_ORIGINAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.1 Modified F1 Score to measure (PENGDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\n",
      "1700966802.8688896 Started.\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "\n",
      "0.0 Processing chunk size 200:\n",
      "\n",
      "0.0\t0.0\tOverlap [0.1] 20\n",
      "\n",
      "0.0019981861114501953\t0.0019981861114501953\t\tstory 1: .Question: What was the amount of children murdered?\n",
      "Predicted Answer:  The article states that there were 1,900 children murdered in India between 2010 and 2015.\n",
      "Actual Answer: 19 \n",
      "F1 Score: 0.0000\n",
      "Modified F1: 0.0794\n",
      "\n",
      ".Question: When was Pandher sentenced to death?\n",
      "Predicted Answer:  Pandh\n",
      "Actual Answer: February.\n",
      "\n",
      "F1 Score: 0.0000\n",
      "Modified F1: 0.1429\n",
      "\n",
      ".Question: The court aquitted Moninder Singh Pandher of what crime?\n",
      "Predicted Answer:  Moninder Singh Pandh\n",
      "Actual Answer: rape and murder \n",
      "F1 Score: 0.0000\n",
      "Modified F1: 0.4762\n",
      "\n",
      ".Question: who was acquitted\n",
      "Predicted Answer:  Moninder Singh Pandh\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "F1 Score: 0.6667\n",
      "Modified F1: 0.9444\n",
      "\n",
      ".Question: who was sentenced\n",
      "Predicted Answer:  The high court upheld the death sentence of Moninder Singh Pandh\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "F1 Score: 0.2857\n",
      "Modified F1: 0.5737\n",
      "\n",
      ".Question: What was Moninder Singh Pandher acquitted for?\n",
      "Predicted Answer:  Moninder Singh Pandh\n",
      "Actual Answer: the killing of a teen \n",
      "F1 Score: 0.0000\n",
      "Modified F1: 0.3889\n",
      "\n",
      ".Question: Who was sentenced to death in February?\n",
      "Predicted Answer:  Moniinder Singh Pandh\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "F1 Score: 0.3333\n",
      "Modified F1: 0.9248\n",
      "\n",
      ".Question: how many people died\n",
      "Predicted Answer:  The context does not provide information on how many people died.\n",
      "Actual Answer: 19 \n",
      "F1 Score: 0.0000\n",
      "Modified F1: 0.0000\n",
      "\n",
      ".Question: How many children and young women were murdered?\n",
      "Predicted Answer:  1\n",
      "Actual Answer: 19 \n",
      "F1 Score: 0.0000\n",
      "Modified F1: 0.6667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template_original = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use ten words maximum and keep the answer as concise as possible. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT_ORIGINAL = PromptTemplate.from_template(template_original)\n",
    "\n",
    "############## Important Parameters ##############\n",
    "max_stories = 100\n",
    "chunk_sizes = [200]\n",
    "overlap_percentages = [0.1]  # Expressed as percentages (0.1 = 10%)\n",
    "random_seed = 123\n",
    "output_file_path = \"results/scores_newf1.txt\"\n",
    "model_location = \"C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "# model_location = \"/Users/wk77/Library/CloudStorage/OneDrive-DrexelUniversity/Documents/data/gpt4all/models/gpt4all-falcon-q4_0.gguf\"\n",
    "\n",
    "##################################################\n",
    "\n",
    "# logging.basicConfig(level=logging.WARNING)  # This will show only warnings and errors\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "set_seed(random_seed)\n",
    "\n",
    "# Results storage\n",
    "f1_results = defaultdict(list)\n",
    "em_results = defaultdict(list)\n",
    "text_results = []\n",
    "\n",
    "# Initialize the language model and the QA chain\n",
    "llm = GPT4All(model=model_location, max_tokens=2048, seed=random_seed)\n",
    "\n",
    "# HuggingFace Instruct Embeddings parameters\n",
    "instruct_embedding_model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "instruct_embedding_model_kwargs = {'device': 'cpu'}\n",
    "instruct_embedding_encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# The following code would iterate over the stories and questions to calculate the scores\n",
    "start_time = time.time()\n",
    "print(f\"{start_time} Started.\")\n",
    "\n",
    "################ Main Function ################\n",
    "newsqa_loop_test(data, llm, output_file_path, chunk_sizes, overlap_percentages, max_stories, instruct_embedding_model_name,\n",
    "                 instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, QA_CHAIN_PROMPT_ORIGINAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Revised Prompt\n",
    "Duplication for checking the consistency between different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700958240.5511186 Started.\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "\n",
      "0.0 Processing chunk size 200:\n",
      "\n",
      "0.0\t0.0\tOverlap [0.1] 20\n",
      "\n",
      "0.0\t0.0\t\tstory 1: .Question: What was the amount of children murdered?\n",
      "Predicted Answer:  The amount of children murdered is not provided in the given context.\n",
      "Actual Answer: 19 \n",
      "F1 Score: 0.0000\n",
      "\n",
      ".Question: When was Pandher sentenced to death?\n",
      "Predicted Answer:  Pandh\n",
      "Actual Answer: February.\n",
      "\n",
      "F1 Score: 0.0000\n",
      "\n",
      ".Question: The court aquitted Moninder Singh Pandher of what crime?\n",
      "Predicted Answer:  The court aquitted Moninder Singh Pandher of the crime of murder.\n",
      "Actual Answer: rape and murder \n",
      "F1 Score: 0.0000\n",
      "\n",
      ".Question: who was acquitted\n",
      "Predicted Answer:  Moninder Singh Pandh\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "F1 Score: 0.6667\n",
      "\n",
      ".Question: who was sentenced\n",
      "Predicted Answer:  The high court upheld the sentence of the person who committed the crime.\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "F1 Score: 0.0000\n",
      "\n",
      ".Question: What was Moninder Singh Pandher acquitted for?\n",
      "Predicted Answer:  Moninder Singh Pandh\n",
      "Actual Answer: the killing of a teen \n",
      "F1 Score: 0.0000\n",
      "\n",
      ".Question: Who was sentenced to death in February?\n",
      "Predicted Answer:  Moninder Singh Pandher was sentenced to death in February.\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "F1 Score: 0.5000\n",
      "\n",
      ".Question: how many people died\n",
      "Predicted Answer:  The context states that the teen was one of 1, 9 victims, but it does not provide information on how many people died.\n",
      "Actual Answer: 19 \n",
      "F1 Score: 0.0000\n",
      "\n",
      ".Question: How many children and young women were murdered?\n",
      "Predicted Answer:  1\n",
      "Actual Answer: 19 \n",
      "F1 Score: 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# New template that only let the model answer questions from the original text.\n",
    "template_new = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use ten words maximum and keep the answer as concise as possible. \n",
    "Use only consecutive words that appeared in the context to answer questions. \\n\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT_NEW = PromptTemplate.from_template(template_new)\n",
    "\n",
    "############## Important Parameters ##############\n",
    "max_stories = 100\n",
    "chunk_sizes = [200]\n",
    "overlap_percentages = [0.1]  # Expressed as percentages (0.1 = 10%)\n",
    "random_seed = 123\n",
    "output_file_path = \"results/scores_new_1.txt\"\n",
    "model_location = \"C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "# model_location = \"/Users/wk77/Library/CloudStorage/OneDrive-DrexelUniversity/Documents/data/gpt4all/models/gpt4all-falcon-q4_0.gguf\"\n",
    "\n",
    "##################################################\n",
    "\n",
    "# logging.basicConfig(level=logging.WARNING)  # This will show only warnings and errors\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "set_seed(random_seed)\n",
    "\n",
    "# Results storage\n",
    "f1_results = defaultdict(list)\n",
    "em_results = defaultdict(list)\n",
    "text_results = []\n",
    "\n",
    "# HuggingFace Instruct Embeddings parameters\n",
    "instruct_embedding_model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "instruct_embedding_model_kwargs = {'device': 'cpu'}\n",
    "instruct_embedding_encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# The following code would iterate over the stories and questions to calculate the scores\n",
    "start_time = time.time()\n",
    "print(f\"{start_time} Started.\")\n",
    "\n",
    "newsqa_loop_test(data, llm, output_file_path, chunk_sizes, overlap_percentages, max_stories, instruct_embedding_model_name,\n",
    "                 instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, QA_CHAIN_PROMPT_NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700958771.2228317 Started.\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "\n",
      "0.0 Processing chunk size 200:\n",
      "\n",
      "0.0\t0.0\tOverlap [0.1] 20\n",
      "\n",
      "0.0\t0.0\t\tstory 1: .Question: What was the amount of children murdered?\n",
      "Predicted Answer:  The amount of children murdered is not provided in the given context.\n",
      "Actual Answer: 19 \n",
      "F1 Score: 0.0000\n",
      "\n",
      ".Question: When was Pandher sentenced to death?\n",
      "Predicted Answer:  Pandh\n",
      "Actual Answer: February.\n",
      "\n",
      "F1 Score: 0.0000\n",
      "\n",
      ".Question: The court aquitted Moninder Singh Pandher of what crime?\n",
      "Predicted Answer:  The court aquitted Moninder Singh Pandher of the crime of murder.\n",
      "Actual Answer: rape and murder \n",
      "F1 Score: 0.0000\n",
      "\n",
      ".Question: who was acquitted\n",
      "Predicted Answer:  Moninder Singh Pandh\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "F1 Score: 0.6667\n",
      "\n",
      ".Question: who was sentenced\n",
      "Predicted Answer:  The high court upheld the sentence of the person who committed the crime.\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "F1 Score: 0.0000\n",
      "\n",
      ".Question: What was Moninder Singh Pandher acquitted for?\n",
      "Predicted Answer:  Moninder Singh Pandh\n",
      "Actual Answer: the killing of a teen \n",
      "F1 Score: 0.0000\n",
      "\n",
      ".Question: Who was sentenced to death in February?\n",
      "Predicted Answer:  Moninder Singh Pandher was sentenced to death in February.\n",
      "Actual Answer: Moninder Singh Pandher \n",
      "F1 Score: 0.5000\n",
      "\n",
      ".Question: how many people died\n",
      "Predicted Answer:  The context states that the teen was one of 1, 9 victims, but it does not provide information on how many people died.\n",
      "Actual Answer: 19 \n",
      "F1 Score: 0.0000\n",
      "\n",
      ".Question: How many children and young women were murdered?\n",
      "Predicted Answer:  1\n",
      "Actual Answer: 19 \n",
      "F1 Score: 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############## Important Parameters ##############\n",
    "max_stories = 100\n",
    "chunk_sizes = [200]\n",
    "overlap_percentages = [0.1]  # Expressed as percentages (0.1 = 10%)\n",
    "random_seed = 123\n",
    "output_file_path = \"results/scores_new_2.txt\"\n",
    "model_location = \"C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "# model_location = \"/Users/wk77/Library/CloudStorage/OneDrive-DrexelUniversity/Documents/data/gpt4all/models/gpt4all-falcon-q4_0.gguf\"\n",
    "\n",
    "##################################################\n",
    "\n",
    "# logging.basicConfig(level=logging.WARNING)  # This will show only warnings and errors\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "set_seed(random_seed)\n",
    "\n",
    "# Results storage\n",
    "f1_results = defaultdict(list)\n",
    "em_results = defaultdict(list)\n",
    "text_results = []\n",
    "\n",
    "# HuggingFace Instruct Embeddings parameters\n",
    "instruct_embedding_model_name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "instruct_embedding_model_kwargs = {'device': 'cpu'}\n",
    "instruct_embedding_encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# The following code would iterate over the stories and questions to calculate the scores\n",
    "start_time = time.time()\n",
    "print(f\"{start_time} Started.\")\n",
    "\n",
    "newsqa_loop_test(data, llm, output_file_path, chunk_sizes, overlap_percentages, max_stories, instruct_embedding_model_name,\n",
    "                 instruct_embedding_model_kwargs, instruct_embedding_encode_kwargs, QA_CHAIN_PROMPT_NEW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Reset Embeddings Dimension Value in Vectorstore\n",
    "Problem: Dimensionality error\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "InvalidDimensionException                 Traceback (most recent call last)\n",
    "~\\AppData\\Local\\Temp\\ipykernel_18668\\111973739.py in <module>\n",
    "     57                 all_splits = text_splitter.split_text(story['text'])\n",
    "     58                 # print(f\"[after split]\")\n",
    "---> 59                 vectorstore = Chroma.from_texts(texts=all_splits, embedding=word_embed)\n",
    "     60                 # print(f\"[after vector store]\")\n",
    "     61                 qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectorstore.as_retriever(), return_source_documents=True)\n",
    "\n",
    "~\\AppData\\Roaming\\Python\\Python39\\site-packages\\langchain\\vectorstores\\chroma.py in from_texts(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\n",
    "    574             **kwargs,\n",
    "    575         )\n",
    "--> 576         chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n",
    "    577         return chroma_collection\n",
    "    578 \n",
    "\n",
    "~\\AppData\\Roaming\\Python\\Python39\\site-packages\\langchain\\vectorstores\\chroma.py in add_texts(self, texts, metadatas, ids, **kwargs)\n",
    "    233                 )\n",
    "    234         else:\n",
    "--> 235             self._collection.upsert(\n",
    "    236                 embeddings=embeddings,\n",
    "    237                 documents=texts,\n",
    "\n",
    "~\\AppData\\Roaming\\Python\\Python39\\site-packages\\chromadb\\api\\models\\Collection.py in upsert(self, ids, embeddings, metadatas, documents)\n",
    "    296         )\n",
    "...\n",
    "--> 552             raise InvalidDimensionException(\n",
    "    553                 f\"Embedding dimension {dim} does not match collection dimensionality {collection['dimension']}\"\n",
    "    554             )\n",
    "\n",
    "InvalidDimensionException: Embedding dimension 384 does not match collection dimensionality 768\n",
    "```\n",
    "\n",
    "Solution: Reset the Chroma db collection following this thread:\n",
    "https://github.com/langchain-ai/langchain/issues/\n",
    "\n",
    "```python\n",
    "# If can't find the directory\n",
    "from chromadb.errors import InvalidDimensionException\n",
    "\n",
    "try:\n",
    "    docsearch = Chroma.from_documents(documents=..., embedding=...)\n",
    "except InvalidDimensionException:\n",
    "    Chroma().delete_collection()\n",
    "    docsearch = Chroma.from_documents(documents=..., embedding=...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset vectorstore directly\n",
    "Chroma().delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "1. GPT4All Langchain Demo: https://gist.github.com/segestic/4822f3765147418fc084a250598c1fe6\n",
    "2. Sematic - use GPT4ALL, FAISS & HuggingFace Embeddings for local context-enhanced question answering https://www.youtube.com/watch?v=y32vbJkabCw\n",
    "3. Model used for InstructEmbeddings: [Hugging Face](https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1)\n",
    "4. Reset the Chroma db collection: https://github.com/langchain-ai/langchain/issues/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
