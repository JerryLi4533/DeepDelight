# NewsQA Process, Chunk and Overlap Sizes
11/1/2023 \
Lixiao Yang

## Option 1: Data + Default Code
**Mainly use the default settings, only change the model part to do the experiment** \ 
Overall process: 
1. select small test sets stories 
2. match question id with news 
3. use code to compile json files 
4. modify source code files 
5. add new models to train
6. add new files for creating loop for different chunk and overlap sizes
- Try to mannually set up the environment, but can not transform all the scripts into Python 3. The following is the comment from the aurthur:
  ```python
        # Correct discrepancies in stories.
        # Problems are caused by using several programming languages and libraries.
        # When ingesting the stories, we started with Python 2.
        # After dealing with unicode issues, we tried switching to Python 3.
        # That caused inconsistency problems so we switched back to Python 2.
        # Furthermore, when crowdsourcing, JavaScript and HTML templating perturbed the stories.
        # So here we map the text to be compatible with the indices.
        ```
- Fixed docker setup issues
  - base image too old
  - JDK version does not match
  - WORKDIR problem
- Local data used:
  - 10 from train_story_ids
  - 2 from test_story_ids
  - 1 from stories_requiring_extra_newlines, stories_requiring_two_extra_newlines, stories_to_decode_specially each
  - 5 from dev_story_ids

**Current problem** 
Having trouble identifying the mapping process among multiple intraconnected Python 2 files, since the default setting does not support transforming it into Python 3, the model changing and future loop run can be a problem.

## Option 2: Data Only + Existing Code
**Only use the news and question files, using existing GPT4ALL model and framework to process all the data** \ 
Overall process: 
1. select small test sets stories
2. match question id with news 
3. use code to compile json and csv files 
4. write new code to handle json file 
5. write loop code 
6. use existing GPT4ALL scripts to loop different chunk and overlap sizes

[See 6.2 Option 2 file for the loop code for option 2]

## Options Comparison
- Option 1
  - Pros
    - All the matching and compile process are encapsulated for all steps
    - Use docker to create environment
    - Include complete exception handling functions
    - Multiple additional attributes might be useful for further analysis
  - Cons
    - All the scripts are writing in Python 2 version and can not be revised into Python 3
    - Integrated files add additional difficulty to modify codes
    - Compatibility issue with different Python 3 libraries
- Option 2
  - Pros
    - Current GPT4ALL work can be reused with minimal changes
    - Get rid of the Python 2 restriction in original scripts
    - Compiled json/csv file incorporates all the information needed
  - Cons
    - Might not be able to utilize some of the attributes in the compiled file
    - Additional codes needed for handling the new file structure

## Option 1 running logs

Docker settings in root directory:

```shell
# Use a more recent Miniconda base image
FROM continuumio/miniconda3:latest

# Install JDK.
RUN apt-get update && apt-get install --yes default-jdk

RUN echo ". /opt/conda/etc/profile.d/conda.sh" >> ~/.bashrc
# Setup the Python environment.
RUN conda create --yes --name newsqa python=2.7 "pandas>=0.19.2" cython
RUN echo "conda activate newsqa" >> ~/.bashrc

WORKDIR /usr/src/newsqa
COPY requirements.txt ./
RUN /bin/bash --login -c "conda list && yes | pip install --requirement requirements.txt"

ADD https://nlp.stanford.edu/software/stanford-postagger-2015-12-09.zip /usr/downloads/

# Clean up existing files (there can be problems if they've already been extracted outside of the Docker container).
# Run the unit tests to test and extract the data.
# CMD /bin/bash --login -c "rm --force combined-newsqa-data-*.csv maluuba/newsqa/newsqa-data-*.csv && \
#                          cp --no-clobber /usr/downloads/* maluuba/newsqa/ && \
#                          python -m unittest discover ."
```

**Current problem**: Having trouble identifying the mapping process among multiple intraconnected python 2 files, since the default setting does not support transforming it into Python 3, the model changing and future loop run can be a problem.

Docker running log:

```shell
PS C:\Users\24075> cd 'C:\CNN news\New folder\'
PS C:\CNN news\New folder> docker build -t maluuba/newsqa .
[+] Building 1.0s (15/15) FINISHED                                                                       docker:default
 => [internal] load .dockerignore                                                                                  0.0s
 => => transferring context: 2B                                                                                    0.0s
 => [internal] load build definition from Dockerfile                                                               0.0s
 => => transferring dockerfile: 1.04kB                                                                             0.0s
 => [internal] load metadata for docker.io/continuumio/miniconda3:latest                                           0.3s
 => [1/9] FROM docker.io/continuumio/miniconda3:latest@sha256:db9f536d96d49fe21b5f4ac3252781bb0d2a3b58dab2d8e4434  0.0s
 => https://nlp.stanford.edu/software/stanford-postagger-2015-12-09.zip                                            0.6s
 => [internal] load build context                                                                                  0.0s
 => => transferring context: 37B                                                                                   0.0s
 => CACHED [2/9] RUN apt-get update && apt-get install --yes default-jdk                                           0.0s
 => CACHED [3/9] RUN echo ". /opt/conda/etc/profile.d/conda.sh" >> ~/.bashrc                                       0.0s
 => CACHED [4/9] RUN conda create --yes --name newsqa python=2.7 "pandas>=0.19.2" cython                           0.0s
 => CACHED [5/9] RUN echo "conda activate newsqa" >> ~/.bashrc                                                     0.0s
 => CACHED [6/9] WORKDIR /usr/src/newsqa                                                                           0.0s
 => CACHED [7/9] COPY requirements.txt ./                                                                          0.0s
 => CACHED [8/9] RUN /bin/bash --login -c "conda list && yes | pip install --requirement requirements.txt"         0.0s
 => CACHED [9/9] ADD https://nlp.stanford.edu/software/stanford-postagger-2015-12-09.zip /usr/downloads/           0.0s
 => exporting to image                                                                                             0.0s
 => => exporting layers                                                                                            0.0s
 => => writing image sha256:b0411b423b77ae48422d501fd7ac8f2e44f5b9d495d2ecd5cb8ef3ae54d8435a                       0.0s
 => => naming to docker.io/maluuba/newsqa                                                                          0.0s

What's Next?
  View a summary of image vulnerabilities and recommendations → docker scout quickview
PS C:\CNN news\New folder> docker run --rm -it -v ${PWD}:/usr/src/newsqa --name newsqa maluuba/newsqa
(newsqa) root@de201d57bf5a:/usr/src/newsqa# python maluuba/newsqa/data_generator.py
[INFO] 2023-11-02 04:24:57,476 - data_processing.py::__init__
Loading dataset from `/usr/src/newsqa/maluuba/newsqa/newsqa-data-v1.csv`...
[INFO] 2023-11-02 04:24:57,476 - data_processing.py::load_combined
Loading data from `/usr/src/newsqa/maluuba/newsqa/newsqa-data-v1.csv`...
[INFO] 2023-11-02 04:24:57,518 - data_processing.py::__init__
Loading stories from `/usr/src/newsqa/maluuba/newsqa/cnn_stories.tgz`...
Getting story texts: 100%|█████████████████████████████████████████████████████| 7.00/7.00 [00:16<00:00, 2.42s/ stories]
Setting story texts: 100%|██████████████████████████████████████████████████| 95.0/95.0 [00:00<00:00, 18.6k questions/s]
[INFO] 2023-11-02 04:25:14,541 - data_processing.py::__init__
Done loading dataset.
[INFO] 2023-11-02 04:25:14,591 - data_processing.py::dump
Packaging dataset to `combined-newsqa-data-v1.json`.
Building json: 100%|████████████████████████████████████████████████████████| 95.0/95.0 [00:00<00:00, 28.7k questions/s]
Traceback (most recent call last):
  File "maluuba/newsqa/data_generator.py", line 36, in <module>
    newsqa_data.dump(path='combined-newsqa-data-v1.json')
  File "/usr/src/newsqa/maluuba/newsqa/data_processing.py", line 311, in dump
    data = json.dumps(data, ensure_ascii=False, separators=(',', ':'), encoding='utf-8')
  File "/opt/conda/envs/newsqa/lib/python2.7/json/__init__.py", line 251, in dumps
    sort_keys=sort_keys, **kw).encode(obj)
  File "/opt/conda/envs/newsqa/lib/python2.7/json/encoder.py", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/opt/conda/envs/newsqa/lib/python2.7/json/encoder.py", line 270, in iterencode
    return _iterencode(o, 0)
  File "/opt/conda/envs/newsqa/lib/python2.7/json/encoder.py", line 184, in default
    raise TypeError(repr(o) + " is not JSON serializable")
TypeError: ValueError('./cnn/stories/002a083c3893b1fde734280b9eec28d428a02d2b.story not found in any story ID set.',) is not JSON serializable
```
Proposed next step: continue to find mapping codes and original files to manually update the story ID, then try to update the model part, but Python 2's incompatibility can cause potential problems.
