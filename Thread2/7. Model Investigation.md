# Model Investigation for LangChain
11/16/2023 </br>
Lixiao Yang

This file provides some information for different models able to use for NewsQA dataset further experiment under the LangChain framework, with some additional information. All detailed information can be found in LangChain and GPT4ALL documentation.

- LLMs for LangChain: https://python.langchain.com/docs/integrations/llms/
- GPT4ALL documentation: https://gpt4all.io/index.html
- GPT4ALL github: https://github.com/nomic-ai/gpt4all

## GPT4ALL Models Comparison
GPT4ALL already include optimized GPU operations and also enable the minimal change possiblity to the current code.
| Model Name | Size (GB) | RAM (GB) | Key Features | QA on Documents |
|------------|-----------|----------|--------------|-----------------|
| mistral-7b-openorca.Q4_0.gguf | 3.83 | 8 | Fast chat model, OpenOrca dataset finetuning, by Mistral AI | Moderate |
| mistral-7b-instruct-v0.1.Q4_0.gguf | 3.83 | 8 | Fast, instruction-following, uncensored, by Mistral AI | Moderate |
| gpt4all-falcon-q4_0.gguf | 3.92 | 8 | Very fast, instruction-based, TII and Nomic AI collaboration | Good |
| wizardlm-13b-v1.2.Q4_0.gguf | 6.86 | 16 | Large, instruction-based, long responses, Microsoft & Peking University | Very Good |
| nous-hermes-llama2-13b.Q4_0.gguf | 6.86 | 16 | Extremely capable, instruction-based, long responses, by Nous Research | Very Good |
| gpt4all-13b-snoozy-q4_0.gguf | 6.86 | 16 | High-quality responses, instruction-based, Nomic AI | Very Good |
| mpt-7b-chat-merges-q4_0.gguf | 3.54 | 8 | Novel architecture, chat-focused, by Mosaic ML | Moderate |
| orca-mini-3b-gguf2-q4_0.gguf | 1.84 | 4 | Compact, instruction-based, novel dataset, Orca Research Paper focus | Moderate |
| replit-code-v1_5-3b-q4_0.gguf | 1.74 | 4 | Specialized in code completion, trained on a Stack subset | Low |
| starcoder-q4_0.gguf | 8.37 | 4 | Code completion focused, trained on a subset of Stack | Low |
| rift-coder-v0-7b-q4_0.gguf | 3.56 | 8 | Dedicated to code completion | Low |
| all-MiniLM-L6-v2-f16.gguf | 0.04 | 1 | Sbert model, primarily for embeddings | Low |

*Note: QA on Documents result evaluation might be subjective.*

## Other LangChain LLMs
Below LLMs are selected within the LangChain framework, and only potential useful models result are kept. But considering different models also means potential code and process modifications based on different APIs and functions.

| LLM | Key Features | QA on Documents |
|-------|---------|-----------------|
| Anthropic | Supports invoke, async invoke, stream, and async stream | Very Good |
| Anyscale | Full support for all operations including async and batch processes | Very Good |
| OpenAI | Comprehensive support including streaming and batch operations | Excellent |
| OpenLM | Full support for all operations including async and batch processes | Excellent |
| VertexAI | Supports invoke, async invoke, stream, and batch (excluding async stream) | Very Good |
| VertexAIModelGarden | Supports invoke, async invoke, and batch (excluding stream and async stream) | Very Good |

*Note: QA on Documents result evaluation might be subjective.*

Models like OpenAI and OpenLM are noted for their advanced capabilities in understanding and generating contextually relevant responses. Anthropic, Anyscale, VertexAI, and VertexAIModelGarden are also noted for their strong capabilities in language processing and understanding.
