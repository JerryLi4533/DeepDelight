{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking and Overlaping Sizes with QAConv\n",
    "\n",
    "Lixiao Yang\\\n",
    "10/25/2023\n",
    "\n",
    "QAConv Dataset: https://github.com/salesforce/QAConv\n",
    "\n",
    "\n",
    "Updates:\n",
    "1. **NewsQA dataset**: require docker / local environment setup and compile the news (larger complete dataset size)\n",
    "     - CSV data format\n",
    "     - Needs docker / local environment setup to package the dataset\n",
    "     - CNN Dataset: This dataset contains the documents and accompanying questions from the news articles of CNN. There are approximately 90k documents and 380k questions.\n",
    "       - Questions (200MB), stories (150MB)\n",
    "       - Original news stored in .story format, needs to compile before reading, pure text\n",
    "     - Daily Mail: This dataset contains the documents and accompanying questions from the news articles of Daily Mail. There are approximately 197k documents and 879k questions.\n",
    "       - Questions (500MB), stories (358MB)\n",
    "     - Compatibility for GPT4ALL and LangChain: Needs separate generator, processing, split, tokenize \n",
    "\n",
    "\n",
    "2. **QAConv dataset**: has open-sourced data\n",
    "     - JSON data format\n",
    "       - QA data (40MB), questions (7MB)\n",
    "     - Uses pretrained / fine-tuned model (huggingface library)\n",
    "     - Compatibility for GPT4ALL and LangChain: Needs separate retriver\n",
    "     - Baseline: BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries and modules\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original JSON data\n",
    "with open('./QAConv-V1.1/article_segment.json', 'r') as file:\n",
    "    original_data = json.load(file)\n",
    "\n",
    "# Get a list of all the keys (record IDs) in the data\n",
    "all_keys = list(original_data.keys())\n",
    "\n",
    "# Determine the number of records you want to sample (e.g., 0.1% of the data)\n",
    "num_samples = int(len(all_keys) * 0.001)\n",
    "\n",
    "# Randomly sample a subset of the keys without replacement\n",
    "sampled_keys = random.sample(all_keys, num_samples)\n",
    "\n",
    "# Use the sampled keys to get the corresponding records from the data\n",
    "sampled_data = {key: original_data[key] for key in sampled_keys}\n",
    "\n",
    "# Create a directory to hold the text files\n",
    "os.makedirs('./QAConv-V1.1/preprocessed_text_files', exist_ok=True)\n",
    "\n",
    "# Process each entry in the sampled data\n",
    "for key, entry in sampled_data.items():\n",
    "    texts = []\n",
    "    for item in entry.get('prev_ctx', []):\n",
    "        texts.append(item['text'])\n",
    "    for item in entry.get('seg_dialog', []):\n",
    "        texts.append(item['text'])\n",
    "    # Combine the texts into a single string\n",
    "    combined_text = ' '.join(texts)\n",
    "    # Write the combined text to a text file\n",
    "    with open(f'./QAConv-V1.1/preprocessed_text_files/{key}.txt', 'w') as file:\n",
    "        file.write(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Need to load profiles.\n",
      "Need to load profiles.\n",
      "Need to load profiles.\n",
      "100%|██████████| 18/18 [00:04<00:00,  4.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use DirectoryLoader to load the preprocessed text files\n",
    "loader = DirectoryLoader('./QAConv-V1.1/preprocessed_text_files', glob=\"*.txt\", show_progress=True, use_multithreading=True)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining chunking stategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Chunking Configurations\n",
    "fixed_configs = [(size, overlap) for size in [200, 400, 800, 1600] for overlap in [0, 0.1, 0.2, 0.3]]\n",
    "\n",
    "# Semantic Structure Chunking Configurations\n",
    "# These will be handled differently in the loop as they require different splitting methods\n",
    "semantic_configs = ['sentence', 'paragraph', 'section']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results DataFrame to store the performance metrics\n",
    "results_df = pd.DataFrame(columns=['Chunking Strategy', 'Chunk Size', 'Overlap', 'EM', 'F1'])\n",
    "\n",
    "for config in fixed_configs:\n",
    "    chunk_size, overlap = config\n",
    "    # Adjust the text_splitter for fixed chunking configurations\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=int(chunk_size*overlap))\n",
    "\n",
    "    all_splits = text_splitter.split_documents(data)\n",
    "    \n",
    "    # Store\n",
    "    vectorstore = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings())\n",
    "    \n",
    "    # Retrieve\n",
    "    embeddings = GPT4AllEmbeddings()\n",
    "\n",
    "    gpt4all_falcon_model = \"C:/Users/24075/AppData/Local/nomic.ai/GPT4All/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "\n",
    "    llm = GPT4All(model=gpt4all_falcon_model ,max_tokens=2048)\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectorstore.as_retriever())\n",
    "    \n",
    "    template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "    Use three sentences maximum and keep the answer as concise as possible. \n",
    "    Also provide me the source for your answer. Explain how to get the answer step by step.\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "    QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "    # Assume em and f1 are the calculated Exact Match and F1 scores for this configuration\n",
    "    results_df = results_df.append({\n",
    "        'Chunking Strategy': 'Fixed',\n",
    "        'Chunk Size': chunk_size,\n",
    "        'Overlap': overlap,\n",
    "        'EM': em,\n",
    "        'F1': f1\n",
    "    }, ignore_index=True)\n",
    "\n",
    "# Semantic chunking will require a different text splitting approach which you would need to implement\n",
    "for config in semantic_configs:\n",
    "    # ... rest of your experiment code ...\n",
    "\n",
    "    # Assume em and f1 are the calculated Exact Match and F1 scores for this configuration\n",
    "    results_df = results_df.append({\n",
    "        'Chunking Strategy': 'Semantic',\n",
    "        'Chunk Size': config,\n",
    "        'Overlap': 'N/A',\n",
    "        'EM': em,\n",
    "        'F1': f1\n",
    "    }, ignore_index=True)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('experiment_results.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
